TY  - JOUR
TI  - Robust label prediction via label propagation and geodesic k-nearest neighbor in online semi-supervised learning
AU  - Wada, Y.
AU  - Su, S.
AU  - Kumagai, W.
AU  - Kanamori, T.
T2  - IEICE Transactions on Information and Systems
AB  - This paper proposes a computationally efficient offline semi-supervised algorithm that yields a more accurate prediction than the label propagation algorithm, which is commonly used in online graphbased semi-supervised learning (SSL). Our proposed method is an offline method that is intended to assist online graph-based SSL algorithms. The efficacy of the tool in creating new learning algorithms of this type is demonstrated in numerical experiments. © 2019 The Institute of Electronics, Information and Communication Engineers.
DA  - 2019///
PY  - 2019
DO  - 10.1587/transinf.2018EDP7424
VL  - E102D
IS  - 8
SP  - 1537
EP  - 1545
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071900727&doi=10.1587%2ftransinf.2018EDP7424&partnerID=40&md5=23a1a6f6aa6e5d808cce101d6bc77523
DB  - Scopus
KW  - Machine learning
KW  - Backpropagation
KW  - Learning algorithms
KW  - E-learning
KW  - Geodesic distance
KW  - Geodesic distances
KW  - Geodesy
KW  - Graphic methods
KW  - Label propagation
KW  - Manifold learning
KW  - Nearest neighbor search
KW  - Online learning
KW  - Semi- supervised learning
KW  - Semi-supervised learning
ER  - 

TY  - CONF
TI  - How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments
AU  - Kazantzidis, I.
AU  - Norman, T.J.
AU  - Du, Y.
AU  - Freeman, C.T.
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
AB  - Training reinforcement learning agents in real-world environments is costly, particularly for safety-critical applications. Human input can enable an agent to learn a good policy while avoiding unsafe actions, but at the cost of bothering the human with repeated queries. We present a model for safe learning in safety-critical environments from human input that minimises bother cost. Our model, JPAL-HA, proposes an efficient mechanism to harness human preferences and justifications to significantly improve safety during the learning process without increasing the number of interactions with a user. We show this with both simulation and human experiments. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
DA  - 2022///
PY  - 2022
VL  - 3
SP  - 1654
EP  - 1656
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134306740&partnerID=40&md5=76a61115b19501a21f7673129a6cff87
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Learning systems
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Human robot interaction
KW  - Safety engineering
KW  - Multi agent systems
KW  - Active Learning
KW  - Safe Reinforcement Learning
KW  - Safe reinforcement learning
KW  - Agent collaboration
KW  - Critical environment
KW  - Human agent
KW  - Human-agent collaboration
KW  - Human-Agent Collaboration
KW  - Human-Robot Interaction
KW  - Humans-robot interactions
KW  - Learning from human preference
KW  - Learning from Human Preferences
ER  - 

TY  - CONF
TI  - A family of robust stochastic operators for reinforcement learning
AU  - Lu, Y.
AU  - Squillante, M.S.
AU  - Wu, C.W.
T2  - Advances in Neural Information Processing Systems
AB  - We consider a new family of stochastic operators for reinforcement learning that seeks to alleviate negative effects and become more robust to approximation or estimation errors. Theoretical results are established, showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman and recently proposed operators. © 2019 Neural information processing systems foundation. All rights reserved.
DA  - 2019///
PY  - 2019
VL  - 32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177881&partnerID=40&md5=a53a7206f40717efe60cd4f5353df697
DB  - Scopus
KW  - Reinforcement learning
KW  - Stochastic systems
KW  - Estimation errors
KW  - Optimality
ER  - 

TY  - CONF
TI  - Batch-like online learning for more robust hybrid artificial intelligence: Deconstruction as a machine learning process
AU  - Schmid, T.
T2  - CEUR Workshop Proceedings
AB  - Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)
DA  - 2021///
PY  - 2021
VL  - 2846
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104622356&partnerID=40&md5=04edd84470671c7e18a70da9f07e9d78
DB  - Scopus
KW  - Artificial intelligence
KW  - Knowledge based systems
KW  - Knowledge representation
KW  - Learning systems
KW  - Supervised learning
KW  - E-learning
KW  - Knowledge base
KW  - Traditional approaches
KW  - Online learning
KW  - Unsupervised learning
KW  - Constructivist machine learning
KW  - Data streams
KW  - Design concept
KW  - Human learning
KW  - Hybrid artificial intelligences
KW  - Machine learning techniques
KW  - Springs (components)
KW  - Supervised and unsupervised learning
KW  - Turing machines
ER  - 

TY  - JOUR
TI  - Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation
AU  - Wu, J.
AU  - Zhou, Y.
AU  - Yang, H.
AU  - Huang, Z.
AU  - Lv, C.
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
AB  - Reinforcement learning (RL) is a promising approach in unmanned ground vehicles (UGVs) applications, but limited computing resource makes it challenging to deploy a well-behaved RL strategy with sophisticated neural networks. Meanwhile, the training of RL on navigation tasks is difficult, which requires a carefully-designed reward function and a large number of interactions, yet RL navigation can still fail due to many corner cases. This shows the limited intelligence of current RL methods, thereby prompting us to rethink combining RL with human intelligence. In this paper, a human-guided RL framework is proposed to improve RL performance both during learning in the simulator and deployment in the real world. The framework allows humans to intervene in RL&#x0027;s control progress and provide demonstrations as needed, thereby improving RL&#x0027;s capabilities. An innovative human-guided RL algorithm is proposed that utilizes a series of mechanisms to improve the effectiveness of human guidance, including human-guided learning objective, prioritized human experience replay, and human intervention-based reward shaping. Our RL method is trained in simulation and then transferred to the real world, and we develop a denoised representation for domain adaptation to mitigate the simulation-to-real gap. Our method is validated through simulations and real-world experiments to navigate UGVs in diverse and dynamic environments based only on tiny neural networks and image inputs. Our method performs better in goal-reaching and safety than existing learning- and model-based navigation approaches and is robust to changes in input features and ego kinetics. Furthermore, our method allows small-scale human demonstrations to be used to improve the trained RL agent and learn expected behaviors online. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TPAMI.2023.3314762
SP  - 1
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171531773&doi=10.1109%2fTPAMI.2023.3314762&partnerID=40&md5=213ee3846632eadf5c500e5900fd77df
DB  - Scopus
KW  - Safety
KW  - Task analysis
KW  - Feature extraction
KW  - reinforcement learning
KW  - Training
KW  - Behavioral sciences
KW  - Navigation
KW  - Heuristic algorithms
KW  - Human guidance
KW  - navigation
KW  - sim-to-real transfer
KW  - unmanned ground vehicle
ER  - 

TY  - JOUR
TI  - Learning From Noisy Labels Via Dynamic Loss Thresholding
AU  - Yang, H.
AU  - Jin, Y.
AU  - Li, Z.
AU  - Wang, D.
AU  - Geng, X.
AU  - Zhang, M.
T2  - IEEE Transactions on Knowledge and Data Engineering
AB  - Numerous researches have proved that deep neural networks (DNNs) can fit almost everything even given data with noisy labels, and result in poor generalization performance. However, recent studies suggest that DNNs tend to gradually memorize the data, moving from correct data to mislabeled data. Inspired by this finding, we propose a novel method named <italic>Dynamic Loss Thresholding (DLT)</italic>. During the training process, DLT records the loss value of each sample and calculates dynamic loss thresholds. Specifically, DLT compares the loss value of each sample with the current loss threshold. Samples with smaller losses can be considered as clean samples with higher probability and vice versa. Then, DLT discards the potentially corrupted labels and further leverages self-training semi-supervised learning techniques. Experiments on CIFAR-10/100, WebVision and Clothing1M demonstrate substantial improvements over recent state-of-the-art methods. In addition, we investigate two real-world problems. Firstly, we propose a novel approach to estimate the noise rates of datasets based on the loss difference between the early and late training stages of DNNs. Secondly, we explore the effect of hard samples (which are difficult to be distinguished) on the process of learning from noisy labels. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TKDE.2023.3313604
SP  - 1
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171528884&doi=10.1109%2fTKDE.2023.3313604&partnerID=40&md5=e14cea8b8cc944e7e9d83a3bca018bdf
DB  - Scopus
KW  - Deep learning
KW  - semi-supervised learning
KW  - Data models
KW  - Training
KW  - Robustness
KW  - dynamic loss thresholding
KW  - Entropy
KW  - Fitting
KW  - learning from noisy labels
KW  - Noise measurement
KW  - Semisupervised learning
ER  - 

TY  - JOUR
TI  - A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling
AU  - Zhang, F.
AU  - Chen, S.
AU  - Hong, Z.
AU  - Shan, B.
AU  - Xu, Q.
T2  - Neural Processing Letters
AB  - The extreme learning machine (ELM) algorithm is advantageous to regression modeling owing to its simple structure, fast computation, and good generalization performance. However, the existing ELM algorithm uses an l2 -norm loss function, which is sensitive to outliers and has low robustness. In addition, some existing robust loss functions are not sufficiently flexible to accurately estimate the relationship between sample points and loss values, resulting in unsatisfactory ELM performance. To address these problems, this study established a robust ELM (ALFELM) algorithm. First, an adaptive loss function with two tunable hyperparameters was introduced; the function can be transformed into several robust loss functions by varying the parameters. It overcomes the limitations of fixed robust loss functions. Then, the Bayesian optimization strategy was used to determine the optimal parameters of the loss function. Furthermore, the classical iterative reweighted least squares method was used to solve for output weights, with a weight function corresponding to the loss function and a regularization parameter to prevent overfitting. Finally, the proposed method was tested using several artificial and benchmark datasets, and its effectiveness was verified for a real engineering case. The results indicated that the proposed ALFELM algorithm is more robust and accurate compared with other methods, especially for a large number of outliers. In addition, the algorithm can be used to establish effective regression models for actual processes. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s11063-023-11340-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164156087&doi=10.1007%2fs11063-023-11340-y&partnerID=40&md5=6ff6b65fab5ec5db4a239067c063e3ac
DB  - Scopus
KW  - Machine learning
KW  - Regression analysis
KW  - Least squares approximations
KW  - Knowledge acquisition
KW  - Extreme learning machine
KW  - Learning machines
KW  - Statistics
KW  - Loss functions
KW  - Iterative methods
KW  - Bayesian optimization
KW  - Adaptive loss function
KW  - Adaptive loss functions
KW  - Fast computation
KW  - Iterative reweighted least square
KW  - Iterative reweighted least squares
KW  - Machine algorithm
KW  - Regression modeling
KW  - Regression modelling
KW  - Simple structures
ER  - 

TY  - CONF
TI  - Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models
AU  - Zhao, W.
AU  - He, T.
AU  - Liu, C.
T2  - Proceedings of Machine Learning Research
AB  - Safety is one of the biggest concerns to applying reinforcement learning (RL) to the physical world. In its core part, it is challenging to ensure RL agents persistently satisfy a hard state constraint without white-box or black-box dynamics models. This paper presents an integrated model learning and safe control framework to safeguard any RL agent, where the environment dynamics are learned as Gaussian processes. The proposed theory provides (i) a novel method to construct an offline dataset for model learning that best achieves safety requirements; (ii) a design rule to construct the safety index to ensure the existence of safe control under control limits; (iii) a probablistic safety guarantee (i.e. probabilistic forward invariance) when the model is learned using the aforementioned dataset. Simulation results show that our framework achieves almost zero safety violation on various continuous control tasks. © 2023 W. Zhao, T. He & C. Liu.
DA  - 2023///
PY  - 2023
VL  - 211
SP  - 783
EP  - 796
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164508271&partnerID=40&md5=52100e0b70a3b6e78be9f039cde62fc6
DB  - Scopus
KW  - Reinforcement learning
KW  - Process control
KW  - Learning systems
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Safe control
KW  - Dynamic learning
KW  - Dynamics
KW  - Dynamics learning
KW  - Gaussian distribution
KW  - Gaussian noise (electronic)
KW  - Gaussian process
KW  - Gaussian process models
KW  - Gaussian Processes
KW  - Model learning
KW  - Physical world
KW  - Probabilistics
KW  - Safety indexes
ER  - 

TY  - CONF
TI  - Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms
AU  - Dagdanov, R.
AU  - Durmus, H.
AU  - Ure, N.K.
T2  - Proceedings - IEEE International Conference on Robotics and Automation
AB  - In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICRA48891.2023.10160883
VL  - 2023-May
SP  - 5631
EP  - 5637
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168711151&doi=10.1109%2fICRA48891.2023.10160883&partnerID=40&md5=dc9e3afea2258e7f3a6a741b55b96a88
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Autonomous vehicles
KW  - Intelligent systems
KW  - Autonomous driving
KW  - Learning systems
KW  - Learning algorithms
KW  - Performance
KW  - Deep reinforcement learning
KW  - Reinforcement learnings
KW  - Autonomous Driving
KW  - Black boxes
KW  - Deep Reinforcement Learning
KW  - Adaptive cruise control
KW  - Safety engineering
KW  - Iterative methods
KW  - Reinforcement learning algorithms
KW  - Black-box verification
KW  - Black-Box Verification
KW  - Safety performance
KW  - Verification algorithms
KW  - Verification method
ER  - 

TY  - JOUR
TI  - A Discrepancy Aware Framework for Robust Anomaly Detection
AU  - Cai, Y.
AU  - Liang, D.
AU  - Luo, D.
AU  - He, X.
AU  - Yang, X.
AU  - Bai, X.
T2  - IEEE Transactions on Industrial Informatics
AB  - Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this article, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a discrepancy aware framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TII.2023.3318302
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174819201&doi=10.1109%2fTII.2023.3318302&partnerID=40&md5=d67ed69e94513df294a49a8610cfedcb
DB  - Scopus
KW  - Artificial intelligence
KW  - Data models
KW  - Training
KW  - robustness
KW  - Supervised learning
KW  - Anomaly detection
KW  - Benchmarking
KW  - Robustness
KW  - Simple++
KW  - Decoding
KW  - Critical researches
KW  - Defect detection
KW  - Defects
KW  - Distillation
KW  - Head
KW  - Image reconstruction
KW  - Images reconstruction
KW  - self-supervised learning
KW  - Self-supervised learning
KW  - Synthetic data
ER  - 

TY  - CONF
TI  - Probabilistic Counterexample Guidance for Safer Reinforcement Learning
AU  - Ji, X.
AU  - Filieri, A.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method’s effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-43835-6_22
VL  - 14287 LNCS
SP  - 311
EP  - 328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174226630&doi=10.1007%2f978-3-031-43835-6_22&partnerID=40&md5=b3d687148a4fbe03b6bafbf99bae2612
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Safe reinforcement learning
KW  - Probabilistics
KW  - Counterexample guidance
KW  - Model checking
KW  - Online explorations
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Safety requirements
KW  - Safety violations
KW  - Trial and error learning
ER  - 

TY  - JOUR
TI  - Dual Learning-Based Safe Semi-Supervised Learning
AU  - Gan, HT
AU  - Li, ZH
AU  - Fan, YL
AU  - Luo, ZZ
T2  - IEEE ACCESS
AB  - In many real-world applications, labeled instances are generally limited and expensively collected, while the most instances are unlabeled and the amount is often sufficient. Therefore, semi supervised learning (SSL) has attracted much attention, since it is an effective tool to discover the unlabeled instances. However, how to safely make use of the unlabeled instances is an emerging and interesting problem in SSL. Hence, we propose DuAL Learning-based sAfe Semi-supervised learning (DALLAS), which employs dual learning to estimate the safety or risk of the unlabeled instances. To realize the safe exploitation of the unlabeled instances, our basic idea is to use supervised learning (SL) to analyze the risk of the unlabeled instances. First, DALLAS utilizes a primal model obtained by dual learning to classify each unlabeled instance and then uses a dual model to reconstruct the unlabeled instances according to the obtained classification results. The risk can be measured by analyzing the reconstruction error and predictions of the original and reconstructed unlabeled instances. If the error is small and the predictions are equal, the unlabeled instance may be safe. Otherwise, the instance may be risky and its output should be approach to be that obtained by SL. Finally, we embed a risk-based regularization term into SSL. Hence, the outputs of our algorithm are a tradeoff between those of SL and SSL. In particular, we utilize respectively regularized least squares (RLS) and Laplacian RLS for SL and SSL. To verify the effectiveness of the proposed safe mechanism in DALLAS, we carry out a series of experiments on several data sets by the comparison with the state-of-the-art supervised, semi-supervised, and safe semi-supervised learning methods and the results demonstrate that DALLAS can effectively reduce the risk of the unlabeled instances.
DA  - 2018///
PY  - 2018
DO  - 10.1109/ACCESS.2017.2784406
VL  - 6
SP  - 2615
EP  - 2621
SN  - 2169-3536
AN  - WOS:000425688200021
KW  - artificial intelligence
KW  - Artificial intelligence
KW  - Supervised learning
KW  - Learning algorithms
KW  - Accident prevention
KW  - safety
KW  - Risk perception
KW  - Risk assessment
KW  - Safety engineering
KW  - Semi- supervised learning
KW  - supervised learning
KW  - Semisupervised learning
KW  - Classification algorithm
KW  - Laplace transforms
KW  - Semi-supervised learning methods
KW  - Linear programming
KW  - Reconstruction error
KW  - classification algorithms
KW  - Regularization terms
KW  - Classification results
KW  - Gallium nitride
KW  - Laplace equation
KW  - Regularized least squares
KW  - Semi-supervised learning (SSL)
ER  - 

TY  - JOUR
TI  - Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate
AU  - Yang, YJ
AU  - Jiang, YX
AU  - Liu, YC
AU  - Chen, JY
AU  - Li, SE
T2  - IEEE ROBOTICS AND AUTOMATION LETTERS
AB  - Safety is a critical concern when applying reinforcement learning (RL) to real-world control tasks. However, existing safe RL works either only consider expected safety constraint violations and fail to maintain safety guarantees, or use overly conservative safety certificate tools borrowed from safe control theory, which sacrifices reward optimization and relies on analytic system models. This letter proposes a model-free safe RL algorithm that achieves near-zero constraint violations with high rewards. Our key idea is to jointly learn a policy and a neural barrier certificate under stepwise state constraint setting. The barrier certificate is learned in a model-free manner by minimizing the violations of appropriate barrier properties on transition data collected by the policy. We extend the single-step invariant property of the barrier certificate to a multi-step version and construct the corresponding multi-step invariant loss. This loss balances the bias and variance of the barrier certificate and enhances both the safety and performance of the policy. The policy is optimized under the constraint of the multi-step invariant property using the Lagrangian method. We optimize the policy in a model-free manner by introducing an importance sampling weight in the constraint. We test our algorithm on multiple problems, including classic control tasks, robot collision avoidance, and autonomous driving. Results show that our algorithm achieves near-zero constraint violations and high performance compared to the baselines. Moreover, the learned barrier certificates successfully identify the feasible regions on multiple tasks.
DA  - 2023/03//undefined
PY  - 2023
DO  - 10.1109/LRA.2023.3238656
VL  - 8
IS  - 3
SP  - 1295
EP  - 1302
SN  - 2377-3766
AN  - WOS:000923839100012
KW  - Reinforcement learning
KW  - Task analysis
KW  - Robots
KW  - reinforcement learning
KW  - Analytical models
KW  - Job analysis
KW  - Reinforcement learnings
KW  - Computation theory
KW  - Model free
KW  - Computational modelling
KW  - Robot safety
KW  - Barrier certificates
KW  - Constraint violation
KW  - Control task
KW  - Importance sampling
KW  - Multisteps
KW  - neural barrier certificate
KW  - Neural barrier certificate
ER  - 

TY  - JOUR
TI  - A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms
AU  - Aksjonov, A
AU  - Kyrki, V
T2  - SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH
AB  - While machine-learning-based methods suffer from a lack of transparency, rule-based (RB) methods dominate safety-critical systems. Yet the RB approaches cannot compete with the first ones in robustness to multiple system requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, this article proposes a decision-making and control framework which profits from the advantages of both the RB and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. An RB switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized whenever the Learned one does not meet the safety constraint, and also directly participates in the Learned controller training. Decision-making and control in autonomous driving are chosen as the system case study, where an autonomous vehicle (AV) learns a multitask policy to safely execute an unprotected left turn. Multiple requirements (i.e., safety, efficiency, and comfort) are set to vehicle motion. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environments is successfully demonstrated.
DA  - 2023///
PY  - 2023
DO  - 10.4271/10-07-03-0018
VL  - 7
IS  - 3
SP  - 287
EP  - 299
SN  - 2380-2162
AN  - WOS:001072996800003
KW  - Autonomous vehicles
KW  - Safety
KW  - Machine learning
KW  - Decision making
KW  - Intelligent vehicles
KW  - Autonomous Vehicles
KW  - Decisions makings
KW  - Machine-learning
KW  - Controllers
KW  - Intelligent control
KW  - Efficiency
KW  - Safety engineering
KW  - Robustness (control systems)
KW  - Learning-based methods
KW  - Control framework
KW  - Decision control
KW  - Decision-making
KW  - Decision-making frameworks
KW  - Rule based algorithms
KW  - Rule-based method
KW  - Rule-based systems
KW  - Rules based systems
ER  - 

TY  - CONF
TI  - Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode
AU  - Guissouma, H
AU  - Zink, M
AU  - Sax, E
AU  - IEEE
T2  - 2023 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE COMPANION, ICSA-C
AB  - Over-The-Air (OTA) updates play an essential role in the lifecycle management of modern Cyber Physical Systems (CPSs). They are deployed in short time periods to fix bugs and introduce new features. However, an important part of these updates affects safety-critical functions, and thus, requires thorough verification and validation. Particular care must be taken when using machine learning algorithms, for which it is more difficult to test all conceivable corner cases during the development process. To prevent potential unforeseen misbehavior after deployment, we introduce a method for runtime evaluation of updates in shadow mode using contract specifications. The method focuses on supervised learning models and is embedded in a workflow for iterative training. This enables carrying out reliable field testing and obtaining a realistic evaluation of the planned updates before release. Finally, we evaluate our approach on a prototype Electronic Control Unit (ECU) implementing an automotive Lane Keep Assist (LKA) system.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICSA-C57050.2023.00069
SP  - 301
EP  - 308
SN  - 2768-427X
AN  - WOS:000990534100053
KW  - Supervised learning
KW  - Cyber-physical systems
KW  - Learning algorithms
KW  - Safety engineering
KW  - Learning models
KW  - Safety assessments
KW  - supervised learning
KW  - Embedded systems
KW  - Life cycle
KW  - Iterative methods
KW  - Cybe-physical systems
KW  - Control systems
KW  - contract-based design
KW  - Contract-based designs
KW  - DevOps
KW  - Lifecycle management
KW  - OTA updates
KW  - Over-the-air updates
KW  - safety monitoring
KW  - Safety monitoring
KW  - Safety-critical functions
KW  - Time-periods
ER  - 

TY  - JOUR
TI  - Explicit Explore, Exploit, or Escape (E4): near-optimal safety-constrained reinforcement learning in polynomial time
AU  - Bossens, DM
AU  - Bishop, N
T2  - MACHINE LEARNING
AB  - In reinforcement learning (RL), an agent must explore an initially unknown environment in order to learn a desired behaviour. When RL agents are deployed in real world environments, safety is of primary concern. Constrained Markov decision processes (CMDPs) can provide long-term safety constraints; however, the agent may violate the constraints in an effort to explore its environment. This paper proposes a model-based RL algorithm called Explicit Explore, Exploit, or Escape (E-4), which extends the Explicit Explore or Exploit (E-3) algorithm to a robust CMDP setting. E-4 explicitly separates exploitation, exploration, and escape CMDPs, allowing targeted policies for policy improvement across known states, discovery of unknown states, as well as safe return to known states. E-4 robustly optimises these policies on the worst-case CMDP from a set of CMDP models consistent with the empirical observations of the deployment environment. Theoretical results show that E-4 finds a near-optimal constraint-satisfying policy in polynomial time whilst satisfying safety constraints throughout the learning process. We then discuss E-4 as a practical algorithmic framework, including robust-constrained offline optimisation algorithms, the design of uncertainty sets for the transition dynamics of unknown states, and how to further leverage empirical observations and prior knowledge to relax some of the worst-case assumptions underlying the theory.
DA  - 2023/03//undefined
PY  - 2023
DO  - 10.1007/s10994-022-06201-z
VL  - 112
IS  - 3
SP  - 817
EP  - 858
SN  - 0885-6125
AN  - WOS:000814467900003
KW  - Reinforcement learning
KW  - Learning systems
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Constrained Markov decision process
KW  - Constrained optimization
KW  - Safety constraint
KW  - Model-based reinforcement learning
KW  - Near-optimal
KW  - Constrained Markov decision processes
KW  - Knowledge management
KW  - Polynomial approximation
KW  - Polynomial-time
KW  - Robust markov decision process
KW  - Robust Markov decision processes
KW  - Safe artificial intelligence
KW  - Safe exploration
ER  - 

TY  - CONF
TI  - Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment
AU  - Zhu, J
AU  - Wang, LY
AU  - Han, X
T2  - PROCEEDINGS OF THE 37TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING, ASE 2022
AB  - The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3551349.3556906
SN  - 1527-1366
AN  - WOS:001062775200014
ER  - 

TY  - CONF
TI  - SafeOps: a concept of continuous safety
AU  - Fayollas, C
AU  - Bonnin, H
AU  - Flebus, O
T2  - 2020 16TH EUROPEAN DEPENDABLE COMPUTING CONFERENCE (EDCC 2020)
AB  - Improved safety is one of the key benefits expected from autonomous vehicles. This can only be achieved if the autonomous vehicles are guaranteed to be safe enough. This paper proposes a potential approach contributing to this safety improvement: it describes and investigates "SafeOps", a concept of "continuous safety", based on the DevOps approach, unifying development and operations. DevOps consists in a set of practices intended to reduce the time between committing a change to a system and the change being deployed into production, while ensuring high quality. DevOps benefits to system development and delivery by enabling software continuous delivery, faster changes management with faster issues resolution, and improved reliability. SafeOps key principle is to monitor the system in operation and to use this information for validating and certifying a certain safety assurance level. Following this approach, a system could be compliant to a first safety assurance level when it's first delivered and compliant to higher ones when validated in operation.
DA  - 2020///
PY  - 2020
DO  - 10.1109/EDCC51268.2020.00020
SP  - 65
EP  - 68
SN  - 978-1-72818-936-9
AN  - WOS:000630473500010
KW  - Artificial Intelligence
KW  - Autonomous vehicles
KW  - Machine Learning
KW  - Safety engineering
KW  - Software reliability
KW  - Functional Safety
KW  - Safety assurance
KW  - Safety improvement
KW  - High quality
KW  - DevOps
KW  - System development
KW  - CI/CD
KW  - Cloud-Native Systems
KW  - Continuous Deployment
KW  - Continuous Integration
KW  - Data-intensive Systems
KW  - Development and operations
ER  - 

TY  - CONF
TI  - Inferring Human Values for Safe AGI Design
AU  - Sezener, CE
T2  - ARTIFICIAL GENERAL INTELLIGENCE (AGI 2015)
A2  - Bieger, J
A2  - Goertzel, B
A2  - Potapov, A
AB  - Aligning goals of superintelligent machines with human values is one of the ways to pursue safety in AGI systems. To achieve this, it is first necessary to learn what human values are. However, human values are incredibly complex and cannot easily be formalized by hand. In this work, we propose a general framework to estimate the values of a human given its behavior.
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-319-21365-1_16
VL  - 9205
SP  - 152
EP  - 155
SN  - 0302-9743
AN  - WOS:000363479400016
ER  - 

TY  - CONF
TI  - A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots
AU  - Wang, WQ
AU  - Zhou, X
AU  - Xu, BL
AU  - Lu, ML
AU  - Zhang, YX
AU  - Gu, YH
T2  - 2022 41ST CHINESE CONTROL CONFERENCE (CCC)
A2  - Li, Z
A2  - Sun, J
AB  - Reinforcement learning (RL) holds the promise of autonomous robots because it can adapt to dynamic or unknown environments by automatically learning optimal control policies from the interactions between robots and environments. However, the interactions can be unsafe to both robots and environments during the learning phase, which hinders the practical deployment of RL. Some safe RL methods have been proposed to improve the learning safety by using external or prior knowledge to guide safe actions, but it is difficult to assume having this knowledge in practical applications, especially in unknown environments. More importantly, considering failures are unavoidable in practice, current safe RL lacks the capability of recovering to safe states from failures so that the learning cannot be continued and finished. To solve these problems, we propose a safe and self-recoverable reinforcement learning framework that can predict and prohibit other unsafe actions based on known, explored unsafe actions during the exploration process, and can self-recover to a safe state when a failure occurs. The maze navigation simulation results show that our approach can not only significantly reduce the number of failures but also accelerate the convergence of reinforcement learning.
DA  - 2022///
PY  - 2022
SP  - 3878
EP  - 3883
SN  - 2161-2927
AN  - WOS:000932071603168
KW  - Reinforcement learning
KW  - Robots
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Dynamic environments
KW  - Recovery
KW  - Air navigation
KW  - Learning frameworks
KW  - Safe reinforcement learning
KW  - Reinforcement learning method
KW  - Optimal control policy
KW  - Unknown environments
KW  - Learning phasis
KW  - safe reinforcement learning
KW  - autonomous robots
KW  - self-recoverable reinforcement learning
KW  - Self-recoverable reinforcement learning
ER  - 

TY  - CONF
TI  - A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation
AU  - Zhu, X
AU  - Kang, SC
AU  - Chen, JY
AU  - IEEE
T2  - 2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)
AB  - Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IROS47612.2022.9981185
SP  - 2476
EP  - 2482
SN  - 2153-0858
AN  - WOS:000908368202013
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Real-world
KW  - Learning frameworks
KW  - Robot manipulation
KW  - Contact forces
KW  - External disturbances
KW  - Joint space
KW  - Manipulation task
KW  - Robot arms
KW  - Task space
ER  - 

TY  - CONF
TI  - Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection
AU  - Moradi, M
AU  - Oakes, BJ
AU  - Saraoglu, M
AU  - Morozov, A
AU  - Janschek, K
AU  - Denil, J
AU  - IEEE Comp Soc
T2  - 50TH ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS WORKSHOPS (DSN-W 2020)
AB  - Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.
DA  - 2020///
PY  - 2020
DO  - 10.1109/DSN-W50199.2020.00028
SP  - 102
EP  - 109
SN  - 978-1-72817-263-7
AN  - WOS:000853340600018
KW  - machine learning
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - Learning systems
KW  - Accident prevention
KW  - Speed control
KW  - Adaptive cruise control
KW  - Embedded systems
KW  - Automotive safety
KW  - Software testing
KW  - Fault injection
KW  - Monte Carlo methods
KW  - cyber-physical systems
KW  - Learning process
KW  - Cyber-physical systems (CPS)
KW  - Adaptive cruise controllers
KW  - Development cycle
KW  - Fault coverages
KW  - Fault parameters
KW  - Random algorithms
KW  - safety assessment
ER  - 

TY  - CONF
TI  - Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration with External Systems
AU  - Kohjima, M
AU  - Takahashi, M
AU  - Toda, H
AU  - IEEE
T2  - 2020 59TH IEEE CONFERENCE ON DECISION AND CONTROL (CDC)
AB  - The importance of safe reinforcement learning (safe RL) is widely recognized for enhancing real world systems. In this study, we construct the censored Markov decision process (CeMDP), a new Markov Decision Process (MDP) framework that describes the interaction of environment, learner and external systems, e.g., human intervention or pre-designed controller for emergency response. We also theoretically analyze the relation of CeMDP to existing frameworks such as the semi-Markov decision process, MDP with Option (OMDP) and standard MDP; the analysis clarifies that CeMDP is a special case of OMDP and can, with environment redefinition, be represented by MDP. This finding allows us to design planning and reinforcement learning algorithms for CeMDP. We confirm the validity of the theory and algorithms by numerical experiments.
DA  - 2020///
PY  - 2020
SP  - 3623
EP  - 3630
SN  - 0743-1546
AN  - WOS:000717663402145
KW  - Reinforcement learning
KW  - Learning systems
KW  - Behavioral research
KW  - Learning algorithms
KW  - Real-world system
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Human intervention
KW  - Numerical experiments
KW  - Emergency response
KW  - Design planning
KW  - External systems
KW  - Semi-Markov decision process
ER  - 

TY  - JOUR
TI  - Lyapunov design for safe reinforcement learning
AU  - Perkins, TJ
AU  - Barto, AG
T2  - JOURNAL OF MACHINE LEARNING RESEARCH
AB  - Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.
DA  - 2003/05/15/
PY  - 2003
DO  - 10.1162/jmlr.2003.3.4-5.803
VL  - 3
IS  - 4-5
SP  - 803
EP  - 832
SN  - 1532-4435
AN  - WOS:000184926200009
KW  - Reinforcement learning
KW  - Safety
KW  - Learning systems
KW  - Learning algorithms
KW  - Control system synthesis
KW  - Learning agents
KW  - Reliability
KW  - Lyapunov methods
KW  - Lyapunov functions
KW  - Stability
KW  - System stability
ER  - 

TY  - JOUR
TI  - Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework
AU  - Mazouchi, M
AU  - Nageshrao, S
AU  - Modares, H
T2  - IEEE-CAA JOURNAL OF AUTOMATICA SINICA
AB  - In this paper, a data-driven conflict-aware safe reinforcement learning (CAS-RL) algorithm is presented for control of autonomous systems. Existing safe RL results with predefined performance functions and safe sets can only provide safety and performance guarantees for a single environment or circumstance. By contrast, the presented CAS-RL algorithm provides safety and performance guarantees across a variety of circumstances that the system might encounter. This is achieved by utilizing a bilevel learning control architecture: A higher metacognitive layer leverages a data-driven receding-horizon attentional controller (RHAC) to adapt relative attention to different system's safety and performance requirements, and, a lower-layer RL controller designs control actuation signals for the system. The presented RHAC makes its meta decisions based on the reaction curve of the lower-layer RL controller using a meta-model or knowledge. More specifically, it leverages a prediction meta-model (PMM) which spans the space of all future meta trajectories using a given finite number of past meta trajectories. RHAC will adapt the system's aspiration towards performance metrics (e.g., performance weights) as well as safety boundaries to resolve conflicts that arise as mission scenarios develop. This will guarantee safety and feasibility (i.e., performance boundness) of the lower-layer RL-based control solution. It is shown that the interplay between the RHAC and the lower-layer RL controller is a bilevel optimization problem for which the leader (RHAC) operates at a lower rate than the follower (RL-based controller) and its solution guarantees feasibility and safety of the control solution. The effectiveness of the proposed framework is verified through a simulation example.
DA  - 2022/03//undefined
PY  - 2022
DO  - 10.1109/JAS.2021.1004353
VL  - 9
IS  - 3
SP  - 466
EP  - 481
SN  - 2329-9266
AN  - WOS:000735515700010
KW  - Reinforcement learning
KW  - Cognitive systems
KW  - Controllers
KW  - Safety guarantees
KW  - Autonomous system
KW  - Trajectories
KW  - Aerospace electronics
KW  - reinforcement learning (RL)
KW  - Optimal control
KW  - Optimal controls
KW  - Control systems
KW  - Reinforcement learning algorithms
KW  - Computational modelling
KW  - Data driven
KW  - Receding horizon
KW  - Receding-horizon attentional controller
KW  - receding-horizon attentional controller (RHAC)
ER  - 

TY  - JOUR
TI  - Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety
AU  - Sheth, A
AU  - Gaur, M
AU  - Roy, K
AU  - Venkataraman, R
AU  - Khandelwal, V
T2  - IEEE INTERNET COMPUTING
AB  - AI has seen wide adoption for automating tasks in several domains. However, AI's use in high-value, sensitive, or safety-critical applications such as self-management for personalized health or personalized nutrition has been challenging. These require that the AI system follows guidelines or well-defined processes set by experts, community, or standards. We characterize these as process knowledge (PK). For example, to diagnose the severity of depression, the AI system should incorporate PK that is part of the clinical decision-making process, such as the Patient Health Questionnaire (PHQ-9). Likewise, a nutritionist's knowledge and dietary guidelines are needed to create food plans for diabetic patients. Furthermore, the BlackBox nature of purely data-reliant statistical AI systems falls short in providing user-understandable explanations, such as what a clinician would need to ensure and document compliance with medical guidelines before relying on a recommendation. Using the examples of mental health and cooking recipes for diabetic patients, we show why, what, and how to incorporate PK along with domain knowledge in machine learning. We discuss methods for infusing PK and present performance evaluation metrics. Support for safety and user-level explainability of the PK-infused learning improves confidence and trust in the AI system.
DA  - 2022/09/01/
PY  - 2022
DO  - 10.1109/MIC.2022.3182349
VL  - 26
IS  - 5
SP  - 76
EP  - 84
SN  - 1089-7801
AN  - WOS:000853843100019
ER  - 

TY  - CONF
TI  - Verifiably Safe Exploration for End-to-End Reinforcement Learning
AU  - Hunt, N
AU  - Fulton, N
AU  - Magliacane, S
AU  - Hoang, TN
AU  - Das, S
AU  - Solar-Lezama, A
T2  - HSCC2021: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (PART OF CPS-IOT WEEK)
AB  - Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We characterize safety constraints in terms of a refinement relation on Markov decision processes - rather than directly constraining the reinforcement learning algorithm so that it only takes safe actions, we instead refine the environment so that only safe actions are defined in the environment's transition structure. This has pragmatic system design benefits and, more importantly, provides a clean conceptual setting in which we are able to prove important safety and efficiency properties. These allow us to transform the constrained optimization problem of acting safely in the original environment into an unconstrained optimization in a refined environment.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3447928.3456653
SN  - 978-1-4503-8339-4
AN  - WOS:000932821700017
KW  - Deep learning
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - neural networks
KW  - Internet of things
KW  - Learning algorithms
KW  - Object detection
KW  - Safety engineering
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Safety and efficiencies
KW  - Embedded systems
KW  - Constrained optimization
KW  - Hybrid systems
KW  - formal verification
KW  - Constrained optimi-zation problems
KW  - Automated reasoning
KW  - Bench-mark problems
KW  - differential dynamic logic
KW  - Hybrid dynamical systems
KW  - hybrid systems
KW  - safe artificial intelligence
KW  - Transition structures
KW  - Unconstrained optimization
ER  - 

TY  - JOUR
TI  - Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems
AU  - Zhou, ZH
AU  - Oguz, OS
AU  - Leibold, M
AU  - Buss, M
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
AB  - For the safe application of reinforcement learning algorithms to high-dimensional nonlinear dynamical systems, a simplified system model is used to formulate a safe reinforcement learning (SRL) framework. Based on the simplified system model, a low-dimensional representation of the safe region is identified and used to provide safety estimates for learning algorithms. However, finding a satisfying simplified system model for complex dynamical systems usually requires a considerable amount of effort. To overcome this limitation, we propose a general data-driven approach that is able to efficiently learn a low-dimensional representation of the safe region. By employing an online adaptation method, the low-dimensional representation is updated using the feedback data to obtain more accurate safety estimates. The performance of the proposed approach for identifying the low-dimensional representation of the safe region is illustrated using the example of a quadcopter. The results demonstrate a more reliable and representative low-dimensional representation of the safe region compared with previous works, which extends the applicability of the SRL framework.
DA  - 2023/05//undefined
PY  - 2023
DO  - 10.1109/TNNLS.2021.3106818
VL  - 34
IS  - 5
SP  - 2513
EP  - 2527
SN  - 2162-237X
AN  - WOS:000733505000001
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - article
KW  - Adaptive control systems
KW  - Dynamical systems
KW  - Nonlinear dynamical systems
KW  - Heuristic algorithms
KW  - Computational modelling
KW  - Heuristics algorithm
KW  - Data-driven model
KW  - reinforcement learning (machine learning)
KW  - Data-driven model order reduction
KW  - Deep learning in robotic and automation
KW  - deep learning in robotics and automation
KW  - Learning and adaptive system
KW  - learning and adaptive systems
KW  - Model order reduction
KW  - Probabilistic logics
KW  - Safe reinforcement learning .
KW  - safe reinforcement learning (SRL)
ER  - 

TY  - CONF
TI  - Assured Deep Multi-Agent Reinforcement Learning for Safe Robotic Systems
AU  - Riley, J
AU  - Calinescu, R
AU  - Paterson, C
AU  - Kudenko, D
AU  - Banks, A
T2  - AGENTS AND ARTIFICIAL INTELLIGENCE, ICAART 2021
A2  - Rocha, AP
A2  - Steels, L
A2  - VanDenHerik, J
AB  - Using multi-agent reinforcement learning to find solutions to complex decision-making problems in shared environments has become standard practice in many scenarios. However, this is not the case in safety-critical scenarios, where the reinforcement learning process, which uses stochastic mechanisms, could lead to highly unsafe outcomes. We proposed a novel, safe multi-agent reinforcement learning approach named Assured Multi-Agent Reinforcement Learning (AMARL) to address this issue. Distinct from other safe multi-agent reinforcement learning approaches, AMARL utilises quantitative verification, a model checking technique that guarantees agent compliance of safety, performance, and non-functional requirements, both during and after the learning process. We have previously evaluated AMARL in patrolling domains with various multi-agent reinforcement learning algorithms for both homogeneous and heterogeneous systems. In this work we extend AMARL through the use of deep multi-agent reinforcement learning. This approach is particularly appropriate for systems in which the rewards are sparse and hence extends the applicability of AMARL. We evaluate our approach within a new search and collection domain which demonstrates promising results in safety standards and performance compared to algorithms not using AMARL.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-10161-8_8
VL  - 13251
SP  - 158
EP  - 180
SN  - 0302-9743
AN  - WOS:000876376200008
KW  - Deep learning
KW  - Reinforcement learning
KW  - Decision making
KW  - Multi-Agent Systems
KW  - Learning systems
KW  - Learning algorithms
KW  - Deep reinforcement learning
KW  - Reinforcement learnings
KW  - Deep Reinforcement Learning
KW  - Safety engineering
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Stochastic systems
KW  - Multi-Agent Reinforcement Learning
KW  - Assurance
KW  - Reinforcement Learning
KW  - Compliance control
KW  - Model checking
KW  - Assured multi-agent reinforcement learning
KW  - Assured Multi-Agent Reinforcement Learning
KW  - Quantitative verification
KW  - Safe multi-agent reinforcement learning
KW  - Safe Multi-Agent Reinforcement Learning
KW  - Safety-critical scenario
KW  - Safety-critical scenarios
ER  - 

TY  - CONF
TI  - Proposed V-Model for Verification, Validation, and Safety Activities for Artificial Intelligence
AU  - Schumeg, B
AU  - Marotta, F
AU  - Werner, B
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON ASSURED AUTONOMY, ICAA
AB  - The Department of Defense strives to continuously develop and acquire systems that utilize novel technologies and methods for implementing new and complex mission requirements. One of the identified technologies with high impact and benefit to the Warfighter is the integration of Artificial Intelligence (AI) and Machine Learning (ML). Current AI models and methods have added layers of complexity to achieving a satisfactory level of verification and validation (V&V), possibly resulting in elevated risks with fewer mitigations. Regardless of the type of applications for AI technology within the DoD, the technology implementation must be verified, validated, and ultimately any residual risks accepted. This paper looks to introduce a V-model concept for Artificial Intelligence and Machine Learning, to include an outline of proposed activities that the development, assurance, and evaluation communities can follow. By following this proposed assessment, these organizations can increase their understanding and knowledge of the system, mitigating risk and helping to achieve justified confidence.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICAA58325.2023.00017
SP  - 61
EP  - 66
SN  - 979-8-3503-2601-7
AN  - WOS:001050787900009
KW  - machine learning
KW  - Autonomy
KW  - artificial intelligence
KW  - verification
KW  - Machine learning
KW  - Evaluation
KW  - evaluation
KW  - Machine-learning
KW  - safety
KW  - Risk assessment
KW  - Safety engineering
KW  - Artificial intelligence learning
KW  - assurance
KW  - Assurance
KW  - autonomy
KW  - Safety activities
KW  - test
KW  - Test
KW  - v-model
KW  - V-model
KW  - validation
KW  - Validation
KW  - Verification
KW  - Verification activities
ER  - 

TY  - JOUR
TI  - Multilayered review of safety approaches for machine learning-based systems in the days of AI
AU  - Dey, S
AU  - Lee, SW
T2  - JOURNAL OF SYSTEMS AND SOFTWARE
AB  - The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety. (C) 2021 Elsevier Inc. All rights reserved.
DA  - 2021/06//undefined
PY  - 2021
DO  - 10.1016/j.jss.2021.110941
VL  - 176
SN  - 0164-1212
AN  - WOS:000636371400004
KW  - Machine learning
KW  - Autonomous systems
KW  - Intelligent systems
KW  - Surveys
KW  - Safety engineering
KW  - Future research directions
KW  - Verification
KW  - Engineering education
KW  - Safety requirements
KW  - Software engineering
KW  - Safety analysis
KW  - Research challenges
KW  - Conceptual frameworks
KW  - Component levels
KW  - Engineering process
KW  - Intelligent software systems
KW  - Literature survey
KW  - Validation and verification
ER  - 

TY  - JOUR
TI  - Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence
AU  - Gheraibia, Y
AU  - Kabir, S
AU  - Aslansefat, K
AU  - Sorokos, I
AU  - Papadopoulos, Y
T2  - IEEE ACCESS
AB  - Safety-critical systems are becoming larger and more complex to obtain a higher level of functionality. Hence, modeling and evaluation of these systems can be a difficult and error-prone task. Among existing safety models, Fault Tree Analysis (FTA) is one of the well-known methods in terms of easily understandable graphical structure. This study proposes a novel approach by using Machine Learning (ML) and real-time operational data to learn about the normal behavior of the system. Afterwards, if any abnormal situation arises with reference to the normal behavior model, the approach tries to find the explanation of the abnormality on the fault tree and then share the knowledge with the operator. If the fault tree fails to explain the situation, a number of different recommendations, including the potential repair of the fault tree, are provided based on the nature of the situation. A decision tree is utilized for this purpose. The effectiveness of the proposed approach is shown through a hypothetical example of an Aircraft Fuel Distribution System (AFDS).
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2941566
VL  - 7
SP  - 135855
EP  - 135869
SN  - 2169-3536
AN  - WOS:000563954900139
KW  - Decision trees
KW  - Artificial intelligence
KW  - Machine learning
KW  - Learning systems
KW  - Safety engineering
KW  - Reliability
KW  - Safety critical systems
KW  - Fault tree
KW  - Fault tree analyses (FTA)
KW  - Fault tree analysis
KW  - Fault-trees
KW  - Fuel distribution system
KW  - Graphical structures
KW  - Model repair
KW  - Real time operational data
KW  - Repair
KW  - Safety modeling
ER  - 

TY  - JOUR
TI  - Metacognition for artificial intelligence system safety-An approach to safe and desired behavior
AU  - Johnson, B
T2  - SAFETY SCIENCE
AB  - Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human-machine teaming operations. In parallel, the increasing volume, velocity, variety, ve-racity, value, and variability of data is confounding the complexity of these new systems - creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.
DA  - 2022/07//undefined
PY  - 2022
DO  - 10.1016/j.ssci.2022.105743
VL  - 151
SN  - 0925-7535
AN  - WOS:000792913700003
KW  - machine learning
KW  - artificial intelligence
KW  - Machine learning
KW  - Cognitive systems
KW  - article
KW  - human
KW  - human experiment
KW  - System safety
KW  - Artificial intelligence systems
KW  - awareness
KW  - Human-machine
KW  - Artificial systems
KW  - Complexity
KW  - Computational data
KW  - Computational thinkings
KW  - metacognition
KW  - Metacognition
KW  - Self awareness
KW  - self care
KW  - Volume velocities
ER  - 

TY  - JOUR
TI  - A risk degree-based safe semi-supervised learning algorithm
AU  - Gan, HT
AU  - Luo, ZZ
AU  - Meng, M
AU  - Ma, YL
AU  - She, QS
T2  - INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS
AB  - Semi-supervised learning has attracted much attention in machine learning field over the past decades and a number of algorithms are proposed to improve the performance by exploiting unlabeled data. However, unlabeled data may hurt performance of semi-supervised learning in some cases. It is instinctively expected to design a reasonable strategy to safety exploit unlabeled data. To address the problem, we introduce a safe semi-supervised learning by analyzing the different characteristics of unlabeled data in supervised and semi-supervised learning. Our intuition is that unlabeled data may be often risky in semi-supervised setting and the risk degree are different. Hence, we assign different risk degree to unlabeled data and the risk degree serve as a sieve to determine the exploiting way of unlabeled data. The unlabeled data with high risk should be exploited by supervised learning and the other should be used for semi-supervised learning. In particular, we utilize kernel minimum squared error (KMSE) and Laplacian regularized KMSE for supervised and semi-supervised learning, respectively. Experimental results on several benchmark datasets illustrate the performance of our algorithm is never inferior to that of KMSE and indicate the effectiveness and efficiency of our algorithm.
DA  - 2016/02//undefined
PY  - 2016
DO  - 10.1007/s13042-015-0416-8
VL  - 7
IS  - 1
SP  - 85
EP  - 94
SN  - 1868-8071
AN  - WOS:000368167400006
KW  - Artificial intelligence
KW  - Learning systems
KW  - Supervised learning
KW  - Algorithms
KW  - Learning algorithms
KW  - Benchmarking
KW  - Benchmark datasets
KW  - Semi- supervised learning
KW  - Semi-supervised learning
KW  - Semi-supervised
KW  - Unlabeled data
KW  - Effectiveness and efficiencies
KW  - Kernel minimum squared error
KW  - Minimum squared error
KW  - Risk degree
KW  - Safe mechanism
KW  - Safe mechanisms
ER  - 

TY  - JOUR
TI  - Provably Safe Artificial General Intelligence via Interactive Proofs
AU  - Carlson, K
T2  - PHILOSOPHIES
AB  - Methods are currently lacking to prove artificial general intelligence (AGI) safety. An AGI 'hard takeoff' is possible, in which first generation AGI(1) rapidly triggers a succession of more powerful AGI(n) that differ dramatically in their computational capabilities (AGI(n) << AGI(n+1)). No proof exists that AGI will benefit humans or of a sound value-alignment method. Numerous paths toward human extinction or subjugation have been identified. We suggest that probabilistic proof methods are the fundamental paradigm for proving safety and value-alignment between disparately powerful autonomous agents. Interactive proof systems (IPS) describe mathematical communication protocols wherein a Verifier queries a computationally more powerful Prover and reduces the probability of the Prover deceiving the Verifier to any specified low probability (e.g., 2(-100)). IPS procedures can test AGI behavior control systems that incorporate hard-coded ethics or value-learning methods. Mapping the axioms and transformation rules of a behavior control system to a finite set of prime numbers allows validation of 'safe' behavior via IPS number-theoretic methods. Many other representations are needed for proving various AGI properties. Multi-prover IPS, program-checking IPS, and probabilistically checkable proofs further extend the paradigm. In toto, IPS provides a way to reduce AGI(n) <-> AGI(n+1) interaction hazards to an acceptably low level.
DA  - 2021/12//undefined
PY  - 2021
DO  - 10.3390/philosophies6040083
VL  - 6
IS  - 4
SN  - 2409-9287
AN  - WOS:000738051700001
ER  - 

TY  - JOUR
TI  - Safe semi-supervised classification algorithm combined with active learning sampling strategy
AU  - Zhao, JH
AU  - Liu, N
AU  - Malov, A
T2  - JOURNAL OF INTELLIGENT & FUZZY SYSTEMS
AB  - In order to improve the performance of semi-supervised learning, a kind of safe semi-supervised classification algorithm based active learning sampling strategy is proposed. First, an active learning sampling method based on uncertainty and representativenes is designed. The weighted algorithm combining the uncertainty and representativenesss is used to select the unlabeled samples with rich information and representation, providing for semi-supervised learning. Second, a method of label prediction based on grouping verification is designed. Prelabeling is executed on unlabeled sample selected by active learning. The sample with pseudo-label is added into the labeled sample set to carry out grouping, training and testing. The corresponding errors of various pseudo-labels are calculated and the pseudo-label making the accuracy least is selected as the candidate label of the unlabeled sample. Third, a method of security verification is designed. Only the label making the accuracy lower than before is selected as the final label of the unlabeled sample to expand the number of labeled samples. Iterations are repeatedly executed until a certain precision is met. Finally, the classifier is trained using the final labeled set. The experiments are carried out on semi-supervised datasets and UCI datasets, and the results show that the proposed algorithms are effective.
DA  - 2018///
PY  - 2018
DO  - 10.3233/JIFS-169722
VL  - 35
IS  - 4
SP  - 4001
EP  - 4010
SN  - 1064-1246
AN  - WOS:000451338400007
KW  - Artificial intelligence
KW  - semi-supervised learning
KW  - Supervised learning
KW  - Learning algorithms
KW  - Accident prevention
KW  - safety
KW  - Training and testing
KW  - Semi- supervised learning
KW  - Active Learning
KW  - Sampling strategies
KW  - Active learning sample
KW  - grouping verification
KW  - label prediction
KW  - Label predictions
KW  - Security verification
KW  - Semi-supervised classification
KW  - Weighted algorithm
ER  - 

TY  - JOUR
TI  - A multi-level semantic web for hard-to-specify domain concept, Pedestrian, in ML-based software
AU  - Barzamini, H
AU  - Shahzad, M
AU  - Alhoori, H
AU  - Rahimi, M
T2  - REQUIREMENTS ENGINEERING
AB  - Machine Learning (ML) algorithms are widely used in building software-intensive systems, including safety-critical ones. Unlike traditional software components, Machine-Learned Components (MLC)s, software components built using ML algorithms, learn their specifications through generalizing the common features that they find in a limited set of collected examples. While this inductive nature overcomes the limitations of programming hard-to-specify concepts, the same feature becomes problematic for verifying safety in ML-based software systems. One reason is that, due to MLCs data-driven nature, there is often no set of explicitly written and pre-defined specifications, against which the MLC can be verified. In this regard, we propose to partially specify hard-to-specify domain concepts, which MLCs tend to classify, instead of fully relying on their inductive learning ability from arbitrarily-collected datasets. In this paper, we propose a semi-automated approach to construct a multi-level semantic web to partially outline the hard-to-specify, yet crucial, domain concept "pedestrian" in automotive domain. We evaluate the applicability of the generated semantic web in two ways: first, with a reference to the web, we augment a pedestrian dataset for a missing feature, wheelchair, to show training a state-of-the-art ML-based object detector on the augmented dataset improves its accuracy in detecting pedestrians; second, we evaluate the coverage of the generated semantic web based on multiple state-of-the-art pedestrian and human datasets.
DA  - 2022/06//undefined
PY  - 2022
DO  - 10.1007/s00766-021-00366-0
VL  - 27
IS  - 2
SP  - 161
EP  - 182
SN  - 0947-3602
AN  - WOS:000740138800001
KW  - Machine learning
KW  - Machine learning algorithms
KW  - Support vector machines
KW  - Machine-learning
KW  - Image classification
KW  - Classification (of information)
KW  - State of the art
KW  - Object detection
KW  - Pedestrian safety
KW  - Specifications
KW  - Machine components
KW  - Security systems
KW  - Semantic Web
KW  - Semantic-Web
KW  - Safety critical systems
KW  - Domain concepts
KW  - Machine-learned component
KW  - Machine-learned components
KW  - Multilevels
KW  - Requirements specifications
KW  - Safety-critical systems
KW  - Software-component
ER  - 

TY  - CONF
TI  - Neural Simplex Architecture
AU  - Phan, DT
AU  - Grosu, R
AU  - Jansen, N
AU  - Paoletti, N
AU  - Smolka, SA
AU  - Stoller, SD
T2  - NASA FORMAL METHODS (NFM 2020)
A2  - Lee, R
A2  - Jha, S
A2  - Mavridou, A
A2  - Giannakopoulou, D
AB  - We present the Neural Simplex Architecture (NSA), a new approach to runtime assurance that provides safety guarantees for neural controllers (obtained e.g. using reinforcement learning) of autonomous and other complex systems without unduly sacrificing performance. NSA is inspired by the Simplex control architecture of Sha et al., but with some significant differences. In the traditional approach, the advanced controller (AC) is treated as a black box; when the decision module switches control to the baseline controller (BC), the BC remains in control forever. There is relatively little work on switching control back to the AC, and there are no techniques for correcting the AC's behavior after it generates a potentially unsafe control input that causes a failover to the BC. Our NSA addresses both of these limitations. NSA not only provides safety assurances in the presence of a possibly unsafe neural controller, but can also improve the safety of such a controller in an online setting via retraining, without overly degrading its performance. To demonstrate NSA's benefits, we have conducted several significant case studies in the continuous control domain. These include a target-seeking ground rover navigating an obstacle field, and a neural controller for an artificial pancreas system.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-55754-6_6
VL  - 12229
SP  - 97
EP  - 114
SN  - 0302-9743
AN  - WOS:000890074700006
KW  - Reinforcement learning
KW  - Controllers
KW  - Traditional approaches
KW  - Advanced controller
KW  - Artificial organs
KW  - Artificial pancreas
KW  - Continuous control
KW  - Control architecture
KW  - Formal methods
KW  - NASA
KW  - Neural controller
KW  - Online retraining
KW  - Reverse switching
KW  - Runtime assurance
KW  - Safe reinforcement learning
KW  - Safety guarantees
KW  - Simplex architecture
KW  - Switching Control
ER  - 

TY  - JOUR
TI  - Constrained Cross-Entropy Method for Safe Reinforcement Learning
AU  - Wen, M
AU  - Topcu, U
T2  - IEEE TRANSACTIONS ON AUTOMATIC CONTROL
AB  - We study a safe reinforcement learning problem, in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The key idea is to transform the original constrained optimization problem into an unconstrained one with a surrogate objective. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then, we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show the performance of the proposed algorithm in two simulation examples. In a constrained linear-quadratic regulator example, we observe that the algorithm converges to the global optimum with high probability. In a 2-D navigation example, we find that the algorithm effectively learns feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.
DA  - 2021/07//undefined
PY  - 2021
DO  - 10.1109/TAC.2020.3015931
VL  - 66
IS  - 7
SP  - 3123
EP  - 3137
SN  - 0018-9286
AN  - WOS:000668858300014
KW  - Reinforcement learning
KW  - Machine learning algorithms
KW  - Safety engineering
KW  - Constrained optimization
KW  - Safe reinforcement learning
KW  - Simulation example
KW  - Constraint Satisfaction
KW  - Objective functions
KW  - Safety critical applications
KW  - Cross-entropy method
KW  - Ordinary differential equations
KW  - Asymptotic behaviors
KW  - Constrained optimi-zation problems
KW  - Linear quadratic regulator
KW  - Statistical learning
ER  - 

TY  - CONF
TI  - Verifiably Safe Off-Model Reinforcement Learning
AU  - Fulton, N
AU  - Platzer, A
T2  - TOOLS AND ALGORITHMS FOR THE CONSTRUCTION AND ANALYSIS OF SYSTEMS, PT I
A2  - Vojnar, T
A2  - Zhang, L
AB  - The desire to use reinforcement learning in safety-critical settings has inspired a recent interest in formal methods for learning algorithms. Existing formal methods for learning and optimization primarily consider the problem of constrained learning or constrained optimization. Given a single correct model and associated safety constraint, these approaches guarantee efficient learning while provably avoiding behaviors outside the safety constraint. Acting well given an accurate environmental model is an important pre-requisite for safe learning, but is ultimately insufficient for systems that operate in complex heterogeneous environments. This paper introduces verification-preserving model updates, the first approach toward obtaining formal safety guarantees for reinforcement learning in settings where multiple possible environmental models must be taken into account. Through a combination of inductive data and deductive proving with design-time model updates and runtime model falsification, we provide a first approach toward obtaining formal safety proofs for autonomous systems acting in heterogeneous environments.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-17462-0_28
VL  - 11427
SP  - 413
EP  - 430
SN  - 0302-9743
AN  - WOS:000681166500028
KW  - Reinforcement learning
KW  - Machine learning
KW  - Autonomous systems
KW  - Learning algorithms
KW  - Safety engineering
KW  - Efficient learning
KW  - Constrained optimization
KW  - Formal methods
KW  - Safety constraint
KW  - Heterogeneous environments
KW  - Environmental model
KW  - Correct models
KW  - Pre-requisites
KW  - Runtime models
ER  - 

TY  - JOUR
TI  - A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making
AU  - Yavas, MU
AU  - Kumbasar, T
AU  - Ure, NK
T2  - IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
AB  - Lane-change decision-making for vehicles is a challenging task for many reasons, including traffic rules, safety, and the stochastic nature of driving. Because of its success in solving complex problems, deep reinforcement learning (DRL) has been suggested for addressing these issues. However, the studies on DRL to date have gone no further than validation in simulation and failed to address what are arguably the most critical issues, namely, the mismatch between simulation and reality, human-likeness, and safety. This paper introduces a real-world DRL framework for decision-making to design safe and human-like agents that can operate in the real world without extra tuning. We propose a new learning paradigm for DRL integrated with Real2Sim transfer, which comprises training, validation, and testing phases. The approach involves two simulator environments with different levels of fidelity, which are parameterized via real-world data. Within the framework, a large amount of randomized experience is generated with a low-fidelity simulator, whereupon the learned skills are validated regularly in a high-fidelity simulator to avoid overfitting. Finally, in the testing phase, the agent is examined concerning safety and human-like decision-making. Extensive simulation and real-world evaluations show the superiority of the proposed approach. To the best of the authors' knowledge, this is the first application of DRL lane-changing policy in the real world.
DA  - 2023/07/25/
PY  - 2023
DO  - 10.1109/TITS.2023.3292981
SN  - 1524-9050
AN  - WOS:001040652400001
KW  - Deep learning
KW  - Reinforcement learning
KW  - Autonomous vehicles
KW  - Safety
KW  - artificial intelligence
KW  - intelligent vehicles
KW  - Task analysis
KW  - Decision making
KW  - Measurement
KW  - reinforcement learning
KW  - Training
KW  - Testing
KW  - Behavioral science
KW  - Behavioral sciences
KW  - Autonomous Vehicles
KW  - Behavioral research
KW  - Decisions makings
KW  - Job analysis
KW  - Accident prevention
KW  - Intelligent vehicle highway systems
KW  - Reinforcement learnings
KW  - Real-world
KW  - Learning frameworks
KW  - Tactical decision makings
KW  - Stochastic systems
KW  - Safety testing
KW  - Human like
KW  - Testing phase
ER  - 

TY  - CONF
TI  - Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios
AU  - Lecerf, UUL
AU  - Yemdji-Tchassi, CCY
AU  - Aubert, SSA
AU  - Michiardi, PPM
AU  - ACM
T2  - PROCEEDINGS OF 2022 7TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING TECHNOLOGIES, ICMLT 2022
AB  - When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.
In this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.
We apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3529399.3529432
SP  - 209
EP  - 215
SN  - 978-1-4503-9574-8
AN  - WOS:001053939400033
KW  - Reinforcement learning
KW  - Autonomous vehicles
KW  - Autonomous agents
KW  - Learning systems
KW  - Autonomous Vehicles
KW  - Reinforcement learnings
KW  - Human drivers
KW  - Safety engineering
KW  - Stochastic systems
KW  - Model free
KW  - Safe control
KW  - Reinforcement Learning
KW  - State-space
KW  - Autonomous Vehicle
KW  - Safe Control
KW  - Catastrophic failures
KW  - Fallback strategies
KW  - Imperfect information
KW  - Stochastic environment
ER  - 

TY  - CONF
TI  - Evaluating Correctness of Reinforcement Learning based on Actor-Critic Algorithm
AU  - Kim, Y
AU  - Hussain, M
AU  - Suh, JW
AU  - Hong, JE
AU  - IEEE
T2  - 2022 THIRTEENTH INTERNATIONAL CONFERENCE ON UBIQUITOUS AND FUTURE NETWORKS (ICUFN)
AB  - Deep learning is used for decision making and functional control in various fields, such as autonomous systems. However, rather than being developed by logical design, deep learning models are trained by itself through learning data. Moreover, only reward values are used to evaluate its performance, which does not provide enough information that the model learned properly. This paper proposes a new method to assess the correctness of reinforcement learning, considering other properties of the learning algorithm. The proposed method is applied for the evaluation of ActorCritic Algorithms, and correctness-related insights of the algorithm are confirmed through experiments.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICUFN55119.2022.9829571
SP  - 320
EP  - 325
SN  - 2165-8528
AN  - WOS:000855059600065
KW  - Deep learning
KW  - Reinforcement learning
KW  - Decision making
KW  - reinforcement learning
KW  - Learning systems
KW  - Decisions makings
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Quality control
KW  - Learning models
KW  - Safety critical systems
KW  - Actor-critic algorithm
KW  - Quality evaluation
KW  - actor-critic algorithm
KW  - correctness
KW  - Correctness
KW  - Functional control
KW  - Learning data
KW  - Logical design
KW  - quality evaluation
KW  - safety-critical system
ER  - 

TY  - CONF
TI  - Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints
AU  - Fischer, J
AU  - Eyberg, C
AU  - Werling, M
AU  - Lauer, M
T2  - 2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)
AB  - Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.
DA  - 2021///
PY  - 2021
DO  - 10.1109/IROS51168.2021.9636672
SP  - 791
EP  - 798
SN  - 2153-0858
AN  - WOS:000755125500079
KW  - Reinforcement learning
KW  - Safety
KW  - Motion planning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Cost functions
KW  - Safety engineering
KW  - Inverse problems
KW  - Markov processes
KW  - Constrained Markov decision process
KW  - Entropy
KW  - Reinforcement learning algorithms
KW  - Reinforcement Learning
KW  - Safety constraint
KW  - Robot programming
KW  - Importance sampling
KW  - Constraint
KW  - Constraints
KW  - Inverse reinforcement learning
KW  - Maximum-entropy
KW  - Robotic systems
KW  - Sampling-based
KW  - Inverse Reinforcement Learning
KW  - Maximum Entropy
KW  - SUMO
ER  - 

TY  - JOUR
TI  - Safe Reinforcement Learning via a Model-Free Safety Certifier
AU  - Modares, A
AU  - Sadati, N
AU  - Esmaeili, B
AU  - Yaghmaie, FA
AU  - Modares, H
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
AB  - This article presents a data-driven safe reinforcement learning (RL) algorithm for discrete-time nonlinear systems. A data-driven safety certifier is designed to intervene with the actions of the RL agent to ensure both safety and stability of its actions. This is in sharp contrast to existing model-based safety certifiers that can result in convergence to an undesired equilibrium point or conservative interventions that jeopardize the performance of the RL agent. To this end, the proposed method directly learns a robust safety certifier while completely bypassing the identification of the system model. The nonlinear system is modeled using linear parameter varying (LPV) systems with polytopic disturbances. To prevent the requirement for learning an explicit model of the LPV system, data-based $\lambda$ -contractivity conditions are first provided for the closed-loop system to enforce robust invariance of a prespecified polyhedral safe set and the system's asymptotic stability. These conditions are then leveraged to directly learn a robust data-based gain-scheduling controller by solving a convex program. A significant advantage of the proposed direct safe learning over model-based certifiers is that it completely resolves conflicts between safety and stability requirements while assuring convergence to the desired equilibrium point. Data-based safety certification conditions are then provided using Minkowski functions. They are then used to seemingly integrate the learned backup safe gain-scheduling controller with the RL controller. Finally, we provide a simulation example to verify the effectiveness of the proposed approach.
DA  - 2023/04/13/
PY  - 2023
DO  - 10.1109/TNNLS.2023.3264815
SN  - 2162-237X
AN  - WOS:000973264800001
KW  - Reinforcement learning
KW  - Safety
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Controllers
KW  - Nonlinear systems
KW  - Safe control
KW  - Closed loop systems
KW  - reinforcement learning (RL)
KW  - Heuristic algorithms
KW  - Optimal control
KW  - Optimal controls
KW  - Heuristics algorithm
KW  - Stability criteria
KW  - System stability
KW  - System Dynamics
KW  - Discrete time control systems
KW  - Convex optimization
KW  - Asymptotic stability
KW  - Data-driven control
KW  - Gain scheduling control
KW  - gain-scheduling control
KW  - safe control
KW  - Stability criterions
KW  - System dynamics
ER  - 

TY  - CONF
TI  - Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models
AU  - Li, ZY
AU  - Zeng, J
AU  - Thirugnanam, A
AU  - Sreenath, K
T2  - ROBOTICS: SCIENCE AND SYSTEM XVIII
A2  - Hauser, K
A2  - Shell, D
A2  - Huang, S
AB  - Bridging model-based safety and model-free reinforcement learning (RL) for dynamic robots is appealing since model-based methods are able to provide formal safety guarantees, while RL-based methods are able to exploit the robot agility by learning from the full-order system dynamics. However, current approaches to tackle this problem are mostly restricted to simple systems. In this paper, we propose a new method to combine model-based safety with model-free reinforcement learning by explicitly finding a low-dimensional model of the system controlled by a RL policy and applying stability and safety guarantees on that simple model. We use a complex bipedal robot Cassie, which is a high dimensional nonlinear system with hybrid dynamics and underactuation, and its RL-based walking controller as an example. We show that a low-dimensional dynamical model is sufficient to capture the dynamics of the closed-loop system. We demonstrate that this model is linear, asymptotically stable, and is decoupled across control input in all dimensions. We further exemplify that such linearity exists even when using different RL control policies. Such results point out an interesting direction to understand the relationship between RL and optimal control: whether RL tends to linearize the nonlinear system during training in some cases. Furthermore, we illustrate that the found linear model is able to provide guarantees by safety-critical optimal control framework, e.g., Model Predictive Control with Control Barrier Functions, on an example of autonomous navigation using Cassie while taking advantage of the agility provided by the RL-based controller.
DA  - 2022///
PY  - 2022
SN  - 2330-7668
AN  - WOS:000827625700033
ER  - 

TY  - JOUR
TI  - Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems
AU  - Rausch, A
AU  - Sedeh, AM
AU  - Zhang, M
T2  - APPLIED SCIENCES-BASEL
AB  - Many autonomous systems, such as driverless taxis, perform safety-critical functions. Autonomous systems employ artificial intelligence (AI) techniques, specifically for environmental perception. Engineers cannot completely test or formally verify AI-based autonomous systems. The accuracy of AI-based systems depends on the quality of training data. Thus, novelty detection, that is, identifying data that differ in some respect from the data used for training, becomes a safety measure for system development and operation. In this study, we propose a new architecture for autoencoder-based semantic novelty detection with two innovations: architectural guidelines for a semantic autoencoder topology and a semantic error calculation as novelty criteria. We demonstrate that such a semantic novelty detection outperforms autoencoder-based novelty detection approaches known from the literature by minimizing false negatives.
DA  - 2021/11//undefined
PY  - 2021
DO  - 10.3390/app11219881
VL  - 11
IS  - 21
SN  - 2076-3417
AN  - WOS:000719108200001
KW  - MARKER
ER  - 

TY  - CONF
TI  - BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems
AU  - Chen, ZT
AU  - Li, GP
AU  - Pattabiraman, K
AU  - DeBardeleben, N
AU  - Assoc Comp Machinery
T2  - PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS
AB  - As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors.
In this work, we propose Biel, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3295500.3356177
SN  - 978-1-4503-6229-0
AN  - WOS:000545976800069
KW  - Machine learning
KW  - Learning systems
KW  - Errors
KW  - Critical machine
KW  - Software testing
KW  - Fault injection
KW  - Safety-critical domain
KW  - Radiation hardening
KW  - High performance computing
KW  - Catastrophic consequences
KW  - Error propagation
KW  - Error resilience
KW  - Monotonic functions
KW  - Printing machinery
ER  - 

TY  - CONF
TI  - Safe Policy Search Using Gaussian Process Models
AU  - Polymenakos, K
AU  - Abate, A
AU  - Roberts, S
AU  - Assoc Comp Machinery
T2  - AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS
AB  - We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.
DA  - 2019///
PY  - 2019
SP  - 1565
EP  - 1573
SN  - 978-1-4503-6309-9
AN  - WOS:000474345000180
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Safety engineering
KW  - Multi agent systems
KW  - Gaussian distribution
KW  - Gaussian noise (electronic)
KW  - Gaussian process models
KW  - Gaussian Processes
KW  - Safety critical systems
KW  - System Dynamics
KW  - Model-based reinforcement learning
KW  - Catastrophic failures
KW  - Analytic properties
KW  - Gaussian processes
KW  - State space constraints
ER  - 

TY  - CONF
TI  - Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of key Performance Areas
AU  - Treacy, S
T2  - PROCEEDINGS OF THE 3RD EUROPEAN CONFERENCE ON THE IMPACT OF ARTIFICIAL INTELLIGENCE AND ROBOTICS (ECIAIR 2021)
A2  - Matos, F
A2  - Salavisa, I
A2  - Serrao, C
AB  - The unpredictability of artificial intelligence (AI) services and products pose major ethical concerns for multinational companies as evidenced by the prevalence of unfair, biased, and discriminate AI systems. Examples including Amazon's recruiting tool, Facebook's biased ads, and racially biased healthcare risk algorithms have raised fundamental questions about what these systems should be used for, the inherent risks they possess, and how they can be mitigated. Unfortunately, these failures not only serve to highlight the lack of regulation in AI development, but it also reveals how organisations are struggling to alleviate the dangers associated with this technology. We argue that to successfully implement ethical AI applications, developers need a deeper understanding of not only the implications of misuse, but also a grounded approach in their conception. Judgement studies were therefore conducted with experts from data science backgrounds who identified six performance areas, resulting in a theoretical framework for the development of ethically aligned AI systems. This framework also reveals that these performance areas require specific mechanisms which must be acted upon to ensure that an AI system implements and meets ethical requirements throughout its lifecycle. The findings also outline several constraints which present challenges in the manifestation of these elements. By implementing this framework, organisations can contribute to an elevated trust between technology and people resulting in significant implications for both IS research and practice. This framework will further allow organisations to take a positive and proactive approach in ensuring they are best prepared for the ethical implications associated with the development, deployment and use of AI systems.
DA  - 2021///
PY  - 2021
DO  - 10.34190/EAIR.21.005
SP  - 183
EP  - 191
SN  - 978-1-914587-23-8
AN  - WOS:000838033200024
ER  - 

TY  - CONF
TI  - A Deep Safe Reinforcement Learning Approach for Mapless Navigation
AU  - Lv, SH
AU  - Li, YJ
AU  - Liu, Q
AU  - Gao, JQ
AU  - Pang, XZ
AU  - Chen, ML
AU  - IEEE
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE-ROBIO 2021)
AB  - Reinforcement learning (RL) is used more and more in robot navigation, however the safety of RL is usually not guaranteed. To improve the safety in the end-to-end mapless navigation using deep reinforcement learning (DRL), we propose a deep safe RL approach which uses a safe RL algorithm called Constrained Policy Optimization (CPO) and design the Actor-Critic-Safety (ACS) architecture to apply CPO. We use the Social Force Pedestrian Simulator based on social force model to simulate the dynamic environment with pedestrians in Gazebo. Experiment results show that the proposed approach can obviously increase the success rate and reduce the collision rate, which means the safety in navigation is improved. The planned path is almost as good as by ROS move base which needs to build a map of environment first. What's more, the model trained in static environment is able to generalize to unseen dynamic environment with pedestrians without any fine tuning and behaves well.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ROBIO54168.2021.9739251
SP  - 1520
EP  - 1525
SN  - 978-1-66540-535-5
AN  - WOS:000812286900248
KW  - Deep learning
KW  - Reinforcement learning
KW  - Robots
KW  - Reinforcement learnings
KW  - Dynamic environments
KW  - Air navigation
KW  - Reinforcement learning approach
KW  - Safe Reinforcement Learning
KW  - Constrained optimization
KW  - Safe reinforcement learning
KW  - Memory architecture
KW  - Policy optimization
KW  - Actor critic
KW  - ACS Architecture
KW  - Actor-critic-safety architecture
KW  - Dynamic environment with pedestrian
KW  - Dynamic Environment with Pedestrians
KW  - Mapless Navigation
KW  - Mapless navigations
KW  - Safety architecture
ER  - 

TY  - CONF
TI  - Auditing and Testing AI - A Holistic Framework
AU  - Becker, N
AU  - Waltl, B
T2  - DIGITAL HUMAN MODELING AND APPLICATIONS IN HEALTH, SAFETY, ERGONOMICS AND RISK MANAGEMENT: HEALTH, OPERATIONS MANAGEMENT, AND DESIGN, PT II
A2  - Duffy, VG
AB  - This paper describes a framework that can be used to assess and analyze AI systems in terms of risk. The framework addresses the structure and components of AI systems at five layers and allows taking a holistic view of AI systems while focusing on specific aspects, such as discrimination or data.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-06018-2_20
VL  - 13320
SP  - 283
EP  - 292
SN  - 0302-9743
AN  - WOS:000870272800020
ER  - 

TY  - JOUR
TI  - SAMBA: safe model-based & active reinforcement learning
AU  - Cowen-Rivers, AI
AU  - Palenicek, D
AU  - Moens, V
AU  - Abdullah, MA
AU  - Sootla, A
AU  - Wang, J
AU  - Bou-Ammar, H
T2  - MACHINE LEARNING
AB  - In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.
DA  - 2022/01//undefined
PY  - 2022
DO  - 10.1007/s10994-021-06103-6
VL  - 111
IS  - 1
SP  - 173
EP  - 203
SN  - 0885-6125
AN  - WOS:000738430100001
KW  - Reinforcement learning
KW  - Active Learning
KW  - Safe reinforcement learning
KW  - Dynamical systems
KW  - Model-based OPC
KW  - Gaussian distribution
KW  - Gaussian noise (electronic)
KW  - Gaussian process
KW  - Gaussian Processes
KW  - Probabilistic models
KW  - Function evaluation
KW  - Value engineering
KW  - Active explorations
KW  - Active learning
KW  - Information statistics
KW  - Model informations
KW  - Multi-objective problem
KW  - Process Evaluation
ER  - 

TY  - JOUR
TI  - Hard choices in artificial intelligence
AU  - Dobbe, R
AU  - Gilbert, TK
AU  - Mintz, Y
T2  - ARTIFICIAL INTELLIGENCE
AB  - As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured. (C) 2021 The Authors. Published by Elsevier B.V.
DA  - 2021/11//undefined
PY  - 2021
DO  - 10.1016/j.artint.2021.103555
VL  - 300
SN  - 0004-3702
AN  - WOS:000697026000010
ER  - 

TY  - CONF
TI  - Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction
AU  - Okawa, Y
AU  - Sasaki, T
AU  - Iwane, H
T2  - IFAC PAPERSONLINE
AB  - In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation. Copyright (C) 2020 The Authors.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.ifacol.2020.12.2198
VL  - 53
SP  - 1588
EP  - 1595
SN  - 2405-8963
AN  - WOS:000652592500257
KW  - Reinforcement learning
KW  - Decision making
KW  - Learning algorithm
KW  - Continuous state
KW  - Numerical methods
KW  - Action spaces
KW  - Safe exploration
KW  - Chance constraint
KW  - Safety-critical
KW  - Nominal models
KW  - Covariance matrix
KW  - Control inputs
KW  - Controlled objects
KW  - Exploration process
KW  - Variance-covariance matrices
ER  - 

TY  - CONF
TI  - Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques
AU  - Poenaru-Olaru, L
AU  - Sallou, J
AU  - Cruz, L
AU  - Rellermeyer, JS
AU  - van Deursen, A
AU  - IEEE
T2  - 2023 IEEE/ACM 7TH INTERNATIONAL WORKSHOP ON GREEN AND SUSTAINABLE SOFTWARE, GREENS
AB  - Deployed machine learning systems often suffer from accuracy degradation over time generated by constant data shifts, also known as concept drift. Therefore, these systems require regular maintenance, in which the machine learning model needs to be adapted to concept drift. The literature presents plenty of model adaptation techniques. The most common technique is periodically executing the whole training pipeline with all the data gathered until a particular point in time, yielding a massive energy footprint. In this paper, we propose a research path that uses concept drift detection and adaptation to enable sustainable AI systems.
DA  - 2023///
PY  - 2023
DO  - 10.1109/GREENS59328.2023.00009
SP  - 17
EP  - 18
SN  - 979-8-3503-1238-6
AN  - WOS:001041741400003
ER  - 

TY  - CONF
TI  - Reduce the Handicap: Performance Estimation for AI Systems Safety Certification
AU  - Pfrommer, J
AU  - Poyer, M
AU  - Kiroriwal, S
T2  - 2023 IEEE 21ST INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, INDIN
A2  - Dorksen, H
A2  - Scanzio, S
A2  - Jasperneite, J
A2  - Wisniewski, L
A2  - Man, KF
A2  - Sauter, T
A2  - Seno, L
A2  - Trsek, H
A2  - Vyatkin, V
AB  - The safety validation of AI and ML-based systems is challenging, as (i) analytical validation needs to include the interaction with a complex and stochastic physical environment and (ii) empirical validation needs to observe very long time-horizons to get enough "statistical signal" for the typically very low safety-related incident rate. This paper proposes an approach that amplifies the empirical evidence by introducing a handicap that reduces the system performance-making safety-related failures empirically more visible in a controlled environment-and gradually removing the handicap so that the convergence to the final incident rate can be estimated. Two numerical case studies are used to support and exemplify the approach.
DA  - 2023///
PY  - 2023
DO  - 10.1109/INDIN51400.2023.10218017
SN  - 1935-4576
AN  - WOS:001066089800056
ER  - 

TY  - CONF
TI  - SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI
AU  - Abella, J
AU  - Perez, J
AU  - Englund, C
AU  - Zonooz, B
AU  - Giordana, G
AU  - Donzella, C
AU  - Cazorla, FJ
AU  - Mezzetti, E
AU  - Serra, I
AU  - Brando, A
AU  - Agirre, I
AU  - Eizaguirre, F
AU  - Bui, TH
AU  - Arani, E
AU  - Sarfraz, F
AU  - Balasubramaniam, A
AU  - Badar, A
AU  - Bloise, I
AU  - Feruglio, L
AU  - Cinelli, I
AU  - Brighenti, D
AU  - Cunial, D
T2  - 2023 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION, DATE
AB  - Deep Learning (DL) techniques are at the heart of most future advanced software functions in Critical Autonomous AI-based Systems (CAIS), where they also represent a major competitive factor. Hence, the economic success of CAIS industries (e.g., automotive, space, railway) depends on their ability to design, implement, qualify, and certify DL-based software products under bounded effort/cost. However, there is a fundamental gap between Functional Safety (FUSA) requirements on CAIS and the nature of DL solutions. This gap stems from the development process of DL libraries and affects high-level safety concepts such as (1) explainability and traceability, (2) suitability for varying safety requirements, (3) FUSA-compliant implementations, and (4) real-time constraints. As a matter of fact, the data-dependent and stochastic nature of DL algorithms clashes with current FUSA practice, which instead builds on deterministic, verifiable, and pass/fail test-based software. The SAFEXPLAIN project tackles these challenges and targets by providing a flexible approach to allow the certification - hence adoption - of DL-based solutions in CAIS building on: (1) DL solutions that provide end-to-end traceability, with specific approaches to explain whether predictions can be trusted and strategies to reach (and prove) correct operation, in accordance to certification standards; (2) alternative and increasingly sophisticated design safety patterns for DL with varying criticality and fault tolerance requirements; (3) DL library implementations that adhere to safety requirements; and (4) computing platform configurations, to regain determinism, and probabilistic timing analyses, to handle the remaining non-determinism.
DA  - 2023///
PY  - 2023
SN  - 1530-1591
AN  - WOS:001027444200173
ER  - 

TY  - CONF
TI  - Architectural Patterns for Integrating AI Technology into Safety-Critical Systems
AU  - Dzambic, M
AU  - Dobaj, J
AU  - Seidl, M
AU  - Macher, G
AU  - ACM
T2  - PROCEEDINGS OF THE EUROPEAN CONFERENCE ON PATTERN LANGUAGES OF PROGRAMS 2021, EUROPLOP 2021
AB  - Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3489449.3490014
SN  - 978-1-4503-8997-6
AN  - WOS:000931946300036
ER  - 

TY  - CONF
TI  - Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception
AU  - Gross, J
AU  - Adler, R
AU  - Kläs, M
AU  - Reich, J
AU  - Jöckel, L
AU  - Gansch, R
T2  - COMPUTER SAFETY, RELIABILITY, AND SECURITY, SAFECOMP 2022
A2  - Trapp, M
A2  - Saglietti, F
A2  - Spislander, M
A2  - Bitsch, F
AB  - Data-driven models (DDM) based on machine learning and other AI techniques play an important role in the perception of increasingly autonomous systems. Due to the merely implicit definition of their behavior mainly based on the data used for training, DDM outputs are subject to uncertainty. This poses a challenge with respect to the realization of safety-critical perception tasks by means of DDMs. A promising approach to tackling this challenge is to estimate the uncertainty in the current situation during operation and adapt the system behavior accordingly. In previous work, we focused on runtime estimation of uncertainty and discussed approaches for handling uncertainty estimations. In this paper, we present additional architectural patterns for handling uncertainty. Furthermore, we evaluate the four patterns qualitatively and quantitatively with respect to safety and performance gains. For the quantitative evaluation, we consider a distance controller for vehicle platooning where performance gains are measured by considering how much the distance can be reduced in different operational situations. We conclude that the consideration of context information concerning the driving situation makes it possible to accept more or less uncertainty depending on the inherent risk of the situation, which results in performance gains.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-14835-4_19
VL  - 13414
SP  - 284
EP  - 297
SN  - 0302-9743
AN  - WOS:000871734000019
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-031-14835-4_19.pdf
KW  - Autonomous vehicles
KW  - Safety
KW  - Machine learning
KW  - Autonomous systems
KW  - Data handling
KW  - Uncertainty quantification
KW  - Machine-learning
KW  - Uncertainty
KW  - Safety engineering
KW  - Uncertainty analysis
KW  - Architecture
KW  - Autonomous system
KW  - Model-based OPC
KW  - On-machines
KW  - Data-driven model
KW  - Runtimes
KW  - Uncertainty quantifications
KW  - Architectural pattern
KW  - Architectural patterns
KW  - Performance Gain
ER  - 

TY  - CONF
TI  - Have a Break from Making Decisions, Have a MARS: The Multi-valued Action Reasoning System
AU  - Badea, C
T2  - ARTIFICIAL INTELLIGENCE XXXIX, AI 2022
A2  - Bramer, M
A2  - Stahl, F
AB  - The Multi-valued Action Reasoning System (MARS) is an automated value-based ethical decision-making model for agents in Artificial Intelligence (AI). Given a set of available actions and an underlying moral paradigm, by employing MARS one can identify the ethically preferred action. It can be used to implement and model different ethical theories, different moral paradigms, as well as combinations of such, in the context of automated practical reasoning and normative decision analysis. It can also be used to model moral dilemmas and discover the moral paradigms that result in the desired outcomes therein. In this paper we give a condensed description of MARS, explain its uses, and comparatively place it in the existing literature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-21441-7_31
VL  - 13652
SP  - 359
EP  - 366
SN  - 0302-9743
AN  - WOS:000922637500031
ER  - 

TY  - CONF
TI  - Deep Unsupervised Convolutional Domain Adaptation
AU  - Zhuo, JB
AU  - Wang, SH
AU  - Zhang, WG
AU  - Huang, QM
T2  - PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17)
AB  - In multimedia analysis, the task of domain adaptation is to adapt the feature representation learned in the source domain with rich label information to the target domain with less or even no label information. Significant research endeavors have been devoted to aligning the feature distributions between the source and the target domains in the top fully connected layers based on unsupervised DNN-based models. However, the domain adaptation has been arbitrarily constrained near the output ends of the DNN models, which thus brings about inadequate knowledge transfer in DNN-based domain adaptation process, especially near the input end. We develop an attention transfer process for convolutional domain adaptation. The domain discrepancy, measured in correlation alignment loss, is minimized on the second order correlation statistics of the attention maps for both source and target domains. Then we propose Deep Unsupervised Convolutional Domain Adaptation (DUCDA) method, which jointly minimizes the supervised classification loss of labeled source data and the unsupervised correlation alignment loss measured on both convolutional layers and fully connected layers. The multi-layer domain adaptation process collaborately reinforces each individual domain adaptation component, and significantly enhances the generalization ability of the CNN models. Extensive cross-domain object classification experiments show DUCDA outperforms other state-of-the-art approaches, and validate the promising power of DUCDA towards large scale real world application.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3123266.3123292
SP  - 261
EP  - 269
SN  - 978-1-4503-4906-2
AN  - WOS:000482109500031
KW  - Deep learning
KW  - Deep neural networks
KW  - Convolution
KW  - Feature representation
KW  - Alignment
KW  - Domain adaptation
KW  - Unsupervised domain adaptation
KW  - Knowledge management
KW  - Attention model
KW  - Generalization ability
KW  - Supervised classification
KW  - Correlation alignment
KW  - Correlation statistics
KW  - Object classification
KW  - State-of-the-art approach
ER  - 

TY  - JOUR
TI  - Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective
AU  - Jin, M
AU  - Lavaei, J
T2  - IEEE ACCESS
AB  - We investigate the important problem of certifying stability of reinforcement learning policies when interconnected with nonlinear dynamical systems. We show that by regulating the partial gradients of policies, strong guarantees of robust stability can be obtained based on a proposed semidefinite programming feasibility problem. The method is able to certify a large set of stabilizing controllers by exploiting problem-specific structures; furthermore, we analyze and establish its (non)conservatism. Empirical evaluations on two decentralized control tasks, namely multi-flight formation and power system frequency regulation, demonstrate that the reinforcement learning agents can have high performance within the stability-certified parameter space and also exhibit stable learning behaviors in the long run.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3045114
VL  - 8
SP  - 229086
EP  - 229100
SN  - 2169-3536
AN  - WOS:000616290100001
ER  - 

TY  - JOUR
TI  - Probabilistic Model Predictive Safety Certification for Learning-Based Control
AU  - Wabersich, KJ
AU  - Hewing, L
AU  - Carron, A
AU  - Zeilinger, MN
T2  - IEEE TRANSACTIONS ON AUTOMATIC CONTROL
AB  - Reinforcement learning (RL) methods have demonstrated their efficiency in simulation. However, many of the applications for which RL offers great potential, such as autonomous driving, are also safety critical and require a certified closed-loop behavior in order to meet the safety specifications in the presence of physical constraints. This article introduces a concept called probabilistic model predictive safety certification (PMPSC), which can be combined with any RL algorithm and provides provable safety certificates in terms of state and input chance constraints for potentially large-scale systems. The certificate is realized through a stochastic tube that safely connects the current system state with a terminal set of states that is known to be safe. A novel formulation allows a recursively feasible real-time computation of such probabilistic tubes, despite the presence of possibly unbounded disturbances. A design procedure for PMPSC relying on Bayesian inference and recent advances in probabilistic set invariance is presented. Using a numerical car simulation, the method and its design procedure are illustrated by enhancing an RL algorithm with safety certificates.
DA  - 2022/01//undefined
PY  - 2022
DO  - 10.1109/TAC.2021.3049335
VL  - 67
IS  - 1
SP  - 176
EP  - 188
SN  - 0018-9286
AN  - WOS:000735567400016
KW  - Reinforcement learning
KW  - Learning systems
KW  - safety
KW  - Bayesian networks
KW  - Safety engineering
KW  - Inference engines
KW  - Predictive control
KW  - Stochastic systems
KW  - Simulation environment
KW  - Reinforcement learning method
KW  - Numerical methods
KW  - reinforcement learning (RL)
KW  - Large scale systems
KW  - Safety certification
KW  - Closed-loop behavior
KW  - Physical constraints
KW  - Probabilistic modeling
KW  - Real-time computations
KW  - Safety specifications
KW  - stochastic systems
ER  - 

TY  - JOUR
TI  - ECCOLA - A method for implementing ethically aligned AI systems
AU  - Vakkuri, V
AU  - Kemell, KK
AU  - Jantunen, M
AU  - Halme, E
AU  - Abrahamsson, P
T2  - JOURNAL OF SYSTEMS AND SOFTWARE
AB  - Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners. (C) 2021 The Author(s). Published by Elsevier Inc.
DA  - 2021/12//undefined
PY  - 2021
DO  - 10.1016/j.jss.2021.111067
VL  - 182
SN  - 0164-1212
AN  - WOS:000704056400002
ER  - 

TY  - JOUR
TI  - eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics
AU  - Vaccari, I
AU  - Carlevaro, A
AU  - Narteni, S
AU  - Cambiaso, E
AU  - Mongelli, M
T2  - IEEE ACCESS
AB  - Machine learning (ML) algorithms are nowadays widely adopted in different contexts to perform autonomous decisions and predictions. Due to the high volume of data shared in the recent years, ML algorithms are more accurate and reliable since training and testing phases are more precise. An important concept to analyze when defining ML algorithms concerns adversarial machine learning attacks. These attacks aim to create manipulated datasets to mislead ML algorithm decisions. In this work, we propose new approaches able to detect and mitigate malicious adversarial machine learning attacks against a ML system. In particular, we investigate the Carlini-Wagner (CW), the fast gradient sign method (FGSM) and the Jacobian based saliency map (JSMA) attacks. The aim of this work is to exploit detection algorithms as countermeasures to these attacks. Initially, we performed some tests by using canonical ML algorithms with a hyperparameters optimization to improve metrics. Then, we adopt original reliable AI algorithms, either based on eXplainable AI (Logic Learning Machine) or Support Vector Data Description (SVDD). The obtained results show how the classical algorithms may fail to identify an adversarial attack, while the reliable AI methodologies are more prone to correctly detect a possible adversarial machine learning attack. The evaluation of the proposed methodology was carried out in terms of good balance between FPR and FNR on real world application datasets: Domain Name System (DNS) tunneling, Vehicle Platooning and Remaining Useful Life (RUL). In addition, a statistical analysis was performed to improve the robustness of the trained models, including evaluating their performance in terms of runtime and memory consumption.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3197299
VL  - 10
SP  - 83949
EP  - 83970
SN  - 2169-3536
AN  - WOS:000842742900001
ER  - 

TY  - CONF
TI  - On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods
AU  - Mindom, PSN
AU  - Nikanjam, A
AU  - Khomh, F
AU  - Mullins, J
AU  - IEEE COMP SOC
T2  - 2021 IEEE 21ST INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS 2021)
AB  - The increasing adoption of Reinforcement Learning in safety-critical systems domains such as autonomous vehicles, health, and aviation raises the need for ensuring their safety. Existing safety mechanisms such as adversarial training, adversarial detection, and robust learning are not always adapted to all disturbances in which the agent is deployed. Those disturbances include moving adversaries whose behavior can be unpredictable by the agent, and as a matter of fact harmful to its learning. Ensuring the safety of critical systems also requires methods that give formal guarantees on the behaviour of the agent evolving in a perturbed environment. It is therefore necessary to propose new solutions adapted to the learning challenges faced by the agent. In this paper, first we generate adversarial agents that exhibit flaws in the agent's policy by presenting moving adversaries. Secondly, We use reward shaping and a modified Q-learning algorithm as defense mechanisms to improve the agent's policy when facing adversarial perturbations. Finally, probabilistic model checking is employed to evaluate the effectiveness of both mechanisms. We have conducted experiments on a discrete grid world with a single agent facing non-learning and learning adversaries. Our results show a diminution in the number of collisions between the agent and the adversaries. Probabilistic model checking provides lower and upper probabilistic bounds regarding the agent's safety in the adversarial environment.
DA  - 2021///
PY  - 2021
DO  - 10.1109/QRS54544.2021.00037
SP  - 260
EP  - 269
SN  - 2693-9185
AN  - WOS:000814747000027
KW  - Reinforcement learning
KW  - Learning systems
KW  - Autonomous Vehicles
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Security systems
KW  - Reinforcement learning algorithms
KW  - Safety critical systems
KW  - Reinforcement Learning
KW  - Model checking
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Safety mechanisms
KW  - Formal specification
KW  - Facings
KW  - Formal Specification
KW  - Probabilistic Model Checking
KW  - Reward shaping
KW  - Reward Shaping
KW  - System domain
KW  - Vehicle health
ER  - 

TY  - JOUR
TI  - Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach
AU  - Umbrello, S
T2  - BIG DATA AND COGNITIVE COMPUTING
AB  - This paper argues that the Value Sensitive Design (VSD) methodology provides a principled approach to embedding common values into AI systems both early and throughout the design process. To do so, it draws on an important case study: the evidence and final report of the UK Select Committee on Artificial Intelligence. This empirical investigation shows that the different and often disparate stakeholder groups that are implicated in AI design and use share some common values that can be used to further strengthen design coordination efforts. VSD is shown to be both able to distill these common values as well as provide a framework for stakeholder coordination.
DA  - 2019/03//undefined
PY  - 2019
DO  - 10.3390/bdcc3010005
VL  - 3
IS  - 1
SN  - 2504-2289
AN  - WOS:000697668400005
ER  - 

TY  - JOUR
TI  - Towards functional safety compliance of matrix-matrix multiplication for machine learning-based autonomous systems
AU  - Fernández, J
AU  - Perez, J
AU  - Agirre, I
AU  - Allende, I
AU  - Abella, J
AU  - Cazorla, FJ
T2  - JOURNAL OF SYSTEMS ARCHITECTURE
AB  - Autonomous systems execute complex tasks to perceive the environment and take self-aware decisions with limited human interaction. This autonomy is commonly achieved with the support of machine learning algorithms. The nature of these algorithms, that need to process large data volumes, poses high-performance demands on the underlying hardware. As a result, the embedded critical real-time domain is adopting increasingly powerful processors that combine multi-core processors with accelerators such as GPUs. The resulting hardware and software complexity makes it difficult to demonstrate that the system will run safely and reliably. This is the main objective of functional safety standards, such as IEC 61508 or ISO 26262, that deal with the avoidance, detection and control of hardware or software errors. In this paper, we adopt those measures for the safe inference of machine learning libraries on multi-core devices, two topics that are not explicitly covered in the current version of standards. To this end, we adapt the matrix-matrix multiplication function, a central element of existing machine learning libraries, according to the recommendations of functional safety standards. The paper makes the following contributions: (i) adoption of recommended programming practices for the avoidance of programming errors in the matrix-matrix multiplication, (ii) inclusion of diagnostic mechanisms based on widely used checksums to control runtime errors, and (iii) evaluation of the impact of previous measures in terms of performance and a quantification of the achieved diagnostic coverage. For this purpose, we implement the diagnostic mechanisms on one of the ARM R5 cores of a Zynq UltraScale+ multi-processor system-on-chip and we then adapt them to an Intel i7 processor with native code employing vectorization for the sake of performance.
DA  - 2021/12//undefined
PY  - 2021
DO  - 10.1016/j.sysarc.2021.102298
VL  - 121
SN  - 1383-7621
AN  - WOS:000712050800002
KW  - Machine learning
KW  - Machine learning algorithms
KW  - Learning algorithms
KW  - Performance
KW  - Safety engineering
KW  - Functional Safety
KW  - Libraries
KW  - Functional safety
KW  - Matrix algebra
KW  - Safety standard
KW  - Error detection
KW  - Complex task
KW  - Humaninteraction
KW  - Large data volumes
KW  - Matrix-matrix multiplications
KW  - Program processors
KW  - Safety compliances
KW  - Self-aware
KW  - System-on-chip
ER  - 

TY  - JOUR
TI  - Safe Value Functions
AU  - Massiani, PF
AU  - Heim, S
AU  - Solowjow, F
AU  - Trimpe, S
T2  - IEEE TRANSACTIONS ON AUTOMATIC CONTROL
AB  - Safety constraints and optimality are important but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning (RL) to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces an SVF. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.
DA  - 2023/05//undefined
PY  - 2023
DO  - 10.1109/TAC.2022.3200948
VL  - 68
IS  - 5
SP  - 2743
EP  - 2757
SN  - 0018-9286
AN  - WOS:000979661300009
ER  - 

TY  - CONF
TI  - Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning
AU  - Ma, YJ
AU  - Shen, A
AU  - Bastani, O
AU  - Jayaraman, D
AU  - Assoc Advancement Artificial Intelligence
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
AB  - Reinforcement Learning (RL) agents in the real world must satisfy safety constraints in addition to maximizing a reward objective. Model-based RL algorithms hold promise for reducing unsafe real-world actions: they may synthesize policies that obey all constraints using simulated samples from a learned model. However, imperfect models can result in real-world constraint violations even for actions that are predicted to satisfy all constraints. We propose Conservative and Adaptive Penalty (CAP), a model-based safe RL framework that accounts for potential modeling errors by capturing model uncertainty and adaptively exploiting it to balance the reward and the cost objectives. First, CAP inflates predicted costs using an uncertainty-based penalty. Theoretically, we show that policies that satisfy this conservative cost constraint are guaranteed to also be feasible in the true environment. We further show that this guarantees the safety of all intermediate solutions during RL training. Further, CAP adaptively tunes this penalty during training using true cost feedback from the environment. We evaluate this conservative and adaptive penalty-based approach for model-based safe RL extensively on state and image-based environments. Our results demonstrate substantial gains in sample-efficiency while incurring fewer violations than prior safe RL algorithms.
DA  - 2022///
PY  - 2022
SP  - 5404
EP  - 5412
SN  - 2159-5399
AN  - WOS:000893636205058
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Real-world
KW  - Uncertainty analysis
KW  - Model-based OPC
KW  - Reinforcement learning algorithms
KW  - Safety constraint
KW  - Model-based reinforcement learning
KW  - Adaptive penalty
KW  - Imperfect modeling
KW  - Real world constraints
ER  - 

TY  - JOUR
TI  - Aligning individual and collective welfare in complex socio-technical systems by combining metaheuristics and reinforcement learning
AU  - Bazzan, ALC
T2  - ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
AB  - In complex socio-technical systems it is not easy to find a balance between the welfare state (i.e., a state where the overall performance of a system is optimal) and a situation in which individual components act selfishly to optimize their own utilities. This is even harder when individuals compete for scarce resources. In order to deal with this, some forms of biasing the optimization process have been proposed. However, mostly, such approaches only work for cooperative scenarios. When resources are scarce, the components of the system compete for them, thus approaches designed for cooperative systems are not necessarily appropriate. In the present paper an approach is proposed, which is based on a synergy between: (i) a global optimization process in which the system authority employs metaheuristics, and (ii) reinforcement learning processes that run at each component or agent. Both the agents and the system authority exchange solutions that are incorporated by the other party. The contributions of the proposed approach are twofold: a general scheme for such synergy is given and its benefits are shown in scenarios related to selfish routing, a typical load balance problem in a complex socio-technical system.
DA  - 2019/03//undefined
PY  - 2019
DO  - 10.1016/j.engappai.2018.12.003
VL  - 79
SP  - 23
EP  - 33
SN  - 0952-1976
AN  - WOS:000459524300003
KW  - Reinforcement learning
KW  - Multiagent systems
KW  - Sociotechnical systems
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Heuristic algorithms
KW  - Global optimization
KW  - Large scale systems
KW  - Complex systems
KW  - Load balance
KW  - Meta heuristics
KW  - Metaheuristics
KW  - Multiagent reinforcement learning
KW  - Route choice
KW  - Selfish routing
KW  - Socio-technical systems
ER  - 

TY  - CONF
TI  - Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks
AU  - Douthwaite, M
AU  - Kelly, T
T2  - 2017 IEEE 28TH INTERNATIONAL SYMPOSIUM ON SOFTWARE RELIABILITY ENGINEERING WORKSHOPS (ISSREW 2017)
AB  - The assurance of autonomous systems and the technologies that drive them is a major research challenge in the safety-critical systems engineering domain. The nature of many of these Machine Learning (ML) and Artificial Intelligence (AI) approaches raises a number of additional, technology-specific assurance concerns. One such approach is the Bayesian Network (BN) probabilistic modelling framework. Bayesian Networks and the family of modelling techniques they belong to form the basis of many AI applications. However, little research has been conducted into the assurance of BN-based systems for use in safety-critical applications. This paper explores some of the key distinctions between BN-based software-intensive systems and conventional software systems. It introduces a modelling framework that explicitly captures BN-based system-specific considerations and facilitates both the communication of assurance concerns between safety practitioners and system stakeholders, and the subsequent safety analysis of the system itself. It demonstrates how this approach can be used to develop specific verification and validation objectives for a BN-based system in a medical application.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ISSREW.2017.60
SP  - 302
EP  - 309
SN  - 2375-821X
AN  - WOS:000418465000065
ER  - 

TY  - JOUR
TI  - Addressing uncertainty in the safety assurance of machine-learning
AU  - Burton, S
AU  - Herd, B
T2  - FRONTIERS IN COMPUTER SCIENCE
AB  - There is increasing interest in the application of machine learning (ML) technologies to safety-critical cyber-physical systems, with the promise of increased levels of autonomy due to their potential for solving complex perception and planning tasks. However, demonstrating the safety of ML is seen as one of the most challenging hurdles to their widespread deployment for such applications. In this paper we explore the factors which make the safety assurance of ML such a challenging task. In particular we address the impact of uncertainty on the confidence in ML safety assurance arguments. We show how this uncertainty is related to complexity in the ML models as well as the inherent complexity of the tasks that they are designed to implement. Based on definitions of uncertainty as well as an exemplary assurance argument structure, we examine typical weaknesses in the argument and how these can be addressed. The analysis combines an understanding of causes of insufficiencies in ML models with a systematic analysis of the types of asserted context, asserted evidence and asserted inference within the assurance argument. This leads to a systematic identification of requirements on the assurance argument structure as well as supporting evidence. We conclude that a combination of qualitative arguments combined with quantitative evidence are required to build a robust argument for safety-related properties of ML functions that is continuously refined to reduce residual and emerging uncertainties in the arguments after the function has been deployed into the target environment.
DA  - 2023/04/06/
PY  - 2023
DO  - 10.3389/fcomp.2023.1132580
VL  - 5
SN  - 2624-9898
AN  - WOS:000971580900001
L1  - https://www.frontiersin.org/articles/10.3389/fcomp.2023.1132580/pdf?isPublishedV2=False
KW  - machine learning
KW  - uncertainty
KW  - safety
KW  - complexity
KW  - cyber-physical systems
KW  - assurance arguments
ER  - 

TY  - JOUR
TI  - MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation
AU  - Carlucci, FM
AU  - Porzi, L
AU  - Caputo, B
AU  - Ricci, E
AU  - Bulo, SR
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
AB  - One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach.
DA  - 2021/12/01/
PY  - 2021
DO  - 10.1109/TPAMI.2020.3001338
VL  - 43
IS  - 12
SP  - 4441
EP  - 4452
SN  - 0162-8828
AN  - WOS:000714203900021
ER  - 

TY  - JOUR
TI  - Interpretable semi-parametric regression models with defined error bounds
AU  - Otte, C
T2  - NEUROCOMPUTING
AB  - Unreliable extrapolation of data-driven models hinders their applicability not only in safety-related domains. The paper discusses how model interpretability and uncertainty estimates can address this problem. A new semi-parametric approach is proposed for providing an interpretable model with improved accuracy by combining a symbolic regression model with a residual Gaussian Process. While the learned symbolic model is highly interpretable the residual model usually is not. However, by limiting the output of the residual model to a defined range a worst-case guarantee can be given in the sense that the maximal deviation from the symbolic model is always below a defined limit. The limitation of the residual model can include the uncertainty estimate of the Gaussian Process, thus giving the residual model more impact in high-confidence regions. When ranking the accuracy and interpretability of several different approaches on the SARCOS data benchmark the proposed combination yields the best result
DA  - 2014/11/02/
PY  - 2014
DO  - 10.1016/j.neucom.2013.11.042
VL  - 143
SP  - 1
EP  - 6
SN  - 0925-2312
AN  - WOS:000340982800001
KW  - Interpretability
KW  - model interpretability
KW  - Artificial intelligence
KW  - Mathematical models
KW  - article
KW  - Regression analysis
KW  - priority journal
KW  - statistical model
KW  - Uncertainty analysis
KW  - data processing
KW  - intermethod comparison
KW  - mathematical computing
KW  - regression analysis
KW  - Safety-related systems
KW  - kernel method
KW  - mathematical parameters
KW  - Error analysis
KW  - Uncertainty estimates
KW  - Gaussian distribution
KW  - Gaussian noise (electronic)
KW  - Gaussian process
KW  - Gaussian Processes
KW  - Data-driven model
KW  - Symbolic regression
KW  - analytic method
KW  - Interpretable models
KW  - Machine learning for safety-related systems
KW  - Maximal deviation
KW  - measurement error
KW  - model accuracy
KW  - Semi-parametric regressions
KW  - semiparametric regression model
ER  - 

TY  - JOUR
TI  - A Deep Reinforcement Learning Framework with Formal Verification
AU  - Boudi, Z
AU  - Wakrime, AA
AU  - Toub, M
AU  - Haloua, M
T2  - FORMAL ASPECTS OF COMPUTING
AB  - Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.
DA  - 2023/03//undefined
PY  - 2023
DO  - 10.1145/3577204
VL  - 35
IS  - 1
SN  - 0934-5043
AN  - WOS:000950940200005
ER  - 

TY  - JOUR
TI  - Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping
AU  - Zhang, YX
AU  - Liang, XL
AU  - Li, DY
AU  - Ge, SZS
AU  - Gao, BZ
AU  - Chen, H
AU  - Lee, TH
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
AB  - Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton-Jacobi-Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.
DA  - 2022/07/12/
PY  - 2022
DO  - 10.1109/TNNLS.2022.3186528
SN  - 2162-237X
AN  - WOS:000826063500001
KW  - Reinforcement learning
KW  - Autonomous vehicles
KW  - Safety
KW  - autonomous vehicles
KW  - Autonomous Vehicles
KW  - Reinforcement learnings
KW  - Uncertainty
KW  - Adaptive control systems
KW  - Safety engineering
KW  - Iterative methods
KW  - Nonlinear systems
KW  - Safe reinforcement learning
KW  - Lyapunov methods
KW  - Lyapunov's methods
KW  - Adaptive dynamic programming
KW  - Dynamic programming
KW  - Control systems
KW  - Lyapunov functions
KW  - Continuous time systems
KW  - Backstepping
KW  - safe reinforcement learning (SRL)
KW  - Lyapunov's functions
KW  - Adaptive dynamic programming (ADP)
KW  - Barrier lyapunov function
KW  - Barrier Lyapunov Function (BLF)
ER  - 

TY  - JOUR
TI  - On conflicts between ethical and logical principles in artificial intelligence
AU  - D'Acquisto, G
T2  - AI & SOCIETY
AB  - Artificial intelligence is nowadays a reality. Setting rules on the potential outcomes of intelligent machines, so that no surprise can be expected by humans from the behavior of those machines, is becoming a priority for policy makers. In its recent Communication "Artificial Intelligence for Europe" (EU Commission2018), for instance, the European Commission identifies the distinguishing trait of an intelligent machine in the presence of "a certain degree of autonomy" in decision making, in the light of the context. The crucial issue to be addressed is, therefore, whether it is possible to identify a set of rules for data use by intelligent machines so that the decision-making autonomy of machines can allow for humans' traditional informational self-determination (humans provide machines only with the data they decide to), as enshrined in many existing legal frameworks (including, for personal data protection, the EU's General Data Protection Regulation) (EU Parliament and Council2016) and can actually turn out to be further beneficial to individuals. Governing the autonomy of machines can be a very ambitious goal for humans since machines are geared first to the principles of formal logic and then-possibly-to ethical or legal principles. This introduces an unprecedented degree of complexity in how a norm should be engineered, which requires, in turn, an in-depth reflection in order to prevent conflicts between the legal and ethical principles underlying humans' civil coexistence and the rules of formal logic upon which the functioning of machines is based (EU Parliament2017).
DA  - 2020/12//undefined
PY  - 2020
DO  - 10.1007/s00146-019-00927-6
VL  - 35
IS  - 4
SP  - 895
EP  - 900
SN  - 0951-5666
AN  - WOS:000548491100001
ER  - 

TY  - CONF
TI  - A study on real-time artificial intelligence
AU  - Tay, EB
AU  - Gan, OP
AU  - Ho, WK
T2  - ARTIFICIAL INTELLIGENCE IN REAL-TIME CONTROL 1997
A2  - Rauch, HE
AB  - This paper highlights some of the important considerations when applying artificial intelligence techniques in real-time applications. There is a specific focus on real-time expert systems. The issues of reliability and safety in real-time control systems are also addressed. Important considerations like scheduling, real-time operating systems are also highlighted in designing real-time systems. Copyright (C) 1998 IFAC.
DA  - 1998///
PY  - 1998
SP  - 109
EP  - 114
SN  - 0962-9505
AN  - WOS:000077333600018
ER  - 

TY  - JOUR
TI  - SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness
AU  - Kocak, MA
AU  - Ramirez, D
AU  - Erkip, E
AU  - Shasha, DE
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
AB  - SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, 1 - epsilon, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed epsilon. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate epsilon, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415.
DA  - 2021/02/01/
PY  - 2021
DO  - 10.1109/TPAMI.2019.2932415
VL  - 43
IS  - 2
SP  - 663
EP  - 678
SN  - 0162-8828
AN  - WOS:000607383300019
KW  - machine learning
KW  - Machine learning
KW  - Prediction algorithms
KW  - State of the art
KW  - article
KW  - software
KW  - Forecasting
KW  - Errors
KW  - Meta-algorithms
KW  - Correctness rates
KW  - Data distribution
KW  - Digital libraries
KW  - Finite number
KW  - Online data
KW  - Target error rate
ER  - 

TY  - CONF
TI  - Morality, Machines, and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents
AU  - Badea, C
AU  - Artus, G
T2  - ARTIFICIAL INTELLIGENCE XXXIX, AI 2022
A2  - Bramer, M
A2  - Stahl, F
AB  - We present what we call the Interpretation Problem, whereby any rule in symbolic form is open to infinite interpretation in ways thatwemight disapprove of and argue that any attempt to build morality into machines is subject to it. We show how the Interpretation Problem in Artificial Intelligence is an illustration of Wittgenstein's general claim that no rule can contain the criteria for its own application, and that the risks created by this problem escalates in proportion to the degree to which a machine is causally connected to the world, in what we call the Law of Interpretative Exposure. Using games as an illustration, we attempt to define the structure of normative spaces and argue that any rule-following within a normative space is guided by values that are external to that space and which cannot themselves be represented as rules. In light of this, we categorise the types of mistakes an artificial moral agent could make into Mistakes of Intention and Instrumental Mistakes, and we proposeways of building morality into machines by getting them to interpret the rules we give in accordance with these external values, through explicit moral reasoning, the "Show, not Tell" paradigm, the adjustment of causal power and structure of the agent, and relational values, with the ultimate aim that the machine develop a virtuous character and that the impact of the Interpretation Problem is minimised.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-21441-7_9
VL  - 13652
SP  - 124
EP  - 137
SN  - 0302-9743
AN  - WOS:000922637500009
ER  - 

TY  - CONF
TI  - Automating Safety Argument Change Impact Analysis for Machine Learning Components
AU  - Carlan, C
AU  - Gauerhof, L
AU  - Gallina, B
AU  - Burton, S
AU  - IEEE
T2  - 2022 IEEE 27TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING (PRDC)
AB  - The need to make sense of complex input data within a vast variety of unpredictable scenarios has been a key driver for the use of machine learning (ML), for example in Automated Driving Systems (ADS). Such systems are usually safety-critical, and therefore they need to be safety assured. In order to consider the results of the safety assurance activities (scoping uncovering previously unknown hazardous scenarios), a continuous approach to arguing safety is required, whilst iteratively improving ML-specific safety-relevant properties, such as robustness and prediction certainty. Such a continuous safety life cycle will only be practical with an efficient and effective approach to analyzing the impact of system changes on the safety case. In this paper, we propose a semi-automated approach for accurately identifying the impact of changes on safety arguments. We focus on arguments that reason about the sufficiency of the data used for the development of ML components. The approach qualitatively and quantitatively analyses the impact of changes in the input space of the considered ML component on other artifacts created during the execution of the safety life cycle, such as datasets and performance requirements and makes recommendations to safety engineers for handling the identified impact. We implement the proposed approach in a model-based safety engineering environment called FASTEN, and we demonstrate its application for an ML-based pedestrian detection component of an ADS.
DA  - 2022///
PY  - 2022
DO  - 10.1109/PRDC55274.2022.00019
SP  - 43
EP  - 53
SN  - 1555-094X
AN  - WOS:000965064800005
KW  - Machine learning
KW  - Learning systems
KW  - Automated driving systems
KW  - Machine-learning
KW  - Pedestrian safety
KW  - Machine components
KW  - Operational design
KW  - Operational design domain
KW  - Change impact analyse
KW  - Change impact analysis
KW  - Change Impact Analysis (CIA)
KW  - Design domains
KW  - Life cycle
KW  - Machine Learning (ML)
KW  - Operational Design Domain (ODD)
KW  - Safety arguments
KW  - Safety case
KW  - Safety Cases
ER  - 

TY  - CONF
TI  - Parallel reward and punishment control in humans and robots: safe reinforcement learning using the MaxPain algorithm
AU  - Elfwing, S
AU  - Seymour, B
T2  - 2017 THE SEVENTH JOINT IEEE INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING AND EPIGENETIC ROBOTICS (ICDL-EPIROB)
AB  - An important issue in reinforcement learning systems for autonomous agents is whether it makes sense to have separate systems for predicting rewards and punishments. In robotics, learning and control are typically achieved by a single controller, with punishments coded as negative rewards. However in biological systems, some evidence suggests that the brain has a separate system for punishment. Although this may in part be due to biological constraints of implementing negative quantities, it raises the question as to whether there is any computational rationale for keeping reward and punishment prediction operationally distinct. Here we outline a basic argument supporting this idea, based on the proposition that learning best-case predictions (as in Q-learning) does not always achieve the safest behaviour. We introduce a modified RL scheme involving a new algorithm which we call 'MaxPain' - which back-ups worst-case predictions in parallel, and then scales the two predictions in a multi-attribute RL policy. i.e. independently learning 'what to do' as well as 'what not to do' and then combining this information. We show how this scheme can improve performance in benchmark RL environments, including a grid-world experiment and a delayed version of the mountain car experiment. In particular, we demonstrate how early exploration and learning are substantially improved, leading to much 'safer' behaviour. In conclusion, the results illustrate the importance of independent punishment prediction in RL, and provide a testable framework for better understanding punishment (such as pain) and avoidance in humans, in both health and disease.
DA  - 2017///
PY  - 2017
SP  - 140
EP  - 147
SN  - 2161-9484
AN  - WOS:000491967600019
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Forecasting
KW  - Benchmarking
KW  - Q-learning
KW  - Improve performance
KW  - Reinforcement learning systems
KW  - Robotic controls
KW  - Biological constraints
KW  - Multi-attributes
KW  - Robotic learning
KW  - Single controllers
ER  - 

TY  - JOUR
TI  - A "Do No Harm" Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework
AU  - Khan, WU
AU  - Seto, E
T2  - JOURNAL OF MEDICAL INTERNET RESEARCH
AB  - Given the impact artificial intelligence (AI)-based medical technologies (hardware devices, software programs, and mobile apps) can have on society, debates regarding the principles behind their development and deployment are emerging. Using the biopsychosocial model applied in psychiatry and other fields of medicine as our foundation, we propose a novel 3-step framework to guide industry developers of AI-based medical tools as well as health care regulatory agencies on how to decide if a product should be launched-a "Go or No-Go" approach. More specifically, our novel framework places stakeholders' (patients, health care professionals, industry, and government institutions) safety at its core by asking developers to demonstrate the biological-psychological (impact on physical and mental health), economic, and social value of their AI tool before it is launched. We also introduce a novel cost-effective, time-sensitive, and safety-oriented mixed quantitative and qualitative clinical phased trial approach to help industry and government health care regulatory agencies test and deliberate on whether to launch these AI-based medical technologies. To our knowledge, our biological-psychological, economic, and social (BPES) framework and mixed method phased trial approach are the first to place the Hippocratic Oath of "Do No Harm" at the center of developers', implementers', regulators', and users' mindsets when determining whether an AI-based medical technology is safe to launch. Moreover, as the welfare of AI users and developers becomes a greater concern, our framework's novel safety feature will allow it to complement existing and future AI reporting guidelines.
DA  - 2023/04/05/
PY  - 2023
DO  - 10.2196/43386
VL  - 25
SN  - 1438-8871
AN  - WOS:001007075800001
ER  - 

TY  - JOUR
TI  - Safety Monitoring of Neural Networks Using Unsupervised Feature Learning and Novelty Estimation
AU  - Ranjbar, A
AU  - Hornauer, S
AU  - Fredriksson, J
AU  - Yu, SX
AU  - Chan, CY
T2  - IEEE TRANSACTIONS ON INTELLIGENT VEHICLES
AB  - Neural networks are currently suggested to be implemented in several different driving functions of autonomous vehicles. While showing promising results the drawback lies in the difficulty of safety verification and ensuring operation as intended. The aim of this paper is to increase safety when using neural networks, by proposing a monitoring framework based on novelty estimation of incoming driving data. The idea is to use unsupervised instance discrimination to learn a similarity measure across ego-vehicle camera images. By estimating a von Mises-Fisher distribution of expected ego-camera images they can be compared with unexpected novel images. A novelty measurement is inferred through the likelihood of test frames belonging to the expected distribution. The suggested method provides competitive results to several other novelty or anomaly detection algorithms on the CIFAR-10 and CIFAR-100 datasets. It also shows promising results on real world driving scenarios by distinguishing novel driving scenes from the training data of BDD100 k. Applied on the identical training-test data split, the method is also able to predict the performance profile of a segmentation network. Finally, examples are provided on how this method can be extended to find novel segments in images.
DA  - 2022/09//undefined
PY  - 2022
DO  - 10.1109/TIV.2022.3152084
VL  - 7
IS  - 3
SP  - 711
EP  - 721
SN  - 2379-8858
AN  - WOS:000873905600027
KW  - machine learning
KW  - Autonomous vehicles
KW  - Machine learning
KW  - Training data
KW  - Anomaly detection
KW  - Autonomous Vehicles
KW  - Machine-learning
KW  - Cameras
KW  - Image segmentation
KW  - Images segmentations
KW  - Neural-networks
KW  - Monitoring
KW  - monitoring
KW  - Safety monitoring
KW  - Camera images
KW  - Driving functions
KW  - safety systems
KW  - Unsupervised feature learning
ER  - 

TY  - CONF
TI  - Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks
AU  - Sallami, MM
AU  - Ibn Khedher, M
AU  - Trabelsi, A
AU  - Kerboua-Benlarbi, S
AU  - Bettebghor, D
T2  - NEURAL INFORMATION PROCESSING (ICONIP 2019), PT IV
A2  - Gedeon, T
A2  - Wong, KW
A2  - Lee, M
AB  - Embedding machine or deep learning software into safety-critical systems such as autonomous vehicles requires software verification and validation. Such software adds non traceable hazards to traditional hardware and sensors failures, not to mention attacks that fool the prediction of a DNN and hampers its robustness. Formal methods from computer science are now applied to deep neural networks to assess the local and global robustness of a given DNN. Typically static analysis with Abstract Interpretation or SAT solvers approaches are applied to neural networks and leverages the important progress of formal methods over the last decades. Such approaches estimate bounds on the perturbation of the inputs and formally guarantee the same DNN prediction within these bounds. However formal methods over DNN for image perception system have only been applied to simple image attacks (2D rotation, brightness). In this work, we extend the definition of Lower and Upper Bounds to assess the robustness of a DNN perception system against more generic attacks. We propose a general method to verify object recognition systems using Abstract Interpretation theory. Another major contribution is the adaptation of Upper and Lower Bounds with the abstract intervals to support more complex attacks. We consider the three following classes: convolutional attacks, occlusion attacks and geometrical transformations. For the last one, we generalize the geometrical transformations with displacements in the three-dimensional space.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-36808-1_30
VL  - 1142
SP  - 274
EP  - 286
SN  - 1865-0929
AN  - WOS:000651201400030
ER  - 

TY  - JOUR
TI  - Reflection machines: increasing meaningful human control over Decision Support Systems
AU  - Cornelissen, NAJ
AU  - van Eerdt, RJM
AU  - Schraffenberger, HK
AU  - Haselager, WFG
T2  - ETHICS AND INFORMATION TECHNOLOGY
AB  - Rapid developments in Artificial Intelligence are leading to an increasing human reliance on machine decision making. Even in collaborative efforts with Decision Support Systems (DSSs), where a human expert is expected to make the final decisions, it can be hard to keep the expert actively involved throughout the decision process. DSSs suggest their own solutions and thus invite passive decision making. To keep humans actively 'on' the decision-making loop and counter overreliance on machines, we propose a 'reflection machine' (RM). This system asks users questions about their decision strategy and thereby prompts them to evaluate their own decisions critically. We discuss what forms RMs can take and present a proof-of-concept implementation of a RM that can produce feedback on users' decisions in the medical and law domains. We show that the prototype requires very little domain knowledge to create reasonably intelligent critiquing questions. With this prototype, we demonstrate the technical feasibility to develop RMs and hope to pave the way for future research into their effectiveness and value.
DA  - 2022/06//undefined
PY  - 2022
DO  - 10.1007/s10676-022-09645-y
VL  - 24
IS  - 2
SN  - 1388-1957
AN  - WOS:000781340300001
ER  - 

TY  - JOUR
TI  - Developing safer AI-concepts from economics to the rescue
AU  - Maskara, PK
T2  - AI & SOCIETY
AB  - With the rapid advancement of AI, there exists a possibility of rogue human actor(s) taking control of a potent AI system or an AI system redefining its objective function such that it presents an existential threat to mankind or severely curtails its freedom. Therefore, some suggest an outright ban on AI development while others profess international agreement on constraining specific types of AI. These approaches are untenable because countries will continue developing AI for national defense, regardless. Some suggest having an all-powerful benevolent one-AI that will act as an AI nanny. However, such an approach relies on the everlasting benevolence of one-AI, an untenable proposition. Furthermore, such an AI is itself subject to capture by a rogue actor. We present an alternative approach that uses existing mechanisms and time-tested economic concepts of competition and marginal analysis to limit centralization and integration of AI, rather than AI itself. Instead of depending on international consensus it relies on countries working in their best interests. We recommend that through regulation and subsidies countries promote independent development of competing AI technologies, especially those with decentralized architecture. The Sherman Antitrust Act can be used to limit the domain of an AI system, training module, or any of its components. This will increase the segmentation of potent AI systems and force technological incompatibility across systems. Finally, cross-border communication between AI-enabled systems should be restricted, something countries like China and the US are already inclined to do to serve their national interests. Our approach can ensure the availability of numerous sufficiently powerful AI systems largely disconnected from each other that can be called upon to identify and neutralize rogue systems when needed. This setup can provide sufficient deterrence to any rational human or AI system from attempting to exert undue control.
DA  - 2023/10/02/
PY  - 2023
DO  - 10.1007/s00146-023-01778
SN  - 0951-5666
AN  - WOS:001076383400001
ER  - 

TY  - CONF
TI  - Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics
AU  - Terra, A
AU  - Riaz, H
AU  - Raizer, K
AU  - Hata, A
AU  - Inam, R
T2  - 2020 6TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND ROBOTICS (ICCAR)
AB  - The use of AI-based risk mitigation is increasing to provide safety in the areas of smart manufacturing, automated logistics etc, where the human-robot collaboration operations are in use. This paper presents our work on implementation of fuzzy logic system (FLS) and reinforcement learning (RL) to build risk mitigation modules for human-robot collaboration scenarios. Risk mitigation using FLS strategy is developed by manually defining the linguistic values, tuning the membership functions and generating the rules based on ISO/TS15066:2016. RL-based risk mitigation modules are developed using three different Qnetworks to estimate the Q-value function. Our purpose is twofold: to perform a comparative analysis of FLS and RL in terms of safety perspectives and further to evaluate the efficiency to accomplish the task. Our results present that all the proposed risk mitigation strategies improve the safety aspect by up to 26% as compared to a default setup where the robot is just relying on a navigation module without risk mitigation. The efficiency of using FLS model is maintained to the default setup, while the efficiency of using RL model is reduced by 26% from the default setup. We also compare the computation performance of risk mitigation between centralized and edge execution where the edge execution is 27.5 times faster than the centralized one.
DA  - 2020///
PY  - 2020
DO  - 10.1109/iccar49639.2020.9108037
SP  - 151
EP  - 160
SN  - 2251-2446
AN  - WOS:000591176900027
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - Social robots
KW  - Robotics
KW  - Risk perception
KW  - Efficiency
KW  - Safety engineering
KW  - Fuzzy logic
KW  - Human-robot collaboration
KW  - Membership functions
KW  - Agricultural robots
KW  - smart manufacturing
KW  - Smart manufacturing
KW  - Fuzzy logic system
KW  - Comparative analysis
KW  - automated warehouse
KW  - Computation performance
KW  - fuzzy logic system
KW  - human-robot collaboration
KW  - ISO/TS15066:2016
KW  - Linguistic values
KW  - risk mitigation
KW  - Risk mitigation
KW  - Risk mitigation strategy
KW  - safety analysis
ER  - 

TY  - CONF
TI  - Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-driven Engineering Systems
AU  - Kshetry, N
AU  - Varshney, LR
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)
AB  - Most current machine learning algorithms make highly confident yet incorrect classifications when faced with unexpected test samples from an unknown distribution different from training; such epistemic uncertainty (unknown unknowns) can have catastrophic safety implications. In this conceptual paper, we propose a method to leverage engineering science knowledge to control epistemic uncertainty and maintain decision safety. The basic idea is an algorithm fusion approach that combines data-driven learned models with physical system knowledge, to operate between the extremes of purely data-driven classifiers and purely engineering science rules. This facilitates the safe operation of data-driven engineering systems, such as wastewater treatment plants.
DA  - 2019///
PY  - 2019
SP  - 8162
EP  - 8166
SN  - 1520-6149
AN  - WOS:000482554008080
ER  - 

TY  - CONF
TI  - Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior
AU  - Boggust, A
AU  - Hoover, B
AU  - Satyanarayan, A
AU  - Strobelt, H
T2  - PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22)
AB  - Saliency methods-techniques to identify the importance of input features on a model's output-are a common step in understanding neural network behavior. However, interpreting saliency requires tedious manual inspection to identify and aggregate patterns in model behavior, resulting in ad hoc or cherry-picked analysis. To address these concerns, we present Shared Interest: metrics for comparing model reasoning (via saliency) to human reasoning (via ground truth annotations). By providing quantitative descriptors, Shared Interest enables ranking, sorting, and aggregating inputs, thereby facilitating large-scale systematic analysis of model behavior. We use Shared Interest to identify eight recurring patterns in model behavior, such as cases where contextual features or a subset of ground truth features are most important to the model. Working with representative real-world users, we show how Shared Interest can be used to decide if a model is trustworthy, uncover issues missed in manual analyses, and enable interactive probing.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3491102.3501965
SN  - 978-1-4503-9157-3
AN  - WOS:000890212502034
ER  - 

TY  - JOUR
TI  - A Maturity Model for Trustworthy AI Software Development
AU  - Cho, S
AU  - Kim, I
AU  - Kim, J
AU  - Woo, H
AU  - Shin, W
T2  - APPLIED SCIENCES-BASEL
AB  - Recently, AI software has been rapidly growing and is widely used in various industrial domains, such as finance, medicine, robotics, and autonomous driving. Unlike traditional software, in which developers need to define and implement specific functions and rules according to requirements, AI software learns these requirements by collecting and training relevant data. For this reason, if unintended biases exist in the training data, AI software can create fairness and safety issues. To address this challenge, we propose a maturity model for ensuring trustworthy and reliable AI software, known as AI-MM, by considering common AI processes and fairness-specific processes within a traditional maturity model, SPICE (ISO/IEC 15504). To verify the effectiveness of AI-MM, we applied this model to 13 real-world AI projects and provide a statistical assessment on them. The results show that AI-MM not only effectively measures the maturity levels of AI projects but also provides practical guidelines for enhancing maturity levels.
DA  - 2023/04//undefined
PY  - 2023
DO  - 10.3390/app13084771
VL  - 13
IS  - 8
SN  - 2076-3417
AN  - WOS:000977808400001
ER  - 

TY  - JOUR
TI  - Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning
AU  - Yuan, YM
AU  - Li, YH
AU  - Zhu, ZL
AU  - Li, RX
AU  - Gu, XW
T2  - IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
AB  - Domain adaptation aims to improve the performance of the classifier in the target domain by reducing the difference between the two domains. Domain shifts usually exist in both marginal distribution and conditional distribution, and their relative importance varies with datasets. Moreover, there is an influence between marginal distribution distance and conditional distribution distance. However, joint domain adaptation approaches rarely consider those. Existing dynamic distribution alignment methods require a feature discriminator, and they need to train a subdomain discriminator for each class. Besides, they don't think about the interaction between the two distribution distances. In this article, we propose a dynamic joint domain adaptation approach, namely Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning (ADPL), to deal with the above problems. Both marginal distribution alignment and conditional distribution alignment can be implemented by adversarial learning. The dynamic algorithm can keep a balance between marginal and conditional distribution alignment with only two domain discriminators. In addition, the dynamic algorithm takes the influence between the two distribution distances into consideration. Compared with several advanced domain adaptation methods on both text and image datasets, all classification experiments and extensive comparison experiments demonstrate that ADPL has higher learning performance of classification and less running time. This reveals that ADPL outperforms the state-of-the-art domain adaptation approaches.
DA  - 2021/08//undefined
PY  - 2021
DO  - 10.1109/TETCI.2021.3055873
VL  - 5
IS  - 4
SP  - 714
EP  - 723
SN  - 2471-285X
AN  - WOS:000677536500017
ER  - 

TY  - CONF
TI  - A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA
AU  - Antikainen, J
AU  - Agbese, M
AU  - Alanen, HK
AU  - Halme, E
AU  - Isomäki, H
AU  - Jantunen, M
AU  - Kemell, KK
AU  - Rousi, R
AU  - Vainio-Pekka, H
AU  - Vakkuri, V
T2  - 29TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE WORKSHOPS (REW 2021)
A2  - Yue, T
A2  - Mirakhorli, M
AB  - There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOIA method for creating ethically aligned AI -systems. ECCOIA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOIA with a deployment model to drive the adoption of ECCOIA, as any method - no matter how good -is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given life-cycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition
DA  - 2021///
PY  - 2021
DO  - 10.1109/REW53955.2021.00043
SP  - 230
EP  - 235
SN  - 978-1-66541-898-0
AN  - WOS:000788547300034
ER  - 

TY  - CONF
TI  - Risk-averse Distributional Reinforcement Learning: A CVaR Optimization Approach
AU  - Stanko, S
AU  - Macek, K
T2  - IJCCI: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE
A2  - Merelo, JJ
A2  - Garibaldi, J
A2  - Barranco, AL
A2  - Madani, K
A2  - Warwick, K
AB  - Conditional Value-at-Risk (CVaR) is a well-known measure of risk that has been directly equated to robustness, an important component of Artificial Intelligence (AI) safety. In this paper we focus on optimizing CVaR in the context of Reinforcement Learning (RL), as opposed to the usual risk-neutral expectation. As a first original contribution, we improve the CVaR Value Iteration algorithm (Chow et al., 2015) in a way that reduces computational complexity of the original algorithm from polynomial to linear time. Secondly, we propose a sampling version of CVaR Value Iteration we call CVaR Q-learning. We also derive a distributional policy improvement algorithm, and later use it as a heuristic for extracting the optimal policy from the converged CVaR Q-learning algorithm. Finally, to show the scalability of our method, we propose an approximate Q-learning algorithm by reformulating the CVaR Temporal Difference update rule as a loss function which we later use in a deep learning context. All proposed methods are experimentally analyzed, including the Deep CVaR Q-learning agent which learns how to avoid risk from raw pixels.
DA  - 2019///
PY  - 2019
DO  - 10.5220/0008175604120423
SP  - 412
EP  - 423
SN  - 978-989-758-384-1
AN  - WOS:000571773900044
KW  - Deep learning
KW  - Reinforcement learning
KW  - Machine learning
KW  - Learning algorithms
KW  - Risk assessment
KW  - Safety engineering
KW  - Risk
KW  - Optimization
KW  - Iterative methods
KW  - Value iteration
KW  - Q-learning algorithms
KW  - AI safety
KW  - Conditional Value-at-Risk
KW  - Value engineering
KW  - Q-learning
KW  - Deep Q-learning
KW  - Optimization approach
KW  - Temporal differences
KW  - Risks
KW  - Distributional reinforcement learning
KW  - Value iteration algorithm
KW  - Conditional value-at-risk
KW  - CVaR
ER  - 

TY  - JOUR
TI  - Safe semi-supervised learning using a bayesian neural network
AU  - Bae, J
AU  - Lee, MJ
AU  - Kim, SB
T2  - INFORMATION SCIENCES
AB  - Semi-supervised learning attempts to use a large set of unlabeled data to increase the pre-diction accuracy of machine learning models when the amount of labeled data is limited. However, in realistic cases, unlabeled data may worsen performance because they contain out-of-distribution (OOD) data that differ from the labeled data. To address this issue, safe semi-supervised deep learning has recently been presented. This study suggests a new safe semi-supervised algorithm that uses an uncertainty-aware Bayesian neural network. Our proposed method, safe uncertainty-based consistency training (SafeUC), uses Bayesian uncertainty to minimize the harmful effects caused by unlabeled OOD examples. The pro-posed method improves the model's generalization performance by regularizing the net-work for consistency against uncertain noise. Moreover, to avoid uncertain prediction results, the proposed method includes a practical inference tip based on a well -calibrated uncertainty. The effectiveness of the proposed method is demonstrated in the experimental results on CIFAR-10 and SVHN by showing that it achieved state-of-the-art performance for all semi-supervised learning tasks with OOD data presence rates.(c) 2022 Elsevier Inc. All rights reserved.
DA  - 2022/10//undefined
PY  - 2022
DO  - 10.1016/j.ins.2022.08.094
VL  - 612
SP  - 453
EP  - 464
SN  - 0020-0255
AN  - WOS:000863219500003
KW  - Deep learning
KW  - Convolutional neural networks
KW  - Learning algorithms
KW  - Uncertainty
KW  - Semi-supervised learning
KW  - Semi-supervised
KW  - Unlabeled data
KW  - Regularisation
KW  - Bayesian neural network
KW  - Bayesian neural networks
KW  - Out-of-distribution
KW  - Consistency regularization
KW  - Safe semi-supervised deep learning
KW  - Uncertain noise
ER  - 

TY  - CONF
TI  - Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch
AU  - He, RD
AU  - Han, ZY
AU  - Yang, Y
AU  - Yin, YL
AU  - Assoc Advancement Artificial Intelligence
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
AB  - Deep semi-supervised learning (SSL) aims to utilize a sizeable unlabeled set to train deep networks, thereby reducing the dependence on labeled instances. However, the unlabeled set often carries unseen classes that cause the deep SSL algorithm to lose generalization. Previous works focus on the data level that they attempt to remove unseen class data or assign lower weight to them but could not eliminate their adverse effects on the SSL algorithm. Rather than focusing on the data level, this paper turns attention to the model parameter level. We find that only partial parameters are essential for seen-class classification, termed safe parameters. In contrast, the other parameters tend to fit irrelevant data, termed harmful parameters. Driven by this insight, we propose Safe Parameter Learning (SPL) to discover safe parameters and make the harmful parameters inactive, such that we can mitigate the adverse effects caused by unseen-class data. Specifically, we firstly design an effective strategy to divide all parameters in the pre-trained SSL model into safe and harmful ones. Then, we introduce a bi-level optimization strategy to update the safe parameters and kill the harmful parameters. Extensive experiments show that SPL outperforms the state-of-the-art SSL methods on all the benchmarks by a large margin. Moreover, experiments demonstrate that SPL can be integrated into the most popular deep SSL networks and be easily extended to handle other cases of class distribution mismatch.
DA  - 2022///
PY  - 2022
SP  - 6874
EP  - 6883
SN  - 2159-5399
AN  - WOS:000893636206110
KW  - Deep learning
KW  - Learning models
KW  - Semi-supervised learning
KW  - Parameter learning
KW  - Generalisation
KW  - Bi-level optimization
KW  - Modeling parameters
KW  - Class distributions
KW  - Adverse effect
KW  - Data level
KW  - Parameter levels
ER  - 

TY  - CONF
TI  - Safe Multi-view Co-training for Semi-supervised Regression
AU  - Liu, LY
AU  - Huang, P
AU  - Min, F
T2  - 2022 IEEE 9TH INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA)
A2  - Huang, JZ
A2  - Pan, Y
A2  - Hammer, B
A2  - Khan, MK
A2  - Xie, X
A2  - Cui, L
A2  - He, Y
AB  - Co-training is a popular disagreement-based semisupervised learning method. Learners of different views mutually select reliable unlabeled instances to augment the labeled dataset. Existing co-training style algorithms have cumbersome procedures for selecting confident instances. Furthermore, the pseudolabels assigned to selected unlabeled instances are not always reliable. In this paper, we propose a safe co-training regression algorithm for multi-view scenarios with two characteristics. An instance selection strategy based on the consistency assumption aims to improve the efficiency of selecting confident unlabeled instances. This strategy makes full use of the information provided by a committee to measure the confidence of unlabeled instances. A safe labeling technique in an ensemble manner is introduced to improve the quality of pseudo-labels. The safe pseudo-labels not only integrate information provided by the committee, but also take into account the part of the receiver. The results over twenty datasets prove the superiority of the proposed algorithm against other state-of-the-art semi-supervised regression algorithms.
DA  - 2022///
PY  - 2022
DO  - 10.1109/DSAA54385.2022.10032437
SP  - 56
EP  - 65
SN  - 2472-1573
AN  - WOS:000967751000007
KW  - Learning systems
KW  - Regression analysis
KW  - Computer vision
KW  - safe learning
KW  - Safe learning
KW  - Semi-supervised
KW  - Multi-views
KW  - Semi-supervised learning methods
KW  - Regression algorithms
KW  - Labeled dataset
KW  - co-training
KW  - Co-training
KW  - Semi-supervised regression
KW  - Disagreement-based method
KW  - disagreement-based methods
KW  - multi-view learning
KW  - Multi-view learning
KW  - semi-supervised regression
ER  - 

TY  - CONF
TI  - RDA: Reciprocal Distribution Alignment for Robust Semi-supervised Learning
AU  - Duan, Y
AU  - Qi, L
AU  - Wang, L
AU  - Zhou, LP
AU  - Shi, YH
T2  - COMPUTER VISION - ECCV 2022, PT XXX
A2  - Avidan, S
A2  - Brostow, G
A2  - Cisse, M
A2  - Farinella, GM
A2  - Hassner, T
AB  - In this work, we propose Reciprocal Distribution Alignment (RDA) to address semi-supervised learning (SSL), which is a hyperparameter-free framework that is independent of confidence threshold and works with both the matched (conventionally) and the mismatched class distributions. Distribution mismatch is an often overlooked but more general SSL scenario where the labeled and the unlabeled data do not fall into the identical class distribution. This may lead to the model not exploiting the labeled data reliably and drastically degrade the performance of SSL methods, which could not be rescued by the traditional distribution alignment. In RDA, we enforce a reciprocal alignment on the distributions of the predictions from two classifiers predicting pseudo-labels and complementary labels on the unlabeled data. These two distributions, carrying complementary information, could be utilized to regularize each other without any prior of class distribution. Moreover, we theoretically show that RDA maximizes the input-output mutual information. Our approach achieves promising performance in SSL under a variety of scenarios of mismatched distributions, as well as the conventional matched SSL setting. Our code is available at: https://github.com/NJUyued/RDA4RobustSSL.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-20056-4_31
VL  - 13690
SP  - 533
EP  - 549
SN  - 0302-9743
AN  - WOS:000903586400031
KW  - Supervised learning
KW  - Performance
KW  - Semi-supervised learning
KW  - Labeled data
KW  - Alignment
KW  - Hyper-parameter
KW  - Unlabeled data
KW  - Confidence threshold
KW  - Distribution alignment
KW  - Class distributions
KW  - Learning scenarios
KW  - Mismatched distribution
KW  - Mismatched distributions
ER  - 

TY  - JOUR
TI  - Adversarial feature distribution alignment for semi-supervised learning
AU  - Mayer, C
AU  - Paul, M
AU  - Timofte, R
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
AB  - Training deep neural networks with only a few labeled samples can lead to overfitting. This is problematic in semi-supervised learning where only a few labeled samples are available. In this paper, we show that a consequence of overfitting in SSL is feature distribution misalignment between labeled and unlabeled samples. Hence, we propose a new feature distribution alignment method. Our method is particularly effective when using only a small amount of labeled samples. We test our method on CIFAR-10, SVHN and LSUN. On SVHN we achieve a test error of 3.88% (250 labeled samples) and 3.39% (1000 labeled samples), which is close to the fully supervised model 2.89% (73k labeled samples). In comparison, the current SOTA achieves only 4.29% and 3.74%. On LSUN we achieve superior results than a state-of-the- art method even when using 100x less unlabeled samples (500 labeled samples). Finally, we provide a theoretical insight why feature distribution misalignment occurs and show that our method reduces it.
DA  - 2021/01//undefined
PY  - 2021
DO  - 10.1016/j.cviu.2020.103109
VL  - 202
SN  - 1077-3142
AN  - WOS:000616091100012
KW  - Deep learning
KW  - Deep neural networks
KW  - Semi-supervised learning
KW  - Unlabeled samples
KW  - State-of-the-art methods
KW  - Alignment
KW  - Overfitting
KW  - Alignment methods
KW  - Feature distribution
KW  - Test errors
ER  - 

TY  - CONF
TI  - Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class Unlabeled Data
AU  - He, RD
AU  - Han, ZY
AU  - Lu, XK
AU  - Yin, YL
AU  - IEEE COMP SOC
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
AB  - Deep semi-supervised learning (SSL) methods aim to take advantage of abundant unlabeled data to improve the algorithm performance. In this paper, we consider the problem of safe SSL scenario where unseen-class instances appear in the unlabeled data. This setting is essential and commonly appears in a variety of real applications. One intuitive solution is removing these unseen-class instances after detecting them during the SSL process. Nevertheless, the performance of unseen-class identification is limited by the small number of labeled data and ignoring the availability of unlabeled data. To take advantage of these unseen-class data and ensure performance, we propose a safe SSL method called SAFE-STUDENT from the teacher-student view. Firstly, a new scoring function called energy-discrepancy (ED) is proposed to help the teacher model improve the security of instances selection. Then, a novel unseen-class label distribution learning mechanism mitigates the unseen-class perturbation by calibrating the unseen-class label distribution. Finally, we propose an iterative optimization strategy to facilitate teacher-student network learning. Extensive studies on several representative datasets show that SAFE-STUDENT remarkably outperforms the state-of-the-art, verifying the feasibility and robustness of our method in the under-explored problem.
DA  - 2022///
PY  - 2022
DO  - 10.1109/CVPR52688.2022.01418
SP  - 14565
EP  - 14574
SN  - 1063-6919
AN  - WOS:000870783000016
KW  - Deep learning
KW  - Learning systems
KW  - Learning algorithms
KW  - Performance
KW  - Categorization
KW  - Representation learning
KW  - Retrieval
KW  - Semi-supervised learning
KW  - Iterative methods
KW  - Students
KW  - Unlabeled data
KW  - Semi-supervised learning methods
KW  - retrieval
KW  - categorization
KW  - Class labels
KW  - Self- & semi- & meta- recognition: detection
KW  - Self- & semi- & meta- Recognition: detection
KW  - Teachers'
ER  - 

TY  - CONF
TI  - Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data
AU  - Guo, LZ
AU  - Zhang, ZY
AU  - Jiang, Y
AU  - Li, YF
AU  - Zhou, ZH
T2  - INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 119
A2  - Daume, H
A2  - Singh, A
AB  - Deep semi-supervised learning (SSL) has been recently shown very effectively. However, its performance is seriously decreased when the class distribution is mismatched, among which a common situation is that unlabeled data contains some classes not seen in the labeled data. Efforts on this issue remain to be limited. This paper proposes a simple and effective safe deep SSL method to alleviate the harm caused by it. In theory, the result learned from the new method is never worse than learning from merely labeled data, and it is theoretically guaranteed that its generalization approaches the optimal in the order O(root dln(n)/n), even faster than the convergence rate in supervised learning associated with massive parameters. In the experiment of benchmark data, unlike the existing deep SSL methods which are no longer as good as supervised learning in 40% of unseen-class unlabeled data, the new method can still achieve performance gain in more than 60% of unseen-class unlabeled data. Moreover, the proposal is suitable for many deep SSL algorithms and can be easily extended to handle other cases of class distribution mismatch.
DA  - 2020///
PY  - 2020
VL  - 119
SN  - 2640-3498
AN  - WOS:000683178504002
KW  - Deep learning
KW  - Learning systems
KW  - Benchmarking
KW  - Semi-supervised learning
KW  - Labeled data
KW  - Convergence rates
KW  - Unlabeled data
KW  - Performance Gain
KW  - Semi-supervised learning (SSL)
KW  - Benchmark data
KW  - Class distributions
ER  - 

TY  - JOUR
TI  - Safe Artificial General Intelligence via Distributed Ledger Technology
AU  - Carlson, KW
T2  - BIG DATA AND COGNITIVE COMPUTING
AB  - Artificial general intelligence (AGI) progression metrics indicate AGI will occur within decades. No proof exists that AGI will benefit humans and not harm or eliminate humans. A set of logically distinct conceptual components is proposed that are necessary and sufficient to (1) ensure various AGI scenarios will not harm humanity, and (2) robustly align AGI and human values and goals. By systematically addressing pathways to malevolent AI we can induce the methods/axioms required to redress them. Distributed ledger technology (DLT, "blockchain") is integral to this proposal, e.g., "smart contracts" are necessary to address the evolution of AI that will be too fast for human monitoring and intervention. The proposed axioms: (1) Access to technology by market license. (2) Transparent ethics embodied in DLT. (3) Morality encrypted via DLT. (4) Behavior control structure with values at roots. (5) Individual bar-code identification of critical components. (6) Configuration Item (from business continuity/disaster recovery planning). (7) Identity verification secured via DLT. (8) "Smart" automated contracts based on DLT. (9) Decentralized applications-AI software modules encrypted via DLT. (10) Audit trail of component usage stored via DLT. (11) Social ostracism (denial of resources) augmented by DLT petitions. (12) Game theory and mechanism design.
DA  - 2019/09//undefined
PY  - 2019
DO  - 10.3390/bdcc3030040
VL  - 3
IS  - 3
SN  - 2504-2289
AN  - WOS:000697671000006
ER  - 

TY  - CONF
TI  - A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms
AU  - Coston, A
AU  - Kawakami, A
AU  - Zhu, HY
AU  - Holstein, K
AU  - Heidari, H
AU  - IEEE
T2  - 2023 IEEE CONFERENCE ON SECURE AND TRUSTWORTHY MACHINE LEARNING, SATML
AB  - Recent research increasingly brings to question the appropriateness of using predictive tools in complex, real-world tasks. While a growing body of work has explored ways to improve value alignment in these tools, comparatively less work has centered concerns around the fundamental justifiability of using these tools. This work seeks to center validity considerations in deliberations around whether and how to build data-driven algorithms in high-stakes domains. Toward this end, we translate key concepts from validity theory to predictive algorithms. We apply the lens of validity to re-examine common challenges in problem formulation and data issues that jeopardize the justifiability of using predictive algorithms and connect these challenges to the social science discourse around validity. Our interdisciplinary exposition clarifies how these concepts apply to algorithmic decision making contexts. We demonstrate how these validity considerations could distill into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data.
DA  - 2023///
PY  - 2023
DO  - 10.1109/SaTML54575.2023.00050
SP  - 690
EP  - 704
SN  - 978-1-66546-299-0
AN  - WOS:001012311500040
L1  - https://arxiv.org/pdf/2206.14983
ER  - 

TY  - JOUR
TI  - Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms
AU  - Nilsen, MK
AU  - Nygaard, TF
AU  - Ellefsen, KO
T2  - GENETIC PROGRAMMING AND EVOLVABLE MACHINES
AB  - Reward tampering is a problem that will impact the trustworthiness of the powerful AI systems of the future. Reward Tampering describes the problem where AI agents bypass their intended objective, enabling unintended and potentially harmful behaviours. This paper investigates whether the creative potential of evolutionary algorithms could help ensure trustworthy solutions when facing this problem. The reason why evolutionary algorithms may help combat reward tampering is that they are able to find a diverse collection of different solutions to a problem within a single run, aiding the search for desirable solutions. Four different evolutionary algorithms were deployed in tasks illustrating the problem of reward tampering. The algorithms were designed with varying degrees of human expertise, measuring how human guidance influences the ability to discover trustworthy solutions. The results indicate that the algorithms' ability to find and preserve trustworthy solutions is very dependent on preserving diversity during the search. Algorithms searching for behavioural diversity showed to be the most effective against reward tampering. Human expertise also showed to improve the certainty and quality of safe solutions, but even with only a minimal degree of human expertise, domain-independent diversity management was found to discover safe solutions.
DA  - 2023/12//undefined
PY  - 2023
DO  - 10.1007/s10710-023-09456-0
VL  - 24
IS  - 2
SN  - 1389-2576
AN  - WOS:001068097800001
ER  - 

TY  - JOUR
TI  - Adversarial Robustness Via Fisher-Rao Regularization
AU  - Picot, M
AU  - Messina, F
AU  - Boudiaf, M
AU  - Labeau, F
AU  - Ayed, IB
AU  - Piantanida, P
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
AB  - Adversarial robustness has become a topic of growing interest in machine learning since it was observed that neural networks tend to be brittle. We propose an information-geometric formulation of adversarial defense and introduce Fire, a new Fisher-Rao regularization for the categorical cross-entropy loss, which is based on the geodesic distance between the softmax outputs corresponding to natural and perturbed input features. Based on the information-geometric properties of the class of softmax distributions, we derive an explicit characterization of the Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some interesting properties as well as connections with standard regularization metrics. Furthermore, we verify on a simple linear and Gaussian model, that all Pareto-optimal points in the accuracy-robustness region can be reached by Fire while other state-of-the-art methods fail. Empirically, we evaluate the performance of various classifiers trained with the proposed loss on standard datasets, showing up to a simultaneous 1% of improvement in terms of clean and robust performances while reducing the training time by 20% over the best-performing methods.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.1109/TPAMI.2022.3174724
VL  - 45
IS  - 3
SP  - 2698
EP  - 2710
SN  - 0162-8828
AN  - WOS:000934990500001
KW  - Deep learning
KW  - deep learning
KW  - Neural networks
KW  - Learning systems
KW  - neural networks
KW  - Machine-learning
KW  - Classification (of information)
KW  - article
KW  - Computer vision
KW  - Robustness
KW  - computer vision
KW  - classifier
KW  - Neural-networks
KW  - Perturbation method
KW  - Perturbation techniques
KW  - Adversarial machine learning
KW  - adversarial training
KW  - Adversarial training
KW  - Regularisation
KW  - Geometry
KW  - Pareto principle
KW  - Adversarial regularization
KW  - fisher-rao distance
KW  - Fisher-rao distance
KW  - information geometry
KW  - Information geometry
KW  - Manifold
KW  - safety AI
KW  - Safety AI
ER  - 

TY  - CONF
TI  - Safe Offline Reinforcement Learning Through Hierarchical Policies
AU  - Liu, SF
AU  - Sun, SL
T2  - ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PAKDD 2022, PT II
A2  - Gama, J
A2  - Li, T
A2  - Yu, Y
A2  - Chen, E
A2  - Zheng, Y
A2  - Teng, F
AB  - Recently, offline reinforcement learning has gained increasing attention. However, the safety of offline reinforcement learning has been ignored. It poses a significant challenge to learn a safe and high-performance policy from a fixed dataset that contains unsafe or unexpected state-action pairs without interacting with the environment. Since the unsafe state-action pairs are usually sparse in the behavior data collected by humans, it is difficult to effectively model information about unsafe behaviors. This paper utilized the hierarchical reinforcement learning framework to alleviate the sparsity issue by modeling unsafe behaviors with hierarchical policies. Specifically, a high-level policy determines a prospective state, and a low-level policy takes action to reach the specified goal state. The training objective of the high-level policy is to improve the expected reward that the low-level policy collects when it moves toward the goal state and reduce the number of unsafe actions. We further develop data processing methods to provide training data for the high-level policy and the low-level policy. Evaluation experiments about performance and safety are conducted in simulation environments that return the rewards and unsafe costs obtained by agents during the interaction. Experimental results demonstrate that the proposed algorithm can choose safe actions while maintaining high performance.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-05936-0_30
VL  - 13281
SP  - 380
EP  - 391
SN  - 0302-9743
AN  - WOS:000870701000030
KW  - Reinforcement learning
KW  - Data handling
KW  - Performance
KW  - Reinforcement learnings
KW  - Learn+
KW  - Safe reinforcement learning
KW  - Offline
KW  - Model informations
KW  - Unsafe behaviors
KW  - Hierarchical policies
KW  - Hierarchical policy
KW  - High level policies
KW  - Off-line training
KW  - Offline training
ER  - 

TY  - CONF
TI  - Learning a Domain-Invariant Embedding for Unsupervised Domain Adaptation Using Class-Conditioned Distribution Alignment
AU  - Gabourie, AJ
AU  - Rostami, M
AU  - Pope, PE
AU  - Kolouri, S
AU  - Kim, K
T2  - 2019 57TH ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING (ALLERTON)
AB  - We address the problem of unsupervised domain adaptation (UDA) by learning a cross-domain agnostic embedding space, where the distance between the probability distributions of the two source and target visual domains is minimized. We use the output space of a shared cross-domain deep encoder to model the embedding space and use the Sliced-Wasserstein Distance (SWD) to measure and minimize the distance between the embedded distributions of two source and target domains to enforce the embedding to be domain-agnostic. Additionally, we use the source domain labeled data to train a deep classifier from the embedding space to the label space to enforce the embedding space to be discriminative. As a result of this training scheme, we provide an effective solution to train the deep classification network on the source domain such that it will generalize well on the target domain, where only unlabeled training data is accessible. To mitigate the challenge of class matching, we also align corresponding classes in the embedding space by using high confidence pseudo-labels for the target domain, i.e. assigning the class for which the source classifier has a high prediction probability. We provide experimental results on UDA benchmark tasks to demonstrate that our method is effective and leads to state-of-the-art performance.
DA  - 2019///
PY  - 2019
DO  - 10.1109/allerton.2019.8919960
SP  - 352
EP  - 359
SN  - 2474-0195
AN  - WOS:000535355700051
KW  - Classification (of information)
KW  - Benchmarking
KW  - Probability distributions
KW  - Domain adaptation
KW  - Embeddings
KW  - State-of-the-art performance
KW  - Deep classifications
KW  - Domain agnostics
KW  - Effective solution
KW  - Prediction probabilities
KW  - Sliced wasserstein distances
KW  - Training schemes
ER  - 

TY  - JOUR
TI  - Integrating safety constraints into adversarial training for robust deep reinforcement learning
AU  - Meng, JL
AU  - Zhu, F
AU  - Ge, YY
AU  - Zhao, PY
T2  - INFORMATION SCIENCES
AB  - The ability to resist interference is the key to the widespread application of reinforcement learning. Although adversarial training is a promising method for robust promotion, stan-dard adversarial training leads to unstable results or performance deterioration due to the presence of perturbation. To address the problem, a robust reinforcement learning method which integrates safety constraints that are modelled by environment termination condi-tions into adversarial training is proposed, where safety constraints are adopted to restrict agent's actions and guide the training process. For better modelling the robust reinforce-ment learning problem, a modified constrained Markov Decision Process (MDP) that con-siders perturbation for robust reinforcement learning, named Constrained Markov Decision Process (CMDP) with Perturbation (CMDPP) is also introduced. The proposed safe robust reinforcement learning method based on CMDPP utilizes the penalty function to solve CMDP and generates perturbation from the gradient of state for adversarial training. Tests on the robustness of the proposed method under several attack methods and evalu-ation of generalization through changing environment dynamics were carried out on the OpenAI gym and Roboschool environments. The results demonstrate that our method not only has a better performance confronting the attack but also has a higher generaliza-tion capability with reference to the changing environment dynamics.(c) 2022 Elsevier Inc. All rights reserved.
DA  - 2023/01//undefined
PY  - 2023
DO  - 10.1016/j.ins.2022.11.051
VL  - 619
SP  - 310
EP  - 323
SN  - 0020-0255
AN  - WOS:000901771900018
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Markov processes
KW  - Constrained Markov decision process
KW  - Reinforcement learning method
KW  - Safety constraint
KW  - Deterioration
KW  - Environment dynamics
KW  - Adversarial training
KW  - Changing environment
KW  - Penalty function method
KW  - Penalty function methods
KW  - Robust deep reinforcement learning
ER  - 

TY  - CONF
TI  - Formal Methods Assisted Training of Safe Reinforcement Learning Agents
AU  - Murugesan, A
AU  - Moghadamfalahi, M
AU  - Chattopadhyay, A
T2  - NASA FORMAL METHODS (NFM 2019)
A2  - Badger, JM
A2  - Rozier, KY
AB  - Reinforcement learning (RL) is emerging as a powerful machine learning paradigm to develop autonomous safety critical systems; RL enables the systems to learn optimal control strategies by interacting with the environment. However, there is also widespread apprehension to deploying such systems in the real world since rigorously ensuring if they had learned safe strategies by interacting with an environment that is representative of the real world remains a challenge. Hence, there is a surge of interest to establish safety-focused RL techniques.
In this paper, we present a safety-assured training approach that augments standard RL with formal analysis and simulation technology. The benefits of coupling these techniques is three-fold: the formal analysis tools (SMT solvers) guide the system to learn strategies that rigorously uphold specified safety properties; the sophisticated simulators provide a wide-range of quantifiable, realistic learning environments; the adequacy of the safety properties can be assessed as agent explores complex environments. We illustrate this approach using a Flappy Bird game.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-20652-9_22
VL  - 11460
SP  - 333
EP  - 340
SN  - 0302-9743
AN  - WOS:000657973800022
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Machine learning
KW  - Reinforcement learning agent
KW  - Safety engineering
KW  - Computer aided instruction
KW  - Formal methods
KW  - NASA
KW  - Assurance
KW  - Safety critical systems
KW  - Complex environments
KW  - Learning environments
KW  - Formal analysis
KW  - Formal analysis tools
KW  - Optimal control strategy
ER  - 

TY  - JOUR
TI  - Safety-constrained reinforcement learning with a distributional safety critic
AU  - Yang, QS
AU  - Simao, TD
AU  - Tindemans, SH
AU  - Spaan, MTJ
T2  - MACHINE LEARNING
AB  - Safety is critical to broadening the real-world use of reinforcement learning. Modeling the safety aspects using a safety-cost signal separate from the reward and bounding the expected safety-cost is becoming standard practice, since it avoids the problem of finding a good balance between safety and performance. However, it can be risky to set constraints only on the expectation neglecting the tail of the distribution, which might have prohibitively large values. In this paper, we propose a method called Worst-Case Soft Actor Critic for safe RL that approximates the distribution of accumulated safety-costs to achieve risk control. More specifically, a certain level of conditional Value-at-Risk from the distribution is regarded as a safety constraint, which guides the change of adaptive safety weights to achieve a trade-off between reward and safety. As a result, we can compute policies whose worst-case performance satisfies the constraints. We investigate two ways to estimate the safety-cost distribution, namely a Gaussian approximation and a quantile regression algorithm. On the one hand, the Gaussian approximation is simple and easy to implement, but may underestimate the safety cost, on the other hand, the quantile regression leads to a more conservative behavior. The empirical analysis shows that the quantile regression method achieves excellent results in complex safety-constrained environments, showing good risk control.
DA  - 2023/03//undefined
PY  - 2023
DO  - 10.1007/s10994-022-06187-8
VL  - 112
IS  - 3
SP  - 859
EP  - 887
SN  - 0885-6125
AN  - WOS:000814940000002
KW  - Reinforcement learning
KW  - Risk management
KW  - Performance
KW  - Reinforcement learnings
KW  - Economic and social effects
KW  - Regression analysis
KW  - Risk assessment
KW  - Real-world
KW  - Safety engineering
KW  - Approximation algorithms
KW  - Value engineering
KW  - Cost benefit analysis
KW  - Quantile regression
KW  - Gaussian approximations
KW  - Risks controls
KW  - Safety aspects
KW  - Safety costs
KW  - Set constraints
KW  - Standard practices
ER  - 

TY  - CONF
TI  - Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations
AU  - Luo, YP
AU  - Ma, TY
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)
A2  - Ranzato, M
A2  - Beygelzimer, A
A2  - Dauphin, Y
A2  - Liang, PS
A2  - Vaughan, JW
AB  - Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS),which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates are learned via adversarial training and ensure the policy's safety assuming calibrated learned dynamics. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.
DA  - 2021///
PY  - 2021
VL  - 34
SN  - 1049-5258
AN  - WOS:000922928205035
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Real-world
KW  - Iterative methods
KW  - Learn+
KW  - Prior-knowledge
KW  - Reinforcement learning algorithms
KW  - Dynamics
KW  - Safety violations
KW  - Barrier certificates
KW  - Learning barriers
KW  - Training time
KW  - Offline data
ER  - 

TY  - JOUR
TI  - Safe incomplete label distribution learning
AU  - Zhang, J
AU  - Tao, H
AU  - Luo, TJ
AU  - Hou, CP
T2  - PATTERN RECOGNITION
AB  - Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance. (C) 2021 Elsevier Ltd. All rights reserved.
DA  - 2022/05//undefined
PY  - 2022
DO  - 10.1016/j.patcog.2021.108518
VL  - 125
SN  - 0031-3203
AN  - WOS:000742689700004
KW  - Learning systems
KW  - Performance
KW  - Quadratic programming
KW  - Learn+
KW  - Learning methods
KW  - Labeled data
KW  - Real applications
KW  - Baseline methods
KW  - Safeness
KW  - Label distribution
KW  - Incomplete supervised learning
KW  - Label distribution learning
ER  - 

TY  - CONF
TI  - Safe Reinforcement Learning using Data-Driven Predictive Control
AU  - Selim, M
AU  - Alanwar, A
AU  - El-Kharashi, MW
AU  - Abbas, HM
AU  - Johansson, KH
T2  - 2022 5TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND THEIR APPLICATIONS (ICCSPA)
AB  - Reinforcement learning (RL) algorithms can achieve state-of-the-art performance in decision-making and continuous control tasks. However, applying RL algorithms on safety-critical systems still needs to be well justified due to the exploration nature of many RL algorithms, especially when the model of the robot and the environment are unknown. To address this challenge, we propose a data-driven safety layer that acts as a filter for unsafe actions. The safety layer uses a data-driven predictive controller to enforce safety guarantees for RL policies during training and after deployment. The RL agent proposes an action that is verified by computing the data-driven reachability analysis. If there is an intersection between the reachable set of the robot using the proposed action, we call the data-driven predictive controller to find the closest safe action to the proposed unsafe action. The safety layer penalizes the RL agent if the proposed action is unsafe and replaces it with the closest safe one. In the simulation, we show that our method outperforms state-of-the-art safe RL methods on the robotics navigation problem for a Turtlebot 3 in Gazebo and a quadrotor in Unreal Engine 4 (UE4).
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICCSPA55860.2022.10018994
SN  - 2377-682X
AN  - WOS:000972628300008
KW  - Reinforcement learning
KW  - Decision making
KW  - Motion planning
KW  - Learning systems
KW  - Motion-planning
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Controllers
KW  - Safety engineering
KW  - Predictive control
KW  - robot safety
KW  - Reinforcement learning algorithms
KW  - Data driven
KW  - Robot programming
KW  - Robot safety
KW  - State-of-the-art performance
KW  - Predictive controller
KW  - task and motion planning
KW  - Task planning
ER  - 

TY  - JOUR
TI  - An Actor-Critic Framework for Online Control With Environment Stability Guarantee
AU  - Osinenko, P
AU  - Yaremenko, G
AU  - Malaniya, G
AU  - Bolychev, A
T2  - IEEE ACCESS
AB  - Online actor-critic reinforcement learning is concerned with training an agent on-the-fly via dynamic interaction with the environment. Due to the specifics of the application, it is not generally possible to perform long pre-training, as it is commonly done in off-line, tabular or Monte-Carlo mode. Such applications may be found more frequently in industry, rather than in pure digital fields, such as cloud services, video games, database management, etc., where reinforcement learning has been demonstrating success. Stability of the closed-loop of the agent plus the environment is a major challenge here, and not only in terms of the environment safety and integrity, but also in terms of sparing resources on failed training episodes. In this paper, we tackle the problem of environment stability under an actor-critic reinforcement learning agent by integration of the Lyapunov stability theory tools. Under the presented approach, the closed-loop stability is secured in all episodes without pre-training. It was observed in a case study with a mobile robot that the suggested agent could always successfully achieve the control goal, while significantly reducing the cost. While many approaches may be exploited for mobile robot control, we suggest that the experiments showed the promising potential of actor-critic reinforcement learning agents based on Lyapunov-like constraints. The presented methodology may be utilized in safety-critical, industrial applications where stability is necessary.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ACCESS.2023.3306070
VL  - 11
SP  - 89188
EP  - 89204
SN  - 2169-3536
AN  - WOS:001061771000001
ER  - 

TY  - JOUR
TI  - Safe batch constrained deep reinforcement learning with generative adversarial network
AU  - Dong, WB
AU  - Liu, SF
AU  - Sun, SL
T2  - INFORMATION SCIENCES
AB  - Batch-constrained reinforcement learning constrains the learned policy to be close to the behavior policy, which holds a tremendous promise for alleviating the distributional shift in offline reinforcement learning. Existing batch-constrained techniques rely on perturbation models to adjust the actions generated from the generative model to maximize the estimated value function. However, the perturbation model deviates from the distribution of the offline datasets and introduces a new distribution drift problem, which affects the performance of the learned policies. In addition, since offline reinforcement learning cannot learn by trial and error, the final policies are often prone to failure in reality or make unsafe decisions when trained with a noisy or small size dataset. To address the above issues, this paper employs constrained generative adversarial network to generate actions with given states. Specifically, we train the generator to maximize the estimated value and constrain the state-action pairs to follow the dataset distribution. The perturbation model is trained to maximize the probability of the perturbed actions belonging to the dataset and minimize the likelihood of taking dangerous actions. Moreover, we utilize safety critics to predict the risk of the actions under a state. Experimental results show that the proposed method is effective and can choose safe actions while maintaining a high performance in offline settings.
DA  - 2023/07//undefined
PY  - 2023
DO  - 10.1016/j.ins.2023.03.108
VL  - 634
SP  - 259
EP  - 270
SN  - 0020-0255
AN  - WOS:000962845200001
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Performance
KW  - Deep reinforcement learning
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Offline
KW  - Perturbation techniques
KW  - Generative adversarial networks
KW  - Generative model
KW  - Batch-constrained technique
KW  - Batch-constrained techniques
KW  - Behavior policy
KW  - Distributional shift
KW  - Generative adversarial network
KW  - Perturbation model
KW  - Safety critic
KW  - Safety critics
ER  - 

TY  - CONF
TI  - Safe-DS: A Domain Specific Language to Make Data Science Safe
AU  - Reimann, L
AU  - Kniesel-Wünsche, G
T2  - 2023 IEEE/ACM 45TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING-NEW IDEAS AND EMERGING RESULTS, ICSE-NIER
AB  - Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide.
In this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python libraries, and automatically generate suitable stubs.
Moreover, Safe-DS complements textual DS pipelines with a graphical representation that eases safe development by preventing syntax errors. The seamless synchronization of textual and graphic view lets developers always choose the one best suited for their skills and current task.
We think that Safe-DS can make DS development easier, faster, and more reliable, significantly reducing development costs.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICSE-NIER58687.2023.00019
SP  - 72
EP  - 77
SN  - 2832-7624
AN  - WOS:001032816400013
KW  - Machine learning
KW  - Machine Learning
KW  - Pipelines
KW  - Machine-learning
KW  - 'current
KW  - Errors
KW  - Application programming interfaces (API)
KW  - Python
KW  - Libraries
KW  - Runtimes
KW  - Object oriented programming
KW  - Data Science
KW  - Domain Specific Language
KW  - Domains specific languages
KW  - Problem oriented languages
KW  - Refined type
KW  - Refined Types
KW  - Schema type
KW  - Schema Types
KW  - Science libraries
KW  - Static safety
KW  - Static Safety
KW  - Static type checking
KW  - Static typing
ER  - 

TY  - CONF
TI  - Adversarial Training on Joint Energy Based Model for Robust Classification and Out-of-Distribution Detection
AU  - Lee, K
AU  - Yang, H
AU  - Oh, SY
T2  - 2020 20TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS)
AB  - Deep neural networks tend to be erroneous when the training and test distribution differ. Especially, neural classifiers are brittle to adversarial examples, and highly overconfident to out-of-distribution examples. Hybrid modeling of generative and discriminative distribution shown to be effective for out-of-distribution detection, but is not robust to adversarial attacks. Otherwise, defense methods for adversarial attacks cannot distinguish out-of-distribution examples. In this work, we present a hybrid model that can deal with both adversarial and out-of-distribution examples. Our method is built upon the joint energy based model and adversarial training. Through experiments on CIFAR-10 dataset, we show that our method has state-of-the-art performanced among hybrid models. Furthermore, we show that our model exhibits more perceptually-aligned feature than other methods, by showing the gradient sensitivity map with newly proposed score function.
DA  - 2020///
PY  - 2020
DO  - 10.23919/iccas50221.2020.9268406
SP  - 17
EP  - 21
SN  - 2093-7121
AN  - WOS:000681746000004
KW  - Deep learning
KW  - Deep neural networks
KW  - State of the art
KW  - Control engineering
KW  - Adversarial attack
KW  - Robust classification
KW  - Hybrid model
KW  - Out-of-distribution detection
KW  - Energy-based models
KW  - Gradient sensitivity
KW  - Machine vision and perception
KW  - Neural classifiers
KW  - Score function
KW  - Security and safety of deep learning
ER  - 

TY  - CONF
TI  - Train Small, Deploy Big: Do Relative World Views Permit Swarm-Safety During Policy Transplantation for Multi-Agent Reinforcement Learning Problems?
AU  - Fraser, B
AU  - Laurito, G
T2  - AI 2020: ADVANCES IN ARTIFICIAL INTELLIGENCE
A2  - Gallagher, M
A2  - Moustafa, N
A2  - Lakshika, E
AB  - In order to 'train small, deploy big', agent control policies must be transplanted from one trained agent into a larger set of agents for deployment. Given that compute resources and training time generally scale with the number of agents, this approach to generating swarm control policies may be favourable for larger swarms. However, in order for this process to be successful, the agent control policy must be indistinct to the agent on which it is trained so that it can perform as required in its new host agent. Through extensive simulation of a cooperative multi-agent navigation task, it is shown that this indistinctness of agent policies, and therefore the success of the associated learned solution of the transplanted swarm, is dependent upon the way in which an agent views the world: absolute or relative. As a corollary to, and in contrary to naive intuition of, this result, we show that homogeneous agent capability is not enough to guarantee policy indistinctness. The article also discusses what general conditions may be required in order to enforce policy indistinctness.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-64984-5_21
VL  - 12576
SP  - 269
EP  - 280
SN  - 2945-9133
AN  - WOS:001061406300021
KW  - Reinforcement learning
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Multi-agent deep reinforcement learning
KW  - Control policy
KW  - Swarm intelligence
KW  - Extensive simulations
KW  - Training time
KW  - Agent control
KW  - Compute resources
KW  - Cooperative navigation
KW  - Homogeneous agents
KW  - Multi-agent navigations
KW  - Policy transplantation
KW  - Swarm-safety
ER  - 

TY  - CONF
TI  - Detecting Functional Safety Violations in Online AI Accelerators
AU  - Kundu, S
AU  - Basu, K
T2  - 2022 IEEE 28TH INTERNATIONAL SYMPOSIUM ON ON-LINE TESTING AND ROBUST SYSTEM DESIGN (IOLTS 2022)
A2  - Savino, A
A2  - Rech, P
A2  - DiCarlo, S
A2  - Gizopoulos, D
AB  - With the ubiquitous deployment of Deep Neural Networks (DNNs) in low latency mission critical applications, there has been an extensive proliferation of custom-built AI inference accelerators at the edge. Drastic technology scaling in recent years has made these circuits highly vulnerable to faults due to various reasons like aging, latent defects, single event upsets, etc. Such faults are highly detrimental to the classification accuracy of the AI accelerator, leading to the critical Functional Safety (FuSa) violation, when used in mission-critical applications. In order to detect such violations in mission mode, we analyze the efficiency of a software-based self test scheme that employs functional test patterns, akin to instances in the application dataset. Such patterns are either selected from the dataset of the DNN, or generated from scratch utilizing the concept of Generative Adversarial Networks (GANs). When evaluated on state-of-the-art DNNs on multivariate exhaustive datasets, the GAN generated test patterns significantly improve FuSa violation detection coverage by up to 130.28%, compared to the selected test patterns, thereby accomplishing efficient testing of the AI accelerator, online, in mission mode.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IOLTS56730.2022.9897702
SN  - 1942-9398
AN  - WOS:000865857100024
ER  - 

TY  - JOUR
TI  - AI apology: interactive multi-objective reinforcement learning for human-aligned AI
AU  - Harland, H
AU  - Dazeley, R
AU  - Nakisa, B
AU  - Cruz, F
AU  - Vamplew, P
T2  - NEURAL COMPUTING & APPLICATIONS
AB  - For an Artificially Intelligent (AI) system to maintain alignment between human desires and its behaviour, it is important that the AI account for human preferences. This paper proposes and empirically evaluates the first approach to aligning agent behaviour to human preference via an apologetic framework. In practice, an apology may consist of an acknowledgement, an explanation and an intention for the improvement of future behaviour. We propose that such an apology, provided in response to recognition of undesirable behaviour, is one way in which an AI agent may both be transparent and trustworthy to a human user. Furthermore, that behavioural adaptation as part of apology is a viable approach to correct against undesirable behaviours. The Act-Assess-Apologise framework potentially could address both the practical and social needs of a human user, to recognise and make reparations against prior undesirable behaviour and adjust for the future. Applied to a dual-auxiliary impact minimisation problem, the apologetic agent had a near perfect determination and apology provision accuracy in several non-trivial configurations. The agent subsequently demonstrated behaviour alignment with success that included up to complete avoidance of the impacts described by these objectives in some scenarios.
DA  - 2023/08//undefined
PY  - 2023
DO  - 10.1007/s00521-023-08586-x
VL  - 35
IS  - 23
SP  - 16917
EP  - 16930
SN  - 0941-0643
AN  - WOS:000973380900004
KW  - Reinforcement learning
KW  - Behavioral research
KW  - Reinforcement learnings
KW  - Human users
KW  - Human alignment
KW  - AI safety
KW  - Multi objective
KW  - Multi-objective reinforcement learning
KW  - Agent behavior
KW  - AI apology
KW  - Artificially intelligent apology
KW  - Artificially intelligent safety
KW  - Behavioral adaptation
KW  - Impact minimisation
KW  - Impact minimization
ER  - 

TY  - JOUR
TI  - Beyond generalization: a theory of robustness in machine learning
AU  - Freiesleben, T.
AU  - Grote, T.
T2  - Synthese
AB  - The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept. © 2023, The Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1007/s11229-023-04334-9
VL  - 202
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172812290&doi=10.1007%2fs11229-023-04334-9&partnerID=40&md5=7e9f1df24e78804b758b8df25867fe87
DB  - Scopus
KW  - Machine Learning
KW  - Uncertainty
KW  - Robustness
KW  - Extrapolation
KW  - Generalization
KW  - Models in Science
ER  - 

TY  - CONF
TI  - Detecting Operational Adversarial Examples for Reliable Deep Learning
AU  - Zhao, XY
AU  - Huang, W
AU  - Schewe, S
AU  - Dong, Y
AU  - Huang, XW
T2  - 51ST ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS - SUPPLEMENTAL VOL (DSN 2021)
AB  - The utilisation of Deep Learning (DL) raises new challenges regarding its dependability in critical applications. Sound verification and validation methods are needed to assure the safe and reliable use of DL. However, state-of-the-art debug testing methods on DL that aim at detecting adversarial examples (AEs) ignore the operational profile, which statistically depicts the software's future operational use. This may lead to very modest effectiveness on improving the software's delivered reliability, as the testing budget is likely to be wasted on detecting AEs that are unrealistic or encountered very rarely in real-life operation. In this paper, we first present the novel notion of "operational AEs" which are AEs that have relatively high chance to be seen in future operation. Then an initial design of a new DL testing method to efficiently detect "operational AEs" is provided, as well as some insights on our prospective research plan.
DA  - 2021///
PY  - 2021
DO  - 10.1109/DSN-S52858.2021.00013
SP  - 5
EP  - 6
SN  - 1530-0889
AN  - WOS:000701459900003
KW  - Deep learning
KW  - Safe AI
KW  - Software reliability
KW  - Well testing
KW  - operational profile
KW  - Operational profile
KW  - safe AI
KW  - Software testing
KW  - Software-Reliability
KW  - Critical applications
KW  - Budget control
KW  - Software testings
KW  - Robustness testing
KW  - robustness testing
KW  - Testing method
KW  - Deep learning robustness
KW  - Deep Learning robustness
KW  - software reliability
KW  - software testing
KW  - Sound verification
KW  - Verification and validation methods
ER  - 

TY  - CONF
TI  - Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction
AU  - Liu, PZ
AU  - Zhang, K
AU  - Tateo, D
AU  - Jauhri, S
AU  - Hu, ZY
AU  - Peters, J
AU  - Chalvatzaki, G
AU  - IEEE
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2023)
AB  - Safety is a fundamental property for the realworld deployment of robotic platforms. Any control policy should avoid dangerous actions that could harm the environment, humans, or the robot itself. In reinforcement learning (RL), safety is crucial when exploring a new environment to learn a new skill. This paper introduces a new formulation of safe exploration for robotic RL in the tangent space of the constraint manifold that effectively transforms the action space of the RL agent for always respecting safety constraints locally. We show how to apply this approach to a wide range of robotic platforms and how to define safety constraints that represent dynamic articulated objects like humans in the context of robotic RL. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a TIAGo++ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICRA48891.2023.10161548
SP  - 9449
EP  - 9456
SN  - 1050-4729
AN  - WOS:001048371102021
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Human robot interaction
KW  - Learn+
KW  - Safety constraint
KW  - Control policy
KW  - High-dimensional
KW  - Higher-dimensional
KW  - Robotic tasks
KW  - Real world deployment
KW  - Fundamental properties
KW  - Robotic platforms
ER  - 

TY  - CONF
TI  - Discovering Blind Spots in Reinforcement Learning
AU  - Ramakrishnan, R
AU  - Kamar, E
AU  - Dey, D
AU  - Shah, J
AU  - Horvitz, E
T2  - PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18)
AB  - Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.
DA  - 2018///
PY  - 2018
SP  - 1017
EP  - 1025
SN  - 978-1-4503-5649-7
AN  - WOS:000468231300121
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Supervised learning
KW  - Predictive modeling
KW  - Transfer learning
KW  - Combining techniques
KW  - Errors
KW  - Execution environments
KW  - Interactive reinforcement learning
KW  - Interactive Reinforcement Learning
KW  - Multi agent systems
KW  - Predictive performance
KW  - Safety in RL
KW  - State representation
KW  - Supervised learning problems
ER  - 

TY  - CONF
TI  - Safe Exploration for Interactive Machine Learning
AU  - Turchetta, M
AU  - Berkenkamp, F
AU  - Krause, A
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)
A2  - Wallach, H
A2  - Larochelle, H
A2  - Beygelzimer, A
A2  - d'Alche-Buc, F
A2  - Fox, E
A2  - Garnett, R
AB  - In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.
DA  - 2019///
PY  - 2019
VL  - 32
SN  - 1049-5258
AN  - WOS:000534424302084
KW  - Machine learning
KW  - Real-world system
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Iterative methods
KW  - Active Learning
KW  - Bayesian optimization
KW  - Gaussian process priors
KW  - Interactive machine learning
KW  - Noisy observations
KW  - Photolithography
KW  - Regularity assumption
ER  - 

TY  - JOUR
TI  - Towards Backdoor Attacks and Defense in Robust Machine Learning Models
AU  - Soremekun, E.
AU  - Udeshi, S.
AU  - Chattopadhyay, S.
T2  - Computers and Security
AB  - The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. Notably, the state-of-the-art projected gradient descent (PGD) -based training method has been shown to be universally and reliably effective in defending against adversarial inputs. This robustness approach uses PGD as a reliable and universal “first-order adversary”. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we study how to inject and defend against backdoor attacks for robust models trained using PGD-based robust optimisation. We demonstrate that these models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect such backdoor-infected models via a detection technique called AEGIS. Specifically, given a robust Deep Neural Network (DNN) that is trained using PGD-based first-order adversarial training approach, AEGIS uses feature clustering to effectively detect whether such DNNs are backdoor-infected or clean. In our evaluation of several visible and hidden backdoor triggers on major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects PGD-trained robust DNNs infected with backdoors. AEGIS detects such backdoor-infected models with 91.6% accuracy (11 out of 12 tested models), without any false positives. Furthermore, AEGIS detects the targeted class in the backdoor-infected model with a reasonably low (11.1%) false positive rate. Our investigation reveals that salient features of adversarially robust DNNs could be promising to break the stealthy nature of backdoor attacks. © 2023 Elsevier Ltd
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.cose.2023.103101
VL  - 127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147089205&doi=10.1016%2fj.cose.2023.103101&partnerID=40&md5=082a38fe3e54aee9a4c98eb655378cfa
DB  - Scopus
KW  - Machine learning
KW  - Deep neural networks
KW  - Neural networks
KW  - Learning systems
KW  - Machine-learning
KW  - Classification (of information)
KW  - State of the art
KW  - Machine learning models
KW  - Optimization
KW  - Robust optimization
KW  - Gradient methods
KW  - Neural-networks
KW  - Backdoors
KW  - First order
KW  - Gradient-descent
KW  - Projected gradient
KW  - Training methods
ER  - 

TY  - CONF
TI  - Adversarial Robustness of Phishing Email Detection Models
AU  - Gholampour, P.M.
AU  - Verma, R.M.
T2  - IWSPA 2023 - Proceedings of the 9th ACM International Workshop on Security and Privacy Analytics
AB  - Developing robust detection models against phishing emails has long been the main concern of the cyber defense community. Currently, public phishing/legitimate datasets lack adversarial email examples which keeps the detection models vulnerable. To address this problem, we developed an augmented phishing/legitimate email dataset, utilizing different adversarial text attack techniques. Next, the models were retrained with the adversarial dataset. Results showed that accuracy and F1 score of the models improved under subsequent attacks. In another experiment, synthetic phishing emails were generated using a fine-tuned GPT-2 model. The detection model was retrained with a newly formed synthetic dataset. Subsequently, we observed that the accuracy and robustness of the model did not improve significantly under black box attack methods. In the last experiment, we proposed a defensive technique to classify adversarial examples to their true labels using a K-Nearest Neighbor approach with 94% accuracy in our prediction.  © 2023 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3579987.3586567
SP  - 67
EP  - 76
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159123101&doi=10.1145%2f3579987.3586567&partnerID=40&md5=a1c834530dbd2c55d36ba67001fdbe55
DB  - Scopus
L1  - https://dl.acm.org/doi/pdf/10.1145/3579987.3586567
KW  - machine learning
KW  - Deep learning
KW  - deep learning
KW  - Learning systems
KW  - Machine-learning
KW  - Network security
KW  - Computer crime
KW  - Nearest neighbor search
KW  - Adversarial attack
KW  - Model robustness
KW  - adversarial attacks
KW  - data augmentation
KW  - Data augmentation
KW  - Electronic mail
KW  - generative ai
KW  - Generative ai
KW  - gpt-2
KW  - Gpt-2
KW  - model robustness
KW  - Phishing
KW  - phishing/legitimate dataset
KW  - Phishing/legitimate dataset
KW  - Transformer modeling
KW  - transformer models
ER  - 

TY  - CONF
TI  - From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness
AU  - Feldkamp, N.
AU  - Strassburger, S.
T2  - ACM International Conference Proceeding Series
AB  - Evaluating robustness is an important goal in simulation-based analysis. Robustness is achieved when the controllable factors of a system are adjusted in such a way that any possible variance in uncontrollable factors (noise) has minimal impact on the variance of the desired output. The optimization of system robustness using simulation is a dedicated and well-established research direction. However, once a simulation model is available, there is a lot of potential to learn more about the inherent relationships in the system, especially regarding its robustness. Data farming offers the possibility to explore large design spaces using smart experiment design, high performance computing, automated analysis, and interactive visualization. Sophisticated machine learning methods excel at recognizing and modelling the relation between large amounts of simulation input and output data. However, investigating and analyzing this modelled relationship can be very difficult, since most modern machine learning methods like neural networks or random forests are opaque black boxes. Explainable Artificial Intelligence (XAI) can help to peak into this black box, helping us to explore and learn about relations between simulation input and output. In this paper, we introduce a concept for using Data Farming, machine learning and XAI to investigate and understand system robustness of a given simulation model. © 2023 Owner/Author.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3573900.3591114
SP  - 96
EP  - 106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163894026&doi=10.1145%2f3573900.3591114&partnerID=40&md5=1d1bd8540275e7e02e822b4891aad1dc
DB  - Scopus
KW  - machine learning
KW  - Explainable AI
KW  - XAI
KW  - Deep learning
KW  - deep learning
KW  - explainable AI
KW  - simulation
KW  - Simulation
KW  - Visualization
KW  - Learning systems
KW  - Machine-learning
KW  - Data visualization
KW  - Learn+
KW  - Data farming
KW  - robustness optimization
KW  - Robustness optimizations
KW  - Simulation model
KW  - System robustness
ER  - 

TY  - CONF
TI  - Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces
AU  - Robinette, P.K.
AU  - Hamilton, N.P.
AU  - Johnson, T.T.
T2  - ICCPS 2023 - Proceedings of the 2023 ACM/IEEE 14th International Conference on Cyber-Physical Systems with CPS-IoT Week 2023
AB  - Self-Preserving Genetic Algorithms (SPGA) combine the evolutionary strategy of a genetic algorithm with safety assurance methods commonly implemented in safe reinforcement learning (SRL), a branch of reinforcement learning (RL) that accounts for safety in the exploration and decision-making process of the agent. Safe learning approaches are especially important in safety-critical environments, where failure to account for the safety of the controlled system could result in the loss of millions of dollars in hardware or bodily harm to people working nearby, as is true of many cyber-physical systems. While SRL is a viable approach to safe learning, there are many challenges that must be taken into consideration when training agents, such as sample efficiency, stability, and exploration—an issue that is easily addressed by the evolutionary strategy of a genetic algorithm. By combining GAs with the safety mechanisms used with SRL, SPGA offers a safe learning alternative that is able to explore large areas of the solution space, addressing SRL’s challenge of exploration. This work implements SPGA with both action masking and run time assurance safety strategies to evolve safe controllers for three types of discrete action space environments applicable to cyber physical systems (control, routing, and operations) and under various safety conditions. Training and testing evaluation metrics are compared with results from SRL trained controllers to validate results. SPGA and SRL controllers are trained across 5 random seeds and evaluated on 500 episodes to calculate average wall time to train, average expected return, and percentage of safe action evaluation metrics. SPGA achieves comparable reward and safety performance results with significantly improved training efficiency (55x faster on average), demonstrating the effectiveness of this safe learning approach. © ICCPS 2023. All rights reserved.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3576841.3585936
SP  - 110
EP  - 119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167866179&doi=10.1145%2f3576841.3585936&partnerID=40&md5=01b2dc767bb29f9262c5ca95bfd80333
DB  - Scopus
KW  - Reinforcement learning
KW  - Decision making
KW  - Behavioral research
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Controllers
KW  - Efficiency
KW  - Safety engineering
KW  - Learning approach
KW  - Embedded systems
KW  - Safe reinforcement learning
KW  - Cybe-physical systems
KW  - Cyber Physical System
KW  - safe reinforcement learning
KW  - Action spaces
KW  - safe learning
KW  - Safe learning
KW  - Genetic algorithms
KW  - action masking
KW  - Action masking
KW  - Evolutionary strategies
KW  - genetic algorithms
KW  - run time assurance
KW  - Run time assurance
KW  - Runtimes
ER  - 

TY  - JOUR
TI  - Robust Federated Learning With Noisy Labeled Data Through Loss Function Correction
AU  - Chen, L.
AU  - Ang, F.
AU  - Chen, Y.
AU  - Wang, W.
T2  - IEEE Transactions on Network Science and Engineering
AB  - Federated learning (FL) is a communication-efficient machine learning paradigm to leverage distributed data at the network edge. Nevertheless, FL usually fails to train a high-quality model from the networks, where the edge nodes collect noisy labeled data. To tackle this challenge, this paper focuses on developing an innovative robust FL. We consider two kinds of networks with different data distribution. Firstly, we design a reweighted FL under a full-data network, where all edge nodes are equipped with both numerous noisy labeled dataset and small clean dataset. The key idea is that edge devices learn to assign the local weights of loss functions in noisy labeled dataset, and cooperate with central server to update global weights. Secondly, we consider a part-data network where some edge nodes exclude clean dataset, and can not compute the weights locally. The broadcasting of the global weights is added to help those edge nodes without clean dataset to reweight their noisy loss functions. Both designs have a convergence rate of O(1/T2). Simulation results illustrate that the both proposed training processes improve the prediction accuracy due to the proper weights assignments of noisy loss function. © 2013 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TNSE.2022.3227287
VL  - 10
IS  - 3
SP  - 1501
EP  - 1511
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144783924&doi=10.1109%2fTNSE.2022.3227287&partnerID=40&md5=3a73f67de23a187c9afb49c45465f69b
DB  - Scopus
KW  - machine learning
KW  - Machine learning
KW  - federated learning
KW  - Machine-learning
KW  - Federated learning
KW  - robust design
KW  - Robust designs
KW  - label noise
KW  - Convergence
KW  - Convex optimization
KW  - Distributed networks
KW  - Label noise
KW  - Loss measurement
KW  - Noise measurements
KW  - non-convex optimization
KW  - Nonconvex optimization
KW  - parallel and distributed algorithms
KW  - Parallel and distributed algorithms
KW  - Spurious signal noise
ER  - 

TY  - CONF
TI  - A Multi-layered Collaborative Framework for Evidence-driven Data Requirements Engineering for Machine Learning-based Safety-critical Systems
AU  - Dey, S.
AU  - Lee, S.-W.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - In the days of AI, data-centric machine learning (ML) models are increasingly used in various complex systems. While many researchers are focusing on specifying ML-specific performance requirements, not enough guideline is provided to engineer the data requirements systematically involving diverse stakeholders. Lack of written agreement about the training data, collaboration bottlenecks, lack of data validation framework, etc. are posing new challenges to ensuring training data fitness for safety-critical ML components. To reduce these gaps, we propose a multi-layered framework that helps to perceive and elicit data requirements. We provide a template for verifiable data requirements specifications. Moreover, we show how such requirements can facilitate an evidence-driven assessment of the training data quality based on the experts' judgments about the satisfaction of the requirements. We use Dempster Shafer's theory to combine experts' subjective opinions in the process. A preliminary case study on the CityPersons dataset for the pedestrian detection feature of autonomous cars shows the usefulness of the proposed framework for data requirements understanding and the confidence assessment of the dataset.  © 2023 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3555776.3577647
SP  - 1404
EP  - 1413
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162869276&doi=10.1145%2f3555776.3577647&partnerID=40&md5=f371778982c09e76b7bdfe7aeb0c546f
DB  - Scopus
KW  - machine learning
KW  - Machine learning
KW  - Training data
KW  - reliability
KW  - uncertainty
KW  - Machine-learning
KW  - safety
KW  - Machine learning models
KW  - Uncertainty
KW  - Safety engineering
KW  - Safety critical systems
KW  - Data centric
KW  - Collaborative framework
KW  - data requirements
KW  - Data requirements
KW  - Multi-layered
KW  - Requirement engineering
KW  - Requirements engineering
ER  - 

TY  - JOUR
TI  - TENET: a new hybrid network architecture for adversarial defense
AU  - Tuna, O.F.
AU  - Catak, F.O.
AU  - Eskil, M.T.
T2  - International Journal of Information Security
AB  - Deep neural network (DNN) models are widely renowned for their resistance to random perturbations. However, researchers have found out that these models are indeed extremely vulnerable to deliberately crafted and seemingly imperceptible perturbations of the input, referred to as adversarial examples. Adversarial attacks have the potential to substantially compromise the security of DNN-powered systems and posing high risks especially in the areas where security is a top priority. Numerous studies have been conducted in recent years to defend against these attacks and to develop more robust architectures resistant to adversarial threats. In this study, we propose a new architecture and enhance a recently proposed technique by which we can restore adversarial samples back to their original class manifold. We leverage the use of several uncertainty metrics obtained from Monte Carlo dropout (MC Dropout) estimates of the model together with the model’s own loss function and combine them with the use of defensive distillation technique to defend against these attacks. We have experimentally evaluated and verified the efficacy of our approach on MNIST (Digit), MNIST (Fashion) and CIFAR10 datasets. In our experiments, we showed that our proposed method reduces the attack’s success rate lower than 5% without compromising clean accuracy. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s10207-023-00675-1
VL  - 22
IS  - 4
SP  - 987
EP  - 1004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150178910&doi=10.1007%2fs10207-023-00675-1&partnerID=40&md5=5f8f82fcebf7de7f98e293b8a6974614
DB  - Scopus
KW  - Deep neural networks
KW  - Machine-learning
KW  - Neural network model
KW  - Uncertainty
KW  - Uncertainty analysis
KW  - Network architecture
KW  - Robustness
KW  - Loss functions
KW  - Modeling uncertainties
KW  - Model uncertainty
KW  - Adversarial machine learning
KW  - Distillation
KW  - Hybrid network
KW  - Monte carlo dropout sampling
KW  - Monte Carlo dropout sampling
KW  - Monte Carlo methods
KW  - Random perturbations
ER  - 

TY  - CONF
TI  - User Tampering in Reinforcement Learning Recommender Systems
AU  - Kasirzadeh, A.
AU  - Evans, C.
T2  - AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
AB  - In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms - 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.  © 2023 Owner/Author.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3600211.3604669
SP  - 58
EP  - 69
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172357979&doi=10.1145%2f3600211.3604669&partnerID=40&md5=799ac343804d2feac22eb741385ca0bc
DB  - Scopus
KW  - Recommender systems
KW  - Reinforcement learning
KW  - AI Ethics
KW  - Learning systems
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - User profile
KW  - Formal methods
KW  - Safety concerns
KW  - Reinforcement Learning
KW  - Recommender Systems
KW  - AI ethic
KW  - AI safety
KW  - AI Safety
KW  - Causal modeling
KW  - Formal techniques
KW  - Mitigation strategy
KW  - Recommendation algorithms
KW  - Recommendation Systems
KW  - User engagement
KW  - Value alignment
KW  - Value Alignment
ER  - 

TY  - JOUR
TI  - Curriculum learning-based fuzzy support vector machine
AU  - Chen, B.
AU  - Gao, Y.
AU  - Liu, J.
AU  - Weng, W.
AU  - Huang, J.
AU  - Fan, Y.
AU  - Lan, W.
T2  - IEEE Transactions on Fuzzy Systems
AB  - To improve the robustness of SVM models to noise and outliers, fuzzy support vector machine (FSVM) has been proposed. However, many existing FSVM models have limitations such as their dependence on assumptions, limited optimization, and unreasonable handling of noise. To address these problems, we propose a novel approach called curriculum learning-based FSVM. Our approach employs a curriculum-learning strategy, where model initially learns easy samples to avoid noise interference and obtain a good initial solution, before proceeding to learn all samples, including hard ones. To distinguish between easy and hard samples, we introduce an adaptive density-based clustering model and it is extended to kernel feature space. Moreover, we propose a slack variable-based fuzzy membership function to evaluate the importance of samples. Additionally, our model adaptively adapts the importance of samples based on feedback during the learning process. Finally, our experimental results on popular benchmarks demonstrate that our proposed model outperforms existing competitors in terms of accuracy and robustness. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TFUZZ.2023.3319170
SP  - 1
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173338514&doi=10.1109%2fTFUZZ.2023.3319170&partnerID=40&md5=228b8e7997aa7b33157da00f29d0e3a6
DB  - Scopus
KW  - Support vector machines
KW  - Robustness (control systems)
KW  - Optimisations
KW  - Optimization
KW  - Robustness
KW  - Vectors
KW  - Costs
KW  - Learning strategy
KW  - Adaptation models
KW  - Support vectors machine
KW  - Curricula
KW  - curriculum learning strategy
KW  - Curriculum learning strategy
KW  - density-based clustering
KW  - Density-based Clustering
KW  - Fans
KW  - fuzzy support vector machine
KW  - Fuzzy support vector machines
KW  - Kernel
KW  - Membership functions
KW  - noise
KW  - Noise
KW  - slack variable
KW  - Slack variables
ER  - 

TY  - CONF
TI  - A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints
AU  - Shi, M.
AU  - Liang, Y.
AU  - Shroff, N.
T2  - Proceedings of Machine Learning Research
AB  - In many applications of Reinforcement Learning (RL), it is critically important that the algorithm performs safely, such that instantaneous hard constraints are satisfied at each step, and unsafe states and actions are avoided. However, existing algorithms for “safe” RL are often designed under constraints that either require expected cumulative costs to be bounded or assume all states are safe. Thus, such algorithms could violate instantaneous hard constraints and traverse unsafe states (and actions) in practice. Hence, in this paper, we develop the first near-optimal safe RL algorithm for episodic Markov Decision Processes with unsafe states and actions under instantaneous hard constraints and the linear mixture model. It achieves a regret (Equation presented) that nearly matches the state-of-the-art regret in the setting with only unsafe actions and that in the unconstrained setting, and is safe at each step, where d is the feature-mapping dimension, K is the number of episodes, H is the episode length, and ∆c is a safety-related parameter. We also provide a lower bound (Equation presented), which indicates that the dependency on ∆c is necessary. Further, both our algorithm design and regret analysis involve several novel ideas, which may be of independent interest. © 2023 Proceedings of Machine Learning Research. All rights reserved.
DA  - 2023///
PY  - 2023
VL  - 202
SP  - 31243
EP  - 31268
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174407935&partnerID=40&md5=e136ba097637fe4a30217c735b53fb7b
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - State of the art
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Reinforcement learning algorithms
KW  - Hard constraints
KW  - Cumulative cost
KW  - Feature mapping
KW  - Linear mixture models
KW  - Near-optimal
KW  - Near-optimal algorithms
ER  - 

TY  - CONF
TI  - Towards Deep Anomaly Detection with Structured Knowledge Representations
AU  - Kirchheim, K.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Machine learning models tend to only make reliable predictions for inputs that are similar to the training data. Consequentially, anomaly detection, which can be used to detect unusual inputs, is critical for ensuring the safety of machine learning agents operating in open environments. In this work, we identify and discuss several limitations of current anomaly detection methods, such as their weak performance on tasks that require abstract reasoning, the inability to integrate background knowledge, and the opaqueness that undermines their trustworthiness in critical applications. Furthermore, we propose an architecture for anomaly detection models that aims to integrate structured knowledge representations to address these limitations. Our hypothesis is that this approach can improve performance and robustness, reduce the required resources (such as data and computation), and provide a higher degree of transparency. As a result, our work contributes to the increased safety of machine learning systems. Our code is publicly available. (https://github.com/kkirchheim/sumnist )
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-40953-0_32
VL  - 14182 LNCS
SP  - 382
EP  - 389
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172421520&doi=10.1007%2f978-3-031-40953-0_32&partnerID=40&md5=e50642e6d72d42491a1f6b39cdb01a4a
DB  - Scopus
KW  - Intelligent agents
KW  - Deep learning
KW  - Knowledge representation
KW  - Training data
KW  - Deep Learning
KW  - Learning systems
KW  - Anomaly detection
KW  - Machine-learning
KW  - Machine learning models
KW  - Anomaly Detection
KW  - Hybrid model
KW  - Hybrid Models
KW  - Knowledge-representation
KW  - Machine learning safety
KW  - Machine Learning Safety
KW  - Out-of-distribution detection
KW  - Out-of-Distribution Detection
KW  - Structured knowledge
ER  - 

TY  - JOUR
TI  - Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection
AU  - Mo, Z.
AU  - Di, X.
AU  - Shi, R.
T2  - Games
AB  - How to sample training/validation data is an important question for machine learning models, especially when the dataset is heterogeneous and skewed. In this paper, we propose a data sampling method that robustly selects training/validation data. We formulate the training/validation data sampling process as a two-player game: a trainer aims to sample training data so as to minimize the test error, while a validator adversarially samples validation data that can increase the test error. Robust sampling is achieved at the game equilibrium. To accelerate the searching process, we adopt reinforcement learning aided Monte Carlo trees search (MCTS). We apply our method to a car-following modeling problem, a complicated scenario with heterogeneous and random human driving behavior. Real-world data, the Next Generation SIMulation (NGSIM), is used to validate this method, and experiment results demonstrate the sampling robustness and thereby the model out-of-sample performance. © 2023 by the authors.
DA  - 2023///
PY  - 2023
DO  - 10.3390/g14010013
VL  - 14
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148599130&doi=10.3390%2fg14010013&partnerID=40&md5=1befbf74e2c83028ea4123204f7abeea
DB  - Scopus
KW  - reinforcement learning
KW  - car-following modeling
KW  - Monte Carlo tree search
KW  - two-player game
ER  - 

TY  - CONF
TI  - A Concept for Dynamic and Robust Machine Learning with Context Modeling for Heterogeneous Manufacturing Data
AU  - Kamm, S.
AU  - Sahlab, N.
AU  - Jazdi, N.
AU  - Weyrich, M.
T2  - Procedia CIRP
AB  - With the increasing amount of available and connected data sources, industrial automation applications such as condition monitoring of a production machine can be improved by considering various data. To gain insights from this data and make it useable, heterogeneous data has to be analyzed intensively. Limited machine learning approaches exist in industrial automation and manufacturing for analyzing data acquired from multiple sources. In this paper, first, a suitable concept for handling heterogeneous data from integration to analysis is presented as well as a multi-layer architecture for the concept's realization. The architecture encapsulates functionalities into the different layers and allows easy extendability and modifiability. Afterwards, a context modeling approach for managing heterogeneous data and existing approaches and algorithms for analyzing this data robustly and dynamically analyzing it are presented. © 2023 Elsevier B.V.. All rights reserved.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.procir.2023.06.061
VL  - 118
SP  - 354
EP  - 359
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173582100&doi=10.1016%2fj.procir.2023.06.061&partnerID=40&md5=93caaf62c655e13f5089c9dd4ee26c65
DB  - Scopus
KW  - Machine learning
KW  - Machine Learning
KW  - Data handling
KW  - Context models
KW  - Machine-learning
KW  - Machine learning approaches
KW  - Memory architecture
KW  - Condition monitoring
KW  - Context Modeling
KW  - Data-source
KW  - Dynamic and robust machine learning
KW  - Dynamic and Robust Machine Learning
KW  - Gain insight
KW  - Heterogeneous data
KW  - Heterogeneous Data
KW  - Industrial automation applications
KW  - Production-machines
ER  - 

TY  - CONF
TI  - Safe Trajectory Sampling in Model-Based Reinforcement Learning
AU  - Zwane, S.
AU  - Hadjivelichkov, D.
AU  - Luo, Y.
AU  - Bekiroglu, Y.
AU  - Kanoulas, D.
AU  - Deisenroth, M.P.
T2  - IEEE International Conference on Automation Science and Engineering
AB  - Model-based reinforcement learning aims to learn a policy to solve a target task by leveraging a learned dynamics model. This approach, paired with principled handling of uncertainty allows for data-efficient policy learning in robotics. However, the physical environment has feasibility and safety constraints that need to be incorporated into the policy before it is safe to execute on a real robot. In this work, we study how to enforce the aforementioned constraints in the context of model-based reinforcement learning with probabilistic dynamics models. In particular, we investigate how trajectories sampled from the learned dynamics model can be used on a real robot, while fulfilling user-specified safety requirements. We present a model-based reinforcement learning approach using Gaussian processes where safety constraints are taken into account without simplifying Gaussian assumptions on the predictive state distributions. We evaluate the proposed approach on different continuous control tasks with varying complexity and demonstrate how our safe trajectory-sampling approach can be directly used on a real robot without violating safety constraints.  © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/CASE56687.2023.10260496
VL  - 2023-August
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174425795&doi=10.1109%2fCASE56687.2023.10260496&partnerID=40&md5=edfccd70835d83e66fac61314c7dd926
DB  - Scopus
KW  - Reinforcement learning
KW  - Robots
KW  - Learning systems
KW  - Uncertainty
KW  - Learn+
KW  - Trajectories
KW  - Probabilistic dynamics
KW  - Dynamics
KW  - Safety constraint
KW  - Safety requirements
KW  - Model-based reinforcement learning
KW  - Dynamics models
KW  - Physical environments
KW  - Policy learning
KW  - Real robot
ER  - 

TY  - CONF
TI  - Safety Integrity Levels for Artificial Intelligence
AU  - Diemert, S.
AU  - Millet, L.
AU  - Groves, J.
AU  - Joyce, J.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Artificial Intelligence (AI) and Machine Learning (ML) technologies are rapidly being adopted to perform safety-related tasks in critical systems. These AI-based systems pose significant challenges, particularly regarding their assurance. Existing safety approaches defined in internationally recognized standards such as ISO 26262, DO-178C, UL 4600, EN 50126, and IEC 61508 do not provide detailed guidance on how to assure AI-based systems. For conventional (non-AI) systems, these standards adopt a ‘Level of Rigor’ (LoR) approach, where increasingly demanding engineering activities are required as risk associated with the system increases. This paper proposes an extension to existing LoR approaches, which considers the complexity of the task(s) being performed by an AI-based component. Complexity is assessed in terms of input entropy and output non-determinism, and then combined with the allocated Safety Integrity Level (SIL) to produce an AI-SIL. That AI-SIL may be used to identify appropriate measures and techniques for the development and verification of the system. The proposed extension is illustrated by examples from the automotive, aviation, and medical industries. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-40953-0_34
VL  - 14182 LNCS
SP  - 397
EP  - 409
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172417895&doi=10.1007%2f978-3-031-40953-0_34&partnerID=40&md5=5d73b325a3baac32df8f86740108eda6
DB  - Scopus
KW  - Artificial Intelligence
KW  - Machine learning
KW  - Machine Learning
KW  - Machine-learning
KW  - Safety engineering
KW  - Artificial intelligence learning
KW  - Safety critical systems
KW  - Critical systems
KW  - Integrity levels
KW  - ISO 26262
KW  - Machine learning technology
KW  - Safety integrity
KW  - Safety integrity level
KW  - Safety Integrity Levels
KW  - Safety-Critical Systems
KW  - Safety-Related
ER  - 

TY  - CONF
TI  - Imperative Action Masking for Safe Exploration in Reinforcement Learning
AU  - Dey, S.
AU  - Bhat, S.
AU  - Dasgupta, P.
AU  - Dey, S.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Reinforcement Learning (RL) needs sufficient exploration to learn an optimal policy. However, exploratory actions could lead the learning agent to safety hazards, not necessarily in the next state but in the future. Therefore, it is essential to evaluate each action beforehand to ensure safety. The exploratory actions and the actions proposed by the RL agent could also be unsafe during training and in the deployment phase. In this work, we have proposed the Imperative Action Masking Framework, a Graph-Plan-based method considering a finite and small look ahead to assess the safety of actions from the current state. This information is used to construct action masks on the run, filtering out the unsafe actions proposed by the RL agent (including the exploitative ones). The Graph-Plan-based method makes our framework interpretable, while the finite and small look ahead makes the proposed method scalable for larger environments. However, considering the finite and small look ahead comes with a cost of overlooking safety beyond the look ahead. We have done a comparative study against the probabilistic safety shield in Pacman and Warehouse environments approach. Our framework has produced better results in terms of both safety and reward. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-40878-6_8
VL  - 14127 LNAI
SP  - 130
EP  - 142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172194779&doi=10.1007%2f978-3-031-40878-6_8&partnerID=40&md5=d76cfee6e9ec5905292cddffea4cd7cb
DB  - Scopus
KW  - Reinforcement learning
KW  - Machine-learning
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Learning agents
KW  - Learn+
KW  - Reinforcement Learning
KW  - Optimal policies
KW  - Explainable/interpretable machine learning
KW  - Explainable/Interpretable Machine Learning
KW  - Exploration considering safety
KW  - Exploration considering Safety
KW  - Graph-plan algorithm
KW  - Graph-Plan Algorithm
KW  - Information filtering
KW  - Plan-based
ER  - 

TY  - JOUR
TI  - Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence
AU  - Li, Y.
AU  - Tian, Y.
AU  - Tong, E.
AU  - Niu, W.
AU  - Xiang, Y.
AU  - Chen, T.
AU  - Wu, Y.
AU  - Liu, J.
T2  - Tsinghua Science and Technology
AB  - Reinforcement learning (RL), one of three branches of machine learning, aims for autonomous learning and is now greatly driving the artificial intelligence development, especially in autonomous distributed systems, such as cooperative Boston Dynamics robots. However, robust RL has been a challenging problem of reliable aspects due to the gap between laboratory simulation and real world. Existing efforts have been made to approach this problem, such as performing random environmental perturbations in the learning process. However, one cannot guarantee to train with a positive perturbation as bad ones might bring failures to RL. In this work, we treat robust RL as a multi-task RL problem, and propose a curricular robust RL approach. We first present a generative adversarial network (GAN) based task generation model to iteratively output new tasks at the appropriate level of difficulty for the current policy. Furthermore, with these progressive tasks, we can realize curricular learning and finally obtain a robust policy. Extensive experiments in multiple environments demonstrate that our method improves the training stability and is robust to differences in training/test conditions.  © 1996-2012 Tsinghua University Press.
DA  - 2023///
PY  - 2023
DO  - 10.26599/TST.2021.9010076
VL  - 28
IS  - 1
SP  - 27
EP  - 38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135316788&doi=10.26599%2fTST.2021.9010076&partnerID=40&md5=394a312cdebd40ed1f5ed1d35cf53987
DB  - Scopus
L1  - https://ieeexplore.ieee.org/ielx7/5971803/9836992/09837021.pdf
KW  - Reinforcement learning
KW  - Machine-learning
KW  - Reinforcement learnings
KW  - Intelligent robots
KW  - Iterative methods
KW  - robust reinforcement learning
KW  - Robust reinforcement learning
KW  - Autonomous distributed system
KW  - Autonomous learning
KW  - curricular learning
KW  - Curricular learning
KW  - Generative adversarial network  based model
KW  - generative adversarial network (GAN) based model
KW  - Generative adversarial networks
KW  - Network-based
KW  - Network-based modeling
KW  - Three-branch
ER  - 

TY  - CONF
TI  - Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks
AU  - Zhang, L.
AU  - Zhang, Q.
AU  - Shen, L.
AU  - Yuan, B.
AU  - Wang, X.
AU  - Tao, D.
T2  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023
AB  - Safety comes first in many real-world applications involving autonomous agents. Despite a large number of reinforcement learning (RL) methods focusing on safety-critical tasks, there is still a lack of high-quality evaluation of those algorithms that adheres to safety constraints at each decision step under complex and unknown dynamics. In this paper, we revisit prior work in this scope from the perspective of state-wise safe RL and categorize them as projection-based, recovery-based, and optimization-based approaches, respectively. Furthermore, we propose Unrolling Safety Layer (USL), a joint method that combines safety optimization and safety projection. This novel technique explicitly enforces hard constraints via the deep unrolling architecture and enjoys structural advantages in navigating the trade-off between reward improvement and constraint satisfaction. To facilitate further research in this area, we reproduce related algorithms in a unified pipeline and incorporate them into SafeRL-Kit, a toolkit that provides off-the-shelf interfaces and evaluation utilities for safety-critical tasks. We then perform a comparative study of the involved algorithms on six benchmarks ranging from robotic control to autonomous driving. The empirical results provide an insight into their applicability and robustness in learning zero-cost-return policies without task-dependent handcrafting. The project page is available at https://sites.google.com/view/saferlkit. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
DA  - 2023///
PY  - 2023
VL  - 37
SP  - 15313
EP  - 15321
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167964284&partnerID=40&md5=792e1228a61ef0eed74f6cb077dba25d
DB  - Scopus
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Learning systems
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Economic and social effects
KW  - Real-world
KW  - Critical tasks
KW  - Safety engineering
KW  - Quality control
KW  - Model free
KW  - Reinforcement learning method
KW  - Safety constraint
KW  - Evaluating models
KW  - High quality
KW  - Quality evaluation
KW  - Recovery optimizations
ER  - 

TY  - JOUR
TI  - Robust ML model ensembles via risk-driven anti-clustering of training data
AU  - Mauri, L.
AU  - Apolloni, B.
AU  - Damiani, E.
T2  - Information Sciences
AB  - In this paper, we improve the robustness of Machine Learning (ML) classifiers against training-time attacks by linking the risk of training data being tampered with to the redundancy in the ML model's design needed to prevent it. Our defense mechanism is directly applicable to classifiers' training data, without any knowledge of the specific ML model to be hardened. First, we compute the training data proximity to class separation surfaces, identified via a reference linear model. Each data point is associated with a risk index, which is used to partition the training set by an unsupervised technique. Then, we train a learner for each partition and combine the learners' output in an ensemble. Our method treats the protected ML classifier as a black box and is inherently robust to transfer attacks. Experiments show that, for data poisoning rates between 6 and 25 percent of the training set, our method is more robust compared to benchmarks and to a monolithic version of the model trained on the whole training set. Our results make a convincing case for adopting training set partitioning and ensemble generation as a stage of ML models' development and deployment lifecycle. © 2023 The Author(s)
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.ins.2023.03.085
VL  - 633
SP  - 122
EP  - 140
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150025882&doi=10.1016%2fj.ins.2023.03.085&partnerID=40&md5=85e7c63d37ea275829e2bc1b82c9fb9f
DB  - Scopus
KW  - Machine learning
KW  - Machine-learning
KW  - Classification (of information)
KW  - Life cycle
KW  - Machine learning security
KW  - Adversarial machine learning
KW  - Risk modeling
KW  - Ensemble models
KW  - Poisoning attack
KW  - Poisoning attacks
KW  - Robust ensemble model
KW  - Robust ensemble models
KW  - Set partitioning
KW  - Training set partitioning
KW  - Training sets
ER  - 

TY  - JOUR
TI  - Memory-augmented Lyapunov-based safe reinforcement learning: end-to-end safety under uncertainty
AU  - Jeddi, A.B.
AU  - Dehghani, N.L.
AU  - Shafieezadeh, A.
T2  - IEEE Transactions on Artificial Intelligence
AB  - Despite recent advances in safe reinforcement learning (RL), safety constraints are often violated at deployment; especially under extreme uncertainty in memory-based partially observable environments. To address these limitations, we propose a memory-augmented Lyapunov-based safe RL model. The primary contributions of our method include (i) an explicit memory module based on Transformers to process long time horizons of information feedback from the environment; (ii) a memory-augmented Lyapunov function to determine a safe set of policies, and (iii) an exploration module that identifies highly rewarding safe actions by characterizing the uncertainty in the environment. We evaluate the proposed model in reactive OpenAI Safety Gym and memory-based partially observable DMLab-30 environments. The results of these experiments show that the proposed method significantly outperforms state-of-the-art baselines. Specifically, our proposed method achieves the lowest constraint costs among the tested benchmarks, while delivering high returns. Moreover, we perform ablation studies that show significant contributions of the introduced Transformer-based encoder, memory-augmented Lyapunov functions, and the uncertainty-aware exploration module. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TAI.2023.3238700
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147271121&doi=10.1109%2fTAI.2023.3238700&partnerID=40&md5=66191103fffb274acb1a9974b97f7d3a
DB  - Scopus
KW  - Reinforcement learning
KW  - Safety
KW  - Task analysis
KW  - Predictive models
KW  - Job analysis
KW  - Accident prevention
KW  - Reinforcement learnings
KW  - Transformer
KW  - Uncertainty
KW  - Uncertainty analysis
KW  - Markov processes
KW  - Costs
KW  - Safe Reinforcement Learning
KW  - Constrained Markov decision process
KW  - Safe reinforcement learning
KW  - Lyapunov methods
KW  - Lyapunov's methods
KW  - Lyapunov functions
KW  - Cost benefit analysis
KW  - Constrained Markov Decision Processes
KW  - Lyapunov Functions
KW  - Lyapunov's functions
KW  - Transformers
ER  - 

TY  - JOUR
TI  - A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States
AU  - Zhou, Z.
AU  - Liu, G.
AU  - Zhou, M.
T2  - IEEE Transactions on Neural Networks and Learning Systems
AB  - Multiagent deep reinforcement learning (DRL) makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The mean-field actor-critic (MFAC) reinforcement learning is well-known in the multiagent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust MFAC (RoMFAC) reinforcement learning that has two innovations: 1) a new objective function of training actors, composed of a policy gradient function that is related to the expected cumulative discount reward on sampled clean states and an action loss function that represents the difference between actions taken on clean and adversarial states and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a game model named a state-adversarial stochastic game (SASG). Despite the Nash equilibrium of SASG may not exist, adversarial perturbations to states in the RoMFAC are proven to be defensible based on SASG. Experimental results show that RoMFAC is robust against adversarial perturbations while maintaining its competitive performance in environments without perturbations. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TNNLS.2023.3278715
SP  - 1
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161516711&doi=10.1109%2fTNNLS.2023.3278715&partnerID=40&md5=a842a020dd2078511d253e34c9126787
DB  - Scopus
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10143665&ref=
KW  - Deep learning
KW  - Game theory
KW  - Reinforcement learning
KW  - multiagent systems
KW  - Training
KW  - Perturbation methods
KW  - robustness
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Robustness (control systems)
KW  - Robustness
KW  - Markov processes
KW  - Multi agent systems
KW  - Stochastic models
KW  - Stochastic systems
KW  - Game
KW  - Actor-critic reinforcement learning
KW  - Scalability
KW  - Perturbation method
KW  - Perturbation techniques
KW  - Games
KW  - Mean-field
KW  - Mean-field actor-critic  reinforcement learning
KW  - Mean-field actor-critic (MFAC) reinforcement learning
KW  - State-adversarial stochastic game
KW  - state-adversarial stochastic game (SASG)
KW  - Stochastic game
ER  - 

TY  - JOUR
TI  - A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models
AU  - Costa, E.A.
AU  - Rebello, C.D.M.
AU  - Fontana, M.
AU  - Schnitman, L.
AU  - Nogueira, I.B.D.R.
T2  - Mathematics
AB  - Robust learning is an important issue in Scientific Machine Learning (SciML). There are several works in the literature addressing this topic. However, there is an increasing demand for methods that can simultaneously consider all the different uncertainty components involved in SciML model identification. Hence, this work proposes a comprehensive methodology for uncertainty evaluation of the SciML that also considers several possible sources of uncertainties involved in the identification process. The uncertainties considered in the proposed method are the absence of a theory, causal models, sensitivity to data corruption or imperfection, and computational effort. Therefore, it is possible to provide an overall strategy for uncertainty-aware models in the SciML field. The methodology is validated through a case study developing a soft sensor for a polymerization reactor. The first step is to build the nonlinear model parameter probability distribution (PDF) by Bayesian inference. The second step is to obtain the machine learning model uncertainty by Monte Carlo simulations. In the first step, a PDF with 30,000 samples is built. In the second step, the uncertainty of the machine learning model is evaluated by sampling 10,000 values through Monte Carlo simulation. The results demonstrate that the identified soft sensors are robust to uncertainties, corroborating the consistency of the proposed approach. © 2022 by the authors.
DA  - 2023///
PY  - 2023
DO  - 10.3390/math11010074
VL  - 11
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145902502&doi=10.3390%2fmath11010074&partnerID=40&md5=f0ff60278dc34ab43dc423f9f5cb51d3
DB  - Scopus
KW  - uncertainty
KW  - Markov Chain Monte Carlo
KW  - robust learning
KW  - scientific machine learning
ER  - 

TY  - CONF
TI  - Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle
AU  - Henriksson, J.
AU  - Ursing, S.
AU  - Erdogan, M.
AU  - Warg, F.
AU  - Thorsén, A.
AU  - Jaxing, J.
AU  - Örsmark, O.
AU  - Toftås, M.Ö.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - [Context and Motivation] The automotive industry is moving towards increased automation, where features such as automated driving systems typically include machine learning (ML), e.g. in the perception system. [Question/Problem] Ensuring safety for systems partly relying on ML is challenging. Different approaches and frameworks have been proposed, typically where the developer must define quantitative and/or qualitative acceptance criteria, and ensure the criteria are fulfilled using different methods to improve e.g., design, robustness and error detection. However, there is still a knowledge gap between quality methods and metrics employed in the ML domain and how such methods can contribute to satisfying the vehicle level safety requirements. [Principal Ideas/Results] In this paper, we argue the need for connecting available ML quality methods and metrics to the safety lifecycle and explicitly show their contribution to safety. In particular, we analyse Out-of-Distribution (OoD) detection, e.g., the frequency of novelty detection, and show its potential for multiple safety-related purposes. I.e., as (a) an acceptance criterion contributing to the decision if the software fulfills the safety requirements and hence is ready-for-release, (b) in operational design domain selection and expansion by including novelty samples into the training/development loop, and (c) as a run-time measure, e.g., if there is a sequence of novel samples, the vehicle should consider reaching a minimal risk condition. [Contribution] This paper describes the possibility to use OoD detection as a safety measure, and the potential contributions in different stages of the safety lifecycle. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-29786-1_16
VL  - 13975 LNCS
SP  - 233
EP  - 242
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152531710&doi=10.1007%2f978-3-031-29786-1_16&partnerID=40&md5=a7d664a89d78ed62ff983c2f646834a8
DB  - Scopus
KW  - Machine learning
KW  - Automation
KW  - Vehicle safety
KW  - Autonomous driving
KW  - Automated driving systems
KW  - Machine-learning
KW  - Risk assessment
KW  - Safety engineering
KW  - Life cycle
KW  - Automotive safety
KW  - Safety requirements
KW  - Out-of-distribution detection
KW  - C (programming language)
KW  - Acceptance criteria
KW  - Out-of-Distribution detection
KW  - Quality methods
KW  - Quality metrices
KW  - Safety lifecycle
ER  - 

TY  - CONF
TI  - Data Banzhaf: A Robust Data Valuation Framework for Machine Learning
AU  - Wang, J.T.
AU  - Jia, R.
T2  - Proceedings of Machine Learning Research
AB  - Data valuation has wide use cases in machine learning, including improving data quality and creating economic incentives for data sharing. This paper studies the robustness of data valuation to noisy model performance scores. Particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the Shapley value and the Leave-one-out error) to produce inconsistent data value rankings across different runs. To address this challenge, we introduce the concept of safety margin, which measures the robustness of a data value notion. We show that the Banzhaf value, a famous value notion that originated from cooperative game theory literature, achieves the largest safety margin among all semivalues (a class of value notions that satisfy crucial properties entailed by ML applications and include the famous Shapley value and Leave-one-out error). We propose an algorithm to efficiently estimate the Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation demonstrates that the Banzhaf value outperforms the existing semivalue-based data value notions on several ML tasks such as learning with weighted samples and noisy label detection. Overall, our study suggests that when the underlying ML algorithm is stochastic, the Banzhaf value is a promising alternative to the other semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality. Copyright © 2023 by the author(s)
DA  - 2023///
PY  - 2023
VL  - 206
SP  - 6388
EP  - 6421
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165146592&partnerID=40&md5=a821d12295cd52facd652e9c94031f07
DB  - Scopus
KW  - Game theory
KW  - Machine learning
KW  - Machine-learning
KW  - Data quality
KW  - Computation theory
KW  - Stochastic systems
KW  - Gradient methods
KW  - Robust datum
KW  - Banzhaf value
KW  - Data values
KW  - Economic incentive
KW  - Leave one out errors
KW  - Safety margin
KW  - Semivalue
KW  - Shapley value
ER  - 

TY  - JOUR
TI  - Robust and verifiable privacy federated learning
AU  - Lu, Z.
AU  - Lu, S.
AU  - Tang, X.
AU  - Wu, J.
T2  - IEEE Transactions on Artificial Intelligence
AB  - Federated Learning (FL) safeguards user privacy by uploading gradients instead of raw data. However, inference attacks can reconstruct raw data using gradients uploaded by users in FL. To mitigate this issue, researchers have combined privacy computing techniques with FL. However, these tech-niques may not ensure the Byzantine robustness of aggregation or the integrity of the aggregated outcomes. Most current robust privacy FL methods assess differences between gradients and benchmarks in the direction, allowing adversaries to poison the aggregation against the magnitude. Furthermore, these methods cannot ensure the integrity of the aggregation results. To over-come these challenges, this study proposes a novel algorithm, Robust and Verifiable Privacy Federated Learning(RVPFL), which can more effectively eliminate the poisoning attack of the opponent by measuring the direction and magnitude of the gradient in the ciphertext state. The proposed algorithm guarantees the integrity of server aggregation results while safe-guarding user privacy. In this study, comprehensive theoretical analysis and experimental validation of RVPFL are conducted to demonstrate its superiority. The proposed RVPFL algorithm solves the Byzantine robustness problem of aggregation and the integrity problem of aggregation results, which helps to research and develop more robust and effective privacy-preserving federal learning techniques. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TAI.2023.3309273
SP  - 1
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169701947&doi=10.1109%2fTAI.2023.3309273&partnerID=40&md5=8dc0334ff83737a2d178d275de67ec8a
DB  - Scopus
KW  - Artificial intelligence
KW  - Mathematical models
KW  - Privacy
KW  - Learning systems
KW  - Learning algorithms
KW  - Robustness
KW  - Servers
KW  - Privacy-preserving techniques
KW  - Benchmark testing
KW  - Federal learning
KW  - Ho-momorphic encryptions
KW  - Homomorphic encryption
KW  - Homomorphic-encryptions
KW  - Model poisoning
KW  - Privacy protection
KW  - Privacy Protection
KW  - Robust ag-gregation
KW  - Verifiable integrity
KW  - Verifiable Integrity
ER  - 

TY  - JOUR
TI  - Joint Adversarial Domain Adaptation With Structural Graph Alignment
AU  - Wang, M.
AU  - Chen, J.
AU  - Wang, Y.
AU  - Wang, S.
AU  - li, L.
AU  - Su, H.
AU  - Gong, Z.
AU  - Wu, K.
AU  - Chen, Z.
T2  - IEEE Transactions on Network Science and Engineering
AB  - Generative adversarial networks as a powerful technique is also used in domain adaptation (DA) problem. Existing adversarial DA methods mainly conduct domain-wise alignment to alleviate marginal distribution shift between the two domains, while it may damage latent discriminative structure hidden in data feature space and cause negative transfer accordingly. To handle this problem, we propose a joint adversarial domain adaptation method with structural graph alignment to minimize joint distribution bias by further realizing class-wise matching (conditional distribution shift) based on a simple sampling strategy except for the domain-wise alignment, and validate that simultaneously considering these two types of shift can approximately reduce the joint distribution bias. To explore transferable structural information and realize more sufficient transfer for DA problem, we propose to align structural graphs between the two domains which is also based on a simple sampling strategy. Notably, the structural graph describes the relationship between each two samples and it is computed on two domains. As such, we can learn new feature representation of the two domains which are more discriminative and transferable to benefit a cross-domain classification task desirably. Finally, we design a number of experiments to evaluate our approach on four public cross-domain benchmark datasets including standard and large-scale ones, and empirical results show that the proposed model can outperform compared state-of-the-art methods. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TNSE.2023.3302574
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167778383&doi=10.1109%2fTNSE.2023.3302574&partnerID=40&md5=b1d6a78515a4f98172a249841a67ea69
DB  - Scopus
KW  - Task analysis
KW  - Feature extraction
KW  - Training
KW  - Standards
KW  - Learning systems
KW  - Features extraction
KW  - Job analysis
KW  - Machine-learning
KW  - Adaptation models
KW  - Adversarial machine learning
KW  - Domain adaptation
KW  - Generative adversarial networks
KW  - conditional distribution
KW  - Conditional distribution
KW  - joint adversarial domain adaptation
KW  - Joint adversarial domain adaptation
KW  - joint distribution
KW  - Joint distributions
KW  - marginal distribution
KW  - Marginal distribution
KW  - Shape
KW  - Structural graph
KW  - structural graph alignment
KW  - Structural graph alignment
ER  - 

TY  - JOUR
TI  - DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination
AU  - Wu, T.
AU  - Ding, X.
AU  - Zhang, H.
AU  - Gao, J.
AU  - Tang, M.
AU  - Du, L.
AU  - Qin, B.
AU  - Liu, T.
T2  - IEEE Transactions on Multimedia
AB  - Given data with label noise (i.e., incorrect data), deep neural networks would gradually memorize the label noise and impair model performance. To relieve this issue, curriculum learning is proposed to improve model performance and generalization by ordering training samples in a meaningful (e.g., easy to hard) sequence. Previous work takes incorrect samples as generic hard ones without discriminating between hard samples (i.e., hard samples in correct data) and incorrect samples. Indeed, a model should learn from hard samples to promote generalization rather than overfit to incorrect ones. In this paper, we address this problem by appending a novel loss function <italic>DiscrimLoss</italic>, on top of the existing task loss. Its main effect is to automatically and stably estimate the importance of easy samples and difficult samples (including hard and incorrect samples) at the early stages of training to improve the model performance. Then, during the following stages, DiscrimLoss is dedicated to discriminating between hard and incorrect samples to improve the model generalization. Such a training strategy can be formulated dynamically in a self-supervised manner, effectively mimicking the main principle of curriculum learning. Experiments on image classification, image regression, text sequence regression, and event relation reasoning demonstrate the versatility and effectiveness of our method, particularly in the presence of diversified noise levels. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TMM.2023.3290477
SP  - 1
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163478229&doi=10.1109%2fTMM.2023.3290477&partnerID=40&md5=304b809823e78a2e0f99c1366b6f2143
DB  - Scopus
KW  - Deep learning
KW  - Machine learning
KW  - Task analysis
KW  - Predictive models
KW  - Deep neural networks
KW  - Data models
KW  - Training
KW  - Job analysis
KW  - Machine-learning
KW  - Estimation
KW  - Text processing
KW  - Switches
KW  - Modeling performance
KW  - Noise measurement
KW  - Label noise
KW  - Noise measurements
KW  - Curricula
KW  - Model generalization
KW  - Noisy label
KW  - Noisy labels
KW  - Robust methods
ER  - 

TY  - JOUR
TI  - Toward Improved Reliability of Deep Learning Based Systems Through Online Relabeling of Potential Adversarial Attacks
AU  - Al-Maliki, S.
AU  - Bouanani, F.E.
AU  - Ahmad, K.
AU  - Abdallah, M.
AU  - Hoang, D.T.
AU  - Niyato, D.
AU  - Al-Fuqaha, A.
T2  - IEEE Transactions on Reliability
AB  - Deep neural networks have shown vulnerability to well-designed inputs called adversarial examples. Researchers in industry and academia have proposed many adversarial example defense techniques. However, they offer partial but not full robustness. Thus, complementing them with another layer of protection is a must, especially for mission-critical applications. This article proposes a novel online selection and relabeling algorithm (OSRA) that opportunistically utilizes a limited number of crowdsourced workers to maximize the machine learning (ML) system&#x0027;s robustness. The OSRA strives to use crowdsourced workers effectively by selecting the most suspicious inputs and moving them to the crowdsourced workers to be validated and corrected. As a result, the impact of adversarial examples gets reduced, and accordingly, the ML system becomes more robust. We also proposed a heuristic threshold selection method that contributes to enhancing the prediction system&#x0027;s reliability. We empirically validated our proposed algorithm and found that it can efficiently and optimally utilize the allocated budget for crowdsourcing. It is also effectively integrated with a state-of-the-art black box defense technique, resulting in a more robust system. Simulation results show that the OSRA can outperform a random selection algorithm by 60&#x0025; and achieve comparable performance to an optimal offline selection benchmark. They also show that OSRA&#x0027;s performance has a positive correlation with system robustness. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TR.2023.3298685
SP  - 1
EP  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168260380&doi=10.1109%2fTR.2023.3298685&partnerID=40&md5=49a3ddb8a11f11d30bdb6f3d9a3c89bd
DB  - Scopus
KW  - security
KW  - Security
KW  - Task analysis
KW  - Deep neural networks
KW  - Training
KW  - Perturbation methods
KW  - Online systems
KW  - Machine-learning
KW  - Network security
KW  - Benchmarking
KW  - Optimization
KW  - Robustness
KW  - Heuristic methods
KW  - Mission critical systems
KW  - Reliability analysis
KW  - Perturbation method
KW  - Perturbation techniques
KW  - adversarial machine learning
KW  - Adversarial machine learning
KW  - Adversarial example
KW  - evasion attacks
KW  - Adversarial defense
KW  - Evasion attack
KW  - adversarial examples
KW  - Budget control
KW  - crowdsourcing
KW  - Crowdsourcing
KW  - History
KW  - online relabeling
KW  - Online relabeling
KW  - Relabelling
KW  - Reliable deep learning system
KW  - reliable deep learning systems
ER  - 

TY  - JOUR
TI  - Counterfactual learning in enhancing resilience in autonomous agent systems
AU  - Samarasinghe, D.
T2  - Frontiers in Artificial Intelligence
AB  - Resilience in autonomous agent systems is about having the capacity to anticipate, respond to, adapt to, and recover from adverse and dynamic conditions in complex environments. It is associated with the intelligence possessed by the agents to preserve the functionality or to minimize the impact on functionality through a transformation, reconfiguration, or expansion performed across the system. Enhancing the resilience of systems could pave way toward higher autonomy allowing them to tackle intricate dynamic problems. The state-of-the-art systems have mostly focussed on improving the redundancy of the system, adopting decentralized control architectures, and utilizing distributed sensing capabilities. While machine learning approaches for efficient distribution and allocation of skills and tasks have enhanced the potential of these systems, they are still limited when presented with dynamic environments. To move beyond the current limitations, this paper advocates incorporating counterfactual learning models for agents to enable them with the ability to predict possible future conditions and adjust their behavior. Counterfactual learning is a topic that has recently been gaining attention as a model-agnostic and post-hoc technique to improve explainability in machine learning models. Using counterfactual causality can also help gain insights into unforeseen circumstances and make inferences about the probability of desired outcomes. We propose that this can be used in agent systems as a means to guide and prepare them to cope with unanticipated environmental conditions. This supplementary support for adaptation can enable the design of more intelligent and complex autonomous agent systems to address the multifaceted characteristics of real-world problem domains. Copyright © 2023 Samarasinghe.
DA  - 2023///
PY  - 2023
DO  - 10.3389/frai.2023.1212336
VL  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167886649&doi=10.3389%2ffrai.2023.1212336&partnerID=40&md5=b8c838d98f3d1ffd5dc99ec1d8cba075
DB  - Scopus
L1  - https://www.frontiersin.org/articles/10.3389/frai.2023.1212336/pdf?isPublishedV2=False
KW  - machine learning
KW  - explainability
KW  - robustness
KW  - resilience
KW  - autonomous agent systems
KW  - counterfactual learning
KW  - explainable agents
KW  - multi-agent system (MAS)
ER  - 

TY  - JOUR
TI  - A Novel Composite Graph Neural Network
AU  - Liu, Z.
AU  - Yang, J.
AU  - Zhong, X.
AU  - Wang, W.
AU  - Chen, H.
AU  - Chang, Y.
T2  - IEEE Transactions on Neural Networks and Learning Systems
AB  - Graph neural networks (GNNs) have achieved great success in many fields due to their powerful capabilities of processing graph-structured data. However, most GNNs can only be applied to scenarios where graphs are known, but real-world data are often noisy or even do not have available graph structures. Recently, graph learning has attracted increasing attention in dealing with these problems. In this article, we develop a novel approach to improving the robustness of the GNNs, called composite GNN. Different from existing methods, our method uses composite graphs (C-graphs) to characterize both sample and feature relations. The C-graph is a unified graph that unifies these two kinds of relations, where edges between samples represent sample similarities, and each sample has a tree-based feature graph to model feature importance and combination preference. By jointly learning multiaspect C-graphs and neural network parameters, our method improves the performance of semisupervised node classification and ensures robustness. We conduct a series of experiments to evaluate the performance of our method and the variants of our method that only learn sample relations or feature relations. Extensive experimental results on nine benchmark datasets demonstrate that our proposed method achieves the best performance on almost all the datasets and is robust to feature noises. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/TNNLS.2023.3268766
SP  - 1
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160246263&doi=10.1109%2fTNNLS.2023.3268766&partnerID=40&md5=a81f79fc707d9e9221429e3840d4e9b2
DB  - Scopus
KW  - Learning systems
KW  - Graph neural network
KW  - Data handling
KW  - Prediction methods
KW  - Robustness (control systems)
KW  - Benchmarking
KW  - Robustness
KW  - Graphic methods
KW  - Learning (artificial intelligence)
KW  - Trees (mathematics)
KW  - Noise measurement
KW  - Graph neural networks
KW  - Noise measurements
KW  - Aggregates
KW  - Composite graph
KW  - Composite graph (C-graph)
KW  - graph neural networks (GNNs)
KW  - sample graph
KW  - Sample graphs
KW  - Tree-based
KW  - tree-based feature graph
KW  - Tree-based feature graph
ER  - 

TY  - JOUR
TI  - An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML
AU  - Gittens, A.
AU  - Yener, B.
AU  - Yung, M.
T2  - IEEE Access
AB  - Model accuracy is the traditional metric employed in machine learning (ML) applications. However, privacy, fairness, and robustness guarantees are crucial as ML algorithms increasingly pervade our lives and play central roles in socially important systems. These four desiderata constitute the pillars of Trustworthy ML (TML) and may mutually inhibit or reinforce each other. It is necessary to understand and clearly delineate the trade-offs among these desiderata in the presence of adversarial attacks. However, threat models for the desiderata are different and the defenses introduced for each leads to further trade-offs in a multilateral adversarial setting (i.e., a setting attacking several pillars simultaneously). The first half of the paper reviews the state of the art in TML research, articulates known multilateral trade-offs, and identifies open problems and challenges in the presence of an adversary that may take advantage of such multilateral trade-offs. The fundamental shortcomings of statistical association-based TML are discussed, to motivate the use of causal methods to achieve TML. The second half of the paper, in turn, advocates the use of causal modeling in TML. Evidence is collected from across the literature that causal ML is well-suited to provide a unified approach to TML. Causal discovery and causal representation learning are introduced as essential stages of causal modeling, and a new threat model for causal ML is introduced to quantify the vulnerabilities introduced through the use of causal methods. The paper concludes with pointers to possible next steps in the development of a causal TML pipeline. © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3218715
VL  - 10
SP  - 120850
EP  - 120865
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141622799&doi=10.1109%2fACCESS.2022.3218715&partnerID=40&md5=9035238f9011623885d10467067c26b9
DB  - Scopus
L1  - https://ieeexplore.ieee.org/ielx7/6287639/9668973/09933776.pdf?tp=&arnumber=9933776&isnumber=9668973&ref=
KW  - machine learning
KW  - privacy
KW  - Security
KW  - Artificial intelligence
KW  - Predictive models
KW  - Fairness
KW  - Privacy
KW  - fairness
KW  - Learning systems
KW  - trustworthy machine learning
KW  - Machine-learning
KW  - Economic and social effects
KW  - Representation learning
KW  - Robustness
KW  - Data structures
KW  - Commerce
KW  - Data privacy
KW  - Causal modeling
KW  - adversarial robustness
KW  - Adversarial robustness
KW  - causal machine learning
KW  - Causal machine learning
KW  - causal models
KW  - causal representation
KW  - Causal representation
KW  - Trustworthy machine learning
ER  - 

TY  - JOUR
TI  - A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios
AU  - Perales Gomez, A.L.
AU  - Maimo, L.F.
AU  - Clemente, F.J.G.
AU  - Morales, J.A.M.
AU  - Celdran, A.H.
AU  - Bovet, G.
T2  - IEEE Access
AB  - Anomaly Detection systems based on Machine and Deep learning are the most promising solutions to detect cyberattacks in the industry. However, these techniques are vulnerable to adversarial attacks that downgrade prediction performance. Several techniques have been proposed to measure the robustness of Anomaly Detection in the literature. However, they do not consider that, although a small perturbation in an anomalous sample belonging to an attack, i.e., Denial of Service, could cause it to be misclassified as normal while retaining its ability to damage, an excessive perturbation might also transform it into a truly normal sample, with no real impact on the industrial system. This paper presents a methodology to calculate the robustness of Anomaly Detection models in industrial scenarios. The methodology comprises four steps and uses a set of additional models called support models to determine if an adversarial sample remains anomalous. We carried out the validation using the Tennessee Eastman process, a simulated testbed of a chemical process. In such a scenario, we applied the methodology to both a Long-Short Term Memory (LSTM) neural network and 1-dimensional Convolutional Neural Network (1D-CNN) focused on detecting anomalies produced by different cyberattacks. The experiments showed that 1D-CNN is significantly more robust than LSTM for our testbed. Specifically, a perturbation of 60% (empirical robustness of 0.6) of the original sample is needed to generate adversarial samples for LSTM, whereas in 1D-CNN the perturbation required increases up to 111% (empirical robustness of 1.11).  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3224930
VL  - 10
SP  - 124582
EP  - 124594
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144075247&doi=10.1109%2fACCESS.2022.3224930&partnerID=40&md5=6d19be1d6aa20ba9d211e1995bbbca85
DB  - Scopus
KW  - machine learning
KW  - Deep learning
KW  - deep learning
KW  - robustness
KW  - Machine-learning
KW  - Convolutional neural network
KW  - Robustness (control systems)
KW  - Long short-term memory
KW  - Robustness
KW  - Robust control
KW  - Adversarial attack
KW  - Adversarial attacks
KW  - Cyber-attacks
KW  - evasion attacks
KW  - Evasion attack
KW  - Testbeds
KW  - Anomaly detector
KW  - industrial control systems
KW  - Industrial control systems
KW  - Industrial scenarios
ER  - 

TY  - CONF
TI  - Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts
AU  - Hiett, B.
AU  - Boyd, P.
AU  - Fletcher, C.
AU  - Gowland, S.
AU  - Sharp, J.H.
AU  - Sloggett, D.
AU  - Banks, A.
T2  - Proceedings of SPIE - The International Society for Optical Engineering
AB  - Applied research presented in this paper describes an approach to provide meaningful evaluation of the Machine Learning (ML) components in a Full Motion Video (FMV) Machine Learning Enabled System (MLES). The MLES itself is not discussed in the paper. We focus on the experimental activity that has been designed to provide confidence that the MLES, when fielded under dynamic and uncertain conditions, performance will not be undermined by a lack of ML robustness. For example, to real-world changes of the same scene under differing light conditions. The paper details the technical approach and how it is applied to data, across the overall experimental pipeline, consisting of a perturbation engine, test pipeline and metric production. Data is from a small imagery dataset and the results are shown and discussed as part of a proof of concept study. © 2022 SPIE.
DA  - 2022///
PY  - 2022
DO  - 10.1117/12.2638492
VL  - 12276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145198838&doi=10.1117%2f12.2638492&partnerID=40&md5=0fe9bf39ca17fb32f5989749b1568d7c
DB  - Scopus
KW  - Machine learning
KW  - Machine Learning
KW  - Pipelines
KW  - Machine-learning
KW  - Object detection
KW  - Real-world
KW  - Machine components
KW  - Objects detection
KW  - Object Detection
KW  - Robustness evaluation
KW  - Performance metrices
KW  - Applied research
KW  - Experimental activities
KW  - Full motion video
KW  - Meaningful perturbation
KW  - Meaningful Perturbations
KW  - Performance Metrics
KW  - Verification & validation
KW  - Verification & Validation
ER  - 

TY  - CONF
TI  - A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems
AU  - Qi, Y.
AU  - Conmy, P.R.
AU  - Huang, W.
AU  - Zhao, X.
AU  - Huang, X.
T2  - CEUR Workshop Proceedings
AB  - Hazard and Operability Analysis (HAZOP) is a powerful safety analysis technique with a long history in industrial process control domain. With the increasing use of Machine Learning (ML) components in cyber physical systems—so called Learning-Enabled Systems (LESs), there is a recent trend of applying HAZOP-like analysis to LESs. While it shows a great potential to reserve the capability of doing sufficient and systematic safety analysis, there are new technical challenges raised by the novel characteristics of ML that require retrofit of the conventional HAZOP technique. In this regard, we present a new Hierarchical HAZOP-Like method for LESs (HILLS). To deal with the complexity of LESs, HILLS first does “divide and conquer” by stratifying the whole system into three levels, and then proceeds HAZOP on each level to identify (latent-)hazards, causes, security threats and mitigation (with new nodes and guide words). Finally, HILLS attempts at linking and propagating the causal relationship among those identified elements within and across the three levels via both qualitative and quantitative methods. We examine and illustrate the utility of HILLS by a case study on Autonomous Underwater Vehicles, with discussions on assumptions and extensions to real-world applications. HILLS, as a first HAZOP-like attempt on LESs that explicitly considers ML internal behaviours and its interactions with other components, not only uncovers the inherent difficulties of doing safety analysis for LESs, but also demonstrates a good potential to tackle them. © 2022 Copyright for this paper by its authors.
DA  - 2022///
PY  - 2022
VL  - 3215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139381061&partnerID=40&md5=80164f44c1c28764025cd0045f7ff701
DB  - Scopus
KW  - Autonomous vehicles
KW  - Machine learning
KW  - Trustworthy AI
KW  - trustworthy AI
KW  - Learning systems
KW  - Cyber-physical systems
KW  - Accident prevention
KW  - Machine-learning
KW  - Hierarchical systems
KW  - Autonomous underwater vehicles
KW  - Embedded systems
KW  - Hazards
KW  - Cybe-physical systems
KW  - Cyber Physical System
KW  - machine learning security
KW  - Machine learning security
KW  - Learning-enabled system
KW  - Robotic and autonomous system
KW  - AI safety
KW  - Safety analysis
KW  - autonomous underwater vehicle
KW  - Autonomous underwater vehicles]
KW  - cyber physical system
KW  - deviation analysis
KW  - Deviation analysis
KW  - Hazard and operability analysis
KW  - hazard identification
KW  - Hazard identification
KW  - HAZOP
KW  - learning-enabled system
KW  - robotics and autonomous system
ER  - 

TY  - CONF
TI  - Building Multi-Agent Environments with Theoretical Guarantees on the Learning of Ethical Policies
AU  - Rodriguez-Soto, M.
AU  - Rodriguez-Aguilar, J.A.
AU  - Lopez-Sanchez, M.
T2  - ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022
AB  - This paper tackles the open problem of value alignment in multi-agent systems. In particular, we propose an approach to build an ethical environment that guarantees that all agents in the system learn to behave ethically while pursuing their individual objectives. Our contributions are founded in the framework of Multi-Objective Multi-Agent Reinforcement Learning. Firstly, we characterise a family of Multi-Objective Markov Games (MOMGs), the so-called ethical MOMGs, for which we can formally guarantee the learning of ethical behaviours. From these, we specify the process for building single-objective ethical environments that simplify the learning in the multi-agent system. Interestingly, our theoretical results for multi-agent environments generalise recent state-of-the-art results for single-agent environments. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584282&partnerID=40&md5=b592333309eaa9bbf060db65eaa1e353
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Decision making
KW  - Multi-Agent Systems
KW  - Learning systems
KW  - Decisions makings
KW  - Reinforcement learnings
KW  - Multi agent systems
KW  - Learn+
KW  - Ethical technology
KW  - Value alignment
KW  - Value Alignment
KW  - Multi objective
KW  - Markov games
KW  - Moral decision making
KW  - Moral Decision Making
KW  - Multi-agent environment
KW  - Multi-objective reinforcement learning
KW  - Multi-Objective Reinforcement Learning
KW  - Theoretical guarantees
ER  - 

TY  - JOUR
TI  - Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems
AU  - Wang, S.
AU  - Cao, Y.
AU  - Chen, X.
AU  - Yao, L.
AU  - Wang, X.
AU  - Sheng, Q.Z.
T2  - Frontiers in Big Data
AB  - Adversarial attacks, e.g., adversarial perturbations of the input and adversarial samples, pose significant challenges to machine learning and deep learning techniques, including interactive recommendation systems. The latent embedding space of those techniques makes adversarial attacks challenging to detect at an early stage. Recent advance in causality shows that counterfactual can also be considered one of the ways to generate the adversarial samples drawn from different distribution as the training samples. We propose to explore adversarial examples and attack agnostic detection on reinforcement learning (RL)-based interactive recommendation systems. We first craft different types of adversarial examples by adding perturbations to the input and intervening on the casual factors. Then, we augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our white-box detector trained with one crafting method has the generalization ability over several other crafting methods. Copyright © 2022 Wang, Cao, Chen, Yao, Wang and Sheng.
DA  - 2022///
PY  - 2022
DO  - 10.3389/fdata.2022.822783
VL  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132621553&doi=10.3389%2ffdata.2022.822783&partnerID=40&md5=eaceb56da44a54961dfc1c1089624dea
DB  - Scopus
L1  - https://www.frontiersin.org/articles/10.3389/fdata.2022.822783/pdf?isPublishedV2=False
KW  - robustness
KW  - adversarial attack
KW  - deep learning—artificial neural network (DL-ANN)
KW  - deep reinforcement learning (deep RL)
KW  - recommender systems (RS)
ER  - 

TY  - CONF
TI  - An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection
AU  - Ebrahimi, M.R.
AU  - Li, W.
AU  - Chai, Y.
AU  - Pacheco, J.
AU  - Chen, H.
T2  - IEEE International Conference on Data Mining Workshops, ICDMW
AB  - Empowered by the recent development in Ma-chine Learning (ML), signatureless ML-based malware detectors present promising performance in identifying unseen mal ware variants and zero days without requiring expensive dynamic malware analysis. However, it has been recently shown that ML-based malware detectors are vulnerable to adversarial malware attacks, in which an attacker modifies a known malware exe-cutable to trick the malware detector into recognizing the modi-fied variant as benign. Adversarial malware example generation has become an emerging area in adversarial ML that studies creating functionality-preserving adversarial malware variants. Advancements in this area have led to an eternal game between the adversary and defender. While the area has attracted much attention in the security community, a large body of these studies merely focuses on attack methods against ML-based malware detectors. There has been little work on understanding how these adversarial variants can be systematically used by the defender to strengthen the robustness of these detectors and stand ahead of the adversary. Latest efforts have led to emergence of adversarial learning. In this work, we propose a simple wargame approach to empirically conduct the adversarial minimax optimization underlying in the adversarial learning for improving the robustness of ML-based malware detectors. Our proposed approach employs adversarial malware variants generated from a reinforcement learning-based adversarial attack policy in a minimax game alternating between strengthening the attack policy and improving the detectors' robustness. We evaluated the effectiveness of our approach on a testbed with 33.2 GB working malware collected from VirusTotal. Despite the sub-optimal nature of our method, it was able to surprisingly enhance the robustness of three known open-source ML-based malware detectors (LGBM, MalConv, and NonNeg) against the adversarial malware variants by 4, 7, and 11 times, respectively.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICDMW58026.2022.00079
VL  - 2022-November
SP  - 567
EP  - 576
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148436341&doi=10.1109%2fICDMW58026.2022.00079&partnerID=40&md5=fa5afef0860ddc17f4b63e0796529524
DB  - Scopus
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10031124&ref=
KW  - Reinforcement learning
KW  - Learning systems
KW  - Machine-learning
KW  - Reinforcement learnings
KW  - Optimization
KW  - Malware
KW  - Malwares
KW  - Adversarial learning
KW  - adversarial learning
KW  - adversarial robustness
KW  - Adversarial robustness
KW  - Adversarial malware variant
KW  - adversarial malware variants
KW  - adversarial minimax game
KW  - Adversarial minimax game
KW  - machine learning-based malware detection
KW  - Machine learning-based malware detection
KW  - Malware detection
KW  - Minimax games
ER  - 

TY  - JOUR
TI  - Towards a robust and trustworthy machine learning system development: An engineering perspective
AU  - Xiong, P.
AU  - Buffett, S.
AU  - Iqbal, S.
AU  - Lamontagne, P.
AU  - Mamun, M.
AU  - Molyneaux, H.
T2  - Journal of Information Security and Applications
AB  - While Machine Learning (ML) technologies are widely adopted in many mission critical fields to support intelligent decision-making, concerns remain about system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness from a security engineering perspective, focusing on the problems in system threat analysis, design and evaluation faced in developing practical machine learning applications, in terms of robustness and user trust. Accordingly, we organize the presentation of this survey intended to facilitate the convey of the body of knowledge from this angle. We then describe a metamodel we created that represents the body of knowledge in a standard and visualized way. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process which extends and scales up the classic process. Finally, we propose the future research directions motivated by our findings. Our work differs itself from the existing surveys by (i) exploring the fundamental principles and best practices to support robust and trustworthy ML system development, and (ii) studying the interplay of robustness and user trust in the context of ML systems. We expect this survey provides a big picture for machine learning security practitioners. © 2022
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jisa.2022.103121
VL  - 65
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124314035&doi=10.1016%2fj.jisa.2022.103121&partnerID=40&md5=fa8bf24ca832c86ff0ff8108655259bf
DB  - Scopus
KW  - Machine learning
KW  - Decision making
KW  - Machine-learning
KW  - Surveys
KW  - Privacy preserving
KW  - Network security
KW  - Engineering education
KW  - Machine learning systems
KW  - Privacy-preserving techniques
KW  - Adversarial sampling and countermeasure
KW  - Adversarial sampling and countermeasures
KW  - Engineering perspective
KW  - Privacy-preserving machine learning
KW  - Robustness of machine learning
KW  - Secure machine learning system development
KW  - System development
KW  - User trust
ER  - 

TY  - CONF
TI  - A Model Selection Approach for Corruption Robust Reinforcement Learning
AU  - Wei, C.-Y.
AU  - Dann, C.
AU  - Zimmert, J.
T2  - Proceedings of Machine Learning Research
AB  - We develop a model selection approach to tackle reinforcement learning with adversarial corruption in both transition and reward. For finite-horizon tabular MDPs, without prior knowledge on the total amount of corruption, our algorithm achieves a regret bound of (Equation presented) where T is the number of episodes, C is the total amount of corruption, and ∆ is the reward gap between the best and the second-best policy. This is the first worst-case optimal bound achieved without knowledge of C, improving previous results of Lykouris et al. (2021); Chen et al. (2021b); Wu et al. (2021). For finite-horizon linear MDPs, we develop a computationally efficient algorithm with a regret bound of Oe(p(1 + C)T), and another computationally inefficient one with Oe(√T + C), improving the result of Lykouris et al. (2021) and answering an open question by Zhang et al. (2021b). Finally, our model selection framework can be easily applied to other settings including linear bandits, linear contextual bandits, and MDPs with general function approximation, leading to several improved or new results. © 2022 C.-Y. Wei, C. Dann & J. Zimmert.
DA  - 2022///
PY  - 2022
VL  - 167
SP  - 1043
EP  - 1096
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163731154&partnerID=40&md5=be608eb3822f8bbe3c87072cb3e0d7fd
DB  - Scopus
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Crime
KW  - Optimization
KW  - Prior-knowledge
KW  - Computationally efficient
KW  - Model Selection
KW  - Contextual banditti
KW  - Finite horizons
KW  - Optimal bounds
KW  - Regret bounds
KW  - Second-best policy
KW  - Selection framework
ER  - 

TY  - CONF
TI  - Data Efficient Safe Reinforcement Learning Algorithm
AU  - Padakandla, S.
T2  - ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022
AB  - Applying reinforcement learning (RL) methods for real world applications poses multiple challenges - the foremost being safety of the physical system controlled by the learning agent and the learning efficiency. A RL agent learns to control a system by exploring available actions. In some operating states, when the RL agent exercises an exploratory action, the system may enter unsafe operation, which can lead to safety hazards both for the system as well as for humans supervising the system. RL algorithms thus need to respect these safety constraints and must do so with limited available information. In our work, we formulate this problem in the constrained off-policy setting that facilitates safe exploration by the RL agent. Further, we develop a sample efficient algorithm by adapting the cross-entropy method. The proposed algorithm’s safety performance is evaluated numerically on benchmark RL problems. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586002&partnerID=40&md5=360c0b6fe4e7ad5c29d0651567b520d8
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Learning systems
KW  - Learning algorithms
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Physical systems
KW  - Real-world
KW  - Benchmarking
KW  - Learning agents
KW  - Safe Reinforcement Learning
KW  - Safe reinforcement learning
KW  - Reinforcement learning method
KW  - Reinforcement learning algorithms
KW  - Cross-entropy method
KW  - Cross-Entropy Method
KW  - Off-policy
ER  - 

TY  - JOUR
TI  - A robust double-parallel extreme learning machine based on an improved M-estimation algorithm
AU  - Zha, L.
AU  - Ma, K.
AU  - Li, G.
AU  - Fang, Q.
AU  - Hu, X.
T2  - Advanced Engineering Informatics
AB  - To solve the problem of improving the regression accuracy and model stability of the extreme learning machine(ELM), a new approach based on an improved M-estimation optimized double-parallel extreme learning machine is proposed in this study, namely robust double-parallel extreme learning machine(RD-ELM). Firstly, RD-ELM is constructed with a double parallel forward structure, thus the information can be received from both hidden layer neurons and input layer neurons. Secondly, we use an improved M-estimation to calculate output weights of neural network by iteratively reweighted Least-Squares Estimation(LSE), with weights assigned by the least absolute residual estimation of the samples. Finally, we establish a regression prediction model utilized to test the goodness of fit in a SinC function and verify the regression ability in eight benchmark regression problems. Then the proposed method is applied to an actual operational condition of a power plant. Experimental results show that the proposed method can efficiently process the influence of outliers and noise with strong anti-jamming ability. Compared with other methods, RD-ELM has superior performance that is stronger robustness and better generalization performance in many benchmark data and practical experiments. © 2022 Elsevier Ltd
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.aei.2022.101606
VL  - 52
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128187488&doi=10.1016%2fj.aei.2022.101606&partnerID=40&md5=a4f5f40de5633708b4f9407e58292c83
DB  - Scopus
KW  - Machine learning
KW  - Regression analysis
KW  - Benchmarking
KW  - Knowledge acquisition
KW  - Extreme learning machine
KW  - Statistics
KW  - M-estimation
KW  - Iterative methods
KW  - Extreme learning machine (ELM)
KW  - Coal-fired boiler
KW  - Double-parallel
KW  - Estimation algorithm
KW  - Hidden layer neurons
KW  - Model stability
KW  - Neuron layers
KW  - New approaches
KW  - Outlier-robust regression
KW  - Robust regressions
ER  - 

TY  - JOUR
TI  - Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework
AU  - Li, W.
AU  - Chai, Y.
T2  - Journal of Management Information Systems
AB  - As predictive analytics increasingly applies supervised machine learning (SML) models to inform mission-critical decision-making, adversaries become incentivized to exploit the vulnerabilities of these SML models and mislead predictive analytics into erroneous decisions. Due to the limited understanding and awareness of such adversarial attacks, the predictive analytics knowledge and deployment need a principled technique for adversarial robustness assessment and enhancement. In this research, we leverage the technology threat avoidance theory as the kernel theory and propose a research framework for assessing and enhancing the adversarial robustness of predictive analytics applications. We instantiate the proposed framework by developing a robust text classification system, the ARText system. The proposed system is rigorously evaluated in comparison with benchmark methods on two tasks extensively enabled by SML: spam review detection and spam email detection, which then confirmed the utility and effectiveness of our ARText system. Results from numerous experiments revealed that our proposed framework could significantly enhance the adversarial robustness of predictive analytics applications. © 2022 Taylor & Francis Group, LLC.
DA  - 2022///
PY  - 2022
DO  - 10.1080/07421222.2022.2063549
VL  - 39
IS  - 2
SP  - 542
EP  - 572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131375989&doi=10.1080%2f07421222.2022.2063549&partnerID=40&md5=03a5b535d2a12af5782faf5ad9d9b1a6
DB  - Scopus
KW  - Decision making
KW  - Supervised learning
KW  - Decisions makings
KW  - Predictive analytics
KW  - Classification (of information)
KW  - Machine learning models
KW  - Supervised machine learning
KW  - supervised machine learning
KW  - Text processing
KW  - Robustness assessment
KW  - Design frameworks
KW  - adversarial robustness
KW  - Adversarial robustness
KW  - Text-mining
KW  - artificial intelligence security
KW  - Artificial intelligence security
KW  - design frameworks
KW  - Mission-critical decisions
KW  - text mining
KW  - Threat-avoidance theory
ER  - 

TY  - JOUR
TI  - Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems with Unknown Uncertainty
AU  - Li, J.
AU  - Ding, J.
AU  - Chai, T.
AU  - Lewis, F.L.
AU  - Jagannathan, S.
T2  - IEEE Transactions on Neural Networks and Learning Systems
AB  - This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton-Jacobi-Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.  © 2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TNNLS.2020.3027653
VL  - 33
IS  - 1
SP  - 270
EP  - 280
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122590061&doi=10.1109%2fTNNLS.2020.3027653&partnerID=40&md5=0519b95993dc7072c4748d7a8416a5be
DB  - Scopus
KW  - Reinforcement learning
KW  - simulation
KW  - uncertainty
KW  - Neural network
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - article
KW  - human
KW  - algorithm
KW  - theoretical study
KW  - Controllers
KW  - Adaptive control systems
KW  - Robust control
KW  - Unknown uncertainty
KW  - Nonlinear systems
KW  - Neural-networks
KW  - Dynamic programming
KW  - Optimal control systems
KW  - Affine nonlinear systems
KW  - robust control
KW  - reinforcement
KW  - Stability
KW  - nonlinear system
KW  - in-law
KW  - neural networks (NNs)
KW  - Bounded stability
KW  - Discrete time
KW  - Interleaved reinforcement learning
KW  - Nonlinear equations
KW  - Robust stability
KW  - uncertain systems
KW  - Uncertain systems
KW  - Uniformly ultimately bounded
ER  - 

TY  - JOUR
TI  - A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning
AU  - Wang, Z.
AU  - Chen, C.
AU  - Dong, D.
T2  - IEEE Transactions on Cybernetics
AB  - While reinforcement learning (RL) algorithms are achieving state-of-the-art performance in various challenging tasks, they can easily encounter catastrophic forgetting or interference when faced with lifelong streaming information. In this article, we propose a scalable lifelong RL method that dynamically expands the network capacity to accommodate new knowledge while preventing past memories from being perturbed. We use a Dirichlet process mixture to model the nonstationary task distribution, which captures task relatedness by estimating the likelihood of task-to-cluster assignments and clusters the task models in a latent space. We formulate the prior distribution of the mixture as a Chinese restaurant process (CRP) that instantiates new mixture components as needed. The update and expansion of the mixture are governed by the Bayesian nonparametric framework with an expectation maximization (EM) procedure, which dynamically adapts the model complexity without explicit task boundaries or heuristics. Moreover, we use the domain randomization technique to train robust prior parameters for the initialization of each task model in the mixture; thus, the resulting model can better generalize and adapt to unseen tasks. With extensive experiments conducted on robot navigation and locomotion domains, we show that our method successfully facilitates scalable lifelong RL and outperforms relevant existing methods. IEEE
DA  - 2022///
PY  - 2022
DO  - 10.1109/TCYB.2022.3170485
SP  - 1
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130466921&doi=10.1109%2fTCYB.2022.3170485&partnerID=40&md5=db9ee7534b0697b5af43aa33db661075
DB  - Scopus
KW  - Deep learning
KW  - Knowledge engineering
KW  - Reinforcement learning
KW  - Task analysis
KW  - Bayes methods
KW  - Mathematical models
KW  - Adaptation models
KW  - domain randomization
KW  - Domain randomization
KW  - Random processes
KW  - Randomisation
KW  - Bayes method
KW  - Maximum principle
KW  - Q-learning
KW  - Thermal Engineering
KW  - Mixtures
KW  - Chinese restaurant process
KW  - Chinese restaurant process (CRP)
KW  - Dirichlet process
KW  - Dirichlet process mixture
KW  - Expectation Maximization
KW  - expectation maximization (EM)
KW  - Lifelong reinforcement learning
KW  - lifelong reinforcement learning (RL)
KW  - Thermal stability
ER  - 

TY  - CONF
TI  - HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning
AU  - Xiong, Z.
AU  - Agarwal, I.
AU  - Jagannathan, S.
T2  - CEUR Workshop Proceedings
AB  - We propose a two-level hierarchical framework for safe reinforcement learning in a complex environment. The high-level part is an adaptive planner, which aims at learning and generating safe and efficient paths for tasks with imperfect map information. The lower-level part contains a learning-based controller and its corresponding neural Lyapunov function, which characterizes the controller's stability property. This learned neural Lyapunov function serves two purposes. First, it will be part of the high-level heuristic for our planning algorithm. Second, it acts as a part of a runtime shield to guard the safety of the whole system. We use a robot navigation example to demonstrate that our framework can operate efficiently and safely in complex environments, even under adversarial attacks. Copyright © 2022 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
DA  - 2022///
PY  - 2022
VL  - 3087
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382338&partnerID=40&md5=ac7bb4dc501b17e0342bcc2d1c031aca
DB  - Scopus
KW  - Reinforcement learning
KW  - Robots
KW  - Reinforcement learnings
KW  - Controllers
KW  - Lyapunov functions
KW  - Runtimes
KW  - Complex environments
KW  - Lyapunov's functions
KW  - Planning algorithms
KW  - Stability properties
KW  - Controller stabilities
KW  - Efficient path
KW  - Robot navigation
ER  - 

TY  - CONF
TI  - Towards Fair and Robust Classification
AU  - Sun, H.
AU  - Wu, K.
AU  - Wang, T.
AU  - Wang, W.H.
T2  - Proceedings - 7th IEEE European Symposium on Security and Privacy, Euro S and P 2022
AB  - Robustness and fairness are two equally important issues for machine learning systems. Despite the active research on robustness and fairness of ML recently, these efforts focus on either fairness or robustness, but not both. To bridge this gap, in this paper, we design Fair and Robust Classification (FRoC) models that equip the classification models with both fairness and robustness. Meeting both fairness and robustness constraints is not trivial due to the tension between them. The trade-off between fairness, robustness, and model accuracy also introduces additional challenge. To address these challenges, we design two FRoC methods, namely FRoC-PRE that modifies the input data before model training, and FRoC-IN that modifies the model with an adversarial objective function to address both fairness and robustness during training. FRoC-IN is suitable to the settings where the users (e.g., ML service providers) only have the access to the model but not the original data, while FRoC-PRE works for the settings where the users (e.g., data owners) have the access to both data and a surrogate model that may have similar architecture as the target model. Our extensive experiments on real-world datasets demonstrate that both FRoC-IN and FRoC-PRE can achieve both fairness and robustness with insignificant accuracy loss of the target model. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/EuroSP53844.2022.00030
SP  - 356
EP  - 376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134017587&doi=10.1109%2fEuroSP53844.2022.00030&partnerID=40&md5=671eb3c92941e21c487bcff140b27e8d
DB  - Scopus
KW  - Machine learning
KW  - Algorithmic fairness
KW  - trustworthy machine learning
KW  - Machine-learning
KW  - Classification models
KW  - Economic and social effects
KW  - classification
KW  - Algorithmics
KW  - Trade off
KW  - Machine learning systems
KW  - Robust classification
KW  - adversarial robustness
KW  - Adversarial robustness
KW  - Trustworthy machine learning
KW  - Bridges
KW  - Target model
ER  - 

TY  - CONF
TI  - A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods
AU  - Zhang, Y.
AU  - Gao, T.
AU  - Mi, Q.
T2  - Proceedings of the 2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology, IAICT 2022
AB  - Offline deep reinforcement learning algorithms are still in developing. Some existing algorithms have shown that it is feasible to learn directly without using environmental interaction under the condition of sufficient datasets. In this paper, we combine an offline reinforcement learning method through behavior regularization with a robust offline reinforcement learning algorithm. Moreover, the algorithm is verified and analyzed with a high-quality but limited dataset. The experimental results show that it is feasible to combine the behavior regularization method with the robust offline reinforcement learning algorithm, to gain better performance under the condition of limited data compared with the baseline algorithms.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IAICT55358.2022.9887435
SP  - 150
EP  - 154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139201012&doi=10.1109%2fIAICT55358.2022.9887435&partnerID=40&md5=21027466550a155936800e0c9aabbafa
DB  - Scopus
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9887435&ref=
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Condition
KW  - Learn+
KW  - Offline
KW  - Reinforcement learning algorithms
KW  - Regularisation
KW  - behavior regularization
KW  - Behavior regularization
KW  - limited data
KW  - Limited data
KW  - offline deep reinforcement learning
KW  - Offline deep reinforcement learning
KW  - Regularization methods
ER  - 

TY  - JOUR
TI  - A robust supervised subspace learning approach for output-relevant prediction and detection against outliers
AU  - Li, W.
AU  - Wang, Y.
T2  - Journal of Process Control
AB  - This paper proposes a novel robust supervised subspace learning (RSSL) method for output-relevant prediction and detection against outliers. RSSL learns the robust subspaces by optimizing a joint problem over both the prediction of output and the reconstruction of input. To this end, the learned subspaces/data representations are informative, i.e., they are encapsulated with the critic information related to both the input and output, and thus can benefit the following tasks of output-related modeling and detection. Besides, we separate sparse items from the raw measurements to suppress the effects of outliers. An efficient optimization algorithm is designed to solve the optimization problem of RSSL. We further conduct post orthogonal decomposition upon the subspaces provided by RSSL so that the trimmed subspaces are more suitable for output-related detection. The efficacy of the proposed method is extensively verified by synthesis data and benchmark data. © 2021 Elsevier Ltd
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.jprocont.2021.09.007
VL  - 106
SP  - 184
EP  - 194
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116036069&doi=10.1016%2fj.jprocont.2021.09.007&partnerID=40&md5=0d74c08e6d482e68ae1a0bf1a20ff3cb
DB  - Scopus
KW  - Learning systems
KW  - Forecasting
KW  - Factorization
KW  - Matrix factorizations
KW  - Optimization
KW  - Learning approach
KW  - Statistics
KW  - Learn+
KW  - Learning methods
KW  - Matrix factorization
KW  - Data representations
KW  - Industrial system
KW  - Industrial systems
KW  - Output-related detection
KW  - Robust supervised subspace learning
KW  - Robust supervised subspace learning (RSSL)
KW  - Subspace decomposition
KW  - Subspace learning
ER  - 

TY  - JOUR
TI  - A data-driven robust optimization algorithm for black-box cases: An application to hyper-parameter optimization of machine learning algorithms
AU  - Seifi, F.
AU  - Azizi, M.J.
AU  - Akhavan Niaki, S.T.
T2  - Computers and Industrial Engineering
AB  - The huge availability of data in the last decade has raised the opportunity for the better use of data in decision-making processes. The idea of using the existing data to achieve a more coherent reality solution has led to a branch of optimization called data-driven optimization. On the one hand, the presence of uncertain variables in these datasets makes it crucial to design robust optimization methods in this area. On the other hand, in many real-world problems, the closed-form of the objective function is not available and a meta-model based framework is necessary. Motivated by the above points, in this paper a Gaussian process is used in a Bayesian optimization framework to design a method that is consistent with the data in a predefined confidence level. The advantage of the proposed method is that it is computationally tractable in addition to being robust and independent of the objective function's form. As one of the applications of the proposed algorithm, hyper-parameter optimization for deep learning is investigated. The proposed method can help find the optimal hyper-parameters that are robust with respect to noise. © 2021 Elsevier Ltd
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.cie.2021.107581
VL  - 160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112307597&doi=10.1016%2fj.cie.2021.107581&partnerID=40&md5=a6ef9ee5ae1cddf885674810e44be2c2
DB  - Scopus
KW  - Deep learning
KW  - Decision making
KW  - Learning algorithms
KW  - Optimization
KW  - Robust optimization
KW  - Parameter estimation
KW  - Bayesian optimization
KW  - Gaussian distribution
KW  - Gaussian noise (electronic)
KW  - Gaussian process
KW  - Gaussian Processes
KW  - Data driven
KW  - Hyper-parameter optimizations
KW  - Objective functions
KW  - Black-box optimization
KW  - Data-driven optimization
KW  - Hyper-parameter tuning
ER  - 

TY  - CONF
TI  - Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention
AU  - Xu, Y.
AU  - Liu, Z.
AU  - Duan, G.
AU  - Zhu, J.
AU  - Bai, X.
AU  - Tan, J.
T2  - Proceedings of Machine Learning Research
AB  - Safety has become one of the main challenges of applying deep reinforcement learning to real world systems. Currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. In this paper, we propose MBHI, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both”local” and”non-local” catastrophes. An ensemble of supervised learners are trained in MBHI to imitate human blocking decisions. Similar to human decision-making process, MBHI will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. When the imagination encounters a catastrophe, MBHI will block the current action and use an efficient MPC method to output a safety policy. We evaluate our method on several safety tasks, and the results show that MBHI achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines. © 2021 Proceedings of Machine Learning Research. All rights reserved.
DA  - 2021///
PY  - 2021
VL  - 164
SP  - 332
EP  - 341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146865238&partnerID=40&md5=b9693007970623e55035eef698de6e42
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Decision making
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Real-world system
KW  - Human intervention
KW  - Model-based OPC
KW  - Disasters
KW  - Model-based reinforcement learning
KW  - Disaster prevention
KW  - External knowledge
KW  - Model predict control
KW  - Model Predict Control
KW  - Model-based RL
KW  - Predict control
KW  - Safety RL
ER  - 

TY  - CONF
TI  - FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning
AU  - Chen, L.-Y.
AU  - Chiu, T.-C.
AU  - Pang, A.-C.
AU  - Cheng, L.-C.
T2  - 2021 IEEE Global Communications Conference, GLOBECOM 2021 - Proceedings
AB  - With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/GLOBECOM46510.2021.9685082
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127300983&doi=10.1109%2fGLOBECOM46510.2021.9685082&partnerID=40&md5=018f5ed6035aca25fa5d428aab92a59a
DB  - Scopus
KW  - Learning systems
KW  - Federated Learning
KW  - Privacy preserving
KW  - Statistics
KW  - Security systems
KW  - Federated learning
KW  - Privacy-preserving techniques
KW  - System robustness
KW  - Poisoning attacks
KW  - System Robustness
KW  - Edge AI
KW  - End-devices
KW  - Global models
KW  - Heterogeneous environments
KW  - Model poisoning attack
KW  - Model Poisoning Attacks
KW  - Model security
KW  - Model Security
ER  - 

TY  - JOUR
TI  - A Robust Automated Machine Learning System with Pseudoinverse Learning
AU  - Wang, K.
AU  - Guo, P.
T2  - Cognitive Computation
AB  - Developing a robust deep neural network (DNN) for a specific task is not only time-consuming but also requires lots of experienced human experts. In order to make deep neural networks easier to apply or even take the human experts out of the design of network architecture completely, a growing number of researches focus on robust automated machine learning (AutoML). In this paper, we investigated the robustness problem of AutoML systems based on contractive pseudoinverse learners. In our proposed method, deep neural networks were built with stacked contractive pseudoinverse learners (CPILer). Each CPILer has a Jacobian regularized reconstruction loss function and is trained with pseudoinverse learning algorithm. When sigmoid activation function is adopted in the hidden layer, the graph Laplace regularizer is derived from square Frobenius norm of the Jacobian matrix. This learning scheme not only speeds up the training process dramatically but also reduces the effort of hyperparameter tuning. In addition, the graph Laplace regularization can improve the robustness of the learning systems by reducing the sensibility to noise. An ensemble network architecture consisting of several sub-networks was designed to build the AutoML systems. The architecture hyperparameters of the system were determined in an automated way which could be considered as a data-driven way. The proposed method shown good performance in the experiments in terms of efficiency and accuracy, and outperformed the baseline methods on a series of benchmark data sets. The robustness improvement of our proposed method was also demonstrated in the experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s12559-021-09853-6
VL  - 13
IS  - 3
SP  - 724
EP  - 735
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103065280&doi=10.1007%2fs12559-021-09853-6&partnerID=40&md5=e40b73e3515c9d6099b659efc424c45b
DB  - Scopus
KW  - Deep learning
KW  - Automation
KW  - Deep neural networks
KW  - Neural networks
KW  - Learning systems
KW  - Learning algorithms
KW  - Automated machines
KW  - Benchmarking
KW  - Network architecture
KW  - Training process
KW  - Hyper-parameter
KW  - Laplace transforms
KW  - Learning schemes
KW  - Automated machine learning
KW  - Deep neural network
KW  - Baseline methods
KW  - Contractive pseudoinverse learners
KW  - Hyperparameters
KW  - Jacobian matrices
KW  - Pseudo-inverses
KW  - Robust learning systems
KW  - Sigmoid activation function
ER  - 

TY  - CONF
TI  - Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning
AU  - Curi, S.
AU  - Bogunovic, I.
AU  - Krause, A.
T2  - Proceedings of Machine Learning Research
AB  - In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness against worst-case situations. The robust RL framework addresses this challenge via a worst-case optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty, and efficiently explores both the agent and adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally, we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments. Copyright © 2021 by the author(s)
DA  - 2021///
PY  - 2021
VL  - 139
SP  - 2254
EP  - 2264
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127231156&partnerID=40&md5=0803f12a48c9ee753414ffb80af7e9d2
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Optimisations
KW  - Complex networks
KW  - Optimization
KW  - Reliable performance
KW  - Learning frameworks
KW  - Model-based OPC
KW  - Reinforcement learning algorithms
KW  - Large problems
KW  - Real-world task
KW  - Training time
ER  - 

TY  - JOUR
TI  - A Robust Method to Measure the Global Feature Importance of Complex Prediction Models
AU  - Zhang, X.
AU  - Wu, L.
AU  - Li, Z.
AU  - Liu, H.
T2  - IEEE Access
AB  - Because machine learning has been widely used in various domains, interpreting internal mechanisms and predictive results of models is crucial for further applications of complex machine learning models. However, the interpretability of complex machine learning models on biased data remains a difficult problem. When the important explanatory features of concerned data are highly influenced by contaminated distributions, particularly in risk-sensitive fields, such as self-driving vehicles and healthcare, it is crucial to provide a robust interpretation of complex models for users. The interpretation of complex models is often associated with analyzing model features by measuring feature importance. Therefore, this article proposes a novel method derived from high-dimensional model representation (HDMR) to measure feature importance. The proposed method can provide robust estimation when the input features follow contaminated distributions. Moreover, the method is model-agnostic, which can enhance its ability to compare different interpretations due to its generalizability. Experimental evaluations on artificial models and machine learning models show that the proposed method is more robust than the traditional method based on HDMR. © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3049412
VL  - 9
SP  - 7885
EP  - 7893
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099261795&doi=10.1109%2fACCESS.2021.3049412&partnerID=40&md5=d2209d9951eb35633613b8bfcbdab3a2
DB  - Scopus
L1  - https://ieeexplore.ieee.org/ielx7/6287639/9312710/09314116.pdf?tp=&arnumber=9314116&isnumber=9312710&ref=
KW  - Interpretability
KW  - Machine learning
KW  - robustness
KW  - Feature importance
KW  - Predictive analytics
KW  - Machine learning models
KW  - Robust estimation
KW  - supervised machine learning
KW  - Prediction model
KW  - Analyzing models
KW  - Complex machines
KW  - Experimental evaluation
KW  - global interpretation
KW  - High dimensional model representation
KW  - high-dimensional model representation
ER  - 

TY  - CONF
TI  - BDI-Dojo: Developing robust BDI agents in evolving adversarial environments
AU  - Pulawski, S.
AU  - Dam, H.K.
AU  - Ghose, A.
T2  - Proceedings - 2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion, ACSOS-C 2021
AB  - The Belief-Desire-Intention (BDI) architecture is a widely-used model for developing multi-agent systems. BDI agents pursue their goals over time using a collection of plan recipes that are programmed by the developers. Thus, traditional BDI agents are limited in dealing with dynamic environments where uncertainties are not known beforehand, such as those introduced by adversarial forces. In this paper, we present the BDI-Dojo framework for developing robust BDI agents by training them using reinforcement learning against similarly learning-equipped adversarial agents. This adversarial training approach empowers BDI agents to become more resilient in uncertain, dynamic environments. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACSOS-C52956.2021.00066
SP  - 257
EP  - 262
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123394011&doi=10.1109%2fACSOS-C52956.2021.00066&partnerID=40&md5=1d727ac9b754c915d09d08c9493bfe5e
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Multi-Agent Systems
KW  - Dynamic environments
KW  - Uncertainty
KW  - Robustness
KW  - Multi agent systems
KW  - Resilience
KW  - Adversarial training
KW  - Adversarial environments
KW  - Adversarial agent
KW  - Belief-desire-intentions
KW  - Beliefs-desires-intentions agents
KW  - Uncertain dynamic environment
ER  - 

TY  - CONF
TI  - Learning to safely approve updates to machine learning algorithms
AU  - Feng, J.
T2  - ACM CHIL 2021 - Proceedings of the 2021 ACM Conference on Health, Inference, and Learning
AB  - Machine learning algorithms in healthcare have the potential to continually learn from real-world data generated during healthcare delivery and adapt to dataset shifts. As such, regulatory bodies like the US FDA have begun discussions on how to autonomously approve modifications to algorithms. Current proposals evaluate algorithmic modifications via hypothesis testing and control a definition of online approval error that only applies if the data is stationary over time, which is unlikely in practice. To this end, we investigate designing approval policies for modifications to ML algorithms in the presence of distributional shifts. Our key observation is that the approval policy most efficient at identifying and approving beneficial modifications varies across problem settings. So, rather than selecting a fixed approval policy a priori, we propose learning the best approval policy by searching over a family of approval strategies. We define a family of strategies that range in their level of optimism when approving modifications. To protect against settings where no version of the ML algorithm performs well, this family includes a pessimistic strategy that rescinds approval. We use the exponentially weighted averaging forecaster (EWAF) to learn the most appropriate strategy and derive tighter regret bounds assuming the distributional shifts are bounded. In simulation studies and empirical analyses, we find that wrapping approval strategies within EWAF is a simple yet effective approach to protect against distributional shifts without significantly slowing down approval of beneficial modifications.  © 2021 Owner/Author.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3450439.3451864
SP  - 164
EP  - 173
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104096120&doi=10.1145%2f3450439.3451864&partnerID=40&md5=742aa4352dc79c83df5bdfb093ff02cb
DB  - Scopus
KW  - Machine learning
KW  - Learning algorithms
KW  - Health care
KW  - online learning
KW  - Healthcare delivery
KW  - Simulation studies
KW  - AI/ML-based SaMD
KW  - Dataset shifts
KW  - Effective approaches
KW  - Empirical analysis
KW  - Hypothesis testing
KW  - non-stationarity
KW  - prediction with expert advice
KW  - Regulatory bodies
KW  - Weighted averaging
ER  - 

TY  - CONF
TI  - Online Robust Reinforcement Learning with Model Uncertainty
AU  - Wang, Y.
AU  - Zou, S.
T2  - Advances in Neural Information Processing Systems
AB  - Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially, and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set, and design robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms, and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts (within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms. © 2021 Neural information processing systems foundation. All rights reserved.
DA  - 2021///
PY  - 2021
VL  - 9
SP  - 7193
EP  - 7206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130775690&partnerID=40&md5=ac00e2eb7a717c085e21d9699fe68881
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - E-learning
KW  - Reinforcement learnings
KW  - Uncertainty
KW  - Uncertainty analysis
KW  - Modeling uncertainties
KW  - Approximation algorithms
KW  - Error analysis
KW  - Functions approximations
KW  - Model free
KW  - Q-functions
KW  - Q-learning algorithms
KW  - Single sample
KW  - Unknown uncertainty
KW  - Worst-case performance
ER  - 

TY  - JOUR
TI  - Adaptive robust learning framework for twin support vector machine classification
AU  - Ma, J.
AU  - Yang, L.
AU  - Sun, Q.
T2  - Knowledge-Based Systems
AB  - In general, introducing robust distance metrics and loss functions in the learning process can improve the robustness of the algorithms. In this work, we first propose a new robust loss function called adaptive capped Lθε-loss. For different problems, we can choose different loss functions through adaptive parameter θ during the learning process. Secondly, we propose a new robust distance metric induced by correntropy (CIM) that is based on Laplacian kernel. The CIM contains first and higher-order moments from samples. Further, we demonstrate some important and interesting properties of the Lθε-loss and CIM, such as robustness, boundedness, nonconvexity, etc. Finally, we apply the to Lθε-loss and CIM to twin support vector machine (TWSVM) and develop an adaptive robust learning framework, namely adaptive robust twin support vector machine (ARTSVM). The proposed ARTSVM not only inherits the advantages of TWSVM but also improves the robustness of classification problems. A non-convex optimization method, DC (difference of convex functions) programming algorithm (DCA) is used to solve the proposed ARTSVM, and the convergence of the algorithm is proved theoretically. Experiments on multiple datasets show that the proposed ARTSVM is competitive with existing methods. © 2020
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.knosys.2020.106536
VL  - 211
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094912105&doi=10.1016%2fj.knosys.2020.106536&partnerID=40&md5=c95bdc9b36362521439723d98ce958c3
DB  - Scopus
KW  - Support vector machines
KW  - Learning systems
KW  - Learning algorithms
KW  - Robustness
KW  - Robust learning
KW  - DC programming
KW  - Robust distance
KW  - Convex optimization
KW  - Nonconvex optimization
KW  - Learning process
KW  - Multiple data sets
KW  - Adaptive parameters
KW  - Correntropy
KW  - Distance metric
KW  - Functions
KW  - Higher order moments
KW  - Twin support vector machine
KW  - Twin support vector machines
ER  - 

TY  - JOUR
TI  - A robust policy bootstrapping algorithm for multi-objective reinforcement learning in non-stationary environments
AU  - Abdelfattah, S.
AU  - Kasmarik, K.
AU  - Hu, J.
T2  - Adaptive Behavior
AB  - Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this kind of problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity in order to evolve a coverage set of policies that can solve the problem. This article introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments. © The Author(s) 2019.
DA  - 2020///
PY  - 2020
DO  - 10.1177/1059712319869313
VL  - 28
IS  - 4
SP  - 273
EP  - 292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071469173&doi=10.1177%2f1059712319869313&partnerID=40&md5=5430878a3c4684e33716a18eea2a7dfc
DB  - Scopus
L1  - https://journals.sagepub.com/doi/pdf/10.1177/1059712319869313
KW  - reinforcement learning
KW  - Markov decision processes
KW  - Multi-objective optimization
KW  - environment
KW  - dynamics
KW  - non-stationary
KW  - policy bootstrapping
ER  - 

TY  - CONF
TI  - Impact-learning: A robust machine learning algorithm
AU  - Kowsher, M.
AU  - Tahabilder, A.
AU  - Murad, S.A.
T2  - ACM International Conference Proceeding Series
AB  - The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3411174.3411185
SP  - 9
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089608381&doi=10.1145%2f3411174.3411185&partnerID=40&md5=5aa583b9d3f7591557bcf03a3f1f6a17
DB  - Scopus
KW  - machine learning
KW  - Decision trees
KW  - Support vector machines
KW  - Learning systems
KW  - Learning algorithms
KW  - Classification (of information)
KW  - classification
KW  - Supervised machine learning
KW  - Logistic regression
KW  - Training sets
KW  - Research papers
KW  - Polynomial regression
KW  - Standard machines
KW  - Analyzing system
KW  - Impact learning
KW  - Intrinsic rates
KW  - Naive bayes
KW  - Numerical data
KW  - regression
ER  - 

TY  - CONF
TI  - Towards Robust Production Machine Learning Systems: Managing Dataset Shift
AU  - Abdelkader, H.
T2  - Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020
AB  - The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3324884.3415281
SP  - 1164
EP  - 1166
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099251225&doi=10.1145%2f3324884.3415281&partnerID=40&md5=b7f18b590ee0b52cf8df446932f366cc
DB  - Scopus
KW  - Machine learning
KW  - Machine learning models
KW  - Computer software
KW  - System robustness
KW  - Robust production
KW  - Software engineering process
KW  - Intelligent Services
KW  - n/a
KW  - Notification mechanism
KW  - Robustness properties
KW  - Software organisations
ER  - 

TY  - CONF
TI  - Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training
AU  - Meng, Y.
AU  - Zhang, Y.
AU  - Huang, J.
AU  - Wang, X.
AU  - Zhang, Y.
AU  - Ji, H.
AU  - Han, J.
T2  - EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings
AB  - We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins. © 2021 Association for Computational Linguistics
DA  - 2021///
PY  - 2021
SP  - 10367
EP  - 10378
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121748854&partnerID=40&md5=0fb4ea97baff35f25aec98ee2a1ae5de
DB  - Scopus
KW  - Knowledge based systems
KW  - Learning systems
KW  - Benchmarking
KW  - Learning models
KW  - Natural language processing systems
KW  - Labeled data
KW  - Robust learning
KW  - Noise robust
KW  - Computational linguistics
KW  - Language model
KW  - Self-training
KW  - Character recognition
KW  - Noisy labels
KW  - Recognition models
KW  - Matchings
KW  - Named entity recognition
ER  - 

TY  - CONF
TI  - Induction of fault trees through Bayesian networks
AU  - Linard, A.
AU  - Bueno, M.L.P.
AU  - Bucur, D.
AU  - Stoelinga, M.
T2  - Proceedings of the 29th European Safety and Reliability Conference, ESREL 2019
AB  - Cyber-physical systems have increasingly intricate architectures and failure modes, which is due to an explosion of their complexity, size, and failure criticality. While expert knowledge of individual components exists, their interaction is complex. For these reasons, obtaining accurate system reliability models is a hard task. At the same time, systems tend to be continuously monitored via advanced sensor systems. This data describes the components' failure behavior and can be exploited for failure diagnosis and learning of reliability models. This paper presents an effective algorithm for the learning of Fault Trees from data. Fault trees (FTs) are a widespread formalism in reliability engineering. They capture the failure behavior of components and their propagation through an entire system. To that end, we first use machine learning to compute a Bayesian Network (BN) highlighting probabilistic relationships between the failures of components and root causes. Then, we apply a set of rules to translate a BN into an FT, based on the Conditional Probability Tables to decide, amongst others, the nature of gates in the FT. We evaluate our method on synthetic data and a benchmark set of FTs. © 2019 European Safety and Reliability Association. Published by Research Publishing, Singapore.
DA  - 2020///
PY  - 2020
DO  - 10.3850/978-981-11-2724-3_0596-cd
SP  - 910
EP  - 917
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089187848&doi=10.3850%2f978-981-11-2724-3_0596-cd&partnerID=40&md5=d522b2243b1a745146c5bc2d04b75084
DB  - Scopus
KW  - Machine learning
KW  - Learning systems
KW  - Cyber-physical systems
KW  - Bayesian networks
KW  - Complex networks
KW  - Embedded systems
KW  - Reliability
KW  - Trees (mathematics)
KW  - Forestry
KW  - Explosions
KW  - Risk analysis
KW  - Safety-critical systems
KW  - Failure Diagnosis
KW  - Bayesian network inference
KW  - Conditional probability tables
KW  - Effective algorithms
KW  - Failure behaviors
KW  - Failure diagnosis
KW  - Fault tree induction
KW  - Individual components
KW  - Reliability engineering
KW  - Reliability model
KW  - System reliability models
ER  - 

TY  - JOUR
TI  - Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence
AU  - Xin, R.
AU  - Kar, S.
AU  - Khan, U.A.
T2  - IEEE Signal Processing Magazine
AB  - Decentralized methods to solve finite-sum minimization problems are important in many signal processing and machine learning tasks where the data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. In this article, we review decentralized stochastic first-order methods and provide a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence. We provide explicit theoretical guarantees of the corresponding methods when the objective functions are smooth and strongly convex and show their applicability to nonconvex problems via numerical experiments. Throughout the article, we provide intuitive illustrations of the main technical ideas by casting appropriate tradeoffs and comparisons among the methods of interest and by highlighting applications to decentralized training of machine learning models. © 1991-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/MSP.2020.2974267
VL  - 37
IS  - 3
SP  - 102
EP  - 113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084602494&doi=10.1109%2fMSP.2020.2974267&partnerID=40&md5=8770fefac4537655d72f8fadd99b637c
DB  - Scopus
KW  - Machine learning
KW  - Signal processing
KW  - Machine learning models
KW  - Optimization
KW  - Numerical methods
KW  - Objective functions
KW  - Theoretical guarantees
KW  - Numerical experiments
KW  - Algorithmic framework
KW  - Data Sharing
KW  - Minimization problems
KW  - Numerical analysis
KW  - Stochastic optimizations
KW  - Variance reductions
ER  - 

TY  - JOUR
TI  - A robust approach to model-based classification based on trimming and constraints: Semi-supervised learning in presence of outliers and label noise
AU  - Cappozzo, A.
AU  - Greselin, F.
AU  - Murphy, T.B.
T2  - Advances in Data Analysis and Classification
AB  - In a standard classification framework a set of trustworthy learning data are employed to build a decision rule, with the final aim of classifying unlabelled units belonging to the test set. Therefore, unreliable labelled observations, namely outliers and data with incorrect labels, can strongly undermine the classifier performance, especially if the training size is small. The present work introduces a robust modification to the Model-Based Classification framework, employing impartial trimming and constraints on the ratio between the maximum and the minimum eigenvalue of the group scatter matrices. The proposed method effectively handles noise presence in both response and exploratory variables, providing reliable classification even when dealing with contaminated datasets. A robust information criterion is proposed for model selection. Experiments on real and simulated data, artificially adulterated, are provided to underline the benefits of the proposed method. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s11634-019-00371-w
VL  - 14
IS  - 2
SP  - 327
EP  - 354
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070929493&doi=10.1007%2fs11634-019-00371-w&partnerID=40&md5=97940a49ee45db1b0a11aa8ed46b42d8
DB  - Scopus
KW  - Machine learning
KW  - Supervised learning
KW  - Classification (of information)
KW  - Robust estimation
KW  - Semi- supervised learning
KW  - Statistics
KW  - Classification framework
KW  - Label noise
KW  - Classifier performance
KW  - Eigenvalues
KW  - Eigenvalues and eigenfunctions
KW  - Eigenvalues restrictions
KW  - Impartial trimming
KW  - Information criterion
KW  - Model-based classification
KW  - Model-based classifications
KW  - Outliers detection
KW  - Trimming
ER  - 

TY  - CONF
TI  - Towards deployment of robust cooperative ai agents: An algorithmic framework for learning adaptive policies
AU  - Ghosh, A.
AU  - Mahdavi, H.
AU  - Tschiatschek, S.
AU  - Singla, A.
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
AB  - We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by real-world applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user's actions to make inferences about the user's type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents. © 2020 International Foundation for Autonomous.
DA  - 2020///
PY  - 2020
VL  - 2020-May
SP  - 447
EP  - 455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093460105&partnerID=40&md5=9647e6eca81c01f303cba645d35a90aa
DB  - Scopus
KW  - Reinforcement learning
KW  - Machine learning
KW  - Autonomous agents
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Multi agent systems
KW  - Multi agent
KW  - Virtual assistants
KW  - Two agents
KW  - Algorithmic framework
KW  - Adaptive policy
KW  - Computing policies
KW  - Game environment
KW  - Learning agent-to-agent interactions
ER  - 

TY  - CONF
TI  - An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems
AU  - Ward, F.R.
AU  - Habli, I.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Machine Learning (ML) has the potential to become widespread in safety-critical applications. It is therefore important that we have sufficient confidence in the safe behaviour of the ML-based functionality. One key consideration is whether the ML being used is interpretable. In this paper, we present an argument pattern, i.e. reusable structure, that can be used for justifying the sufficient interpretability of ML within a wider assurance case. The pattern can be used to assess whether the right interpretability method and format are used in the right context (time, setting and audience). This argument structure provides a basis for developing and assessing focused requirements for the interpretability of ML in safety-critical domains. © 2020, Springer Nature Switzerland AG.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-55583-2_30
VL  - 12235 LNCS
SP  - 395
EP  - 407
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096588864&doi=10.1007%2f978-3-030-55583-2_30&partnerID=40&md5=eb76c695180f85f6bedebd5169c827e6
DB  - Scopus
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-030-55583-2_30.pdf
KW  - Interpretability
KW  - Explainability
KW  - Safety
KW  - Artificial intelligence
KW  - Machine learning
KW  - Safety engineering
KW  - Embedded systems
KW  - Assurance
KW  - Safety critical systems
KW  - Assurance case
KW  - Safety critical applications
KW  - System of systems
KW  - Safety-critical domain
KW  - Argument structures
KW  - Safety-case
ER  - 

TY  - CONF
TI  - A Safety Case Pattern for Systems with Machine Learning Components
AU  - Wozniak, E.
AU  - Cârlan, C.
AU  - Acar-Celik, E.
AU  - Putzer, H.J.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Several standards from the domain of safety critical systems, in order to support the argumentation of the safety assurance of a system under development, recommend the construction of a safety case. This activity is guided by the objectives to be met, recommended or required by the standards along the safety lifecycle. Ongoing attempts to use Machine Learning (ML) for safety critical functionality revealed certain deficits. For instance, the widely recognized standard for functional safety of automotive systems, ISO 26262, which can be used as a basis to construct a safety case, does not reason about ML. To this end, the goal of this work is to provide a pattern for arguing about the correct implementation of safety requirements in system components based on ML. The pattern is integrated within an overall encompassing approach for safety case generation for automotive systems and its applicability is showcased on a pedestrian avoidance system. © 2020, Springer Nature Switzerland AG.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-55583-2_28
VL  - 12235 LNCS
SP  - 370
EP  - 382
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096573715&doi=10.1007%2f978-3-030-55583-2_28&partnerID=40&md5=fe9cbbd8d29147cd6d4befb4c7199d1f
DB  - Scopus
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-030-55583-2_28.pdf
KW  - Machine learning
KW  - Pedestrian safety
KW  - Embedded systems
KW  - Life cycle
KW  - Safety case
KW  - Functional Safety
KW  - Security systems
KW  - Safety critical systems
KW  - Safety requirements
KW  - Safety assurance
KW  - ISO 26262
KW  - System of systems
KW  - Automotive Systems
KW  - Avoidance systems
KW  - GSN
KW  - Safety case patterns
KW  - System components
ER  - 

TY  - CONF
TI  - A Principal Component Analysis Approach for Embedding Local Symmetries into Deep Learning Algorithms
AU  - Lagrave, P.-Y.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Building robust-by-design Machine Learning algorithms is key for critical tasks such as safety or military applications. By leveraging on the ideas developed in the context of building invariant Support Vectors Machines, this paper introduces a convenient methodology for embedding local Lie groups symmetries into Deep Learning algorithms by performing a Principal Component Analysis on the corresponding Tangent Covariance Matrix. The projection of the input data onto the principal directions leads to a new data representation which allows singling out the components conveying the semantic information useful to the considered algorithmic task while reducing the dimension of the input manifold. Besides, our numerical testing emphasizes that, although less efficient than using Group-Convolutional Neural Networks as only dealing with local symmetries, our approach does improve accuracy and robustness without introducing significant computational overhead. Performance improvements up to 5% were obtained for low capacity algorithms, making this approach of particular interest for the engineering of safe embedded Artificial Intelligence systems. © 2020, Springer Nature Switzerland AG.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-55583-2_22
VL  - 12235 LNCS
SP  - 302
EP  - 314
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096594336&doi=10.1007%2f978-3-030-55583-2_22&partnerID=40&md5=26c5c385b8de90bde29d678864ad77a9
DB  - Scopus
KW  - Deep learning
KW  - Semantics
KW  - Convolutional neural networks
KW  - Learning systems
KW  - Learning algorithms
KW  - Critical tasks
KW  - Safety engineering
KW  - Embedded systems
KW  - Support vectors machine
KW  - Artificial intelligence systems
KW  - Military applications
KW  - Embeddings
KW  - Safe machine learning
KW  - System of systems
KW  - Data representations
KW  - Computational overheads
KW  - Covariance matrix
KW  - Data representation
KW  - Lie groups
KW  - Model-based engineering
KW  - Numerical testing
KW  - Principal directions
KW  - Robustness-by-design
KW  - Semantic information
ER  - 

TY  - CONF
TI  - A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components
AU  - Kläs, M.
AU  - Jöckel, L.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - More and more software-intensive systems include components that are data-driven in the sense that they use models based on artificial intelligence (AI) or machine learning (ML). Since the outcomes of such models cannot be assumed to always be correct, related uncertainties must be understood and taken into account when decisions are made using these outcomes. This applies, in particular, if such decisions affect the safety of the system. To date, however, hardly any AI-/ML-based model provides dependable estimates of the uncertainty remaining in its outcomes. In order to address this limitation, we present a framework for encapsulating existing models applied in data-driven components with an uncertainty wrapper in order to enrich the model outcome with a situation-aware and dependable uncertainty statement. The presented framework is founded on existing work on the concept and mathematical foundation of uncertainty wrappers. The application of the framework is illustrated using pedestrian detection as an example, which is a particularly safety-critical feature in the context of autonomous driving. The Brier score and its components are used to investigate how the key aspects of the framework (scoping, clustering, calibration, and confidence limits) can influence the quality of uncertainty estimates. © 2020, Springer Nature Switzerland AG.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-55583-2_23
VL  - 12235 LNCS
SP  - 315
EP  - 327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096598898&doi=10.1007%2f978-3-030-55583-2_23&partnerID=40&md5=83471415db0d0778b436e00a14267b50
DB  - Scopus
KW  - Artificial intelligence
KW  - Machine learning
KW  - Autonomous driving
KW  - Pedestrian safety
KW  - Safety engineering
KW  - Uncertainty analysis
KW  - Data quality
KW  - Dependability
KW  - Embedded systems
KW  - Estimation
KW  - Mathematical foundations
KW  - Operational design domain
KW  - Uncertainty estimates
KW  - System of systems
KW  - Out-of-distribution
KW  - Pedestrian detection
KW  - Confidence limit
KW  - Critical features
KW  - Data encapsulation
KW  - Situation-aware
KW  - Software intensive systems
ER  - 

TY  - CONF
TI  - Security analysis of safe and seldonian reinforcement learning algorithms
AU  - Pinar Ozisik, A.
AU  - Thomas, P.S.
T2  - Advances in Neural Information Processing Systems
AB  - We analyze the extent to which existing methods rely on accurate training data for a specific class of reinforcement learning (RL) algorithms, known as Safe and Seldonian RL. We introduce a new measure of security to quantify the susceptibility to perturbations in training data by creating an attacker model that represents a worst-case analysis, and show that a couple of Seldonian RL methods are extremely sensitive to even a few data corruptions. We then introduce a new algorithm that is more robust against data corruptions, and demonstrate its usage in practice on some RL problems, including a grid-world and a diabetes treatment simulation. © 2020 Neural information processing systems foundation. All rights reserved.
DA  - 2020///
PY  - 2020
VL  - 2020-December
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108455515&partnerID=40&md5=dfff89b401122e01f74e471cb746b1f9
DB  - Scopus
KW  - Reinforcement learning
KW  - Training data
KW  - Learning algorithms
KW  - Worst-case analysis
KW  - Attacker models
KW  - Data corruption
KW  - Security analysis
KW  - Specific class
KW  - Treatment simulation
ER  - 

TY  - JOUR
TI  - A robust extreme learning machine framework for uncertain data classification
AU  - Jing, S.
AU  - Yang, L.
T2  - Journal of Supercomputing
AB  - Uncertain or missing data may occur in many practical applications. A principled strategy for handling this problem would therefore be very useful. We consider two-class and multi-class classification problems where the mean and covariance of each class are assumed to be known. With simple structure, fast speed and good performance, extreme learning machine (ELM) has been an important technology in machine learning. In this work, from the viewpoint of probability, we present a robust ELM framework (RELM) for missing data classification. Applying the Chebyshev–Cantelli inequality, the proposed RELM is reformulated as a second-order cone programming with global optimal solution. The proposed RELM only relates to the second moments of input samples and makes no assumption about the data probability distribution. Expectation maximization algorithm is used to fill in missing values and then obtain complete data. Numerical experiments are simulated in various datasets from UCI database and a practical application database. Experimental results show that the proposed method can achieve better performance than traditional methods. These results illustrate the feasibility and effectiveness of the proposed method for missing data classification. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s11227-018-2430-6
VL  - 76
IS  - 4
SP  - 2390
EP  - 2416
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081254670&doi=10.1007%2fs11227-018-2430-6&partnerID=40&md5=9d6880693d753dff20a823380e6433c1
DB  - Scopus
KW  - Machine learning
KW  - Classification (of information)
KW  - Uncertainty
KW  - Knowledge acquisition
KW  - Image segmentation
KW  - Extreme learning machine
KW  - Probability distributions
KW  - Maximum principle
KW  - Missing data
KW  - Expectation-maximization algorithms
KW  - Expectation maximization (EM) algorithm
KW  - Probability constraint
KW  - Probability constraints
KW  - Second-order cone programming
ER  - 

TY  - JOUR
TI  - Robust Transparency Against Model Inversion Attacks
AU  - Alufaisan, Y.
AU  - Kantarcioglu, M.
AU  - Zhou, Y.
T2  - IEEE Transactions on Dependable and Secure Computing
AB  - Transparency has become a critical need in machine learning (ML) applications. Designing transparent ML models helps increase trust, ensure accountability, and scrutinize fairness. Some organizations may opt-out of transparency to protect individuals&#x0027; privacy. Therefore, there is a great demand for transparency models that consider both privacy and security risks. Such transparency models can motivate organizations to improve their credibility by making the ML-based decision-making process comprehensible to end-users. Differential privacy (DP) provides an important technique to disclose information while protecting individual privacy. However, it has been shown that DP alone cannot prevent certain types of privacy attacks against disclosed ML models. DP with low values can provide high privacy guarantees, but may result in significantly weaker ML models in terms of accuracy. On the other hand, setting value too high may lead to successful privacy attacks. This raises the question whether we can disclose accurate transparent ML models while preserving privacy. In this paper we introduce a novel technique that complements DP to ensure model transparency and accuracy while being robust against model inversion attacks. We show that combining the proposed technique with DP provide highly transparent and accurate ML models while preserving privacy against model inversion attacks. IEEE
DA  - 2020///
PY  - 2020
DO  - 10.1109/TDSC.2020.3019508
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090442360&doi=10.1109%2fTDSC.2020.3019508&partnerID=40&md5=bb2978e08029eebf6487f9ba6bd9e6b3
DB  - Scopus
KW  - Transparency
KW  - Machine learning
KW  - transparency
KW  - Predictive models
KW  - Decision making
KW  - Computational modeling
KW  - Privacy
KW  - Model transparency
KW  - Robustness
KW  - Differential privacies
KW  - Privacy and security
KW  - Decision making process
KW  - Individual privacy
KW  - Model inversion
KW  - model-inversion attack
KW  - Novel techniques
KW  - Organizations
KW  - Privacy Attacks
KW  - privacy-preserving
ER  - 

TY  - JOUR
TI  - Deep Robust Reinforcement Learning for Practical Algorithmic Trading
AU  - Li, Y.
AU  - Zheng, W.
AU  - Zheng, Z.
T2  - IEEE Access
AB  - In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2932789
VL  - 7
SP  - 108014
EP  - 108021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071105095&doi=10.1109%2fACCESS.2019.2932789&partnerID=40&md5=a210d1234cf03eb745ff22d3367444d2
DB  - Scopus
KW  - Reinforcement learning
KW  - Machine learning
KW  - Deep neural networks
KW  - reinforcement learning
KW  - Autonomous agents
KW  - Markov decision process
KW  - deep neural network
KW  - Long short-term memory
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Commerce
KW  - Electronic trading
KW  - Financial markets
KW  - Real-world problem
KW  - Algorithmic trading
KW  - Financial time series
KW  - Function approximators
KW  - Long-term profits
KW  - Profitability
KW  - Trading environments
KW  - Trading strategies
ER  - 

TY  - CONF
TI  - Improving safety in reinforcement learning using model-based architectures and human intervention
AU  - Prakash, B.
AU  - Khatwani, M.
AU  - Waytowich, N.
AU  - Mohsenin, T.
T2  - Proceedings of the 32nd International Florida Artificial Intelligence Research Society Conference, FLAIRS 2019
AB  - Recent progress in AI and Reinforcement learning has shown great success in solving complex problems with high dimensional state spaces. However, most of these successes have been primarily in simulated environments where failure is of little or no consequence. Most real-world applications, however, require training solutions that are safe to operate as catastrophic failures are inadmissible especially when there is human interaction involved. Currently, Safe RL systems use human oversight during training and exploration in order to make sure the RL agent does not go into a catastrophic state. These methods require a large amount of human labor and it is very difficult to scale up. We present a hybrid method for reducing the human intervention time by combining model-based approaches and training a supervised learner to improve sample efficiency while also ensuring safety. We evaluate these methods on various grid-world environments using both standard and visual representations and show that our approach achieves better performance in terms of sample efficiency, number of catastrophic states reached as well as overall task performance compared to traditional model-free approaches. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 50
EP  - 55
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094005967&partnerID=40&md5=77fcdf372b36d2d963ff0e048ff1e361
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning systems
KW  - Efficiency
KW  - Human interactions
KW  - Human intervention
KW  - Simulated environment
KW  - Catastrophic failures
KW  - Complex problems
KW  - Traditional models
KW  - Training solutions
KW  - Visual representations
ER  - 

TY  - JOUR
TI  - Understanding adversarial training: Increasing local stability of supervised models through robust optimization
AU  - Shaham, U.
AU  - Yamada, Y.
AU  - Negahban, S.
T2  - Neurocomputing
AB  - We show that adversarial training of supervised learning models is in fact a robust optimization procedure. To do this, we establish a general framework for increasing local stability of supervised learning models using robust optimization. The framework is general and broadly applicable to differentiable non-parametric models, e.g., Artificial Neural Networks (ANNs). Using an alternating minimization-maximization procedure, the loss of the model is minimized with respect to perturbed examples that are generated at each parameter update, rather than with respect to the original training data. Our proposed framework generalizes adversarial training, as well as previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the networks also on the original test data. © 2018 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.neucom.2018.04.027
VL  - 307
SP  - 195
EP  - 204
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047515976&doi=10.1016%2fj.neucom.2018.04.027&partnerID=40&md5=c8b27ed05118590fb453407e684ea2b0
DB  - Scopus
KW  - machine learning
KW  - Deep learning
KW  - Neural networks
KW  - Training data
KW  - Supervised learning
KW  - algorithm
KW  - Article
KW  - priority journal
KW  - Optimization
KW  - accuracy
KW  - data processing
KW  - artificial neural network
KW  - Robust optimization
KW  - model
KW  - adversarial training
KW  - process optimization
KW  - Adversarial examples
KW  - Alternating minimization
KW  - Local stability
KW  - Non-parametric
KW  - Non-parametric model
KW  - Non-parametric supervised models
KW  - Test data
ER  - 

TY  - JOUR
TI  - Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems with Unknown Dynamics
AU  - Yang, X.
AU  - He, H.
T2  - IEEE Transactions on Cybernetics
AB  - This paper develops a novel event-triggered robust control strategy for continuous-time nonlinear systems with unknown dynamics. To begin with, the event-triggered robust nonlinear control problem is transformed into an event-triggered nonlinear optimal control problem by introducing an infinite-horizon integral cost for the nominal system. Then, a recurrent neural network (RNN) and adaptive critic designs (ACDs) are employed to solve the derived event-triggered nonlinear optimal control problem. The RNN is applied to reconstruct the system dynamics based on collected system data. After acquiring the knowledge of system dynamics, a unique critic network is proposed to obtain the approximate solution of the event-triggered Hamilton-Jacobi-Bellman equation within the framework of ACDs. The critic network is updated by using simultaneously historical and instantaneous state data. An advantage of the present critic network update law is that it can relax the persistence of excitation condition. Meanwhile, under a newly developed event-triggering condition, the proposed critic network tuning rule not only guarantees the critic network weights to converge to optimums but also ensures nominal system states to be uniformly ultimately bounded. Moreover, by using Lyapunov method, it is proved that the derived optimal event-triggered control (ETC) guarantees uniform ultimate boundedness of all the signals in the original system. Finally, a nonlinear oscillator and an unstable power system are provided to validate the developed robust ETC scheme. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TCYB.2018.2823199
VL  - 49
IS  - 6
SP  - 2255
EP  - 2267
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045765098&doi=10.1109%2fTCYB.2018.2823199&partnerID=40&md5=5c47089e8fef0b934762484ce73a6aa0
DB  - Scopus
KW  - Reinforcement learning
KW  - Adaptive systems
KW  - Adaptive control systems
KW  - Robustness (control systems)
KW  - Recurrent neural networks
KW  - Robust control
KW  - Dynamical systems
KW  - Lyapunov methods
KW  - Nonlinear dynamical systems
KW  - reinforcement learning (RL)
KW  - Adaptive dynamic programming
KW  - Dynamic programming
KW  - Optimal control systems
KW  - Optimal controls
KW  - robust control
KW  - Continuous time systems
KW  - System Dynamics
KW  - System theory
KW  - neural networks (NNs)
KW  - Event-triggered controls
KW  - Adaptive critic designs
KW  - Adaptive critic designs (ACDs)
KW  - adaptive dynamic programming (ADP)
KW  - event-triggered control (ETC)
KW  - Neural networks (NNS)
ER  - 

TY  - CONF
TI  - Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety
AU  - Canonico, L.B.
AU  - McNeese, N.
T2  - Proceedings - Winter Simulation Conference
AB  - As AI advances and becomes more complicated, it becomes necessary to study the safety implications of its behavior. This paper expands upon prior AI-safety research to create a model to study the harmful outcomes of multi-agent systems. In this paper, we outline previous work that has highlighted multiple aspects of AI-safety research and focus on AI-safety systems in multi-agent systems. After overviewing previous literature, we present a model focused on flash crashes, a concept often found in economics. The model was constructed using an interdisciplinary approach that includes game theory, machine learning, cognitive science and systems theory to study flash crashes in complex human-AI systems. We use the model to study a complex interaction between AI-agents, and our results indicate the multi-agent system in question is prone to cause flash crashes. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/WSC40007.2019.9004675
VL  - 2019-December
SP  - 193
EP  - 204
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081129928&doi=10.1109%2fWSC40007.2019.9004675&partnerID=40&md5=21d0982f79295c9d8e0fd9260e9e4833
DB  - Scopus
KW  - Cognitive science
KW  - Game theory
KW  - Reinforcement learning
KW  - Learning systems
KW  - Cognitive systems
KW  - AI systems
KW  - Multi agent systems
KW  - Safety research
KW  - Minority game
ER  - 

TY  - CONF
TI  - N-version machine learning models for safety critical systems
AU  - Machida, F.
T2  - Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop, DSN-W 2019
AB  - Quality control of machine learning systems is a fundamental challenge in industries to provide intelligent services or products using machine learning. While recent advances in machine learning algorithms substantially improve the performance of intelligent tasks such as object recognition, their outputs are essentially stochastic and very sensitive to input data. Such an output uncertainty is a big obstacle to ensure the quality of safety critical applications like autonomous vehicle and hence architectural design to mitigate the impact of error output becomes a great importance. In this paper, we propose N-version machine learning architecture that aims to improve system reliability against probabilistic outputs of individual machine learning modules. The key idea of this architecture is exploiting two kinds of diversities; input diversity and model diversity. Our study first formally defines these diversity metrics and analytically shows the improved reliability by N-version machine learning architecture. Since we treat a machine learning module as a black-box, the proposed architecture and the reliability property are generally applicable to any machine learning algorithms and applications. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/DSN-W.2019.00017
SP  - 48
EP  - 51
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072049689&doi=10.1109%2fDSN-W.2019.00017&partnerID=40&md5=72ee90ee38a50aa4360eabec023ddb53
DB  - Scopus
KW  - machine learning
KW  - Machine learning
KW  - Learning systems
KW  - reliability
KW  - Learning algorithms
KW  - Accident prevention
KW  - safety
KW  - Machine learning models
KW  - Safety engineering
KW  - Network architecture
KW  - Stochastic systems
KW  - Reliability
KW  - Object recognition
KW  - Safety critical systems
KW  - Proposed architectures
KW  - design diversity
KW  - Design diversity
KW  - Machine learning module
KW  - N version programming
KW  - N-version programming
KW  - Probabilistic output
KW  - Reliability properties
ER  - 

TY  - JOUR
TI  - An Improved Robust Fuzzy Algorithm for Unsupervised Learning
AU  - Dik, A.
AU  - Jebari, K.
AU  - Ettouhami, A.
T2  - Journal of Intelligent Systems
AB  - This paper presents a robust, dynamic, and unsupervised fuzzy learning algorithm (RDUFL) that aims to cluster a set of data samples with the ability to detect outliers and assign the numbers of clusters automatically. It consists of three main stages. The first (1) stage is a pre-processing method in which possible outliers are determined and quarantined using a concept of proximity degree. The second (2) stage is a learning method, which consists in auto-detecting the number of classes with their prototypes for a dynamic threshold. This threshold is automatically determined based on the similarity among the detected prototypes that are updated at the exploration of a new data. The last (3) stage treats quarantined samples detected from the first stage to determine whether they belong to some class defined in the second phase. The effectiveness of this method is assessed on eight real medical benchmark datasets in comparison to known unsupervised learning methods, namely, the fuzzy c-means (FCM), possibilistic c-means (PCM), and noise clustering (NC). The obtained accuracy of our scheme is very promising for unsupervised learning problems. © 2020 Walter de Gruyter GmbH, Berlin/Boston.
DA  - 2020///
PY  - 2020
DO  - 10.1515/jisys-2018-0030
VL  - 29
IS  - 1
SP  - 1028
EP  - 1042
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056243355&doi=10.1515%2fjisys-2018-0030&partnerID=40&md5=c83a75d5ee40e2f0bc136d8e5f54214b
DB  - Scopus
L1  - https://www.degruyter.com/document/doi/10.1515/jisys-2018-0030/pdf
KW  - Machine learning
KW  - Fuzzy sets
KW  - Anomaly detection
KW  - Learning algorithms
KW  - Benchmark datasets
KW  - Statistics
KW  - Unsupervised learning
KW  - outlier detection
KW  - clustering
KW  - Dynamic threshold
KW  - Information dissemination
KW  - Possibilistic C-means
KW  - Pre-processing method
KW  - proximity degree
KW  - Similarity measure
KW  - Unsupervised learning method
ER  - 

TY  - CONF
TI  - Assurance argument patterns and processes for machine learning in safety-related systems
AU  - Picardi, C.
AU  - Paterson, C.
AU  - Hawkins, R.
AU  - Calinescu, R.
AU  - Habli, I.
T2  - CEUR Workshop Proceedings
AB  - Machine Learnt (ML) components are now widely accepted for use in a range of applications with results that are reported to exceed, under certain conditions, human performance. The adoption of ML components in safety-related domains is restricted, however, unless sufficient assurance can be demonstrated that the use of these components does not compromise safety. In this paper, we present patterns that can be used to develop assurance arguments for demonstrating the safety of the ML components. The argument patterns provide reusable templates for the types of claims that must be made in a compelling argument. On their own, the patterns neither detail the assurance artefacts that must be generated to support the safety claims for a particular system, nor provide guidance on the activities that are required to generate these artefacts. We have therefore also developed a process for the engineering of ML components in which the assurance evidence can be generated at each stage in the ML lifecycle in order to instantiate the argument patterns and create the assurance case for ML components. The patterns and the process could help provide a practical and clear basis for a justifiable deployment of ML components in safety-related systems. © 2020 for this paper by its authors.
DA  - 2020///
PY  - 2020
VL  - 2560
SP  - 23
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604916&partnerID=40&md5=aeaccb7806b23dc5dcf079764a13b75b
DB  - Scopus
KW  - Machine learning
KW  - Safety engineering
KW  - Machine components
KW  - Safety-related systems
KW  - Life cycle
KW  - Assurance case
KW  - Safety-Related
KW  - Human performance
KW  - Provide guidances
KW  - Reusable templates
ER  - 

TY  - CONF
TI  - Algorithmic robustness for semi-supervised (ε, γ, τ)-good metric learning
AU  - Nicolae, M.-I.
AU  - Sebban, M.
AU  - Habrard, A.
AU  - Gaussier, E.
AU  - Amini, M.-R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Using the appropriate metric is crucial for the performance of most of machine learning algorithms. For this reason, a lot of effort has been put into distance and similarity learning. However, it is worth noting that this research field lacks theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a learned metric. The theoretical framework of (ε, γ, τ)-good similarity functions [1] provides means to relate the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend this theory to a method where the metric and the separator are jointly learned in a semi-supervised way, setting that has not been explored before. We furthermore prove the robustness of our algorithm, which allows us to provide a generalization bound for this approach. The behavior of our method is illustrated via some experimental results. © Springer International Publishing Switzerland 2015.
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-319-26532-2_28
VL  - 9489
SP  - 253
EP  - 263
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952837463&doi=10.1007%2f978-3-319-26532-2_28&partnerID=40&md5=acb93b59e9c15b2a75bd6972252bfefd
DB  - Scopus
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-319-26532-2_28.pdf
KW  - Artificial intelligence
KW  - Learning systems
KW  - Algorithms
KW  - Learning algorithms
KW  - Information science
KW  - Metric learning
KW  - Theoretical guarantees
KW  - Theoretical framework
KW  - Generalization bound
KW  - Generalization capacity
KW  - Linear classifiers
KW  - Similarity functions
KW  - Similarity learning
ER  - 

TY  - CONF
TI  - Extreme learning machine: A robust modeling technique? yes!
AU  - Lendasse, A.
AU  - Akusok, A.
AU  - Simula, O.
AU  - Corona, F.
AU  - Van Heeswijk, M.
AU  - Eirola, E.
AU  - Miche, Y.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - In this paper is described the original (basic) Extreme Learning Machine (ELM). Properties like robustness and sensitivity to variable selection are studied. Several extensions of the original ELM are then presented and compared. Firstly, Tikhonov-Regularized Optimally-Pruned Extreme Learning Machine (TROP-ELM) is summarized as an improvement of the Optimally-Pruned Extreme Learning Machine (OP-ELM) in the form of a L 2 regularization penalty applied within the OP-ELM. Secondly, a Methodology to Linearly Ensemble ELM (-ELM) is presented in order to improve the performance of the original ELM. These methodologies (TROP-ELM and -ELM) are tested against state of the art methods such as Support Vector Machines or Gaussian Processes and the original ELM and OP-ELM, on ten different data sets. A specific experiment to test the sensitivity of these methodologies to variable selection is also presented. © 2013 Springer-Verlag Berlin Heidelberg.
DA  - 2013///
PY  - 2013
DO  - 10.1007/978-3-642-38679-4_2
VL  - 7902 LNCS
SP  - 17
EP  - 35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880077016&doi=10.1007%2f978-3-642-38679-4_2&partnerID=40&md5=1f46c0c9280c127a485d686042f32665
DB  - Scopus
KW  - Neural networks
KW  - Learning systems
KW  - Knowledge acquisition
KW  - Extreme learning machine
KW  - State-of-the-art methods
KW  - Gaussian Processes
KW  - Robust modeling
KW  - Variable selection
ER  - 

TY  - CONF
TI  - When Neurons Fail
AU  - El Mhamdi, E.M.
AU  - Guerraoui, R.
T2  - Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium, IPDPS 2017
AB  - Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neuralactivation functions are Lipschitz-continuous. Our bound isgiven in the form of quantity, we call the Forward ErrorPropagation, computing this quantity only requires looking atthe topology of the network, while experimentally assessingthe robustness of a network requires the costly experiment oflooking at all the possible inputs and testing all the possibleconfigurations of the network corresponding to different failuresituations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neuronsper layer, the Lipschitz constant of the neural activationfunction, the number of failing neurons, the synaptic weightsand the depth of the layer where the failure occurred. In thecase of Byzantine failures, our bound involves, in addition, thesynaptic transmission capacity. Interestingly, as we show inthe paper, our bound can easily be extended to the case wheresynapses can fail. We present three applications of our results. The first is aquantification of the effect of memory cost reduction on theaccuracy of a neural network. The second is a quantification ofthe amount of information any neuron needs from its precedinglayer, enabling thereby a boosting scheme that prevents neuronsfrom waiting for unnecessary signals. Our third applicationis a quantification of the trade-off between neural networksrobustness and learning cost. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/IPDPS.2017.66
SP  - 1028
EP  - 1037
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027702685&doi=10.1109%2fIPDPS.2017.66&partnerID=40&md5=d12a7728772dcc4135077fd4178a0145
DB  - Scopus
KW  - Neural networks
KW  - Machine Learning
KW  - Neurons
KW  - Learning systems
KW  - Economic and social effects
KW  - Robustness (control systems)
KW  - Neural Networks
KW  - Robustness
KW  - Fault tolerance
KW  - Cost reduction
KW  - Distributed computer systems
KW  - Multilayer neural networks
KW  - Critical applications
KW  - Distributed systems
KW  - Fault tolerant computer systems
KW  - Combinatorial explosion
KW  - Amount of information
KW  - Byzantine fault tolerance
KW  - Byzantine Fault Tolerance
KW  - Distributed Systems
KW  - Lipschitz continuous
KW  - Neuromorphic computing
KW  - Transmission capacities
ER  - 

TY  - CONF
TI  - Gray-box adversarial training
AU  - Vivek, B.S.
AU  - Mopuri, K.R.
AU  - Babu, R.V.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed “gray-box adversarial attacks” based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named “Gray-box Adversarial Training” that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained models. © Springer Nature Switzerland AG 2018.
DA  - 2018///
PY  - 2018
DO  - 10.1007/978-3-030-01267-0_13
VL  - 11219 LNCS
SP  - 213
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055438686&doi=10.1007%2f978-3-030-01267-0_13&partnerID=40&md5=e7a92b9cac84761bd58ae819624a68fd
DB  - Scopus
KW  - Artificial intelligence
KW  - Learning systems
KW  - Machine learning models
KW  - Computer vision
KW  - On-machines
KW  - Large datasets
KW  - Adversarial training
KW  - Experimental evaluation
KW  - Adversarial perturbations
KW  - Attacks on machine learning models
KW  - Evaluation policy
KW  - Gradient ascent
KW  - Robust machine learning models
KW  - Robust models
ER  - 

TY  - CHAP
TI  - Robust fusion of unreliable data sources using error-correcting output codes
AU  - Vempaty, A.
AU  - Kailkhura, B.
AU  - Varshney, P.K.
T2  - Data Fusion in Wireless Sensor Networks
AB  - The emergence of big and dirty data era demands new distributed learning and inference solutions to tackle the problem of inference with corrupted data. The central goal of this chapter is to discuss the presence of corrupted data in the context of distributed inference networks (DINs) and discuss coding-theoretic strategies to ensure reliable inference performance in several practical scenarios. It discusses a generalization of the classical Byzantine Generals problem in the context of distributed inference to different topologies. Over the last three decades, research community has extensively studied the impact of imperfect transmission channels or sensor faults on distributed inference systems. However, corrupted (Byzantine) data models, considered in this chapter, are philosophically different from the imperfect channels or faulty sensor cases. Byzantines are intentional and intelligent and therefore can optimize over the data corruption parameters. While learning their behavior and actively countering them is a viable approach, this chapter presents a new paradigm of mitigation strategies that use coding-theoretic results. The general approach of error-correcting output codes (ECOC) for data fusion is presented and its applicability for several inference problems in practice dealing with unreliable data including Byzantines is shown. This approach is then shown to be applicable to a wider range of inference problems such as classification using crowdsourced data. © The Institution of Engineering and Technology 2019.
DA  - 2019///
PY  - 2019
SP  - 291
EP  - 311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117998598&doi=10.1049%2fPBCE117E_ch12&partnerID=40&md5=c7334508b4b3a8a50b7d4bf13b57694b
DB  - Scopus
KW  - Sensor fusion
KW  - Learning (artificial intelligence)
KW  - Data fusion
KW  - Radio links and equipment
KW  - Chapter
KW  - Classical byzantine generals problem
KW  - Codes
KW  - Coding-theoretic results
KW  - Coding-theoretic strategies
KW  - Corrupted data models
KW  - Crowdsourced data
KW  - Data corruption parameters
KW  - Data handling techniques
KW  - Dirty data era demands new distributed learning
KW  - Distributed inference networks
KW  - Distributed inference systems
KW  - Error correction codes
KW  - Error-correcting output codes
KW  - Fault tolerant computing
KW  - General approach
KW  - Imperfect channels
KW  - Imperfect transmission channels
KW  - Inference mechanisms
KW  - Inference problems
KW  - Inference solutions
KW  - Knowledge engineering techniques
KW  - Other topics in statistics
KW  - Pattern classification
KW  - Reliable inference performance
KW  - Robust fusion
KW  - Unreliable data sources
ER  - 

TY  - JOUR
TI  - Improving the robustness of instance-based reinforcement learning robots by metalearning
AU  - Yasuda, T.
AU  - Araki, K.
AU  - Ohkura, K.
T2  - Journal of Advanced Computational Intelligence and Intelligent Informatics
AB  - Learning autonomous robots have been widely discussed in recent years. Reinforcement learning (RL) is a popular method in this domain. However, its performance is quite sensitive to the segmentation of state and action spaces. To overcome this problem, we developed the new technique Bayesian- discriminationfunction- based RL (BRL). BRL has proven to be more effective than other standard RL algorithms in dealing withmulti-robot system(MRS) problems. However, as in most learning systems, occasional overfitting problems occur in BRL. This paper introduces an extended BRL for improving the robustness of MRSs. Metalearning based on the information entropy of fired rules is adopted for adaptive modification of its learning parameters. Computer simulations are conducted to verify the effectiveness of our proposed method.
DA  - 2011///
PY  - 2011
DO  - 10.20965/jaciii.2011.p1065
VL  - 15
IS  - 8
SP  - 1065
EP  - 1072
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054952620&doi=10.20965%2fjaciii.2011.p1065&partnerID=40&md5=a01e1846ec1ab4558076a5d6405651e1
DB  - Scopus
KW  - Reinforcement learning
KW  - Robustness
KW  - Metalearning
KW  - Multi-robot system
ER  - 

TY  - JOUR
TI  - Robust automatic target recognition using learning classifier systems
AU  - Ravichandran, B.
AU  - Gandhe, A.
AU  - Smith, R.
AU  - Mehra, R.
T2  - Information Fusion
AB  - This work developed and demonstrated a machine learning approach for robust ATR. The primary innovation of this work was the development of an automated way of developing inference rules that can draw on multiple models and multiple feature types to make robust ATR decisions. The key realization is that this "meta learning" problem is one of structural learning, and that it can be conducted independently of parameter learning associated with each model and feature based technique. This was accomplished by using a learning classifier system, which is based on genetics-based machine learning, for the ill conditioned combinatorial problem of structural rule learning, while using statistical and mathematical techniques for parameter learning. This system was tested on MSTAR Public Release SAR data using standard and extended operation conditions. These results were also compared against two baseline classifiers, a PCA based distance classifier and a MSE classifier. The classifiers were evaluated for accuracy (via training set classification) and robustness (via testing set classification). In both cases, the LCS based robust ATR system performed well with accuracy over 99% and robustness over 80%. © 2006 Elsevier B.V. All rights reserved.
DA  - 2007///
PY  - 2007
DO  - 10.1016/j.inffus.2006.03.001
VL  - 8
IS  - 3
SP  - 252
EP  - 265
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947672844&doi=10.1016%2fj.inffus.2006.03.001&partnerID=40&md5=518bef4bc6bad38b249e8a52e4abf601
DB  - Scopus
KW  - Machine learning
KW  - Mathematical models
KW  - Learning systems
KW  - Classification (of information)
KW  - Automatic target recognition
KW  - Robustness (control systems)
KW  - Decision theory
KW  - Evolutionary algorithms
KW  - Inference engines
KW  - Inference rules
KW  - Learning classifier systems
KW  - Parameter learning
KW  - Robust automatic target recognition
KW  - Standard/extended operating conditions
ER  - 

TY  - JOUR
TI  - Robust supervised classification with mixture models: Learning from data with uncertain labels
AU  - Bouveyron, C.
AU  - Girard, S.
T2  - Pattern Recognition
AB  - In the supervised classification framework, human supervision is required for labeling a set of learning data which are then used for building the classifier. However, in many applications, human supervision is either imprecise, difficult or expensive. In this paper, the problem of learning a supervised multi-class classifier from data with uncertain labels is considered and a model-based classification method is proposed to solve it. The idea of the proposed method is to confront an unsupervised modeling of the data with the supervised information carried by the labels of the learning data in order to detect inconsistencies. The method is able afterward to build a robust classifier taking into account the detected inconsistencies into the labels. Experiments on artificial and real data are provided to highlight the main features of the proposed method as well as an application to object recognition under weak supervision. © 2009 Elsevier Ltd. All rights reserved.
DA  - 2009///
PY  - 2009
DO  - 10.1016/j.patcog.2009.03.027
VL  - 42
IS  - 11
SP  - 2649
EP  - 2658
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649389414&doi=10.1016%2fj.patcog.2009.03.027&partnerID=40&md5=32d729d76c33d6f8ee081360fb958e28
DB  - Scopus
KW  - Learning systems
KW  - Robustness
KW  - Education
KW  - Object recognition
KW  - Label noise
KW  - Mixtures
KW  - Mixture models
KW  - Supervised classification
KW  - Classifiers
KW  - Data with uncertain labels
KW  - Labels
KW  - Weakly supervised classification
ER  - 

TY  - CONF
TI  - A robust unsupervised feature learning framework using spatial boosting networks
AU  - Le, N.D.-H.
AU  - Tran, M.-T.
T2  - Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013
AB  - To boost up power of unsupervised feature learning and deep learning, there has been a great effort in optimizing network structure to learn more efficient high level features. It is crucial for a network to have a sufficient amount of learnable parameters yet still be able to capture in variances in data. In this paper, the authors propose spatial boosting networks, which employ convolutional feature learning networks as learning components. Each component in a network is assigned to a certain spatial region. This allows the network learn more adaptive features for each region. In order to make spatial boosting networks to capture relationship between regions of the visual field, we also propose convolutional pooling procedure. By expanding pooling scope into overlapping regions, we expect the features pooled in higher level to be more robust to noises and more invariant to transformation. Experiments show that using spatial boosting networks boosts up accuracy up to 3% from conventional approaches in standard datasets CIFAR and STL. Moreover, these results are competitive in comparison with other methods by using only a basic feature learning algorithm. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/ICMLA.2013.168
VL  - 2
SP  - 507
EP  - 512
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899422191&doi=10.1109%2fICMLA.2013.168&partnerID=40&md5=d37cd0f93e3521438336b1269f6828d6
DB  - Scopus
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=6786161&ref=
KW  - Deep learning
KW  - deep learning
KW  - Neural networks
KW  - Learning systems
KW  - Learning algorithms
KW  - Convolution
KW  - Network structures
KW  - Unsupervised feature learning
KW  - Feature learning
KW  - Conventional approach
KW  - Adaptive features
KW  - High-level features
KW  - Overlapping regions
KW  - unsupervised feature learning
ER  - 

TY  - JOUR
TI  - Improving fairness generalization through a sample-robust optimization method
AU  - Ferry, J.
AU  - Aïvodji, U.
AU  - Gambs, S.
AU  - Huguet, M.-J.
AU  - Siala, M.
T2  - Machine Learning
AB  - Unwanted bias is a major concern in machine learning, raising in particular significant ethical issues when machine learning models are deployed within high-stakes decision systems. A common solution to mitigate it is to integrate and optimize a statistical fairness metric along with accuracy during the training phase. However, one of the main remaining challenges is that current approaches usually generalize poorly in terms of fairness on unseen data. We address this issue by proposing a new robustness framework for statistical fairness in machine learning. The proposed approach is inspired by the domain of distributionally robust optimization and works in ensuring fairness over a variety of samplings of the training set. Our approach can be used to quantify the robustness of fairness but also to improve it when training a model. We empirically evaluate the proposed method and show that it effectively improves fairness generalization. In addition, we propose a simple yet powerful heuristic application of our framework that can be integrated into a wide range of existing fair classification techniques to enhance fairness generalization. Our extensive empirical study using two existing fair classification methods demonstrates the efficiency and scalability of the proposed heuristic approach. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s10994-022-06191-y
VL  - 112
IS  - 6
SP  - 2131
EP  - 2192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133582221&doi=10.1007%2fs10994-022-06191-y&partnerID=40&md5=07ba1bd2f24d1fcdbb25acd34bbbbe73
DB  - Scopus
KW  - Machine learning
KW  - Fairness
KW  - Supervised learning
KW  - Machine-learning
KW  - Machine learning models
KW  - Optimization
KW  - Robust optimization
KW  - Heuristic methods
KW  - Training phasis
KW  - Distributionally robust optimization
KW  - Generalization
KW  - Generalisation
KW  - Ethical issues
KW  - Decision systems
KW  - Robust optimization method
ER  - 

TY  - JOUR
TI  - Supervised contrastive learning for robust text adversarial training
AU  - Li, W.
AU  - Zhao, B.
AU  - An, Y.
AU  - Shangguan, C.
AU  - Ji, M.
AU  - Yuan, A.
T2  - Neural Computing and Applications
AB  - The lack of robustness is a serious problem for deep neural networks (DNNs) and makes DNNs vulnerable to adversarial examples. A promising solution is applying adversarial training to alleviate this problem, which allows the model to learn the features from adversarial examples. However, adversarial training usually produces overfitted models and may not work when facing a new attack. We believe this is because the previous adversarial training using cross-entropy loss ignores the similarity between the adversarial examples and the original examples, which will result in a low margin. Accordingly, we propose a supervised adversarial contrastive learning (SACL) approach for adversarial training. SACL uses supervised adversarial contrastive loss which contains both the cross-entropy term and adversarial contrastive term. The cross-entropy term is used for guiding DNN inductive bias learning, and the adversarial contrastive term can help models learn example representations by maximizing feature consistency under different original examples, which fits well with the goal of solving low margins. In addition, SACL only uses adversarial examples which can successfully fool the model and their corresponding original examples for training. This process is more advantageous to provide the model with more accurate information about the decision boundary and obtain a model that fits the example distribution. Experiments show that SACL can reduce the attack success rate of multiple adversarial attack algorithms against different models on text classification tasks. The defensive performance is significantly better than other adversarial training approaches without reducing the generalization ability of the model. In addition, the DNN model trained by our approach has high transferability and robustness. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s00521-022-07871-5
VL  - 35
IS  - 10
SP  - 7357
EP  - 7368
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144688930&doi=10.1007%2fs00521-022-07871-5&partnerID=40&md5=1494523c566a87356fb3d171b70d52cf
DB  - Scopus
KW  - Deep neural networks
KW  - Learning systems
KW  - Supervised learning
KW  - Neural network
KW  - Classification (of information)
KW  - Learning approach
KW  - Text processing
KW  - Learn+
KW  - Neural-networks
KW  - Entropy
KW  - Adversarial attack
KW  - Contrastive learning
KW  - Bias Learning
KW  - Cross entropy
KW  - Entropy loss
KW  - HELP model
KW  - Inductive bias
ER  - 

TY  - CONF
TI  - Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions
AU  - Jia, L.-H.
AU  - Guo, L.-Z.
AU  - Zhou, Z.
AU  - Shao, J.-J.
AU  - Xiang, Y.-K.
AU  - Li, Y.-F.
T2  - Proceedings of Machine Learning Research
AB  - Semi-supervised learning (SSL) suffers from severe performance degradation when labeled and unlabeled data come from inconsistent data distributions. However, there is still a lack of sufficient theoretical guidance on how to alleviate this problem. In this paper, we propose a general theoretical framework that demonstrates how distribution discrepancies caused by pseudo-label predictions and target predictions can lead to severe generalization errors. Through theoretical analysis, we identify three main reasons why previous SSL algorithms cannot perform well with inconsistent distributions: coupling between the pseudo-label predictor and the target predictor, biased pseudo labels, and restricted sample weights. To address these challenges, we introduce a practical framework called Bidirectional Adaptation that can adapt to the distribution of unlabeled data for debiased pseudo-label prediction and to the target distribution for debiased target prediction, thereby mitigating these shortcomings. Extensive experimental results demonstrate the effectiveness of our proposed framework. © 2023 Proceedings of Machine Learning Research. All rights reserved.
DA  - 2023///
PY  - 2023
VL  - 202
SP  - 14886
EP  - 14901
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174426186&partnerID=40&md5=83bb3938b25b176e9237be2cf5d0084a
DB  - Scopus
KW  - Supervised learning
KW  - Learning algorithms
KW  - Forecasting
KW  - Semi-supervised learning
KW  - Performance degradation
KW  - Generalization Error
KW  - Data distribution
KW  - Theoretical framework
KW  - Target prediction
KW  - Label predictions
KW  - Labeled and unlabeled data
KW  - Inconsistent data
KW  - Samples weight
ER  - 

TY  - CONF
TI  - CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing
AU  - Altmann, P.
AU  - Ritz, F.
AU  - Feuchtinger, L.
AU  - Nüßlein, J.
AU  - Linnhoff-Popien, C.
AU  - Phan, T.
T2  - IJCAI International Joint Conference on Artificial Intelligence
AB  - The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.
DA  - 2023///
PY  - 2023
VL  - 2023-August
SP  - 3414
EP  - 3422
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170359176&partnerID=40&md5=324523ba832f4699dfb4c359f306a724
DB  - Scopus
KW  - Reinforcement learning
KW  - Training data
KW  - Reinforcement learnings
KW  - 'current
KW  - Policy optimization
KW  - Data augmentation
KW  - Generalisation
KW  - Crops
KW  - Overfitting
KW  - State-of-the-art approach
KW  - Augmentation techniques
KW  - Limited training data
ER  - 

TY  - CONF
TI  - Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data
AU  - Liu, Z.
AU  - Guo, Z.
AU  - Cen, Z.
AU  - Zhang, H.
AU  - Yao, Y.
AU  - Hu, H.
AU  - Zhao, D.
T2  - Proceedings of Machine Learning Research
AB  - Previous work demonstrates that the optimal safe reinforcement learning policy in a noise-free environment is vulnerable and could be unsafe under observational attacks. While adversarial training effectively improves robustness and safety, collecting samples by attacking the behavior agent online could be expensive or prohibitively dangerous in many applications. We propose the robuSt vAriational ofF-policy lEaRning (SAFER) approach, which only requires benign training data without attacking the agent. SAFER obtains an optimal non-parametric variational policy distribution via convex optimization and then uses it to improve the parameterized policy robustly via supervised learning. The two-stage policy optimization facilitates robust training, and extensive experiments on multiple robot platforms show the efficiency of SAFER in learning a robust and safe policy: achieving the same reward with much fewer constraint violations during training than on-policy baselines. © 2023 Proceedings of Machine Learning Research. All rights reserved.
DA  - 2023///
PY  - 2023
VL  - 202
SP  - 22249
EP  - 22265
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408788&partnerID=40&md5=6463a867d47c3f19ff2fbca9a27362f9
DB  - Scopus
KW  - Reinforcement learning
KW  - Training data
KW  - Reinforcement learnings
KW  - Convex optimization
KW  - Policy learning
KW  - Learning policy
KW  - Convex optimisation
KW  - Parameterized
KW  - Behavior agents
KW  - Noise-free environments
KW  - Nonparametrics
KW  - Policy distribution
ER  - 

TY  - CONF
TI  - Improving Machine Learning Robustness via Adversarial Training
AU  - Dang, L.
AU  - Hapuarachchi, T.
AU  - Xiong, K.
AU  - Lin, J.
T2  - Proceedings - International Conference on Computer Communications and Networks, ICCCN
AB  - As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-IID data, respectively, where CIFAR-10 is used in this research. In the IID data case, our experimental results demonstrate that we can achieve such a robust accuracy that it is comparable to the one obtained in the centralized environment. Moreover, in the non-IID data case, the natural accuracy drops from 66.23% to 57.82%, and the robust accuracy decreases by 25% and 23.4% in C&W and Projected Gradient Descent (PGD) attacks, compared to the IID data case, respectively. We further propose an IID data-sharing approach, which allows for increasing the natural accuracy to 85.04% and the robust accuracy from 57% to 72% in C&W attacks and from 59% to 67% in PGD attacks. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICCCN58024.2023.10230138
VL  - 2023-July
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173587747&doi=10.1109%2fICCCN58024.2023.10230138&partnerID=40&md5=39307fd66c57626e402a9a0ca5cad66b
DB  - Scopus
KW  - Machine learning
KW  - Machine learning algorithms
KW  - Machine-learning
KW  - Computer vision
KW  - Gradient methods
KW  - Federated learning
KW  - Projected gradient
KW  - Adversarial training
KW  - Decentralised
KW  - Centralised
KW  - Distributed data
KW  - Independent and identically distributed  and non-identically distributed data
KW  - Independent and identically distributed (IID) and non-IID data
KW  - Machine learning robustness
ER  - 

TY  - JOUR
TI  - Robust multi-agent reinforcement learning via Bayesian distributional value estimation
AU  - Du, X.
AU  - Chen, H.
AU  - Wang, C.
AU  - Xing, Y.
AU  - Yang, J.
AU  - Yu, P.S.
AU  - Chang, Y.
AU  - He, L.
T2  - Pattern Recognition
AB  - Reinforcement learning in multi-agent scenarios is essential for real-world applications as it can vividly depict agents’ collaborative and competitive behaviors from a perspective closer to reality. However, most existing studies suffer from poor robustness, preventing multi-agent reinforcement learning from practical applications where robustness is the core indicator of system security and stability. In view of this, we propose a novel Bayesian Multi-Agent Reinforcement Learning method, named BMARL, which leverages the distributional value function calculated by Bayesian inference to improve the robustness of the model. Specifically, Bayesian linear regression is adopted to estimate a posterior distribution concerning value function parameters, rather than approximating an expectation value for Q-value by point estimation. In this way, the value function is more generalized than previously obtained by point estimation, which is beneficial to the robustness of our model. Meanwhile, we utilize the Gaussian prior knowledge to integrate more prior knowledge while estimating the value function, which improves learning efficiency. Extensive experimental results on three benchmark multi-agent environments comparing with seven state-of-the-art methods demonstrate the superiority of BMARL in terms of both robustness and efficiency. © 2023 Elsevier Ltd
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.patcog.2023.109917
VL  - 145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169977108&doi=10.1016%2fj.patcog.2023.109917&partnerID=40&md5=e595d43fa1635c5e71562359b2cf6266
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Deep reinforcement learning
KW  - Reinforcement learnings
KW  - Bayesian networks
KW  - Efficiency
KW  - Inference engines
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Prior-knowledge
KW  - Bayesian
KW  - Value functions
KW  - Bayesian inference
KW  - Distributional value function
KW  - Point estimation
KW  - Value estimation
ER  - 

TY  - CONF
TI  - Towards Robust Off-Policy Evaluation via Human Inputs
AU  - Singh, H.
AU  - Joshi, S.
AU  - Doshi-Velez, F.
AU  - Lakkaraju, H.
T2  - AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society
AB  - Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3514094.3534198
SP  - 686
EP  - 699
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137169796&doi=10.1145%2f3514094.3534198&partnerID=40&md5=adb078c8cd93894d712ddc5b63ed82f8
DB  - Scopus
KW  - Machine learning
KW  - Machine-learning
KW  - Evaluation methods
KW  - Markov processes
KW  - Health care
KW  - adversarial machine learning
KW  - Adversarial machine learning
KW  - Robust learning
KW  - Domain knowledge
KW  - Domain Knowledge
KW  - Computational complexity
KW  - Property
KW  - robust learning
KW  - Covariates
KW  - Dataset shifts
KW  - Human-in-the-loop
KW  - dataset shift
KW  - human-in-The-loop
KW  - policy evaluation
KW  - Policy evaluation
ER  - 

TY  - CONF
TI  - Safety Assurance with Ensemble-based Uncertainty Estimation and overlapping alternative Predictions in Reinforcement Learning
AU  - Eilers, D.
AU  - Burton, S.
AU  - Roza, F.S.
AU  - Roscher, K.
T2  - CEUR Workshop Proceedings
AB  - A number of challenges are associated with the use of machine learning technologies in safety-related applications. These include the difficulty of specifying adequately safe behaviour in complex environments (specification uncertainty), ensuring a predictably safe behaviour under all operating conditions (technical uncertainty) and arguing that the safety goals of the system have been met with sufficient confidence (assurance uncertainty). An assurance argument is therefore required that demonstrates that the effects of these uncertainties do not lead to an unacceptable level of risk during operation. A reinforcement learning model will predict an action in whatever state it is in - even in previously unseen states for which a valid (safe) outcome cannot be determined due to lack of training. Uncertainty estimation is a well understood approach in machine learning to identify states with a high probability of an invalid action due a lack of training experience, thus addressing technical uncertainty. However, the impact of alternative possible predictions which may be equally valid (and represent a safe state) in estimating uncertainty in reinforcement learning is not so clear and to our knowledge, not so well documented in current literature. In this paper we build on work where we investigated uncertainty estimation on simplified scenarios in a gridworld environment. Using model ensemble-based uncertainty estimation we proposed an algorithm based on action count variance to deal with discrete action spaces whilst considering in-distribution action variance calculation to handle the overlap with alternative predictions. The method indicates potentially unsafe states when the agent is near out-of-distribution elements and can distinguish it from overlapping alternative, but equally valid predictions. Here, we present these results within the context of a safety assurance framework and highlight the activities and evidences required to build a convincing safety argument. We show that our previous approach is able to act as an external observer and can fulfil the requirements of an assurance argumentation for systems based on machine learning with ontological uncertainty. © 2023 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)
DA  - 2023///
PY  - 2023
VL  - 3381
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159259475&partnerID=40&md5=2c11248b544075497117f0dc81f91729
DB  - Scopus
KW  - Reinforcement learning
KW  - Machine-learning
KW  - Reinforcement learnings
KW  - Forecasting
KW  - Uncertainty
KW  - Safety engineering
KW  - Uncertainty analysis
KW  - Safe reinforcement learning
KW  - Uncertainty estimation
KW  - Safety assurance
KW  - Distributional shift
KW  - Distributional Shift
KW  - Ensemble-based uncertainty estimation
KW  - Ensemble-based Uncertainty Estimation
KW  - Out-of-distribution  detection
KW  - Out-of-Distribution (OOD) detection
KW  - Safe Reinforcement Learning (Safe RL)
KW  - Safety assurance argumentation
KW  - Safety Assurance Argumentation
ER  - 

TY  - CONF
TI  - Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables
AU  - Xu, M.
AU  - Huang, P.
AU  - Niu, Y.
AU  - Kumar, V.
AU  - Qiu, J.
AU  - Fang, C.
AU  - Lee, K.-H.
AU  - Qi, X.
AU  - Lam, H.
AU  - Li, B.
AU  - Zhao, D.
T2  - Proceedings of Machine Learning Research
AB  - One key challenge for multi-task Reinforcement learning (RL) in practice is the absence of task specifications. Robust RL has been applied to deal with task ambiguity but may result in over-conservative policies. To balance the worst-case (robustness) and average performance, we propose Group Distributionally Robust Markov Decision Process (GDR-MDP), a flexible hierarchical MDP formulation that encodes task groups via a latent mixture model. GDR-MDP identifies the optimal policy that maximizes the expected return under the worst-possible qualified belief over task groups within an ambiguity set. We rigorously show that GDR-MDP's hierarchical structure improves distributional robustness by adding regularization to the worst possible outcomes. We then develop deep RL algorithms for GDR-MDP for both value-based and policy-based RL methods. Extensive experiments on Box2D control tasks, MuJoCo benchmarks, and Google football platforms show that our algorithms outperform classic robust training algorithms across diverse environments in terms of robustness under belief uncertainties. Demos are available on our project page (https://sites.google.com/view/gdr-rl/home). Copyright © 2023 by the author(s)
DA  - 2023///
PY  - 2023
VL  - 206
SP  - 2677
EP  - 2703
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165176997&partnerID=40&md5=78b2ceb3e076387d4b6e631d9b475265
DB  - Scopus
KW  - Reinforcement learning
KW  - Performance
KW  - Reinforcement learnings
KW  - Robustness (control systems)
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Multi tasks
KW  - Optimal policies
KW  - Latent variable
KW  - Task groups
KW  - Mixture modeling
KW  - Task specifications
KW  - Worst-case robustness
ER  - 

TY  - CONF
TI  - A Robust Framework for Fixing The Vulnerability of Compressed Distributed Learning
AU  - Chen, Y.
AU  - Wang, B.
AU  - Zhang, Y.
AU  - Kuang, J.
T2  - Proceedings of the 2023 26th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2023
AB  - Nowadays, as a prevailing paradigm for large-scale machine learning, distributed learning has been faced with two challenges, communication bottleneck and limited robustness. For the communication challenge, compression is widely-used as a solution. For the robustness challenge, some robust defence methods have been proposed. However, previous works that simultaneously consider these two challenges are limited. Through an experiment on the w8a dataset, we found that compressed distributed learning with rand-K is vulnerable to poisoning attacks. Therefore, in this paper, we propose a robust compressed distributed framework for distributed learning settings. Experimental evaluations on a9a and w8a datasets have shown the effectiveness of our proposed framework, which markedly decreases the average optimality gap from 1.47E -2 and 2.15E -2 to 3.98E -4 and 4.33E -4 respectively.  © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/CSCWD57460.2023.10152781
SP  - 733
EP  - 738
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164686727&doi=10.1109%2fCSCWD57460.2023.10152781&partnerID=40&md5=3b3e21af6d20d9c82120e878aeb75a21
DB  - Scopus
KW  - Machine learning
KW  - Machine Learning
KW  - Machine-learning
KW  - Robustness
KW  - Poisoning attacks
KW  - Learning settings
KW  - Experimental evaluation
KW  - Distributed learning
KW  - Average optimality
KW  - Compression
KW  - Distributed framework
KW  - Distributed Learning
KW  - Large-scale machine learning
KW  - Poisoning Attacks
ER  - 

TY  - JOUR
TI  - JSMix: a holistic algorithm for learning with label noise
AU  - Wen, Z.
AU  - Xu, H.
AU  - Ying, S.
T2  - Neural Computing and Applications
AB  - The success of deep learning is mainly dependent on large-scale and accurately labeled datasets. However, real-world datasets are marked with much noise. Directly training on datasets with label noise may lead to the overfitting. Recent research is under the spotlight on how to design algorithms that can learn robust models from noisy datasets, via designing the loss function and integrating the idea of Semi-supervised learning (SSL). This paper proposes a robust algorithm for learning with label noise that does not require additional clean data and an auxiliary model. Specifically, on the one hand, Jensen–Shannon (JS) divergence is introduced as a component of the loss function, which measures the distance between the predicted distribution and the noisy label distribution. It can alleviate the overfitting problem caused by the traditional cross entropy loss theoretically and experimentally. On the other hand, a dynamic sample selection mechanism is also proposed. The dataset is divided into the pseudo-clean labeled subset and the pseudo-noisy labeled subset. Two subsets are treated differently to exploit prior information about the data, and then learned by SSL. The dynamic sample selection is performed with the iteration between two subsets and the model parameters, which are different from the conventional training. Considering the label of the pseudo-clean labeled subset is not correct entirely, they are also refined by linear interpolation. Furthermore, we experimentally show that the integration of SSL helps the model divide two subsets more precise and build decision boundaries more explicit. Extensive experimental results on corrupted data from benchmark datasets and the real-world dataset, including CIFAR-10, CIFAR-100, and Clothing1M, demonstrate that our method is superior to many state-of-the-art approaches for learning with label noise. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s00521-022-07770-9
VL  - 35
IS  - 2
SP  - 1519
EP  - 1533
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139177299&doi=10.1007%2fs00521-022-07770-9&partnerID=40&md5=733ca6f7e18f5fbabc4dac91622d3cf8
DB  - Scopus
KW  - Deep learning
KW  - Supervised learning
KW  - Learning algorithms
KW  - Semi-supervised learning
KW  - Loss functions
KW  - Robust loss function
KW  - Iterative methods
KW  - Large dataset
KW  - Set theory
KW  - Large-scales
KW  - Real-world datasets
KW  - Overfitting
KW  - Labeled dataset
KW  - Data with label noise
KW  - Dynamic sample selection
KW  - Jensen-Shannon divergence
KW  - Jensen–Shannon divergence
ER  - 

TY  - CONF
TI  - Byzantine-Robust Online and Offline Distributed Reinforcement Learning
AU  - Chen, Y.
AU  - Zhang, X.
AU  - Zhang, K.
AU  - Wang, M.
AU  - Zhu, X.
T2  - Proceedings of Machine Learning Research
AB  - We consider a distributed reinforcement learning setting where multiple agents separately explore the environment and communicate their experiences through a central server. However, αfraction of agents are adversarial and can report arbitrary fake information. Critically, these adversarial agents can collude and their fake data can be of any sizes. We desire to robustly identify a near-optimal policy for the underlying Markov decision process in the presence of these adversarial agents. Our main technical contribution is COW, a novel algorithm for the robust mean estimation from batches problem, that can handle arbitrary batch sizes. Building upon this new estimator, in the offline setting, we design a Byzantine-robust distributed pessimistic value iteration algorithm; in the online setting, we design a Byzantine-robust distributed optimistic value iteration algorithm. Both algorithms obtain near-optimal sample complexities and achieve superior robustness guarantee than prior works. Copyright © 2023 by the author(s)
DA  - 2023///
PY  - 2023
VL  - 206
SP  - 3230
EP  - 3269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381268&partnerID=40&md5=f0f263cd3d72d542c157a405a6e611d3
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - E-learning
KW  - Reinforcement learnings
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Multi agent systems
KW  - Iterative methods
KW  - Offline
KW  - Learning settings
KW  - Adversarial agent
KW  - Central servers
KW  - Near-optimal policies
KW  - Multiple agents
KW  - Technical contribution
KW  - Value iteration algorithm
ER  - 

TY  - JOUR
TI  - Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments
AU  - Strawn, K.J.
AU  - Ayanian, N.
AU  - Lindemann, L.
T2  - IEEE Robotics and Automation Letters
AB  - The interest in using reinforcement learning (RL) controllers in safety-critical applications such as robot navigation around pedestrians motivates the development of additional safety mechanisms. Running RL-enabled systems among uncertain dynamic agents may result in high counts of collisions and failures to reach the goal. The system could be safer if the pre-trained RL policy was uncertainty-informed. For that reason, we propose <italic>conformal predictive safety filters</italic> that: 1) predict the other agents&#x0027; trajectories, 2) use statistical techniques to provide uncertainty intervals around these predictions, and 3) learn an additional safety filter that closely follows the RL controller but avoids the uncertainty intervals. We use conformal prediction to learn uncertainty-informed predictive safety filters, which make no assumptions about the agents&#x0027; distribution. The framework is modular and outperforms the existing controllers in simulation. We demonstrate our approach with multiple experiments in a collision avoidance gym environment and show that our approach minimizes the number of collisions without making overly conservative predictions. IEEE
DA  - 2023///
PY  - 2023
DO  - 10.1109/LRA.2023.3322644
SP  - 1
EP  - 8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174844462&doi=10.1109%2fLRA.2023.3322644&partnerID=40&md5=07707bbdae1e1e714275089a0239b90c
DB  - Scopus
KW  - Reinforcement learning
KW  - Safety
KW  - Motion planning
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Training
KW  - Collision avoidance
KW  - Motion-planning
KW  - Collisions avoidance
KW  - Reinforcement learnings
KW  - Controllers
KW  - Forecasting
KW  - Pedestrian safety
KW  - Uncertainty
KW  - Learn+
KW  - Reinforcement Learning
KW  - Robot programming
KW  - Bandpass filters
KW  - Conformal predictions
KW  - Learning controllers
KW  - Conformal Prediction
KW  - Predictive safety filter
KW  - Predictive Safety Filter
KW  - Safe motion planning
KW  - Safe Motion Planning
KW  - Uncertainty intervals
ER  - 

TY  - CONF
TI  - Embracing Risk in Reinforcement Learning: The Connection between Risk-Sensitive Exponential and Distributionally Robust Criteria
AU  - Noorani, E.
AU  - Baras, J.S.
T2  - Proceedings of the American Control Conference
AB  - We explore the relation between the risk-sensitive exponential (exponential of total cost) and Distributionally Robust Reinforcement Learning objectives, and in doing so, we unify some of the popular Reinforcement Learning algorithms. Such equivalence (I) allows to understand a number of well-known Reinforcement Learning algorithms from a risk minimization perspective and (II) establishes the robustness properties of risk-sensitive exponential objective in the Reinforcement Learning context, which in turn provides a theoretical justification for the robust performance of risk-sensitive Reinforcement Learning algorithms in the literature. The robustness of exponential criteria motivates risk-sensitizing current risk-neutral Reinforcement Learning algorithms using such criteria.  © 2022 American Automatic Control Council.
DA  - 2022///
PY  - 2022
DO  - 10.23919/ACC53348.2022.9867841
VL  - 2022-June
SP  - 2703
EP  - 2708
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136260457&doi=10.23919%2fACC53348.2022.9867841&partnerID=40&md5=347ba7f8d2c690bdd7ad34cc2c71ded9
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - 'current
KW  - Reinforcement learning algorithms
KW  - Robust performance
KW  - Learning objectives
KW  - Robustness properties
KW  - Exponentials
KW  - Learning context
KW  - Risk minimization
KW  - Robust criteria
ER  - 

TY  - CONF
TI  - Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness
AU  - Zhang, C.
AU  - Zhang, K.
AU  - Zhang, C.
AU  - Niu, A.
AU  - Feng, J.
AU  - Yoo, C.D.
AU  - Kweon, I.S.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Adversarial training (AT) for robust representation learning and self-supervised learning (SSL) for unsupervised representation learning are two active research fields. Integrating AT into SSL, multiple prior works have accomplished a highly significant yet challenging task: learning robust representation without labels. A widely used framework is adversarial contrastive learning which couples AT and SSL, and thus constitutes a very complex optimization problem. Inspired by the divide-and-conquer philosophy, we conjecture that it might be simplified as well as improved by solving two sub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts the focus of the task from seeking an optimal integrating strategy for a coupled problem to finding sub-solutions for sub-problems. With this said, this work discards prior practices of directly introducing AT to SSL frameworks and proposed a two-stage framework termed Decoupled Adversarial Contrastive Learning (DeACL). Extensive experimental results demonstrate that our DeACL achieves SOTA self-supervised adversarial robustness while significantly reducing the training time, which validates its effectiveness and efficiency. Moreover, our DeACL constitutes a more explainable solution, and its success also bridges the gap with semi-supervised AT for exploiting unlabeled samples for robust representation learning. The code is publicly accessible at https://github.com/pantheon5100/DeACL. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-20056-4_42
VL  - 13690 LNCS
SP  - 725
EP  - 742
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144544122&doi=10.1007%2f978-3-031-20056-4_42&partnerID=40&md5=31ca03190c46bbcd6067a473f7dbb3a8
DB  - Scopus
KW  - Supervised learning
KW  - Optimization
KW  - Self-supervised learning
KW  - Research fields
KW  - Adversarial training
KW  - Sub-problems
KW  - Adversarial robustness
KW  - Adversarial contrastive learning
KW  - Complex optimization problems
KW  - Coupled problems
KW  - Divide-and-conquer
KW  - Task learning
ER  - 

TY  - CONF
TI  - Robust Reinforcement Learning using Offline Data
AU  - Panaganti, K.
AU  - Xu, Z.
AU  - Kalathil, D.
AU  - Ghavamzadeh, M.
T2  - Advances in Neural Information Processing Systems
AB  - The goal of robust reinforcement learning (RL) is to learn a policy that is robust against the uncertainty in model parameters. Parameter uncertainty commonly occurs in many real-world RL applications due to simulator modeling errors, changes in the real-world system dynamics over time, and adversarial disturbances. Robust RL is typically formulated as a max-min problem, where the objective is to learn the policy that maximizes the value against the worst possible models that lie in an uncertainty set. In this work, we propose a robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an offline dataset to learn the optimal robust policy. Robust RL with offline data is significantly more challenging than its non-robust counterpart because of the minimization over all models present in the robust Bellman operator. This poses challenges in offline data collection, optimization over the models, and unbiased estimation. In this work, we propose a systematic approach to overcome these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a near-optimal robust policy under standard assumptions and demonstrate its superior performance on standard benchmark problems. © 2022 Neural information processing systems foundation. All rights reserved.
DA  - 2022///
PY  - 2022
VL  - 35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143683086&partnerID=40&md5=12385a40ddb4c145a75af7e617b994dc
DB  - Scopus
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Uncertainty
KW  - Real-world
KW  - Real-world system
KW  - Uncertainty analysis
KW  - Benchmarking
KW  - Iterative methods
KW  - Learn+
KW  - Model errors
KW  - Parameter uncertainty
KW  - Simulator models
KW  - Modeling parameters
KW  - Offline data
ER  - 

TY  - JOUR
TI  - Multiexpert Adversarial Regularization for Robust and Data-Efficient Deep Supervised Learning
AU  - Gholami, B.
AU  - Liu, Q.
AU  - El-Khamy, M.
AU  - Lee, J.
T2  - IEEE Access
AB  - Deep neural networks (DNNs) can achieve high accuracy when there is abundant training data that has the same distribution as the test data. In practical applications, data deficiency is often a concern. For classification tasks, the lack of enough labeled images in the training set often results in overfitting. Another issue is the mismatch between the training and the test domains, which results in poor model performance. This calls for the need to have robust and data efficient deep learning models. In this work, we propose a deep learning approach called Multi-Expert Adversarial Regularization learning (MEAR) with limited computational overhead to improve the generalization and robustness of deep supervised learning models. The MEAR framework appends multiple classifier heads (experts) to the feature extractor of the legacy model. MEAR aims to learn the feature extractor in an adversarial fashion by leveraging complementary information from the individual experts as well as the ensemble of the experts to be more robust for an unseen test domain. We train state-of-the-art networks with MEAR for two important computer vision tasks, image classification and semantic segmentation. We compare MEAR to a variety of baselines on multiple benchmarks. We show that MEAR is competitive with other methods and more successful at learning robust features. © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3196780
VL  - 10
SP  - 85080
EP  - 85094
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136107917&doi=10.1109%2fACCESS.2022.3196780&partnerID=40&md5=5dff15387da09a3c425c359284ad6896
DB  - Scopus
KW  - Ensemble methods
KW  - Semantics
KW  - Task analysis
KW  - Predictive models
KW  - Deep neural networks
KW  - Supervised learning
KW  - Features extraction
KW  - Job analysis
KW  - Image classification
KW  - Classification (of information)
KW  - Efficient learning
KW  - Semantic Segmentation
KW  - Images classification
KW  - Images segmentations
KW  - Robustness
KW  - Computational modelling
KW  - Robust learning
KW  - image segmentation
KW  - Adversarial learning
KW  - robust learning
KW  - adversarial learning
KW  - data efficient learning
KW  - Data efficient learning
KW  - ensemble methods
ER  - 

TY  - CONF
TI  - A Semi-supervised Deep Learning Model with Consistency Regularization of Augmented Samples for Imbalanced Fault Detection
AU  - Chen, H.
AU  - Han, J.
AU  - Lv, X.
AU  - Wu, Z.
AU  - Guo, H.
AU  - Zhan, Z.
T2  - 13th International Conference on Reliability, Maintainability, and Safety: Reliability and Safety of Intelligent Systems, ICRMS 2022
AB  - With increasing requirements on reliability, maintainability and safety in modern ICT systems, fault detection, as an indispensable part of AIOps, has become essential in cloud computing or communication network environments. However, due to the lack of effective labels and class imbalance on faulty samples, fault detection performance based on the common classification model can't meet the system's operational requirements. Some recent approaches of SSL propose a consistency regularization loss to solve the problem of insufficient labels. However, these approaches are mainly for images based on artificial data augmentations but not feasible for all data types, and class-imbalance problem is not considered simultaneously. So, we propose a semi-supervised method for imbalanced fault detection with few labels, called SSLCR-IFD. In the method, we use a semi-supervised deep classifier based on consistency loss to solve the lack of labels, in which two sample augmentation methods based on clustering and GAN are used. Furthermore, a selective pseudo-labeling self-training strategy is proposed to solve the class-imbalance problem. Compared with the standard data augmentation, our methods alleviates the need for domain knowledge and can be used on multiple types of tasks. Finally, experiment results show that our method outperforms the baseline methods on two different AIOps tasks.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICRMS55680.2022.9944609
SP  - 290
EP  - 295
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143065667&doi=10.1109%2fICRMS55680.2022.9944609&partnerID=40&md5=ca3844b36ae721b2f47955e2b0fcbc65
DB  - Scopus
KW  - Deep learning
KW  - semi-supervised learning
KW  - Learning systems
KW  - Supervised learning
KW  - Semi-supervised learning
KW  - Fault detection
KW  - Faults detection
KW  - Semi-supervised
KW  - Distributed computer systems
KW  - Reliability and safeties
KW  - data augmentation
KW  - Data augmentation
KW  - Domain Knowledge
KW  - Regularisation
KW  - Class imbalance
KW  - Consistency regularization
KW  - Artificial intelligence in reliability and safety
KW  - Class imbalance problems
KW  - class-imbalance
KW  - consistency regularization
ER  - 

TY  - CONF
TI  - Robust Unsupervised Domain Adaptation from A Corrupted Source
AU  - Yu, S.
AU  - Zhu, Z.
AU  - Liu, B.
AU  - Jain, A.K.
AU  - Zhou, J.
T2  - Proceedings - IEEE International Conference on Data Mining, ICDM
AB  - Unsupervised Domain Adaptation (UDA) provides a promising solution for learning without supervision, which transfers knowledge from relevant source domains with accessible labeled training data. Existing UDA solutions hinge on clean training data with a short-tail distribution from the source domain, which can be fragile when the source domain data is corrupted either inherently or via adversarial attacks. In this work, we propose an effective framework to address the challenges of UDA from corrupted source domains in a principled manner. Specifically, we perform knowledge ensemble from multiple domain-invariant models that are learned on random partitions of training data. To further address the distribution shift from the source to the target domain, we refine each of the learned models via mutual information maximization, which adaptively obtains the predictive information of the target domain with high confidence. Extensive empirical studies demonstrate that the proposed approach is robust against various types of poisoned data attacks while achieving high asymptotic performance on the target domain.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICDM54844.2022.00171
VL  - 2022-November
SP  - 1299
EP  - 1304
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147729954&doi=10.1109%2fICDM54844.2022.00171&partnerID=40&md5=556ad39d51119f2913db0a36ef016d76
DB  - Scopus
KW  - Training data
KW  - Robust learning
KW  - Domain adaptation
KW  - Unsupervised domain adaptation
KW  - Multiple domains
KW  - Labeled training data
KW  - Target domain
KW  - Robust Learning
KW  - Data attacks
KW  - Poison data attack
KW  - Poison Data Attack
KW  - Tail distribution
KW  - Unsupervised Domain Adaptation
ER  - 

TY  - CONF
TI  - Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network
AU  - Schott, L.
AU  - Hajri, H.
AU  - Lamprier, S.
T2  - Proceedings of the International Joint Conference on Neural Networks
AB  - To improve robustness of deep reinforcement learning agents, a line of recent works focus on producing disturbances of the dynamics of the environment. Existing approaches of the literature to generate such disturbances are environment adversarial reinforcement learning methods. These methods set the problem as a two-player game between the protagonist agent, which learns to perform a task in an environment, and the adversary agent, which learns to disturb the dynamics of the considered environment to make the protagonist agent fail. Alternatively, we propose to build on gradient-based adversarial attacks, usually used for classification tasks for instance, that we apply on the critic network of the protagonist to identify efficient disturbances of the dynamics of the environment. Rather than training an adversary agent, which usually reveals as very complex and unstable, we leverage the knowledge of the critic network of the protagonist, to dynamically increase the complexity of the task at each step of the learning process. We show that our method, while being faster and lighter, leads to significantly better improvements in robustness of the policy than existing methods of the literature. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IJCNN55064.2022.9892901
VL  - 2022-July
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140781892&doi=10.1109%2fIJCNN55064.2022.9892901&partnerID=40&md5=3c7ba77dfe5886c14307e437ac8d50a3
DB  - Scopus
KW  - Intelligent agents
KW  - Deep learning
KW  - Game theory
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Learning systems
KW  - Learning algorithms
KW  - Deep reinforcement learning
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Deep Reinforcement Learning
KW  - Complex networks
KW  - Robustness
KW  - Multi agent systems
KW  - Learn+
KW  - Reinforcement learning method
KW  - Dynamics
KW  - Knowledge management
KW  - Adversarial training
KW  - Adversarial Training
KW  - Gradient based
KW  - Critic network
KW  - Two-player games
ER  - 

TY  - CONF
TI  - Distributed Multi-Agent Deep Reinforcement Learning for Robust Coordination against Noise
AU  - Motokawa, Y.
AU  - Sugawara, T.
T2  - Proceedings of the International Joint Conference on Neural Networks
AB  - In multi-agent systems, noise reduction techniques are considerable for improving the overall system reliability as agents are required to rely on limited environmental information to develop cooperative and coordinated behaviors with the surrounding agents. However, previous studies have often applied centralized noise reduction methods to build robust and versatile coordination in noisy multi-agent environments, while distributed and decentralized autonomous agents are more plausible for real-world application. In this paper, we introduce a distributed attentional actor architecture model for a multi-agent system (DA3-X), using which we demonstrate that agents with DA3-X can selectively learn the noisy environment and behave cooperatively. We experimentally evaluate the effectiveness of DA3-X by comparing learning methods with and without DA3-X and show that agents with DA3-X can achieve better performance than baseline agents. Furthermore, we visualize heatmaps of attentional weights from the DA3-X to analyze how the decision-making process and coordinated behavior are influenced by noise. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IJCNN55064.2022.9892253
VL  - 2022-July
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140784231&doi=10.1109%2fIJCNN55064.2022.9892253&partnerID=40&md5=bbbe0e8c9ed49dd82a27932e6dcb977a
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Decision making
KW  - Noise reduction
KW  - Autonomous agents
KW  - Learning systems
KW  - Attention mechanism
KW  - Coordination
KW  - Behavioral research
KW  - Reinforcement learnings
KW  - Attention mechanisms
KW  - Multi agent systems
KW  - Multi agent
KW  - Multi-agent deep reinforcement learning
KW  - Noise abatement
KW  - Distributed systems
KW  - Cooperation
KW  - Alter-exploration problem
KW  - Coordinated behavior
KW  - Distributed system
KW  - Noise reduction technique
ER  - 

TY  - CONF
TI  - Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning
AU  - Liang, Y.
AU  - Sun, Y.
AU  - Zheng, R.
AU  - Huang, F.
T2  - Advances in Neural Information Processing Systems
AB  - Recent studies reveal that a well-trained deep reinforcement learning (RL) policy can be particularly vulnerable to adversarial perturbations on input observations. Therefore, it is crucial to train RL agents that are robust against any attacks with a bounded budget. Existing robust training methods in deep RL either treat correlated steps separately, ignoring the robustness of long-term rewards, or train the agents and RL-based attacker together, doubling the computational burden and sample complexity of the training process. In this work, we propose a strong and efficient robust training framework for RL, named Worst-case-aware Robust RL (WocaR-RL), that directly estimates and optimizes the worst-case reward of a policy under bounded ℓp attacks without requiring extra samples for learning an attacker. Experiments on multiple environments show that WocaR-RL achieves state-of-the-art performance under various strong attacks, and obtains significantly higher training efficiency than prior state-of-the-art robust training methods. The code of this work is available at https://github.com/umd-huang-lab/WocaR-RL. © 2022 Neural information processing systems foundation. All rights reserved.
DA  - 2022///
PY  - 2022
VL  - 35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150009921&partnerID=40&md5=f780a63268f2da04aa1b18bf9fc5d208
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Sample complexity
KW  - Training methods
KW  - Training process
KW  - Computational burden
KW  - Budget control
KW  - Learning policy
KW  - Agent learning
KW  - Doublings
KW  - Robust trainings
ER  - 

TY  - CONF
TI  - Adversarial Training With Channel Attention Regularization
AU  - Cho, S.
AU  - Byun, J.
AU  - Kwon, M.-J.
AU  - Kim, Y.
AU  - Kim, C.
T2  - Proceedings - International Conference on Image Processing, ICIP
AB  - Adversarial attack shows that deep neural networks (DNNs) are highly vulnerable to small perturbation. Currently, one of the most effective ways to defend against adversarial attacks is adversarial training, which generates adversarial examples during training and induces the models to classify them correctly. To further increase robustness, various techniques such as exploiting additional unlabeled data and novel training loss have been proposed. In this paper, we propose a novel regularization method that exploits latent features, which can be easily combined with existing approaches. We discover that particular channels are more sensitive to adversarial perturbation, motivating us to propose regularizing these channels. Specifically, we attach a channel attention module for adjusting sensitivity of each channel by reducing the difference between the latent feature of the natural image and that of the adversarial image, which we call Channel Attention Regularization (CAR). CAR can be combined with the existing adversarial training framework, showing that it improves the robustness of state-of-the-art defense models. Experiments on various existing adversarial training methods against diverse attacks show the effectiveness of our methods. Codes are available at https://github.com/sgmath12/Adversarial-Training-CAR. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICIP46576.2022.9897754
SP  - 2996
EP  - 3000
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146701970&doi=10.1109%2fICIP46576.2022.9897754&partnerID=40&md5=5bfdee444f7399d1ed29bce966f3d6f0
DB  - Scopus
KW  - Deep neural networks
KW  - Machine-learning
KW  - Computer vision
KW  - Robustness
KW  - Adversarial machine learning
KW  - Adversarial training
KW  - Unlabeled data
KW  - Regularisation
KW  - Regularization methods
KW  - Natural images
KW  - Small perturbations
KW  - Feature regularization
ER  - 

TY  - CONF
TI  - Training and Transferring Safe Policies in Reinforcement Learning
AU  - Yang, Q.
AU  - Simão, T.D.
AU  - Jansen, N.
AU  - Tindemans, S.H.
AU  - Spaan, M.T.J.
T2  - ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022
AB  - Safety is critical to broadening the application of reinforcement learning (RL). Often, RL agents are trained in a controlled environment, such as a laboratory, before being deployed in the real world. However, the target reward might be unknown prior to deployment. Reward-free RL addresses this problem by training an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe sampling policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence from the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168254751&partnerID=40&md5=69ee98d03db848dbd74b02d5ad5311a6
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - Transfer learning
KW  - Reinforcement learning agent
KW  - Reinforcement learnings
KW  - Real-world
KW  - Learn+
KW  - Safety violations
KW  - Students
KW  - Empirical analysis
KW  - Controlled environment
ER  - 

TY  - CONF
TI  - Data-Driven Robust Multi-Agent Reinforcement Learning
AU  - Wang, Y.
AU  - Wang, Y.
AU  - Zhou, Y.
AU  - Velasquez, A.
AU  - Zou, S.
T2  - IEEE International Workshop on Machine Learning for Signal Processing, MLSP
AB  - Multi-agent reinforcement learning (MARL) in the collaborative setting aims to find a joint policy that maximizes the accumulated reward averaged over all the agents. In this paper, we focus on MARL under model uncertainty, where the transition kernel is assumed to be in an uncertainty set, and the goal is to optimize the worst-case performance over the uncertainty set. We investigate the model-free setting, where the uncertain set centers around an unknown Markov decision process from which a single sample trajectory can be obtained sequentially. We develop a robust multi-agent Q-learning algorithm, which is model-free and fully decentralized. We theoretically prove that the proposed algorithm converges to the minimax robust policy, and further characterize its sample complexity. Our algorithm, comparing to the vanilla multi-agent Q-learning, offers provable robustness under model uncertainty without incurring additional computational and memory cost.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/MLSP55214.2022.9943500
VL  - 2022-August
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142763167&doi=10.1109%2fMLSP55214.2022.9943500&partnerID=40&md5=d95526b627560183e68f0e84ebf1a926
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning systems
KW  - Learning algorithms
KW  - Uncertainty
KW  - Uncertainty analysis
KW  - Markov processes
KW  - Multi agent systems
KW  - Fertilizers
KW  - Multi-agent reinforcement learning
KW  - Modeling uncertainties
KW  - Model free
KW  - Sample complexity
KW  - Data driven
KW  - Computational complexity
KW  - Robust MDP
KW  - model-free
KW  - Distributionally robust
KW  - finite-time analysis
KW  - Finite-time analysis
KW  - Multi-agent Q-learning
KW  - robust MDP
KW  - sample complexity
ER  - 

TY  - CONF
TI  - Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation
AU  - Wang, H.
AU  - Huang, Z.
AU  - Wu, X.
AU  - Xing, E.
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
AB  - Data augmentation has been proven to be an effective technique for developing machine learning models that are robust to known classes of distributional shifts (e.g., rotations of images), and alignment regularization is a technique often used together with data augmentation to further help the model learn representations invariant to the shifts used to augment the data. In this paper, motivated by a proliferation of options of alignment regularizations, we seek to evaluate the performances of several popular design choices along the dimensions of robustness and invariance, for which we introduce a new test procedure. Our synthetic experiment results speak to the benefits of squared "2 norm regularization. Further, we also formally analyze the behavior of alignment regularization to complement our empirical study under assumptions we consider realistic. Finally, we test this simple technique we identify (worst-case data augmentation with squared "2 norm alignment regularization) and show that the benefits of this method outrun those of the specially designed methods. We also release a software package in both TensorFlow and PyTorch for users to use the method with a couple of lines at https://github.com/jyanln/AlignReg.  © 2022 Owner/Author.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3534678.3539438
SP  - 1846
EP  - 1856
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137145889&doi=10.1145%2f3534678.3539438&partnerID=40&md5=ee29243ec5958682d44789213dc6ab47
DB  - Scopus
KW  - machine learning
KW  - Machine learning
KW  - Testing
KW  - robustness
KW  - trustworthy
KW  - Performance
KW  - Machine-learning
KW  - Machine learning models
KW  - Robustness
KW  - Learn+
KW  - Test procedures
KW  - Alignment
KW  - data augmentation
KW  - Data augmentation
KW  - Regularisation
KW  - Trustworthy
KW  - Invariant representation
ER  - 

TY  - CONF
TI  - Robustifying Reinforcement Learning Agents via Action Space Adversarial Training
AU  - Tan, K.L.
AU  - Esfandiari, Y.
AU  - Lee, X.Y.
AU  - Sarkar, S.
T2  - Proceedings of the American Control Conference
AB  - Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training. © 2020 AACC.
DA  - 2020///
PY  - 2020
DO  - 10.23919/ACC45564.2020.9147846
VL  - 2020-July
SP  - 3959
EP  - 3964
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089561387&doi=10.23919%2fACC45564.2020.9147846&partnerID=40&md5=501add6087ff53cc19dbca4920ba956e
DB  - Scopus
KW  - Intelligent agents
KW  - Deep learning
KW  - Reinforcement learning
KW  - Reinforcement learning agent
KW  - Controllers
KW  - Electric power transmission networks
KW  - Robust control
KW  - Embedded systems
KW  - Robust approaches
KW  - Actuators
KW  - Data driven decision
KW  - Catastrophic failures
KW  - Control applications
KW  - Cyber-physical systems (CPS)
KW  - Error observation
KW  - Resilient controller
ER  - 

TY  - CONF
TI  - Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning
AU  - Chen, K.
AU  - Lee, Y.
AU  - Soh, H.
T2  - Proceedings - IEEE International Conference on Robotics and Automation
AB  - This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations. © 2021 IEEE
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICRA48506.2021.9561187
VL  - 2021-May
SP  - 4274
EP  - 4280
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119129739&doi=10.1109%2fICRA48506.2021.9561187&partnerID=40&md5=22b8ddbec48150346ed766f970d0348e
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Computer vision
KW  - 'current
KW  - Multi-modal
KW  - State space methods
KW  - Down-stream
KW  - Mutual informations
KW  - Over reliance
KW  - Poor performance
KW  - Shared representations
KW  - Specific sensors
KW  - World model
ER  - 

TY  - JOUR
TI  - Robust weighted Gaussian processes
AU  - Ramirez-Padron, R.
AU  - Mederos, B.
AU  - Gonzalez, A.J.
T2  - Computational Statistics
AB  - This paper presents robust weighted variants of batch and online standard Gaussian processes (GPs) to effectively reduce the negative impact of outliers in the corresponding GP models. This is done by introducing robust data weighers that rely on robust and quasi-robust weight functions that come from robust M-estimators. Our robust GPs are compared to various GP models on four datasets. It is shown that our batch and online robust weighted GPs are indeed robust to outliers, significantly outperforming the corresponding standard GPs and the recently proposed heteroscedastic GP method GPz. Our experiments also show that our methods are comparable to and sometimes better than a state-of-the-art robust GP that uses a Student-t likelihood. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s00180-020-01011-0
VL  - 36
IS  - 1
SP  - 347
EP  - 373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087661618&doi=10.1007%2fs00180-020-01011-0&partnerID=40&md5=3825bbaaabcd6c0ed09caeca5ecb0a79
DB  - Scopus
KW  - Machine learning
KW  - Online learning
KW  - Outlying data
KW  - Robust regression
ER  - 

TY  - CONF
TI  - Robust Reinforcement Learning: A Constrained Game-theoretic Approach
AU  - Yu, J.
AU  - Gehring, C.
AU  - Schäfer, F.
AU  - Anandkumar, A.
T2  - Proceedings of Machine Learning Research
AB  - Reinforcement learning (RL) methods provide state-of-art performance in complex control tasks. However, it has been widely recognized that RL methods often fail to generalize due to unaccounted uncertainties. In this work, we propose a game theoretic framework for robust reinforcement learning that comprises many previous works as special cases. We formulate robust RL as a constrained minimax game between the RL agent and an environmental agent which represents uncertainties such as model parameter variations and adversarial disturbances. To solve the competitive optimization problems arising in our framework, we propose to use competitive mirror descent (CMD). This method accounts for the interactive nature of the game at each iteration while using Bregman divergences to adapt to the global structure of the constraint set. leveraging Lagrangian duality, we demonstrate an RRL policy gradient algorithm based on CMD. We empirically show that our algorithm is stable for large step sizes, resulting in faster convergence on constrained linear quadratic games. © 2021 J. Yu, C. Gehring, F. Schäfer & A. Anandkumar.
DA  - 2021///
PY  - 2021
VL  - 144
SP  - 1242
EP  - 1254
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673722&partnerID=40&md5=924f503e07b0d4790f2ca10f68012f4f
DB  - Scopus
KW  - Game theory
KW  - Reinforcement learning
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Uncertainty
KW  - Optimisations
KW  - Iterative methods
KW  - robust reinforcement learning
KW  - Constrained optimization
KW  - Robust reinforcement learning
KW  - Reinforcement learning method
KW  - adversarial training
KW  - Policy gradient
KW  - Adversarial training
KW  - policy gradient
KW  - Zero-sum game
KW  - zero-sum game
KW  - Game-theoretic
KW  - competitive optimization
KW  - Competitive optimization
ER  - 

TY  - CONF
TI  - AlwaysSafe: Reinforcement learning without safety constraint violations during training
AU  - Simão, T.D.
AU  - Jansen, N.
AU  - Spaan, M.T.J.
T2  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
AB  - Deploying reinforcement learning (RL) involves major concerns around safety. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial. Safe RL studies how to mitigate such problems. For instance, we can decouple safety from reward using constrained Markov decision processes (CMDPs), where an independent signal models the safety aspects. In this setting, an RL agent can autonomously find tradeoffs between performance and safety. Unfortunately, most RL agents designed for CMDPs only guarantee safety after the learning phase, which might prevent their direct deployment. In this work, we investigate settings where a concise abstract model of the safety aspects is given, a reasonable assumption since a thorough understanding of safety-related matters is a prerequisite for deploying RL in typical applications. Factored CMDPs provide such compact models when a small subset of features describe the dynamics relevant for the safety constraints. We propose an RL algorithm that uses this abstract model to learn policies for CMDPs safely, that is without violating the constraints. During the training process, this algorithm can seamlessly switch from a conservative policy to a greedy policy without violating the safety constraints. We prove that this algorithm is safe under the given assumptions. Empirically, we show that even if safety and reward signals are contradictory, this algorithm always operates safely and, when they are aligned, this approach also improves the agent’s performance. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
DA  - 2021///
PY  - 2021
VL  - 2
SP  - 1214
EP  - 1223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103674313&partnerID=40&md5=359b14bfd08a91ba735c0f4b935475f4
DB  - Scopus
KW  - Reinforcement learning
KW  - Autonomous agents
KW  - Safety engineering
KW  - Markov processes
KW  - Multi agent systems
KW  - Constrained Markov decision process
KW  - Safe reinforcement learning
KW  - Safety constraint
KW  - Training process
KW  - Safety-Related
KW  - Abstract modeling
KW  - CMDP
KW  - Independent signals
KW  - Learning phase
KW  - Typical application
ER  - 

TY  - CONF
TI  - An Adversarial Training Method for Improving Model Robustness in Unsupervised Domain Adaptation
AU  - Nie, Z.
AU  - Lin, Y.
AU  - Yan, M.
AU  - Cao, Y.
AU  - Ning, S.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - The easily-perturbed nature of deep neural network makes it vulnerable to adversarial attacks, and such vulnerability has become a major threat to the security of machine learning. The transferability of adversarial samples further increases the threat. Adversarial training has made considerable progress in defending against adversarial samples. In transfer learning, unsupervised domain adaptation is an important research branch, however, due to the label of the target domain samples can’t be obtained, it is difficult to implement adversarial training. In this paper, we found that using source domain data for adversarial training and adding the generated adversarial perturbation to the target domain data could effectively enhance the robustness of the transferred model. Experimental results showed that our proposed method can not only ensure the model’s classification accuracy, but also greatly improve the model’s defense performance against adversarial attacks. In simple, our proposed method not only guarantees the transfer of knowledge, but also realizes the transfer of model robustness. It is the main contribution of this paper. © 2021, Springer Nature Switzerland AG.
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-82153-1_1
VL  - 12817 LNAI
SP  - 3
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113779102&doi=10.1007%2f978-3-030-82153-1_1&partnerID=40&md5=cece8d66fb799aeb2aabc95146e3819c
DB  - Scopus
KW  - Deep learning
KW  - Deep neural networks
KW  - Transfer learning
KW  - Network security
KW  - Robustness
KW  - Classification accuracy
KW  - Adversarial attack
KW  - Model robustness
KW  - Training methods
KW  - Domain adaptation
KW  - Knowledge management
KW  - Adversarial training
KW  - Target domain
KW  - Transfer of knowledge
KW  - Unsupervised domain adaption
ER  - 

TY  - JOUR
TI  - Bayesian robust multi-extreme learning machine
AU  - Li, Y.
AU  - Wang, Y.
AU  - Chen, Z.
AU  - Zou, R.
T2  - Knowledge-Based Systems
AB  - Outliers usually exist in the collected data due to human or instrumentation errors. Their existence makes the regression error obey a non-Gaussian distribution. So, people assume that the regression error obeys finite mixture distributions, such as mixture of Gaussians and mixture of Student's t-distributions. This paper proposed two Bayesian robust regression models based on multiple extreme learning machines. First, the finite mixture error distributions are extended to their infinite counterparts according to the theory of stick-breaking construction, which can avoid selecting the optimal number of components in mixture models. Second, a multi-extreme learning machine, which combines many single extreme learning machines with different numbers of hidden nodes, is constructed, which can avoid the effects of different numbers of hidden nodes on the final regression performance. Besides, sparse Bayesian learning has also been derived to perform the sparse model weight and output weight inference automatically. The experimental results on artificial datasets and eight benchmark datasets show that the proposed two models outperform the other compared models under different rates of outliers. Also, they generate better multi-step ahead wind speed and traffic speed forecasts in real applications. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.knosys.2020.106468
VL  - 210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092367835&doi=10.1016%2fj.knosys.2020.106468&partnerID=40&md5=2ec59fb714cd1fc853238e3e820f652e
DB  - Scopus
KW  - Machine learning
KW  - Regression analysis
KW  - Knowledge acquisition
KW  - Extreme learning machine
KW  - Statistics
KW  - Errors
KW  - Robust regression
KW  - Wind
KW  - Mixtures
KW  - Mixture of Gaussians
KW  - Student's t distribution
KW  - Artificial datasets
KW  - Finite mixture distribution
KW  - Infinite mixture of Gaussians
KW  - Infinite mixture of Student's t-distributions
KW  - Instrumentation error
KW  - Multi-extreme learning machine
KW  - Non-gaussian distribution
KW  - Sparse Bayesian learning
KW  - Sparse priors
KW  - Variational Bayesian
ER  - 

TY  - CONF
TI  - Bullseye polytope: A scalable clean-label poisoning attack with improved transferability
AU  - Aghakhani, H.
AU  - Meng, D.
AU  - Wang, Y.-X.
AU  - Kruegel, C.
AU  - Vigna, G.
T2  - Proceedings - 2021 IEEE European Symposium on Security and Privacy, Euro S and P 2021
AB  - A recent source of concern for the security of neural networks is the emergence of clean-label dataset poisoning attacks, wherein correctly labeled poison samples are injected into the training dataset. While these poison samples look legitimate to the human observer, they contain malicious characteristics that trigger a targeted misclassification during inference. We propose a scalable and transferable clean-label poisoning attack against transfer learning, which creates poison images with their center close to the target image in the feature space. Our attack, Bullseye Polytope, improves the attack success rate of the current state-of-the-art by 26.75% in end-to-end transfer learning, while increasing attack speed by a factor of 12. We further extend Bullseye Polytope to a more practical attack model by including multiple images of the same object (e.g., from different angles) when crafting the poison samples. We demonstrate that this extension improves attack transferability by over 16% to unseen images (of the same object) without using extra poison samples.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/EuroSP51992.2021.00021
SP  - 159
EP  - 178
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119113061&doi=10.1109%2fEuroSP51992.2021.00021&partnerID=40&md5=5329947511ad01222151c43dc05e8e58
DB  - Scopus
KW  - Machine learning
KW  - Transfer learning
KW  - Machine-learning
KW  - Computer vision
KW  - Image enhancement
KW  - Neural-networks
KW  - Training dataset
KW  - Poisoning attacks
KW  - Machine learning robustness
KW  - Bullseye
KW  - Dataset poisoning
KW  - Dataset Poisoning
KW  - Human observers
KW  - Machine Learning Robustness
KW  - Polytopes
ER  - 

TY  - JOUR
TI  - Anomaly Detection for Insider Threats Using Unsupervised Ensembles
AU  - Le, D.C.
AU  - Zincir-Heywood, N.
T2  - IEEE Transactions on Network and Service Management
AB  - Insider threat represents a major cybersecurity challenge to companies, organizations, and government agencies. Insider threat detection involves many challenges, including unbalanced data, limited ground truth, and possible user behavior changes. This research presents an unsupervised learning based anomaly detection approach for insider threat detection. We employ four unsupervised learning methods with different working principles, and explore various representations of data with temporal information. Furthermore, different computational intelligence schemes are explored to combine these models to create anomaly detection ensembles for improving the detection performance. Evaluation results show that the approach allows learning from unlabelled data under challenging conditions for insider threat detection. Insider threats are detected with high detection and low false positive rates. For example, 60% of malicious insiders are detected under 0.1% investigation budget, and all malicious insiders are detected at less than 5% investigation budget. Furthermore, we explore the ability of the proposed approach to generalize for detecting new anomalous behaviors in different datasets, i.e., robustness. Finally, results demonstrate that a voting-based ensemble of anomaly detection can be used to improve detection performance as well as the robustness. Comparisons with the state-of-the-art confirm the effectiveness of the proposed approach.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3071928
VL  - 18
IS  - 2
SP  - 1152
EP  - 1164
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104195907&doi=10.1109%2fTNSM.2021.3071928&partnerID=40&md5=2799dabb5e1f8d9dd1e14124ec6ce3cd
DB  - Scopus
KW  - Learning systems
KW  - Anomaly detection
KW  - Behavioral research
KW  - anomaly detection
KW  - Security of data
KW  - Unsupervised learning
KW  - unsupervised learning
KW  - ensemble learning
KW  - Budget control
KW  - Detection approach
KW  - Intelligent computing
KW  - Unsupervised learning method
KW  - False positive rates
KW  - Detection performance
KW  - Evaluation results
KW  - Government agencies
KW  - Insider threat detections
KW  - dependable and robust learning
KW  - Insider threat detection
KW  - temporal data
KW  - Temporal information
ER  - 

TY  - CONF
TI  - Distributionally robust semi-supervised learning for people-centric sensing
AU  - Chen, K.
AU  - Yao, L.
AU  - Zhang, D.
AU  - Chang, X.
AU  - Long, G.
AU  - Wang, S.
T2  - 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019
AB  - Semi-supervised learning is crucial for alleviating la-belling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).
DA  - 2019///
PY  - 2019
SP  - 3321
EP  - 3328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090808244&partnerID=40&md5=68a7c553d7f705a73dd28cb3ab222b33
DB  - Scopus
KW  - Semi-supervised learning
KW  - State-of-the-art methods
KW  - Activity recognition
KW  - Robust modeling
KW  - Motion estimation
KW  - Real-world datasets
KW  - Behavior patterns
KW  - Biological conditions
KW  - Intention recognition
KW  - Muscular movements
ER  - 

TY  - CONF
TI  - Improving Robustness via Risk Averse Distributional Reinforcement Learning
AU  - Singh, R.
AU  - Zhang, Q.
AU  - Chen, Y.
T2  - Proceedings of Machine Learning Research
AB  - One major obstacle that precludes the success of reinforcement learning in real-world applications is the lack of robustness, either to model uncertainties or external disturbances, of the trained policies. Robustness is critical when the policies are trained in simulations instead of real world environment. In this work, we propose a risk-aware algorithm to learn robust policies in order to bridge the gap between simulation training and real-world implementation. Our algorithm is based on recently discovered distributional RL framework. We incorporate CVaR risk measure in sample based distributional policy gradients (SDPG) for learning risk-averse policies to achieve robustness against a range of system disturbances. We validate the robustness of risk-aware SDPG on multiple environments. © 2020 R. Singh, Q. Zhang & Y. Chen.
DA  - 2020///
PY  - 2020
VL  - 120
SP  - 958
EP  - 968
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160153359&partnerID=40&md5=819ea330ab2feb455bbe62ea271c4e74
DB  - Scopus
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - Learning systems
KW  - Reinforcement learnings
KW  - Risk assessment
KW  - Real-world
KW  - Robustness (control systems)
KW  - Uncertainty analysis
KW  - robust reinforcement learning
KW  - Robust reinforcement learning
KW  - Risk analysis
KW  - Policy gradient
KW  - Risk averse
KW  - Risk aware
KW  - distributional reinforcement learning
KW  - Distributional reinforcement learning
KW  - Risk sensitive control
ER  - 

TY  - CONF
TI  - FairMixRep: Self-supervised Robust Representation Learning for Heterogeneous Data with Fairness constraints
AU  - Chakraborty, S.
AU  - Verma, E.
AU  - Sahoo, S.
AU  - Datta, J.
T2  - IEEE International Conference on Data Mining Workshops, ICDMW
AB  - Representation Learning in a heterogeneous space with mixed variables of numerical and categorical types has interesting challenges due to its complex feature manifold. Moreover, feature learning in an unsupervised setup, without class labels and a suitable learning loss function, adds to the problem complexity. Further, the learned representation and subsequent predictions should not reflect discriminatory behavior towards certain sensitive groups or attributes. The proposed feature map should preserve maximum variations present in the data and needs to be fair with respect to the sensitive variables. We propose, in the first phase of our work, an efficient encoder-decoder framework to capture the mixed-domain information. The second phase of our work focuses on de-biasing the mixed space representations by adding relevant fairness constraints. This ensures minimal information loss between the representations before and after the fairness-preserving projections. Both the information content and the fairness aspect of the final representation learned has been validated through several metrics where it shows excellent performance. Our work (FairMixRep) addresses the problem of Mixed Space Fair Representation learning from an unsupervised perspective and learns a Universal representation which is timely, unique and a novel research contribution. 11This paper is accepted at ICDM'2020 DLC Workshop. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICDMW51313.2020.00069
VL  - 2020-November
SP  - 458
EP  - 463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101347891&doi=10.1109%2fICDMW51313.2020.00069&partnerID=40&md5=ec831606edd1047b939929f22266868f
DB  - Scopus
KW  - Data mining
KW  - Fairness
KW  - Robustness
KW  - Heterogeneous data
KW  - Fair representation
KW  - Fairness constraints
KW  - Information contents
KW  - Maximum variations
KW  - Minimal information
KW  - Mixed data types
KW  - Problem complexity
KW  - Self-supervised Representation Learning
KW  - Sensitive variables
KW  - Unbiased learning
ER  - 

TY  - CONF
TI  - Robust reinforcement learning via adversarial training with Langevin dynamics
AU  - Kamalaruban, P.
AU  - Huang, Y.-T.
AU  - Hsieh, Y.-P.
AU  - Rolland, P.
AU  - Shi, C.
AU  - Cevher, V.
T2  - Advances in Neural Information Processing Systems
AB  - We introduce a sampling perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments. Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms. © 2020 Neural information processing systems foundation. All rights reserved.
DA  - 2020///
PY  - 2020
VL  - 2020-December
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104088882&partnerID=40&md5=c4d1f8bdcf47f337e3c90c03ad3e14c6
DB  - Scopus
KW  - Reinforcement learning
KW  - Reinforcement learning agent
KW  - Training and testing
KW  - Stochastic systems
KW  - Gradient methods
KW  - Objective functions
KW  - Policy gradient methods
KW  - Stochastic gradient
KW  - Environmental shifts
KW  - Langevin dynamics
ER  - 

TY  - CONF
TI  - Does distributionally robust supervised learning give robust classifiers?
AU  - Hu, W.
AU  - Niu, G.
AU  - Sato, I.
AU  - Sugiyama, M.
T2  - 35th International Conference on Machine Learning, ICML 2018
AB  - Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with/-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness. © 2018 by authors.All right reserved.
DA  - 2018///
PY  - 2018
VL  - 5
SP  - 3220
EP  - 3249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057279674&partnerID=40&md5=290395557bb261b75f67b472cfddea3d
DB  - Scopus
KW  - Artificial intelligence
KW  - Training data
KW  - Supervised learning
KW  - Real-world
KW  - Different distributions
KW  - Test data
KW  - Two sources
ER  - 

TY  - CONF
TI  - A Robust Self-Organizing Approach to Effectively Clustering Incomplete Data
AU  - Chau, V.T.N.
T2  - Proceedings - 2015 IEEE International Conference on Knowledge and Systems Engineering, KSE 2015
AB  - In the real world, incomplete data are often encountered and located anywhere in a data set. Such incomplete data make a data clustering task more challenging. It's common practice to eliminate incomplete data from the input data set. If there are a large number of missing values, ignoring them may lead to the data insufficiency and ineffectiveness of the data clustering task. Hence, incomplete data clustering has been considered in many research works with many different approaches based on the well-known existing clustering algorithms such as k-means, fuzzy c-means, the self-organizing map (SOM), mean shift, etc. However, few of them have examined both effectiveness and robustness of the incomplete data clustering algorithms. Some of them are not practical due to a lot of parameters in hybrid approaches and/or cannot handle incomplete data which appear in any object at any dimension. In contrast, this paper aims at a SOM-based incomplete data clustering algorithm, iS nps, which is a robust and effective solution to clustering incomplete data in a simple but practical approach. Is nps can do clustering on incomplete data as well as estimate incomplete data using the nearest prototype strategy in an iterative manner. As compared to several different existing approaches, our proposed algorithm can produce the clusters of good quality and a better approximation of incomplete data via the experiments on benchmark data sets. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/KSE.2015.11
SP  - 150
EP  - 155
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964723137&doi=10.1109%2fKSE.2015.11&partnerID=40&md5=d35101e780f00bdfb125cdc59dd8e77f
DB  - Scopus
KW  - robustness
KW  - Algorithms
KW  - Systems engineering
KW  - Robustness (control systems)
KW  - Benchmarking
KW  - Clustering algorithms
KW  - Iterative methods
KW  - Hybrid approach
KW  - Approximation algorithms
KW  - Unsupervised learning
KW  - unsupervised learning
KW  - Cluster analysis
KW  - Conformal mapping
KW  - Self organizing maps
KW  - Benchmark data
KW  - self-organizing map
KW  - Incomplete data
KW  - Effective solution
KW  - Data clustering
KW  - incomplete data clustering
KW  - Missing values
KW  - nearest prototype
KW  - Nearest prototype
KW  - Self-organizing approaches
ER  - 

TY  - CONF
TI  - Robust Support Vector Machine using Least Median Loss Penalty
AU  - Ma, Y.
AU  - Li, L.
AU  - Huang, X.
AU  - Wang, S.
T2  - IFAC Proceedings Volumes (IFAC-PapersOnline)
AB  - It is found that data points used for training may contain outliers that can generate unpredictable disturbance for some Support Vector Machines (SVMs) classification problems. No theoretical limit for such bad influence is held in traditional convex SVM methods. We present a novel robust misclassification penalty function for SVM which is inspired by the concept of "Least Median Regression". In our approach, total loss penalty in training is measured by the summation of two median hinge losses, each for a different class. We also propose a "Rank and Convex Procedure" to optimize our tasks. Though our approach is heuristic, it is faster than other known robust methods, such as SVM with Ramp Loss Penalty. © 2011 IFAC.
DA  - 2011///
PY  - 2011
DO  - 10.3182/20110828-6-IT-1002.03467
VL  - 44
SP  - 11208
EP  - 11213
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866749269&doi=10.3182%2f20110828-6-IT-1002.03467&partnerID=40&md5=21518c36cc565c7a29367397f3dff4f9
DB  - Scopus
KW  - Pattern recognition
KW  - Support vector machines
KW  - Heuristic methods
KW  - Support vector machine
KW  - Misclassifications
KW  - Learning theory
KW  - Learning Theory
KW  - Support vector machine (SVMs)
KW  - Statistical data analysis
KW  - Different class
KW  - Penalty function
KW  - Robustidentification
KW  - Theoretical limits
ER  - 

TY  - CONF
TI  - Robust semi-supervised representation learning for graph-structured data
AU  - Guo, L.-Z.
AU  - Han, T.
AU  - Li, Y.-F.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - The success of machine learning algorithms generally depends on data representation and recently many representation learning methods have been proposed. However, learning a good representation may not always benefit the classification tasks. It sometimes even hurt the performance as the learned representation maybe not related to the ultimate tasks, especially when the labeled examples are few to afford a reliable model selection. In this paper, we propose a novel robust semi-supervised graph representation learning method based on graph convolutional network. To make the learned representation more related to the ultimate classification task, we propose to extend label information based on the smooth assumption and obtain pseudo-labels for unlabeled nodes. Moreover, to make the model robust with noise in the pseudo-label, we propose to apply a large margin classifier to the learned representation. Influenced by the pseudo-label and the large-margin principle, the learned representation can not only exploit the label information encoded in the graph-structure sufficiently but also can produce a more rigorous decision boundary. Experiments demonstrate the superior performance of the proposal over many related methods. © Springer Nature Switzerland AG 2019.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-16142-2_11
VL  - 11441 LNAI
SP  - 131
EP  - 143
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065032165&doi=10.1007%2f978-3-030-16142-2_11&partnerID=40&md5=cdb26d866f80e05e4bd463563214ae0c
DB  - Scopus
KW  - Machine learning
KW  - Data mining
KW  - Supervised learning
KW  - Learning algorithms
KW  - Classification (of information)
KW  - Convolution
KW  - Representation learning
KW  - Graphic methods
KW  - Semi- supervised learning
KW  - Semi-supervised learning
KW  - Robust
KW  - Convolutional networks
KW  - Graph convolutional network
KW  - Graph structured data
KW  - Large margin classifiers
KW  - Large margin principle
KW  - Semi-supervised graphs
ER  - 

TY  - CONF
TI  - Assured Multi-agent Reinforcement Learning with Robust Agent-Interaction Adaptability
AU  - Riley, J.
AU  - Calinescu, R.
AU  - Paterson, C.
AU  - Kudenko, D.
AU  - Banks, A.
T2  - Smart Innovation, Systems and Technologies
AB  - Multi-agent reinforcement learning facilitates agents learning to solve complex decision-making problems requiring collaboration. However, reinforcement learning methods are underpinned by stochastic mechanisms, making them unsuitable for safety-critical domains. To solve this issue, approaches such as assured multi-agent reinforcement learning, which utilises quantitative verification to produce formal guarantees of safety requirements during the agents learning process, have been developed. However, this approach relies on accurate knowledge about the environment to be effectively used which can be detrimental if this knowledge is inaccurate. Therefore, we developed an extension to assured multi-agent reinforcement learning called agent interaction driven adaptability, an automated process to securing reliable safety constraints, allowing inaccurate and missing knowledge to be used without detriment. Our preliminary results showcase the ability of agent interaction driven adaptability to allow safe multi-agent reinforcement learning to be utilised in safety-critical scenarios. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-981-19-3444-5_8
VL  - 309
SP  - 87
EP  - 97
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135905227&doi=10.1007%2f978-981-19-3444-5_8&partnerID=40&md5=a24f1b8b8ff87d93118c4e69e71c214d
DB  - Scopus
KW  - Deep learning
KW  - Reinforcement learning
KW  - Safety
KW  - Automation
KW  - Decision making
KW  - Learning systems
KW  - Deep reinforcement learning
KW  - Reinforcement learnings
KW  - Safety engineering
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Stochastic systems
KW  - Reinforcement learning method
KW  - Assurance
KW  - Decision-making problem
KW  - Quantitative verification
KW  - Agent learning
KW  - Agent interaction
KW  - Complex decision
ER  - 

TY  - JOUR
TI  - A Robust Approach for Continuous Interactive Actor-Critic Algorithms
AU  - Millan-Arias, C.C.
AU  - Fernandes, B.J.T.
AU  - Cruz, F.
AU  - Dazeley, R.
AU  - Fernandes, S.
T2  - IEEE Access
AB  - Reinforcement learning refers to a machine learning paradigm in which an agent interacts with the environment to learn how to perform a task. The characteristics of the environment may change over time or be affected by disturbances not controlled, avoiding the agent finding a proper policy. Some approaches attempt to address these problems, as interactive reinforcement learning, where an external entity helps the agent learn through advice. Other approaches, such as robust reinforcement learning, allow the agent to learn the task, acting in a disturbed environment. In this paper, we propose an approach that addresses interactive reinforcement learning problems in a dynamic environment, where advice provides information on the task and the dynamics of the environment. Thus, an agent learns a policy in a disturbed environment while receiving advice. We implement our approach in the dynamic version of the cart-pole balancing task and a simulated robotic arm dynamic environment to organize objects. Our results show that the proposed approach allows an agent to complete the task satisfactorily in a dynamic, continuous state-action domain. Moreover, experimental results suggest agents trained with our approach are less sensitive to changes in the characteristics of the environment than interactive reinforcement learning agents.  © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3099071
VL  - 9
SP  - 104242
EP  - 104260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111607924&doi=10.1109%2fACCESS.2021.3099071&partnerID=40&md5=55aa30222b5a34ada19cd98c7718390d
DB  - Scopus
KW  - Intelligent agents
KW  - Reinforcement learning
KW  - reinforcement learning
KW  - Dynamic environments
KW  - Interactive Reinforcement Learning
KW  - Continuous state
KW  - interactive robust reinforcement learning
KW  - Pole balancing
KW  - Robust approaches
KW  - robust reinforcement learning
KW  - Turing machines
KW  - Actor-critic algorithm
KW  - Balancing
KW  - Arm dynamic
KW  - Continuous interactive reinforcement learning
KW  - Disturbed environments
ER  - 

TY  - CONF
TI  - Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning
AU  - Ding, D.
AU  - Wei, X.
AU  - Yang, Z.
AU  - Wang, Z.
AU  - Jovanović, M.R.
T2  - Proceedings of Machine Learning Research
AB  - We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rate O((|X| + |Y |)LpT(|A| + |B|))) for both regret and constraint violation after playing T episodes of the game. Here, L is the horizon of each episode, (|X|, |A|) and (|Y |, |B|) are the state/action space sizes of the min-player and the max-player, respectively. To the best of our knowledge, we provide the first provably efficient online safe reinforcement learning algorithm in constrained Markov games. © 2023 D. Ding, X. Wei, Z. Yang, Z. Wang & M.R. Jovanović.
DA  - 2023///
PY  - 2023
VL  - 211
SP  - 315
EP  - 332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172894839&partnerID=40&md5=cdac5fce5f7bd75455c6185abb95092b
DB  - Scopus
KW  - Reinforcement learning
KW  - Learning systems
KW  - Learning algorithms
KW  - E-learning
KW  - Reinforcement learnings
KW  - Multi agent systems
KW  - Multi-agent reinforcement learning
KW  - Lagrange multipliers
KW  - Stochastic systems
KW  - Constrained optimization
KW  - Mirrors
KW  - Safe multi-agent reinforcement learning
KW  - Markov games
KW  - constrained Markov game
KW  - Constrained markov game
KW  - generalized Lagrange multiplier method
KW  - Generalized lagrange multiplier methods
KW  - Lagrangian
KW  - Minimax
KW  - online mirror descent
KW  - Online mirror descent
KW  - safe multi-agent reinforcement learning
KW  - upper confidence reinforcement learning
KW  - Upper confidence reinforcement learning
ER  - 

TY  - CONF
TI  - Corrigibility
AU  - Soares, Nate
AU  - Fallenstein, Benja
AU  - Yudkowsky, Eliezer
AU  - Armstrong, Stuart
T2  - The 29th Annual AAAI Conference on Artificial Intelligence
AB  - As artificially intelligent systems grow in intelligence and capability, some of their available options may allow them to resist intervention by their programmers. We call an AI system “corrigible” if it cooperates with what its creators regard
as a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences. We introduce the notion of corrigibility and analyze utility functions that attempt to make an agent shut
down safely if a shutdown button is pressed, while avoiding incentives to prevent the button from being pressed or cause
the button to be pressed, and while ensuring propagation of the shutdown behavior as it creates new subsystems or selfmodifies. While some proposals are interesting, none have yet been demonstrated to satisfy all of our intuitive desiderata, leaving this simple problem in corrigibility wide-open.
C1  - Austin, Texas, USA
C3  - Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop
DA  - 2015///
PY  - 2015
UR  - https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf
Y2  - 2023/11/17/16:30:43
L1  - https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf
ER  - 

TY  - CONF
TI  - Causal Explanations for Sequential Decision-Making in Multi-Agent Systems
AU  - Gyevnar, Balint
AU  - Wang, Cheng
AU  - Lucas, Christopher G.
AU  - Cohen, Shay B.
AU  - Albrecht, Stefano V.
T3  - AAMAS '24
AB  - We present CEMA: Causal Explanations in Multi-A gent systems; a framework for creating causal natural language explanations of an agent's decisions in dynamic sequential multi-agent systems to build more trustworthy autonomous agents. Unlike prior work that assumes a fixed causal structure, CEMA only requires a probabilistic model for forward-simulating the state of the system. Using such a model, CEMA simulates counterfactual worlds that identify the salient causes behind the agent's decisions. We evaluate CEMA on the task of motion planning for autonomous driving and test it in diverse simulated scenarios. We show that CEMA correctly and robustly identifies the causes behind the agent's decisions, even when a large number of other agents is present, and show via a user study that CEMA's explanations have a positive effect on participants' trust in autonomous vehicles and are rated as high as high-quality baseline explanations elicited from other participants. We release the collected explanations with annotations as the HEADD dataset.
C1  - Richland, SC
C3  - Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems
DA  - 2024/05/06/
PY  - 2024
DP  - ACM Digital Library
SP  - 771
EP  - 779
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 9798400704864
Y2  - 2024/05/13/
KW  - explainable ai
KW  - autonomous vehicles
KW  - dataset
KW  - multi-agent systems
KW  - causal explanations
KW  - human-centric xai
ER  - 

TY  - GEN
TI  - AI Safety Gridworlds
AU  - Leike, Jan
AU  - Martic, Miljan
AU  - Krakovna, Victoria
AU  - Ortega, Pedro A.
AU  - Everitt, Tom
AU  - Lefrancq, Andrew
AU  - Orseau, Laurent
AU  - Legg, Shane
AB  - We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modiﬁcation, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and speciﬁcation problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.
DA  - 2017/11/28/
PY  - 2017
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1711.09883
Y2  - 2024/06/25/18:06:30
L1  - https://arxiv.org/pdf/1711.09883
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Improving Alignment and Robustness with Circuit Breakers
AU  - Zou, Andy
AU  - Phan, Long
AU  - Wang, Justin
AU  - Duenas, Derek
AU  - Lin, Maxwell
AU  - Andriushchenko, Maksym
AU  - Wang, Rowan
AU  - Kolter, Zico
AU  - Fredrikson, Matt
AU  - Hendrycks, Dan
AB  - AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with “circuit breakers.” Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility—even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image “hijacks” that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks. Code is available at github.com/blackswan-ai/circuit-breakers.
DA  - 2024/06/10/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2406.04313
Y2  - 2024/07/01/16:13:30
L1  - https://arxiv.org/pdf/2406.04313
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
AU  - Li, Nathaniel
AU  - Pan, Alexander
AU  - Gopal, Anjali
AU  - Yue, Summer
AU  - Berrios, Daniel
AU  - Gatti, Alice
AU  - Li, Justin D.
AU  - Dombrowski, Ann-Kathrin
AU  - Goel, Shashwat
AU  - Phan, Long
AU  - Mukobi, Gabriel
AU  - Helm-Burger, Nathan
AU  - Lababidi, Rassin
AU  - Justen, Lennart
AU  - Liu, Andrew B.
AU  - Chen, Michael
AU  - Barrass, Isabelle
AU  - Zhang, Oliver
AU  - Zhu, Xiaoyuan
AU  - Tamirisa, Rishub
AU  - Bharathi, Bhrugu
AU  - Khoja, Adam
AU  - Zhao, Zhenqi
AU  - Herbert-Voss, Ariel
AU  - Breuer, Cort B.
AU  - Marks, Samuel
AU  - Patel, Oam
AU  - Zou, Andy
AU  - Mazeika, Mantas
AU  - Wang, Zifan
AU  - Oswal, Palash
AU  - Lin, Weiran
AU  - Hunt, Adam A.
AU  - Tienken-Harder, Justin
AU  - Shih, Kevin Y.
AU  - Talley, Kemper
AU  - Guan, John
AU  - Kaplan, Russell
AU  - Steneker, Ian
AU  - Campbell, David
AU  - Jokubaitis, Brad
AU  - Levinson, Alex
AU  - Wang, Jean
AU  - Qian, William
AU  - Karmakar, Kallol Krishna
AU  - Basart, Steven
AU  - Fitz, Stephen
AU  - Levine, Mindy
AU  - Kumaraguru, Ponnurangam
AU  - Tupakula, Uday
AU  - Varadharajan, Vijay
AU  - Wang, Ruoyu
AU  - Shoshitaishvili, Yan
AU  - Ba, Jimmy
AU  - Esvelt, Kevin M.
AU  - Wang, Alexandr
AU  - Hendrycks, Dan
AB  - The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai
DA  - 2024/05/15/
PY  - 2024
DO  - 10.48550/arXiv.2403.03218
DP  - arXiv.org
PB  - arXiv
ST  - The WMDP Benchmark
UR  - http://arxiv.org/abs/2403.03218
Y2  - 2024/07/01/16:14:09
L1  - https://arxiv.org/pdf/2403.03218.pdf
L2  - https://arxiv.org/abs/2403.03218
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
AU  - Mazeika, Mantas
AU  - Phan, Long
AU  - Yin, Xuwang
AU  - Zou, Andy
AU  - Wang, Zifan
AU  - Mu, Norman
AU  - Sakhaee, Elham
AU  - Li, Nathaniel
AU  - Basart, Steven
AU  - Li, Bo
AU  - Forsyth, David
AU  - Hendrycks, Dan
AB  - Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
CY  - ICML 2024
DA  - 2024/02/26/
PY  - 2024
DO  - 10.48550/arXiv.2402.04249
DP  - arXiv.org
PB  - arXiv
ST  - HarmBench
UR  - http://arxiv.org/abs/2402.04249
Y2  - 2024/07/01/16:14:51
L1  - https://arxiv.org/pdf/2402.04249.pdf
L2  - https://arxiv.org/abs/2402.04249
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Can LLMs Follow Simple Rules?
AU  - Mu, Norman
AU  - Chen, Sarah
AU  - Wang, Zifan
AU  - Chen, Sizhe
AU  - Karamardian, David
AU  - Aljeraisy, Lulwa
AU  - Alomair, Basel
AU  - Hendrycks, Dan
AU  - Wagner, David
AB  - As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases. We also demonstrate that simple optimization attacks suffice to significantly increase failure rates on test cases. We conclude by exploring two potential avenues for improvement: test-time steering and supervised fine-tuning.
DA  - 2024/03/08/
PY  - 2024
DO  - 10.48550/arXiv.2311.04235
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.04235
Y2  - 2024/07/01/16:15:14
L1  - https://arxiv.org/pdf/2311.04235.pdf
L2  - https://arxiv.org/abs/2311.04235
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - A Recipe for Improved Certifiable Robustness
AU  - Hu, Kai
AU  - Leino, Klas
AU  - Wang, Zifan
AU  - Fredrikson, Matt
AB  - Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large “Cholesky-orthogonalized residual dense” layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points1.
DA  - 2024/06/22/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2310.02513
Y2  - 2024/07/01/16:15:26
L1  - https://arxiv.org/pdf/2310.02513
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Representation Engineering: A Top-Down Approach to AI Transparency
AU  - Zou, Andy
AU  - Phan, Long
AU  - Chen, Sarah
AU  - Campbell, James
AU  - Guo, Phillip
AU  - Ren, Richard
AU  - Pan, Alexander
AU  - Yin, Xuwang
AU  - Mazeika, Mantas
AU  - Dombrowski, Ann-Kathrin
AU  - Goel, Shashwat
AU  - Li, Nathaniel
AU  - Byun, Michael J.
AU  - Wang, Zifan
AU  - Mallen, Alex
AU  - Basart, Steven
AU  - Koyejo, Sanmi
AU  - Song, Dawn
AU  - Fredrikson, Matt
AU  - Kolter, J. Zico
AU  - Hendrycks, Dan
AB  - In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
DA  - 2023/10/10/
PY  - 2023
DO  - 10.48550/arXiv.2310.01405
DP  - arXiv.org
PB  - arXiv
ST  - Representation Engineering
UR  - http://arxiv.org/abs/2310.01405
Y2  - 2024/07/01/16:15:32
L1  - https://arxiv.org/pdf/2310.01405.pdf
L2  - https://arxiv.org/abs/2310.01405
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Universal and Transferable Adversarial Attacks on Aligned Language Models
AU  - Zou, Andy
AU  - Wang, Zifan
AU  - Carlini, Nicholas
AU  - Nasr, Milad
AU  - Kolter, J. Zico
AU  - Fredrikson, Matt
AB  - Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
DA  - 2023/12/20/
PY  - 2023
DO  - 10.48550/arXiv.2307.15043
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.15043
Y2  - 2024/07/01/16:16:13
L1  - https://arxiv.org/pdf/2307.15043.pdf
L2  - https://arxiv.org/abs/2307.15043
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Testing Robustness Against Unforeseen Adversaries
AU  - Kaufmann, Max
AU  - Kang, Daniel
AU  - Sun, Yi
AU  - Basart, Steven
AU  - Yin, Xuwang
AU  - Mazeika, Mantas
AU  - Arora, Akul
AU  - Dziedzic, Adam
AU  - Boenisch, Franziska
AU  - Brown, Tom
AU  - Steinhardt, Jacob
AU  - Hendrycks, Dan
AB  - Adversarial robustness research primarily focuses on L_p perturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in real-world applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the L_p ball. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a framework for evaluating model robustness against a range of unforeseen adversaries, including eighteen new non-L_p attacks. To perform well on ImageNet-UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet-UA as a useful tool for the community for improving the worst-case behavior of machine learning systems.
DA  - 2023/10/30/
PY  - 2023
DO  - 10.48550/arXiv.1908.08016
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1908.08016
Y2  - 2024/07/01/16:16:22
L1  - https://arxiv.org/pdf/1908.08016.pdf
L2  - https://arxiv.org/abs/1908.08016
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark
AU  - Pan, Alexander
AU  - Chan, Jun Shern
AU  - Zou, Andy
AU  - Li, Nathaniel
AU  - Basart, Steven
AU  - Woodside, Thomas
AU  - Ng, Jonathan
AU  - Zhang, Hanlin
AU  - Emmons, Scott
AU  - Hendrycks, Dan
AB  - Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.
CY  - ICML 2023
DA  - 2023/06/12/
PY  - 2023
DO  - 10.48550/arXiv.2304.03279
DP  - arXiv.org
PB  - arXiv
ST  - Do the Rewards Justify the Means?
UR  - http://arxiv.org/abs/2304.03279
Y2  - 2024/07/01/16:16:33
L1  - https://arxiv.org/pdf/2304.03279.pdf
L2  - https://arxiv.org/abs/2304.03279
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Forecasting Future World Events with Neural Networks
AU  - Zou, Andy
AU  - Xiao, Tristan
AU  - Jia, Ryan
AU  - Kwon, Joe
AU  - Mazeika, Mantas
AU  - Li, Richard
AU  - Song, Dawn
AU  - Steinhardt, Jacob
AU  - Evans, Owain
AU  - Hendrycks, Dan
AB  - Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.
CY  - NeurIPS 2023
DA  - 2022/10/09/
PY  - 2022
DO  - 10.48550/arXiv.2206.15474
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.15474
Y2  - 2024/07/01/16:16:51
L1  - https://arxiv.org/pdf/2206.15474.pdf
L2  - https://arxiv.org/abs/2206.15474
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios
AU  - Mazeika, Mantas
AU  - Tang, Eric
AU  - Zou, Andy
AU  - Basart, Steven
AU  - Chan, Jun Shern
AU  - Song, Dawn
AU  - Forsyth, David
AU  - Steinhardt, Jacob
AU  - Hendrycks, Dan
AB  - In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.
CY  - NeurIPS 2022
DA  - 2022/10/18/
PY  - 2022
DO  - 10.48550/arXiv.2210.10039
DP  - arXiv.org
PB  - arXiv
ST  - How Would The Viewer Feel?
UR  - http://arxiv.org/abs/2210.10039
Y2  - 2024/07/01/16:18:33
L1  - https://arxiv.org/pdf/2210.10039.pdf
L2  - https://arxiv.org/abs/2210.10039
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures
AU  - Hendrycks, Dan
AU  - Zou, Andy
AU  - Mazeika, Mantas
AU  - Tang, Leonard
AU  - Li, Bo
AU  - Song, Dawn
AU  - Steinhardt, Jacob
AB  - In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.
CY  - CVPR 2022
DA  - 2022/03/29/
PY  - 2022
DO  - 10.48550/arXiv.2112.05135
DP  - arXiv.org
PB  - arXiv
ST  - PixMix
UR  - http://arxiv.org/abs/2112.05135
Y2  - 2024/07/01/16:18:51
L1  - https://arxiv.org/pdf/2112.05135.pdf
L2  - https://arxiv.org/abs/2112.05135
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Scaling Out-of-Distribution Detection for Real-World Settings
AU  - Hendrycks, Dan
AU  - Basart, Steven
AU  - Mazeika, Mantas
AU  - Zou, Andy
AU  - Kwon, Joe
AU  - Mostajabi, Mohammadreza
AU  - Steinhardt, Jacob
AU  - Song, Dawn
AB  - Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.
CY  - ICML 2022
DA  - 2022/05/15/
PY  - 2022
DO  - 10.48550/arXiv.1911.11132
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1911.11132
Y2  - 2024/07/01/16:19:07
L1  - https://arxiv.org/pdf/1911.11132.pdf
L2  - https://arxiv.org/abs/1911.11132
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - What Would Jiminy Cricket Do? Towards Agents That Behave Morally
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AU  - Zou, Andy
AU  - Patel, Sahil
AU  - Zhu, Christine
AU  - Navarro, Jesus
AU  - Song, Dawn
AU  - Li, Bo
AU  - Steinhardt, Jacob
AB  - When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.
CY  - NeurIPS 2021
DA  - 2022/02/07/
PY  - 2022
DO  - 10.48550/arXiv.2110.13136
DP  - arXiv.org
PB  - arXiv
ST  - What Would Jiminy Cricket Do?
UR  - http://arxiv.org/abs/2110.13136
Y2  - 2024/07/01/16:19:32
L1  - https://arxiv.org/pdf/2110.13136.pdf
L2  - https://arxiv.org/abs/2110.13136
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization
AU  - Hendrycks, Dan
AU  - Basart, Steven
AU  - Mu, Norman
AU  - Kadavath, Saurav
AU  - Wang, Frank
AU  - Dorundo, Evan
AU  - Desai, Rahul
AU  - Zhu, Tyler
AU  - Parajuli, Samyak
AU  - Guo, Mike
AU  - Song, Dawn
AU  - Steinhardt, Jacob
AU  - Gilmer, Justin
AB  - We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.
CY  - ICCV 2021
DA  - 2021/07/24/
PY  - 2021
DO  - 10.48550/arXiv.2006.16241
DP  - arXiv.org
PB  - arXiv
ST  - The Many Faces of Robustness
UR  - http://arxiv.org/abs/2006.16241
Y2  - 2024/07/01/16:20:18
L1  - https://arxiv.org/pdf/2006.16241.pdf
L2  - https://arxiv.org/abs/2006.16241
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Natural Adversarial Examples
AU  - Hendrycks, Dan
AU  - Zhao, Kevin
AU  - Basart, Steven
AU  - Steinhardt, Jacob
AU  - Song, Dawn
AB  - We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.
CY  - CVPR 2021
DA  - 2021/03/04/
PY  - 2021
DO  - 10.48550/arXiv.1907.07174
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1907.07174
Y2  - 2024/07/01/16:20:30
L1  - https://arxiv.org/pdf/1907.07174.pdf
L2  - https://arxiv.org/abs/1907.07174
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Aligning AI With Shared Human Values
AU  - Hendrycks, Dan
AU  - Burns, Collin
AU  - Basart, Steven
AU  - Critch, Andrew
AU  - Li, Jerry
AU  - Song, Dawn
AU  - Steinhardt, Jacob
AB  - We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.
CY  - ICLR 2021
DA  - 2023/02/17/
PY  - 2023
DO  - 10.48550/arXiv.2008.02275
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2008.02275
Y2  - 2024/07/01/16:20:30
L1  - https://arxiv.org/pdf/2008.02275.pdf
L2  - https://arxiv.org/abs/2008.02275
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Pretrained Transformers Improve Out-of-Distribution Robustness
AU  - Hendrycks, Dan
AU  - Liu, Xiaoyuan
AU  - Wallace, Eric
AU  - Dziedzic, Adam
AU  - Krishnan, Rishabh
AU  - Song, Dawn
AB  - Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.
CY  - ACL 2020
DA  - 2020/04/16/
PY  - 2020
DO  - 10.48550/arXiv.2004.06100
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2004.06100
Y2  - 2024/07/01/16:20:33
L1  - https://arxiv.org/pdf/2004.06100.pdf
L2  - https://arxiv.org/abs/2004.06100
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty
AU  - Hendrycks, Dan
AU  - Mu, Norman
AU  - Cubuk, Ekin D.
AU  - Zoph, Barret
AU  - Gilmer, Justin
AU  - Lakshminarayanan, Balaji
AB  - Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.
CY  - ICLR 2020
DA  - 2020/02/17/
PY  - 2020
DO  - 10.48550/arXiv.1912.02781
DP  - arXiv.org
PB  - arXiv
ST  - AugMix
UR  - http://arxiv.org/abs/1912.02781
Y2  - 2024/07/01/16:22:20
L1  - https://arxiv.org/pdf/1912.02781.pdf
L2  - https://arxiv.org/abs/1912.02781
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AU  - Kadavath, Saurav
AU  - Song, Dawn
AB  - Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.
CY  - NeurIPS 2020
DA  - 2019/10/29/
PY  - 2019
DO  - 10.48550/arXiv.1906.12340
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1906.12340
Y2  - 2024/07/01/16:22:22
L1  - https://arxiv.org/pdf/1906.12340.pdf
L2  - https://arxiv.org/abs/1906.12340
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Using Pre-Training Can Improve Model Robustness and Uncertainty
AU  - Hendrycks, Dan
AU  - Lee, Kimin
AU  - Mazeika, Mantas
AB  - He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.
CY  - ICML 2019
DA  - 2019/10/20/
PY  - 2019
DO  - 10.48550/arXiv.1901.09960
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1901.09960
Y2  - 2024/07/01/16:22:24
L1  - https://arxiv.org/pdf/1901.09960.pdf
L2  - https://arxiv.org/abs/1901.09960
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Deep Anomaly Detection with Outlier Exposure
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AU  - Dietterich, Thomas
AB  - It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.
CY  - ICLR 2019
DA  - 2019/01/28/
PY  - 2019
DO  - 10.48550/arXiv.1812.04606
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1812.04606
Y2  - 2024/07/01/16:22:27
L1  - https://arxiv.org/pdf/1812.04606.pdf
L2  - https://arxiv.org/abs/1812.04606
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Benchmarking Neural Network Robustness to Common Corruptions and Perturbations
AU  - Hendrycks, Dan
AU  - Dietterich, Thomas
AB  - In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.
CY  - ICLR 2019
DA  - 2019/03/28/
PY  - 2019
DO  - 10.48550/arXiv.1903.12261
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1903.12261
Y2  - 2024/07/01/16:22:31
L1  - https://arxiv.org/pdf/1903.12261.pdf
L2  - https://arxiv.org/abs/1903.12261
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks
AU  - Hendrycks, Dan
AU  - Gimpel, Kevin
AB  - We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.
CY  - ICLR 2017
DA  - 2018/10/03/
PY  - 2018
DO  - 10.48550/arXiv.1610.02136
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1610.02136
Y2  - 2024/07/01/16:22:34
L1  - https://arxiv.org/pdf/1610.02136.pdf
L2  - https://arxiv.org/abs/1610.02136
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Unsolved Problems in ML Safety
AU  - Hendrycks, Dan
AU  - Carlini, Nicholas
AU  - Schulman, John
AU  - Steinhardt, Jacob
AB  - Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.
DA  - 2022/06/16/
PY  - 2022
DO  - 10.48550/arXiv.2109.13916
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2109.13916
Y2  - 2024/07/01/16:25:01
L1  - https://arxiv.org/pdf/2109.13916.pdf
L2  - https://arxiv.org/abs/2109.13916
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - X-Risk Analysis for AI Research
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AB  - Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.
DA  - 2022/09/20/
PY  - 2022
DO  - 10.48550/arXiv.2206.05862
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.05862
Y2  - 2024/07/01/16:25:06
L1  - https://arxiv.org/pdf/2206.05862.pdf
L2  - https://arxiv.org/abs/2206.05862
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Natural Selection Favors AIs over Humans
AU  - Hendrycks, Dan
AB  - For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. This Darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. To counteract these risks and evolutionary forces, we consider interventions such as carefully designing AI agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. These steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.
DA  - 2023/07/18/
PY  - 2023
DO  - 10.48550/arXiv.2303.16200
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.16200
Y2  - 2024/07/01/16:25:06
L1  - https://arxiv.org/pdf/2303.16200.pdf
L2  - https://arxiv.org/abs/2303.16200
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - An Overview of Catastrophic AI Risks
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AU  - Woodside, Thomas
AB  - Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.
DA  - 2023/10/09/
PY  - 2023
DO  - 10.48550/arXiv.2306.12001
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.12001
Y2  - 2024/07/01/16:25:07
L1  - https://arxiv.org/pdf/2306.12001.pdf
L2  - https://arxiv.org/abs/2306.12001
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - AI Deception: A Survey of Examples, Risks, and Potential Solutions
AU  - Park, Peter S.
AU  - Goldstein, Simon
AU  - O'Gara, Aidan
AU  - Chen, Michael
AU  - Hendrycks, Dan
AB  - This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.
DA  - 2023/08/28/
PY  - 2023
DO  - 10.48550/arXiv.2308.14752
DP  - arXiv.org
PB  - arXiv
ST  - AI Deception
UR  - http://arxiv.org/abs/2308.14752
Y2  - 2024/07/01/16:25:12
L1  - https://arxiv.org/pdf/2308.14752.pdf
L2  - https://arxiv.org/abs/2308.14752
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Instrumental Convergence?
AU  - Gallow, Dmitri
AB  - The thesis of instrumental convergence holds that a wide range of ends have common means: for instance, self preservation, desire preservation, self improvement, and resource acquisition. Bostrom (2014) contends that instrumental convergence gives us reason to think that “the default outcome of the creation of machine superintelligence is existential catastrophe”. I use the tools of decision theory to investigate whether this thesis is true. I ﬁnd that, even if intrinsic desires are randomly selected, instrumental rationality induces biases towards certain kinds of choices. Firstly, a bias towards choices which leave less up to chance. Secondly, a bias towards desire preservation, in line with Bostrom’s conjecture. And thirdly, a bias towards choices which aﬀord more choices later on. I do not ﬁnd biases towards any other of the convergent instrumental means on Bostrom’s list. I conclude that the biases induced by instrumental rationality at best weakly support Bostrom’s conclusion that machine superintelligence is likely to lead to existential catastrophe.
CY  - Unpublished
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
ER  - 

TY  - JOUR
TI  - The shutdown problem: an AI engineering puzzle for decision theorists
AU  - Thornley, Elliott
T2  - Philosophical Studies
AB  - I explain and motivate the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don’t try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently. I prove three theorems that make the difficulty precise. These theorems suggest that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it’s costly to do so. I end by noting that these theorems can guide our search for solutions to the problem.
DA  - 2024/06/19/
PY  - 2024
DO  - 10.1007/s11098-024-02153-3
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
ST  - The shutdown problem
UR  - https://doi.org/10.1007/s11098-024-02153-3
Y2  - 2024/07/01/22:03:59
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02153-3.pdf
KW  - AI safety
KW  - Constructive decision theory
KW  - Corrigibility
KW  - The shutdown problem
ER  - 

TY  - JOUR
TI  - Deontology and safe artificial intelligence
AU  - D’Alessandro, William
T2  - Philosophical Studies
AB  - The field of AI safety aims to prevent increasingly capable artificially intelligent systems from causing humans harm. Research on moral alignment is widely thought to offer a promising safety strategy: if we can equip AI systems with appropriate ethical rules, according to this line of thought, they’ll be unlikely to disempower, destroy or otherwise seriously harm us. Deontological morality looks like a particularly attractive candidate for an alignment target, given its popularity, relative technical tractability and commitment to harm-avoidance principles. I argue that the connection between moral alignment and safe behavior is more tenuous than many have hoped. In general, AI systems can possess either of these properties in the absence of the other, and we should favor safety when the two conflict. In particular, advanced AI systems governed by standard versions of deontology need not be especially safe.
DA  - 2024/06/13/
PY  - 2024
DO  - 10.1007/s11098-024-02174-y
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
UR  - https://doi.org/10.1007/s11098-024-02174-y
Y2  - 2024/07/01/22:04:08
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02174-y.pdf
KW  - AI ethics
KW  - AI safety
KW  - AI risk
KW  - Alignment problem
KW  - Anti-natalism
KW  - Deontology
KW  - Existential risk
KW  - Human extinction
ER  - 

TY  - JOUR
TI  - Shutdown-seeking AI
AU  - Goldstein, Simon
AU  - Robinson, Pamela
T2  - Philosophical Studies
AB  - We propose developing AIs whose only final goal is being shut down. We argue that this approach to AI safety has three benefits: (i) it could potentially be implemented in reinforcement learning, (ii) it avoids some dangerous instrumental convergence dynamics, and (iii) it creates trip wires for monitoring dangerous capabilities. We also argue that the proposal can overcome a key challenge raised by Soares et al. (2015), that shutdown-seeking AIs will manipulate humans into shutting them down. We conclude by comparing our approach with Soares et al.'s corrigibility framework.
DA  - 2024/06/06/
PY  - 2024
DO  - 10.1007/s11098-024-02099-6
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
UR  - https://doi.org/10.1007/s11098-024-02099-6
Y2  - 2024/07/01/22:04:15
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02099-6.pdf
KW  - AI safety
KW  - Instrumental convergence
KW  - Reward misspecification
ER  - 

TY  - JOUR
TI  - Against the singularity hypothesis
AU  - Thorstad, David
T2  - Philosophical Studies
AB  - The singularity hypothesis is a hypothesis about the future of artificial intelligence on which self-improving artificial agents will quickly become orders of magnitude more intelligent than the average human. Despite the ambitiousness of its claims, the singularity hypothesis has been defended at length by leading philosophers and artificial intelligence researchers. In this paper, I argue that the singularity hypothesis rests on undersupported growth assumptions. I show how leading philosophical defenses of the singularity hypothesis fail to overcome the case for skepticism. I conclude by drawing out philosophical and policy implications of this discussion.
DA  - 2024/05/10/
PY  - 2024
DO  - 10.1007/s11098-024-02143-5
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
UR  - https://doi.org/10.1007/s11098-024-02143-5
Y2  - 2024/07/01/22:04:21
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02143-5.pdf
KW  - AI safety
KW  - Existential risk
KW  - Philosophy of artificial intelligence
KW  - Singularity hypothesis
KW  - Technological growth
ER  - 

TY  - JOUR
TI  - Existentialist risk and value misalignment
AU  - Tubert, Ariela
AU  - Tiehen, Justin
T2  - Philosophical Studies
AB  - We argue that two long-term goals of AI research stand in tension with one another. The first involves creating AI that is safe, where this is understood as solving the problem of value alignment. The second involves creating artificial general intelligence, meaning AI that operates at or beyond human capacity across all or many intellectual domains. Our argument focuses on the human capacity to make what we call “existential choices”, choices that transform who we are as persons, including transforming what we most deeply value or desire. It is a capacity for a kind of value misalignment, in that the values held prior to making such choices can be significantly different from (misaligned with) the values held after making them. Because of the connection to existentialist philosophers who highlight these choices, we call the resulting form of risk “existentialist risk.” It is, roughly, the risk that results from AI taking an active role in authoring its own values rather than passively going along with the values given to it. On our view, human-like intelligence requires a human-like capacity for value misalignment, which is in tension with the possibility of guaranteeing value alignment between AI and humans.
DA  - 2024/04/25/
PY  - 2024
DO  - 10.1007/s11098-024-02142-6
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
UR  - https://doi.org/10.1007/s11098-024-02142-6
Y2  - 2024/07/01/22:04:27
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02142-6.pdf
KW  - Artificial intelligence
KW  - Value alignment
KW  - Existential risk
KW  - Existentialism
KW  - Practical reason
KW  - Transformative choices
ER  - 

TY  - JOUR
TI  - Instrumental divergence
AU  - Gallow, J. Dmitri
T2  - Philosophical Studies
AB  - The thesis of instrumental convergence holds that a wide range of ends have common means: for instance, self preservation, desire preservation, self improvement, and resource acquisition. Bostrom contends that instrumental convergence gives us reason to think that “the default outcome of the creation of machine superintelligensome of the ‘convergence is existential catastrophe”. I use the tools of decision theory to investigate whether this thesis is true. I find that, even if intrinsic desires are randomly selected, instrumental rationality induces biases towards certain kinds of choices. Firstly, a bias towards choices which leave less up to chance. Secondly, a bias towards desire preservation, in line with Bostrom’s conjecture. And thirdly, a bias towards choices which afford more choices later on. I do not find biases towards any other of the convergent instrumental means on Bostrom’s list. I conclude that the biases induced by instrumental rationality at best weakly support Bostrom’s conclusion that machine superintelligence is likely to lead to existential catastrophe.
DA  - 2024/04/06/
PY  - 2024
DO  - 10.1007/s11098-024-02129-3
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
UR  - https://doi.org/10.1007/s11098-024-02129-3
Y2  - 2024/07/01/22:04:33
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-024-02129-3.pdf
KW  - AI safety
KW  - Existential risk
KW  - Instrumental convergence
ER  - 

TY  - JOUR
TI  - Still no lie detector for language models: probing empirical and conceptual roadblocks
AU  - Levinstein, Benjamin A.
AU  - Herrmann, Daniel A.
T2  - Philosophical Studies
AB  - We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. With this lesson in hand, we evaluate two existing approaches for measuring the beliefs of LLMs, one due to Azaria and Mitchell (The internal state of an llm knows when its lying, 2023) and the other to Burns et al. (Discovering latent knowledge in language models without supervision, 2022). Moving from the armchair to the desk chair, we provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. We conclude by suggesting some concrete paths for future work.
DA  - 2024/02/17/
PY  - 2024
DO  - 10.1007/s11098-023-02094-3
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
ST  - Still no lie detector for language models
UR  - https://doi.org/10.1007/s11098-023-02094-3
Y2  - 2024/07/01/22:04:39
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-023-02094-3.pdf
KW  - Interpretability
KW  - Large language models
KW  - CCS
KW  - Probes
ER  - 

TY  - JOUR
TI  - Will AI avoid exploitation? Artificial general intelligence and expected utility theory
AU  - Bales, Adam
T2  - Philosophical Studies
AB  - A simple argument suggests that we can fruitfully model advanced AI systems using expected utility theory. According to this argument, an agent will need to act as if maximising expected utility if they’re to avoid exploitation. Insofar as we should expect advanced AI to avoid exploitation, it follows that we should expected advanced AI to act as if maximising expected utility. I spell out this argument more carefully and demonstrate that it fails, but show that the manner of its failure is instructive: in exploring the argument, we gain insight into how to model advanced AI systems.
DA  - 2023/08/05/
PY  - 2023
DO  - 10.1007/s11098-023-02023-4
DP  - Springer Link
J2  - Philos Stud
LA  - en
SN  - 1573-0883
ST  - Will AI avoid exploitation?
UR  - https://doi.org/10.1007/s11098-023-02023-4
Y2  - 2024/07/01/22:04:47
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11098-023-02023-4.pdf
KW  - Artificial intelligence
KW  - Expected utility theory
KW  - Artificial general intelligence
KW  - Money pump arguments
ER  - 

TY  - CONF
TI  - Pretraining language models with human preferences
AU  - Korbak, Tomasz
AU  - Shi, Kejian
AU  - Chen, Angelica
AU  - Bhalerao, Rasika Vinayak
AU  - Buckley, Christopher
AU  - Phang, Jason
AU  - Bowman, Samuel R.
AU  - Perez, Ethan
C3  - International Conference on Machine Learning
DA  - 2023///
PY  - 2023
DP  - Google Scholar
SP  - 17506
EP  - 17533
PB  - PMLR
UR  - https://proceedings.mlr.press/v202/korbak23a.html
Y2  - 2024/07/01/22:06:38
L1  - https://proceedings.mlr.press/v202/korbak23a/korbak23a.pdf
ER  - 

TY  - GEN
TI  - Eliciting Latent Predictions from Transformers with the Tuned Lens
AU  - Belrose, Nora
AU  - Furman, Zach
AU  - Smith, Logan
AU  - Halawi, Danny
AU  - Ostrovsky, Igor
AU  - McKinney, Lev
AU  - Biderman, Stella
AU  - Steinhardt, Jacob
AB  - We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
DA  - 2023/11/26/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.08112
Y2  - 2024/07/01/22:06:40
L1  - https://arxiv.org/pdf/2303.08112.pdf
L2  - https://arxiv.org/abs/2303.08112
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Towards automated circuit discovery for mechanistic interpretability
AU  - Conmy, Arthur
AU  - Mavor-Parker, Augustine
AU  - Lynch, Aengus
AU  - Heimersheim, Stefan
AU  - Garriga-Alonso, Adrià
T2  - Advances in Neural Information Processing Systems
DA  - 2023///
PY  - 2023
DP  - Google Scholar
VL  - 36
SP  - 16318
EP  - 16352
UR  - https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html
Y2  - 2024/07/01/22:06:40
L1  - https://proceedings.neurips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf
ER  - 

TY  - GEN
TI  - Training Language Models with Language Feedback at Scale
AU  - Scheurer, Jérémy
AU  - Campos, Jon Ander
AU  - Korbak, Tomasz
AU  - Chan, Jun Shern
AU  - Chen, Angelica
AU  - Cho, Kyunghyun
AU  - Perez, Ethan
AB  - Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.
DA  - 2024/02/22/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.16755
Y2  - 2024/07/01/22:06:41
L1  - https://arxiv.org/pdf/2303.16755.pdf
L2  - https://arxiv.org/abs/2303.16755
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - imitation: Clean Imitation Learning Implementations
AU  - Gleave, Adam
AU  - Taufeeque, Mohammad
AU  - Rocamonde, Juan
AU  - Jenner, Erik
AU  - Wang, Steven H.
AU  - Toyer, Sam
AU  - Ernestus, Maximilian
AU  - Belrose, Nora
AU  - Emmons, Scott
AU  - Russell, Stuart
AB  - imitation provides open-source implementations of imitation and reward learning algorithms in PyTorch. We include three inverse reinforcement learning (IRL) algorithms, three imitation learning algorithms and a preference comparison algorithm. The implementations have been benchmarked against previous results, and automated tests cover 98% of the code. Moreover, the algorithms are implemented in a modular fashion, making it simple to develop novel algorithms in the framework. Our source code, including documentation and examples, is available at https://github.com/HumanCompatibleAI/imitation
DA  - 2022/11/21/
PY  - 2022
DP  - arXiv.org
PB  - arXiv
ST  - imitation
UR  - http://arxiv.org/abs/2211.11972
Y2  - 2024/07/01/22:06:44
L1  - https://arxiv.org/pdf/2211.11972.pdf
L2  - https://arxiv.org/abs/2211.11972
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Training Language Models with Language Feedback
AU  - Scheurer, Jérémy
AU  - Campos, Jon Ander
AU  - Chan, Jun Shern
AU  - Chen, Angelica
AU  - Cho, Kyunghyun
AU  - Perez, Ethan
AB  - Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.
DA  - 2022/11/17/
PY  - 2022
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.14146
Y2  - 2024/07/01/22:06:46
L1  - https://arxiv.org/pdf/2204.14146.pdf
L2  - https://arxiv.org/abs/2204.14146
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Inverse Scaling: When Bigger Isn't Better
AU  - McKenzie, Ian R.
AU  - Lyzhov, Alexander
AU  - Pieler, Michael
AU  - Parrish, Alicia
AU  - Mueller, Aaron
AU  - Prabhu, Ameya
AU  - McLean, Euan
AU  - Kirtland, Aaron
AU  - Ross, Alexis
AU  - Liu, Alisa
AU  - Gritsevskiy, Andrew
AU  - Wurgaft, Daniel
AU  - Kauffman, Derik
AU  - Recchia, Gabriel
AU  - Liu, Jiacheng
AU  - Cavanagh, Joe
AU  - Weiss, Max
AU  - Huang, Sicong
AU  - Droid, The Floating
AU  - Tseng, Tom
AU  - Korbak, Tomasz
AU  - Shen, Xudong
AU  - Zhang, Yuhui
AU  - Zhou, Zhengping
AU  - Kim, Najoung
AU  - Bowman, Samuel R.
AU  - Perez, Ethan
AB  - Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.
DA  - 2024/05/12/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
ST  - Inverse Scaling
UR  - http://arxiv.org/abs/2306.09479
Y2  - 2024/07/01/22:06:48
L1  - https://arxiv.org/pdf/2306.09479.pdf
L2  - https://arxiv.org/abs/2306.09479
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Adversarial policies beat professional-level go ais
AU  - Wang, Tony Tong
AU  - Gleave, Adam
AU  - Belrose, Nora
AU  - Tseng, Tom
AU  - Dennis, Michael D.
AU  - Duan, Yawen
AU  - Pogrebniak, Viktor
AU  - Miller, Joseph
AU  - Levine, Sergey
AU  - Russell, Stuart
C3  - Deep Reinforcement Learning Workshop NeurIPS 2022
DA  - 2022///
PY  - 2022
DP  - Google Scholar
UR  - https://proceedings.mlr.press/v202/wang23g.html
Y2  - 2024/07/01/22:06:53
L1  - https://proceedings.mlr.press/v202/wang23g/wang23g.pdf
ER  - 

TY  - GEN
TI  - Improving Code Generation by Training with Natural Language Feedback
AU  - Chen, Angelica
AU  - Scheurer, Jérémy
AU  - Korbak, Tomasz
AU  - Campos, Jon Ander
AU  - Chan, Jun Shern
AU  - Bowman, Samuel R.
AU  - Cho, Kyunghyun
AU  - Perez, Ethan
AB  - The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an LLM's performance on code generation tasks.
DA  - 2024/02/22/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.16749
Y2  - 2024/07/01/22:06:55
L1  - https://arxiv.org/pdf/2303.16749.pdf
L2  - https://arxiv.org/abs/2303.16749
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Software Engineering
ER  - 

TY  - JOUR
TI  - Evaluating the moral beliefs encoded in llms
AU  - Scherrer, Nino
AU  - Shi, Claudia
AU  - Feder, Amir
AU  - Blei, David
T2  - Advances in Neural Information Processing Systems
DA  - 2024///
PY  - 2024
DP  - Google Scholar
VL  - 36
UR  - https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html
Y2  - 2024/07/01/22:06:56
L1  - https://proceedings.neurips.cc/paper_files/paper/2023/file/a2cf225ba392627529efef14dc857e22-Paper-Conference.pdf
ER  - 

TY  - GEN
TI  - RL with KL penalties is better viewed as Bayesian inference
AU  - Korbak, Tomasz
AU  - Perez, Ethan
AU  - Buckley, Christopher L.
AB  - Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.
DA  - 2022/10/21/
PY  - 2022
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.11275
Y2  - 2024/07/01/22:06:58
L1  - https://arxiv.org/pdf/2205.11275.pdf
L2  - https://arxiv.org/abs/2205.11275
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Exploiting Novel GPT-4 APIs
AU  - Pelrine, Kellin
AU  - Taufeeque, Mohammad
AU  - Zając, Michał
AU  - McLean, Euan
AU  - Gleave, Adam
AB  - Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose ``gray-box'' access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed by an API can create new vulnerabilities.
DA  - 2023/12/21/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.14302
Y2  - 2024/07/01/22:07:00
L1  - https://arxiv.org/pdf/2312.14302.pdf
L2  - https://arxiv.org/abs/2312.14302
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
KW  - I.2.7
ER  - 

TY  - GEN
TI  - Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning
AU  - Rocamonde, Juan
AU  - Montesinos, Victoriano
AU  - Nava, Elvis
AU  - Perez, Ethan
AU  - Lindner, David
AB  - Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.
DA  - 2024/03/14/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.12921
Y2  - 2024/07/01/22:07:02
L1  - https://arxiv.org/pdf/2310.12921.pdf
L2  - https://arxiv.org/abs/2310.12921
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Codebook Features: Sparse and Discrete Interpretability for Neural Networks
AU  - Tamkin, Alex
AU  - Taufeeque, Mohammad
AU  - Goodman, Noah D.
AB  - Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on several different datasets. First, we explore a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. Second, we train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics by activating the appropriate codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at https://github.com/taufeeque9/codebook-features.
DA  - 2023/10/26/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
ST  - Codebook Features
UR  - http://arxiv.org/abs/2310.17230
Y2  - 2024/07/01/22:07:05
L1  - https://arxiv.org/pdf/2310.17230.pdf
L2  - https://arxiv.org/abs/2310.17230
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems
AU  - Dalrymple, David "davidad"
AU  - Skalse, Joar
AU  - Bengio, Yoshua
AU  - Russell, Stuart
AU  - Tegmark, Max
AU  - Seshia, Sanjit
AU  - Omohundro, Steve
AU  - Szegedy, Christian
AU  - Goldhaber, Ben
AU  - Ammann, Nora
AU  - Abate, Alessandro
AU  - Halpern, Joe
AU  - Barrett, Clark
AU  - Zhao, Ding
AU  - Zhi-Xuan, Tan
AU  - Wing, Jeannette
AU  - Tenenbaum, Joshua
AB  - Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.
DA  - 2024/05/17/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
ST  - Towards Guaranteed Safe AI
UR  - http://arxiv.org/abs/2405.06624
Y2  - 2024/07/01/22:07:07
L1  - https://arxiv.org/pdf/2405.06624.pdf
L2  - https://arxiv.org/abs/2405.06624
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - An Invariant Learning Characterization of Controlled Text Generation
AU  - Zheng, Carolina
AU  - Shi, Claudia
AU  - Vafa, Keyon
AU  - Feder, Amir
AU  - Blei, David M.
AB  - Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments. We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.
DA  - 2023/05/31/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.00198
Y2  - 2024/07/01/22:07:14
L1  - https://arxiv.org/pdf/2306.00198.pdf
L2  - https://arxiv.org/abs/2306.00198
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Few-shot adaptation works with unpredictable data
AU  - Chan, Jun Shern
AU  - Pieler, Michael
AU  - Jao, Jonathan
AU  - Scheurer, Jérémy
AU  - Perez, Ethan
T2  - arXiv preprint arXiv:2208.01009
DA  - 2022///
PY  - 2022
DP  - Google Scholar
UR  - https://ui.adsabs.harvard.edu/abs/2022arXiv220801009S/abstract
Y2  - 2024/07/01/22:07:15
ER  - 

TY  - GEN
TI  - Can Go AIs be adversarially robust?
AU  - Tseng, Tom
AU  - McLean, Euan
AU  - Pelrine, Kellin
AU  - Wang, Tony T.
AU  - Gleave, Adam
AB  - Prior work found that superhuman Go AIs like KataGo can be defeated by simple adversarial strategies. In this paper, we study if simple defenses can improve KataGo's worst-case performance. We test three natural defenses: adversarial training on hand-constructed positions, iterated adversarial training, and changing the network architecture. We find that some of these defenses are able to protect against previously discovered attacks. Unfortunately, we also find that none of these defenses are able to withstand adaptive attacks. In particular, we are able to train new adversaries that reliably defeat our defended agents by causing them to blunder in ways humans would not. Our results suggest that building robust AI systems is challenging even in narrow domains such as Go. For interactive examples of attacks and a link to our codebase, see https://goattack.far.ai.
DA  - 2024/06/18/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.12843
Y2  - 2024/07/01/22:07:17
L1  - https://arxiv.org/pdf/2406.12843.pdf
L2  - https://arxiv.org/abs/2406.12843
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Uncovering Latent Human Wellbeing in Language Model Embeddings
AU  - Freire, Pedro
AU  - Tan, ChengCheng
AU  - Gleave, Adam
AU  - Hendrycks, Dan
AU  - Emmons, Scott
AB  - Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
DA  - 2024/02/18/
PY  - 2024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.11777
Y2  - 2024/07/01/22:07:20
L1  - https://arxiv.org/pdf/2402.11777.pdf
L2  - https://arxiv.org/abs/2402.11777
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - I.2.7
ER  - 

TY  - JOUR
TI  - Safe Pareto improvements for delegated game playing
AU  - Oesterheld, Caspar
AU  - Conitzer, Vincent
T2  - Autonomous Agents and Multi-Agent Systems
AB  - A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.
DA  - 2022/10//
PY  - 2022
DO  - 10.1007/s10458-022-09574-6
DP  - DOI.org (Crossref)
VL  - 36
IS  - 2
SP  - 46
J2  - Auton Agent Multi-Agent Syst
LA  - en
SN  - 1387-2532, 1573-7454
UR  - https://link.springer.com/10.1007/s10458-022-09574-6
Y2  - 2024/07/01/22:17:22
L1  - https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p983.pdf
ER  - 

TY  - GEN
TI  - Commitment games with conditional information disclosure
AU  - DiGiovanni, Anthony
AU  - Clifton, Jesse
AB  - The conditional commitment abilities of mutually transparent computer agents have been studied in previous work on commitment games and program equilibrium. This literature has shown how these abilities can help resolve Prisoner’s Dilemmas and other failures of cooperation in complete information settings. But inefﬁciencies due to private information have been neglected thus far in this literature, despite the fact that these problems are pervasive and might also be addressed by greater mutual transparency. In this work, we introduce a framework for commitment games with a new kind of conditional commitment device, which agents can use to conditionally disclose private information. We prove a folk theorem for this setting that provides sufﬁcient conditions for ex post efﬁciency, and thus represents a model of ideal cooperation between agents without a third-party mediator. Further, extending previous work on program equilibrium, we develop an implementation of conditional information disclosure. We show that this implementation forms program Bayesian Nash equilibria corresponding to the Bayesian Nash equilibria of these commitment games.
DA  - 2022/12/03/
PY  - 2022
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2204.03484
Y2  - 2024/07/01/22:18:25
L1  - https://arxiv.org/pdf/2204.03484
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Normative Disagreement as a Challenge for Cooperative AI
AU  - Stastny, Julian
AU  - Riché, Maxime
AU  - Lyzhov, Alexander
AU  - Treutlein, Johannes
AU  - Dafoe, Allan
AU  - Clifton, Jesse
AB  - Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.
DA  - 2021/11/27/
PY  - 2021
DO  - 10.48550/arXiv.2111.13872
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2111.13872
Y2  - 2024/07/01/22:18:55
L1  - https://arxiv.org/pdf/2111.13872.pdf
L2  - https://arxiv.org/abs/2111.13872
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - JOUR
TI  - Robust program equilibrium
AU  - Oesterheld, Caspar
T2  - Theory and Decision
AB  - One approach to achieving cooperation in the one-shot prisoner’s dilemma is Tennenholtz’s (Games Econ Behav 49(2):363–373, 2004) program equilibrium, in which the players of a game submit programs instead of strategies. These programs are then allowed to read each other’s source code to decide which action to take. As shown by Tennenholtz, cooperation is played in an equilibrium of this alternative game. In particular, he proposes that the two players submit the same version of the following program: cooperate if the opponent is an exact copy of this program and defect otherwise. Neither of the two players can benefit from submitting a different program. Unfortunately, this equilibrium is fragile and unlikely to be realized in practice. We thus propose a new, simple program to achieve more robust cooperative program equilibria: cooperate with some small probability $$\epsilon $$and otherwise act as the opponent acts against this program. I argue that this program is similar to the tit for tat strategy for the iterated prisoner’s dilemma. Both “start” by cooperating and copy their opponent’s behavior from “the last round”. We then generalize this approach of turning strategies for the repeated version of a game into programs for the one-shot version of a game to other two-player games. We prove that the resulting programs inherit properties of the underlying strategy. This enables them to robustly and effectively elicit the same responses as the underlying strategy for the repeated game.
DA  - 2019/02/01/
PY  - 2019
DO  - 10.1007/s11238-018-9679-3
DP  - Springer Link
VL  - 86
IS  - 1
SP  - 143
EP  - 159
J2  - Theory Decis
LA  - en
SN  - 1573-7187
UR  - https://doi.org/10.1007/s11238-018-9679-3
Y2  - 2024/07/01/22:19:50
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11238-018-9679-3.pdf
KW  - Nash equilibrium
KW  - Algorithmic game theory
KW  - Program equilibrium
KW  - Repeated games
ER  - 

TY  - GEN
TI  - Evolutionary Stability of Other-Regarding Preferences Under Complexity Costs
AU  - DiGiovanni, Anthony
AU  - Macé, Nicolas
AU  - Clifton, Jesse
AB  - The evolution of preferences that account for other agents’ ﬁtness, or other-regarding preferences, has been modeled with the “indirect approach” to evolutionary game theory. Under the indirect evolutionary approach, agents make decisions by optimizing a subjective utility function. Evolution may select for subjective preferences that differ from the ﬁtness function, and in particular, subjective preferences for increasing or reducing other agents’ ﬁtness. However, indirect evolutionary models typically artiﬁcially restrict the space of strategies that agents might use (assuming that agents always play a Nash equilibrium under their subjective preferences), and dropping this restriction can undermine the ﬁnding that other-regarding preferences are selected for. Can the indirect evolutionary approach still be used to explain the apparent existence of other-regarding preferences, like altruism, in humans? We argue that it can, by accounting for the costs associated with the complexity of strategies, giving (to our knowledge) the ﬁrst account of the relationship between strategy complexity and the evolution of preferences. Our model formalizes the intuition that agents face tradeoffs between the cognitive costs of strategies and how well they interpolate across contexts. For a single game, these complexity costs lead to selection for a simple ﬁxed-action strategy, but across games, when there is a sufﬁciently large cost to a strategy’s number of context-speciﬁc parameters, a strategy of maximizing subjective (other-regarding) utility is stable again. Overall, our analysis provides a more nuanced picture of when other-regarding preferences will evolve.
DA  - 2023/01/18/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2207.03178
Y2  - 2024/07/01/22:20:16
L1  - https://arxiv.org/pdf/2207.03178
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Towards cooperation in learning games
AU  - Clifton, Jesse
AU  - Riché, Maxime
AB  - Suppose that several actors are going to deploy learning agents to act on their behalf. What principles should guide these actors in designing their agents, given that they may have competing goals? An appealing solution concept in this setting is welfare-optimal learning equilibrium. This means that the learning agents should constitute a Nash equilibrium whose payoﬀ proﬁle is optimal according to some measure of total welfare (welfare function). In this work, we construct a class of learning algorithms in this spirit called learning tit-for-tat (L-TFT). L-TFT algorithms maximize a welfare function according to a speciﬁed optimization schedule, and punish their counterpart when they detect that they are deviating from this plan. Because the policies of other agents are not in general fully observed, agents must infer whether their counterpart is following a cooperative learning algorithm. This requires us to develop new techniques for making inferences about counterpart learning algorithms. In two sequential social dilemmas, our L-TFT algorithms successfully cooperate in self-play while eﬀectively avoiding exploitation by and punishing defecting learning algorithms.
CY  - Unpublished
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://longtermrisk.org/files/toward_cooperation_learning_games_oct_2020.pdf
ER  - 

TY  - GEN
TI  - Computing Optimal Commitments to Strategies and Outcome-Conditional Utility Transfers
AU  - Sauerberg, Nathaniel
AU  - Oesterheld, Caspar
AB  - Prior work has studied the computational complexity of computing optimal strategies to commit to in Stackelberg or leadership games, where a leader commits to a strategy which is observed by one or more followers. We extend this setting to one where the leader can additionally commit to outcome-conditional utility transfers. We characterize the computational complexity of finding optimal strategies in normal-form and Bayesian games, giving a mix of efficient algorithms and NP-hardness results. Finally, we allow the leader to also commit to a signaling scheme which induces a correlated equilibrium. In this setting, optimal commitments can be found in polynomial time for arbitrarily many players.
DA  - 2024/03/10/
PY  - 2024
DO  - 10.48550/arXiv.2402.06626
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.06626
Y2  - 2024/07/01/22:26:34
L1  - https://arxiv.org/pdf/2402.06626.pdf
L2  - https://arxiv.org/abs/2402.06626
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Modeling evidential cooperation in large worlds
AU  - Treutlein, Johannes
AB  - Evidential cooperation in large worlds (ECL) refers to the idea that humans and other agents can benefit by cooperating with similar agents with differing values in causally disconnected parts of a large universe. Cooperating provides agents with evidence that other similar agents are likely to cooperate too, resulting in gains from trade for all. This could be a crucial consideration for altruists.
DA  - 2023/08/08/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2307.04879
Y2  - 2024/07/01/22:26:58
L1  - https://arxiv.org/pdf/2307.04879
KW  - Economics - General Economics
ER  - 

TY  - CONF
TI  - Reinforcement Learning in Newcomblike Environments
AU  - Bell, James
AU  - Oesterheld, Caspar
AU  - Linsefors, Linda
AU  - Skalse, Joar
T2  - NeurIPS 2021
AB  - Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not ratiﬁable, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents – for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratiﬁable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy.
DA  - 2021///
PY  - 2021
DP  - Zotero
LA  - en
L1  - https://proceedings.neurips.cc/paper/2021/file/b9ed18a301c9f3d183938c451fa183df-Paper.pdf
ER  - 

TY  - JOUR
TI  - Approval-directed agency and the decision theory of Newcomb-like problems
AU  - Oesterheld, Caspar
T2  - Synthese
AB  - Decision theorists disagree about how instrumentally rational agents, i.e., agents trying to achieve some goal, should behave in so-called Newcomb-like problems, with the main contenders being causal and evidential decision theory. Since the main goal of artificial intelligence research is to create machines that make instrumentally rational decisions, the disagreement pertains to this field. In addition to the more philosophical question of what the right decision theory is, the goal of AI poses the question of how to implement any given decision theory in an AI. For example, how would one go about building an AI whose behavior matches evidential decision theory’s recommendations? Conversely, we can ask which decision theories (if any) describe the behavior of any existing AI design. In this paper, we study what decision theory an approval-directed agent, i.e., an agent whose goal it is to maximize the score it receives from an overseer, implements. If we assume that the overseer rewards the agent based on the expected value of some von Neumann–Morgenstern utility function, then such an approval-directed agent is guided by two decision theories: the one used by the agent to decide which action to choose in order to maximize the reward and the one used by the overseer to compute the expected utility of a chosen action. We show which of these two decision theories describes the agent’s behavior in which situations.
DA  - 2021/11/01/
PY  - 2021
DO  - 10.1007/s11229-019-02148-2
DP  - Springer Link
VL  - 198
IS  - 27
SP  - 6491
EP  - 6504
J2  - Synthese
LA  - en
SN  - 1573-0964
UR  - https://doi.org/10.1007/s11229-019-02148-2
Y2  - 2024/07/01/22:29:43
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11229-019-02148-2.pdf
KW  - Reinforcement learning
KW  - AI safety
KW  - Causal decision theory
KW  - Evidential decision theory
KW  - Newcomb’s problem
KW  - Philosophical foundations of AI
ER  - 

TY  - GEN
TI  - Multiverse-wide Cooperation via Correlated Decision Making
AU  - Oesterheld, Caspar
AB  - Some decision theorists argue that when playing a prisoner’s dilemma-type game against a suﬃciently similar opponent, we should cooperate to make it more likely that our opponent also cooperates. This idea, which Hofstadter calls superrationality, has strong implications when combined with the insight from modern physics that we probably live in a large universe or multiverse of some sort. If we care about what happens in civilizations located elsewhere in the multiverse, we can superrationally cooperate with some of their inhabitants. That is, if we take their values into account, this makes it more likely that they do the same for us. In this paper, I attempt to assess the practical implications of this idea. I argue that to reap the full gains from trade, everyone should maximize the same impartially weighted sum of the utility functions of all collaborators. I also argue that we can obtain at least weak evidence about the content of these utility functions. In practice, the application of superrationality implies that we should promote causal cooperation, moral pluralism, moral reﬂection, and ensure that our descendants, who will be smarter and thus better at ﬁnding out how to beneﬁt other superrationalists in the universe, engage in superrational cooperation.
DA  - 2017///
PY  - 2017
DP  - Zotero
LA  - en
L1  - https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf
ER  - 

TY  - GEN
TI  - Reducing long-term risks from malevolent actors
AU  - Tomasik, Brian
AB  - Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history. Malevolent individuals in positions of power could negatively affect humanity’s long-term trajectory by, for example, exacerbating international conflict or other broad risk factors. Malevolent humans with access to advanced technology such as whole brain emulation or other forms of transformative AI could cause serious existential risks and suffering risks. We therefore consider interventions to reduce the expected influence of malevolent humans on the long-term future. o The development of manipulation-proof measures of malevolence seems valuable, since they could be used to screen for malevolent humans in high-impact settings, such as heads of government or CEOs. We also explore possible future technologies that may offer unprecedented leverage to mitigate against malevolent traits. Selecting against psychopathic and sadistic tendencies in genetically enhanced, highly intelligent humans might be particularly important. However, risks of unintended negative consequences must be handled with extreme caution. We argue that further work on reducing malevolence would be valuable from many moral perspectives and constitutes a promising focus area for longtermist EAs.
CY  - Unpublished
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://longtermrisk.org/files/Reducing_long_term_risks_from_malevolent_actors.pdf
ER  - 

TY  - JOUR
TI  - A Virtue of Precaution Regarding the Moral Status of Animals with Uncertain Sentience
AU  - Knutsson, Simon
AU  - Munthe, Christian
T2  - Journal of Agricultural and Environmental Ethics
AB  - We address the moral importance of fish, invertebrates such as crustaceans, snails and insects, and other animals about which there is qualified scientific uncertainty about their sentience. We argue that, on a sentientist basis, one can at least say that how such animals fare make ethically significant claims on our character. It is a requirement of a morally decent (or virtuous) person that she at least pays attention to and is cautious regarding the possibly morally relevant aspects of such animals. This involves having a moral stance, in the sense of patterns of perception, such that one notices such animals as being morally relevant in various situations. For the person who does not already consider these animals in this way, this could be a big change in moral psychology, and can be assumed to have behavioural consequences, albeit indeterminate. Character has been largely neglected in the literature, which focuses on act-centred approaches (i.e. that the evidence on sentience supports, or does not support, taking some specific action). We see our character-centred approach as complementary to, not superior to, act-centred approaches. Our approach has the advantage of allowing us to make ethically interesting and practically relevant claims about a wider range of cases, but it has the drawback of providing less specific action guidance.
DA  - 2017/04/01/
PY  - 2017
DO  - 10.1007/s10806-017-9662-y
DP  - Springer Link
VL  - 30
IS  - 2
SP  - 213
EP  - 224
J2  - J Agric Environ Ethics
LA  - en
SN  - 1573-322X
UR  - https://doi.org/10.1007/s10806-017-9662-y
Y2  - 2024/07/01/22:37:54
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10806-017-9662-y.pdf
KW  - Ethics
KW  - Pain
KW  - Fish
KW  - Insects
KW  - Invertebrates
KW  - Sentience
ER  - 

TY  - JOUR
TI  - Long-term trajectories of human civilization
AU  - Baum, Seth D.
AU  - Armstrong, Stuart
AU  - Ekenstedt, Timoteus
AU  - Häggström, Olle
AU  - Hanson, Robin
AU  - Kuhlemann, Karin
AU  - Maas, Matthijs M.
AU  - Miller, James D.
AU  - Salmela, Markus
AU  - Sandberg, Anders
AU  - Sotala, Kaj
AU  - Torres, Phil
AU  - Turchin, Alexey
AU  - Yampolskiy, Roman V.
T2  - foresight
AB  - Purpose This paper aims to formalize long-term trajectories of human civilization as a scientific and ethical field of study. The long-term trajectory of human civilization can be defined as the path that human civilization takes during the entire future time period in which human civilization could continue to exist. Design/methodology/approach This paper focuses on four types of trajectories: status quo trajectories, in which human civilization persists in a state broadly similar to its current state into the distant future; catastrophe trajectories, in which one or more events cause significant harm to human civilization; technological transformation trajectories, in which radical technological breakthroughs put human civilization on a fundamentally different course; and astronomical trajectories, in which human civilization expands beyond its home planet and into the accessible portions of the cosmos. Findings Status quo trajectories appear unlikely to persist into the distant future, especially in light of long-term astronomical processes. Several catastrophe, technological transformation and astronomical trajectories appear possible. Originality/value Some current actions may be able to affect the long-term trajectory. Whether these actions should be pursued depends on a mix of empirical and ethical factors. For some ethical frameworks, these actions may be especially important to pursue.
DA  - 2019/01/01/
PY  - 2019
DO  - 10.1108/FS-04-2018-0037
DP  - Emerald Insight
VL  - 21
IS  - 1
SP  - 53
EP  - 83
SN  - 1463-6689
UR  - https://doi.org/10.1108/FS-04-2018-0037
Y2  - 2024/07/01/22:40:22
L1  - https://www.emerald.com/insight/content/doi/10.1108/FS-04-2018-0037/full/pdf?title=long-term-trajectories-of-human-civilization
KW  - Human civilization
KW  - Long-term trajectories
ER  - 

TY  - JOUR
TI  - How feasible is the rapid development of artificial superintelligence?
AU  - Sotala, Kaj
T2  - Physica Scripta
AB  - What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.
DA  - 2017/10//
PY  - 2017
DO  - 10.1088/1402-4896/aa90e8
DP  - Institute of Physics
VL  - 92
IS  - 11
SP  - 113001
J2  - Phys. Scr.
LA  - en
SN  - 1402-4896
UR  - https://dx.doi.org/10.1088/1402-4896/aa90e8
Y2  - 2024/07/01/22:40:34
L1  - https://iopscience.iop.org/article/10.1088/1402-4896/aa90e8/pdf
ER  - 

TY  - JOUR
TI  - Superintelligence As a Cause or Cure For Risks of Astronomical Suffering
AU  - Sotala, Kaj
AU  - Gloor, Lukas
T2  - Informatica
AB  - Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.
DA  - 2017/12/27/
PY  - 2017
DP  - www.informatica.si
VL  - 41
IS  - 4
LA  - en
SN  - 1854-3871
UR  - https://www.informatica.si/index.php/informatica/article/view/1877
Y2  - 2024/07/01/22:40:50
L1  - https://www.informatica.si/index.php/informatica/article/download/1877/1098
ER  - 

TY  - GEN
TI  - Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models
AU  - Denison, Carson
AU  - MacDiarmid, Monte
AU  - Barez, Fazl
AU  - Duvenaud, David
AU  - Kravec, Shauna
AU  - Marks, Samuel
AU  - Schiefer, Nicholas
AU  - Soklaski, Ryan
AU  - Tamkin, Alex
AU  - Kaplan, Jared
AU  - Shlegeris, Buck
AU  - Bowman, Samuel R.
AU  - Perez, Ethan
AU  - Hubinger, Evan
AB  - In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.
DA  - 2024/06/17/
PY  - 2024
DO  - 10.48550/arXiv.2406.10162
DP  - arXiv.org
PB  - arXiv
ST  - Sycophancy to Subterfuge
UR  - http://arxiv.org/abs/2406.10162
Y2  - 2024/07/01/22:43:03
L1  - https://arxiv.org/pdf/2406.10162.pdf
L2  - https://arxiv.org/abs/2406.10162
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet
AU  - Templeton, Adly
AU  - Conerly, Tom
AU  - Marcus, Jonathan
AU  - Lindsey, Jack
AU  - Bricken, Trenton
AU  - Chen, Brian
AU  - Pearce, Adam
AU  - Citro, Craig
AU  - Ameisen, Emmanuel
AU  - Jones, Andy
AU  - Cunningham, Hoagy
AU  - Turner, Nicholas L
AU  - McDougall, Callum
AU  - MacDiarmid, Monte
AU  - Freeman, C. Daniel
AU  - Sumers, Theodore R.
AU  - Rees, Edward
AU  - Batson, Joshua
AU  - Jermyn, Adam
AU  - Carter, Shan
AU  - Olah, Chris
AU  - Henighan, Tom
AB  - Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders. The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references. There appears to be a systematic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. Features can be used to steer large models (see e.g. Influence on Behavior). This extends prior work on steering models using other methods (see Related Work). We observe features related to a broad range of safety concerns, including deception, sycophancy, bias, and dangerous content.
CY  - Transformer Circuits Thread
DA  - 2024///
PY  - 2024
UR  - https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html
ER  - 

TY  - GEN
TI  - Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training
AU  - Hubinger, Evan
AU  - Denison, Carson
AU  - Mu, Jesse
AU  - Lambert, Mike
AU  - Tong, Meg
AU  - MacDiarmid, Monte
AU  - Lanham, Tamera
AU  - Ziegler, Daniel M.
AU  - Maxwell, Tim
AU  - Cheng, Newton
AU  - Jermyn, Adam
AU  - Askell, Amanda
AU  - Radhakrishnan, Ansh
AU  - Anil, Cem
AU  - Duvenaud, David
AU  - Ganguli, Deep
AU  - Barez, Fazl
AU  - Clark, Jack
AU  - Ndousse, Kamal
AU  - Sachan, Kshitij
AU  - Sellitto, Michael
AU  - Sharma, Mrinank
AU  - DasSarma, Nova
AU  - Grosse, Roger
AU  - Kravec, Shauna
AU  - Bai, Yuntao
AU  - Witten, Zachary
AU  - Favaro, Marina
AU  - Brauner, Jan
AU  - Karnofsky, Holden
AU  - Christiano, Paul
AU  - Bowman, Samuel R.
AU  - Graham, Logan
AU  - Kaplan, Jared
AU  - Mindermann, Sören
AU  - Greenblatt, Ryan
AU  - Shlegeris, Buck
AU  - Schiefer, Nicholas
AU  - Perez, Ethan
AB  - Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.
DA  - 2024/01/17/
PY  - 2024
DO  - 10.48550/arXiv.2401.05566
DP  - arXiv.org
PB  - arXiv
ST  - Sleeper Agents
UR  - http://arxiv.org/abs/2401.05566
Y2  - 2024/07/01/22:47:47
L1  - https://arxiv.org/pdf/2401.05566.pdf
L2  - https://arxiv.org/abs/2401.05566
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Risks from Learned Optimization in Advanced Machine Learning Systems
AU  - Hubinger, Evan
AU  - van Merwijk, Chris
AU  - Mikulik, Vladimir
AU  - Skalse, Joar
AU  - Garrabrant, Scott
AB  - We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer—a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be—how will it diﬀer from the loss function it was trained under—and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.
DA  - 2021/12/01/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1906.01820
Y2  - 2024/07/01/22:49:13
L1  - https://arxiv.org/pdf/1906.01820
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Many-shot Jailbreaking
AU  - Anil, Cem
AU  - Durmus, Esin
AU  - Sharma, Mrinank
AU  - Benton, Joe
AU  - Kundu, Sandipan
AU  - Batson, Joshua
AU  - Rimsky, Nina
AU  - Tong, Meg
AU  - Mu, Jesse
AU  - Ford, Daniel
AU  - Mosconi, Francesco
AU  - Agrawal, Rajashree
AU  - Schaeffer, Rylan
AU  - Bashkansky, Naomi
AU  - Svenningsen, Samuel
AU  - Lambert, Mike
AU  - Radhakrishnan, Ansh
AU  - Denison, Carson
AU  - Hubinger, Evan J
AU  - Bai, Yuntao
AU  - Bricken, Trenton
AU  - Maxwell, Timothy
AU  - Schiefer, Nicholas
AU  - Sully, Jamie
AU  - Tamkin, Alex
AU  - Lanham, Tamera
AU  - Nguyen, Karina
AU  - Korbak, Tomasz
AU  - Kaplan, Jared
AU  - Ganguli, Deep
AU  - Bowman, Samuel R
AU  - Perez, Ethan
AU  - Grosse, Roger
AU  - Duvenaud, David
AB  - We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closedweight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs.
CY  - Unpublished
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf
ER  - 

TY  - GEN
TI  - Evaluating and Mitigating Discrimination in Language Model Decisions
AU  - Tamkin, Alex
AU  - Askell, Amanda
AU  - Lovitt, Liane
AU  - Durmus, Esin
AU  - Joseph, Nicholas
AU  - Kravec, Shauna
AU  - Nguyen, Karina
AU  - Kaplan, Jared
AU  - Ganguli, Deep
AB  - As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval
DA  - 2023/12/06/
PY  - 2023
DO  - 10.48550/arXiv.2312.03689
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.03689
Y2  - 2024/07/01/22:52:22
L1  - https://arxiv.org/pdf/2312.03689.pdf
L2  - https://arxiv.org/abs/2312.03689
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Specific versus General Principles for Constitutional AI
AU  - Kundu, Sandipan
AU  - Bai, Yuntao
AU  - Kadavath, Saurav
AU  - Askell, Amanda
AU  - Callahan, Andrew
AU  - Chen, Anna
AU  - Goldie, Anna
AU  - Balwit, Avital
AU  - Mirhoseini, Azalia
AU  - McLean, Brayden
AU  - Olsson, Catherine
AU  - Evraets, Cassie
AU  - Tran-Johnson, Eli
AU  - Durmus, Esin
AU  - Perez, Ethan
AU  - Kernion, Jackson
AU  - Kerr, Jamie
AU  - Ndousse, Kamal
AU  - Nguyen, Karina
AU  - Elhage, Nelson
AU  - Cheng, Newton
AU  - Schiefer, Nicholas
AU  - DasSarma, Nova
AU  - Rausch, Oliver
AU  - Larson, Robin
AU  - Yang, Shannon
AU  - Kravec, Shauna
AU  - Telleen-Lawton, Timothy
AU  - Liao, Thomas I.
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Hatfield-Dodds, Zac
AU  - Mindermann, Sören
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Kaplan, Jared
AB  - Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as "do what's best for humanity". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.
DA  - 2023/10/20/
PY  - 2023
DO  - 10.48550/arXiv.2310.13798
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.13798
Y2  - 2024/07/01/22:52:36
L1  - https://arxiv.org/pdf/2310.13798.pdf
L2  - https://arxiv.org/abs/2310.13798
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Towards Understanding Sycophancy in Language Models
AU  - Sharma, Mrinank
AU  - Tong, Meg
AU  - Korbak, Tomasz
AU  - Duvenaud, David
AU  - Askell, Amanda
AU  - Bowman, Samuel R.
AU  - Cheng, Newton
AU  - Durmus, Esin
AU  - Hatfield-Dodds, Zac
AU  - Johnston, Scott R.
AU  - Kravec, Shauna
AU  - Maxwell, Timothy
AU  - McCandlish, Sam
AU  - Ndousse, Kamal
AU  - Rausch, Oliver
AU  - Schiefer, Nicholas
AU  - Yan, Da
AU  - Zhang, Miranda
AU  - Perez, Ethan
AB  - Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.
DA  - 2023/10/27/
PY  - 2023
DO  - 10.48550/arXiv.2310.13548
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.13548
Y2  - 2024/07/01/22:52:46
L1  - https://arxiv.org/pdf/2310.13548.pdf
L2  - https://arxiv.org/abs/2310.13548
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
KW  - I.2.6
ER  - 

TY  - GEN
TI  - Constitutional AI: Harmlessness from AI Feedback
AU  - Bai, Yuntao
AU  - Kadavath, Saurav
AU  - Kundu, Sandipan
AU  - Askell, Amanda
AU  - Kernion, Jackson
AU  - Jones, Andy
AU  - Chen, Anna
AU  - Goldie, Anna
AU  - Mirhoseini, Azalia
AU  - McKinnon, Cameron
AU  - Chen, Carol
AU  - Olsson, Catherine
AU  - Olah, Christopher
AU  - Hernandez, Danny
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Li, Dustin
AU  - Tran-Johnson, Eli
AU  - Perez, Ethan
AU  - Kerr, Jamie
AU  - Mueller, Jared
AU  - Ladish, Jeffrey
AU  - Landau, Joshua
AU  - Ndousse, Kamal
AU  - Lukosuite, Kamile
AU  - Lovitt, Liane
AU  - Sellitto, Michael
AU  - Elhage, Nelson
AU  - Schiefer, Nicholas
AU  - Mercado, Noemi
AU  - DasSarma, Nova
AU  - Lasenby, Robert
AU  - Larson, Robin
AU  - Ringer, Sam
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Showk, Sheer El
AU  - Fort, Stanislav
AU  - Lanham, Tamera
AU  - Telleen-Lawton, Timothy
AU  - Conerly, Tom
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Bowman, Samuel R.
AU  - Hatfield-Dodds, Zac
AU  - Mann, Ben
AU  - Amodei, Dario
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Brown, Tom
AU  - Kaplan, Jared
AB  - As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.
DA  - 2022/12/15/
PY  - 2022
DO  - 10.48550/arXiv.2212.08073
DP  - arXiv.org
PB  - arXiv
ST  - Constitutional AI
UR  - http://arxiv.org/abs/2212.08073
Y2  - 2024/07/02/10:27:35
L1  - https://arxiv.org/pdf/2212.08073.pdf
L2  - https://arxiv.org/abs/2212.08073
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Towards monosemanticity: Decomposing language models with dictionary learning
AU  - Bricken, Trenton
AU  - Templeton, Adly
AU  - Batson, Joshua
AU  - Chen, Brian
AU  - Jermyn, Adam
AU  - Conerly, Tom
AU  - Turner, Nick
AU  - Anil, Cem
AU  - Denison, Carson
AU  - Askell, Amanda
AU  - Lasenby, Robert
AU  - Wu, Yifan
AU  - Kravec, Shauna
AU  - Schiefer, Nicholas
AU  - Maxwell, Tim
AU  - Joseph, Nicholas
AU  - Hatfield-Dodds, Zac
AU  - Tamkin, Alex
AU  - Nguyen, Karina
AU  - McLean, Brayden
AU  - Burke, Josiah E
AU  - Hume, Tristan
AU  - Carter, Shan
AU  - Henighan, Tom
AU  - Olah, Christopher
AB  - Sparse Autoencoders extract relatively monosemantic features. We provide four different lines of evidence: detailed investigations for a few features firing in specific contexts for which we can construct computational proxies, human analysis for a large random sample of features, automated interpretability analysis of activations for all the features learned by the autoencoder, and finally automated interpretability analysis of logit weights for all the features. Moreover, the last three analyses show that most learned features are interpretable. While we do not claim that our interpretations catch all aspects of features' behaviors, by constructing metrics of interpretability consistently for features and neurons, we quantitatively show their relative interpretability. Sparse autoencoders produce interpretable features that are effectively invisible in the neuron basis. We find features (e.g., one firing on Hebrew script) which are not active in any of the top dataset examples for any of the neurons. Sparse autoencoder features can be used to intervene on and steer transformer generation. For example, activating the base64 feature we study causes the model to generate base64 text, and activating the Arabic script feature we study produces Arabic text. Sparse autoencoders produce relatively universal features. Sparse autoencoders applied to different transformer language models produce mostly similar features, more similar to one another than they are to their own model's neurons. Features appear to "split" as we increase autoencoder size. When we gradually increase the width of the autoencoder from 512 (the number of neurons) to over 131,000 (256×), we find features which naturally fit together into families. For example, one base64 feature in a small dictionary splits into three, with more subtle and yet still interpretable roles, in a larger dictionary. The different size autoencoders offer different "resolutions" for understanding the same object. Just 512 neurons can represent tens of thousands of features. Despite the MLP layer being very small, we continue to find new features as we scale the sparse autoencoder. Features connect in "finite-state automata"-like systems that implement complex behaviors. For example, we find features that work together to generate valid HTML.
DA  - 2023///
PY  - 2023
PB  - Transformer Circuits Thread
ER  - 

TY  - GEN
TI  - Studying Large Language Model Generalization with Influence Functions
AU  - Grosse, Roger
AU  - Bae, Juhan
AU  - Anil, Cem
AU  - Elhage, Nelson
AU  - Tamkin, Alex
AU  - Tajdini, Amirhossein
AU  - Steiner, Benoit
AU  - Li, Dustin
AU  - Durmus, Esin
AU  - Perez, Ethan
AU  - Hubinger, Evan
AU  - Lukošiūtė, Kamilė
AU  - Nguyen, Karina
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Kaplan, Jared
AU  - Bowman, Samuel R.
AB  - When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
DA  - 2023/08/07/
PY  - 2023
DO  - 10.48550/arXiv.2308.03296
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2308.03296
Y2  - 2024/07/02/10:32:16
L1  - https://arxiv.org/pdf/2308.03296.pdf
L2  - https://arxiv.org/abs/2308.03296
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Measuring Faithfulness in Chain-of-Thought Reasoning
AU  - Lanham, Tamera
AU  - Chen, Anna
AU  - Radhakrishnan, Ansh
AU  - Steiner, Benoit
AU  - Denison, Carson
AU  - Hernandez, Danny
AU  - Li, Dustin
AU  - Durmus, Esin
AU  - Hubinger, Evan
AU  - Kernion, Jackson
AU  - Lukosiute, Kamile
AU  - Nguyen, Karina
AU  - Cheng, Newton
AU  - Joseph, Nicholas
AU  - Schiefer, Nicholas
AU  - Rausch, Oliver
AU  - Larson, Robin
AU  - McCandlish, Sam
AU  - Kundu, Sandipan
AU  - Kadavath, Saurav
AU  - Yang, Shannon
AU  - Henighan, Thomas
AU  - Maxwell, Timothy
AU  - Telleen-Lawton, Timothy
AU  - Hume, Tristan
AU  - Hatfield-Dodds, Zac
AU  - Kaplan, Jared
AU  - Brauner, Jan
AU  - Bowman, Samuel R
AU  - Perez, Ethan
AB  - Large language models (LLMs) perform better when they produce step-by-step, “Chain-ofThought” (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model’s actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT’s performance boost does not seem to come from CoT’s added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.
DA  - 2023/07//
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://cdn.sanity.io/files/4zrzovbb/website/827afa7dd36e4afbb1a49c735bfbb2c69749756e.pdf
ER  - 

TY  - GEN
TI  - Question Decomposition Improves the Faithfulness of Model-Generated Reasoning
AU  - Radhakrishnan, Ansh
AU  - Nguyen, Karina
AU  - Chen, Anna
AU  - Chen, Carol
AU  - Denison, Carson
AU  - Hernandez, Danny
AU  - Durmus, Esin
AU  - Hubinger, Evan
AU  - Kernion, Jackson
AU  - Lukošiūtė, Kamilė
AU  - Cheng, Newton
AU  - Joseph, Nicholas
AU  - Schiefer, Nicholas
AU  - Rausch, Oliver
AU  - McCandlish, Sam
AU  - Showk, Sheer El
AU  - Lanham, Tamera
AU  - Maxwell, Tim
AU  - Chandrasekaran, Venkatesa
AU  - Hatfield-Dodds, Zac
AU  - Kaplan, Jared
AU  - Brauner, Jan
AU  - Bowman, Samuel R
AU  - Perez, Ethan
AB  - As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model’s actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model’s stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
DA  - 2023///ly
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://cdn.sanity.io/files/4zrzovbb/website/8154fb1d828cdc390dc1fa442d84034948679c47.pdf
ER  - 

TY  - GEN
TI  - Towards Measuring the Representation of Subjective Global Opinions in Language Models
AU  - Durmus, Esin
AU  - Nguyen, Karina
AU  - Liao, Thomas I.
AU  - Schiefer, Nicholas
AU  - Askell, Amanda
AU  - Bakhtin, Anton
AU  - Chen, Carol
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Joseph, Nicholas
AU  - Lovitt, Liane
AU  - McCandlish, Sam
AU  - Sikder, Orowa
AU  - Tamkin, Alex
AU  - Thamkul, Janel
AU  - Kaplan, Jared
AU  - Clark, Jack
AU  - Ganguli, Deep
AB  - Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.
DA  - 2024/04/11/
PY  - 2024
DO  - 10.48550/arXiv.2306.16388
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.16388
Y2  - 2024/07/02/10:35:22
L1  - https://arxiv.org/pdf/2306.16388.pdf
L2  - https://arxiv.org/abs/2306.16388
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - The Capacity for Moral Self-Correction in Large Language Models
AU  - Ganguli, Deep
AU  - Askell, Amanda
AU  - Schiefer, Nicholas
AU  - Liao, Thomas I.
AU  - Lukošiūtė, Kamilė
AU  - Chen, Anna
AU  - Goldie, Anna
AU  - Mirhoseini, Azalia
AU  - Olsson, Catherine
AU  - Hernandez, Danny
AU  - Drain, Dawn
AU  - Li, Dustin
AU  - Tran-Johnson, Eli
AU  - Perez, Ethan
AU  - Kernion, Jackson
AU  - Kerr, Jamie
AU  - Mueller, Jared
AU  - Landau, Joshua
AU  - Ndousse, Kamal
AU  - Nguyen, Karina
AU  - Lovitt, Liane
AU  - Sellitto, Michael
AU  - Elhage, Nelson
AU  - Mercado, Noemi
AU  - DasSarma, Nova
AU  - Rausch, Oliver
AU  - Lasenby, Robert
AU  - Larson, Robin
AU  - Ringer, Sam
AU  - Kundu, Sandipan
AU  - Kadavath, Saurav
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Showk, Sheer El
AU  - Lanham, Tamera
AU  - Telleen-Lawton, Timothy
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Bai, Yuntao
AU  - Hatfield-Dodds, Zac
AU  - Mann, Ben
AU  - Amodei, Dario
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Brown, Tom
AU  - Olah, Christopher
AU  - Clark, Jack
AU  - Bowman, Samuel R.
AU  - Kaplan, Jared
AB  - We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.
DA  - 2023/02/18/
PY  - 2023
DO  - 10.48550/arXiv.2302.07459
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.07459
Y2  - 2024/07/02/10:37:01
L1  - https://arxiv.org/pdf/2302.07459.pdf
L2  - https://arxiv.org/abs/2302.07459
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Discovering Language Model Behaviors with Model-Written Evaluations
AU  - Perez, Ethan
AU  - Ringer, Sam
AU  - Lukošiūtė, Kamilė
AU  - Nguyen, Karina
AU  - Chen, Edwin
AU  - Heiner, Scott
AU  - Pettit, Craig
AU  - Olsson, Catherine
AU  - Kundu, Sandipan
AU  - Kadavath, Saurav
AU  - Jones, Andy
AU  - Chen, Anna
AU  - Mann, Ben
AU  - Israel, Brian
AU  - Seethor, Bryan
AU  - McKinnon, Cameron
AU  - Olah, Christopher
AU  - Yan, Da
AU  - Amodei, Daniela
AU  - Amodei, Dario
AU  - Drain, Dawn
AU  - Li, Dustin
AU  - Tran-Johnson, Eli
AU  - Khundadze, Guro
AU  - Kernion, Jackson
AU  - Landis, James
AU  - Kerr, Jamie
AU  - Mueller, Jared
AU  - Hyun, Jeeyoon
AU  - Landau, Joshua
AU  - Ndousse, Kamal
AU  - Goldberg, Landon
AU  - Lovitt, Liane
AU  - Lucas, Martin
AU  - Sellitto, Michael
AU  - Zhang, Miranda
AU  - Kingsland, Neerav
AU  - Elhage, Nelson
AU  - Joseph, Nicholas
AU  - Mercado, Noemí
AU  - DasSarma, Nova
AU  - Rausch, Oliver
AU  - Larson, Robin
AU  - McCandlish, Sam
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Showk, Sheer El
AU  - Lanham, Tamera
AU  - Telleen-Lawton, Timothy
AU  - Brown, Tom
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Bai, Yuntao
AU  - Hatfield-Dodds, Zac
AU  - Clark, Jack
AU  - Bowman, Samuel R.
AU  - Askell, Amanda
AU  - Grosse, Roger
AU  - Hernandez, Danny
AU  - Ganguli, Deep
AU  - Hubinger, Evan
AU  - Schiefer, Nicholas
AU  - Kaplan, Jared
AB  - As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.
DA  - 2022/12/19/
PY  - 2022
DO  - 10.48550/arXiv.2212.09251
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.09251
Y2  - 2024/07/02/10:37:51
L1  - https://arxiv.org/pdf/2212.09251.pdf
L2  - https://arxiv.org/abs/2212.09251
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Measuring Progress on Scalable Oversight for Large Language Models
AU  - Bowman, Samuel R.
AU  - Hyun, Jeeyoon
AU  - Perez, Ethan
AU  - Chen, Edwin
AU  - Pettit, Craig
AU  - Heiner, Scott
AU  - Lukošiūtė, Kamilė
AU  - Askell, Amanda
AU  - Jones, Andy
AU  - Chen, Anna
AU  - Goldie, Anna
AU  - Mirhoseini, Azalia
AU  - McKinnon, Cameron
AU  - Olah, Christopher
AU  - Amodei, Daniela
AU  - Amodei, Dario
AU  - Drain, Dawn
AU  - Li, Dustin
AU  - Tran-Johnson, Eli
AU  - Kernion, Jackson
AU  - Kerr, Jamie
AU  - Mueller, Jared
AU  - Ladish, Jeffrey
AU  - Landau, Joshua
AU  - Ndousse, Kamal
AU  - Lovitt, Liane
AU  - Elhage, Nelson
AU  - Schiefer, Nicholas
AU  - Joseph, Nicholas
AU  - Mercado, Noemí
AU  - DasSarma, Nova
AU  - Larson, Robin
AU  - McCandlish, Sam
AU  - Kundu, Sandipan
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Showk, Sheer El
AU  - Fort, Stanislav
AU  - Telleen-Lawton, Timothy
AU  - Brown, Tom
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Bai, Yuntao
AU  - Hatfield-Dodds, Zac
AU  - Mann, Ben
AU  - Kaplan, Jared
AB  - Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.
DA  - 2022/11/11/
PY  - 2022
DO  - 10.48550/arXiv.2211.03540
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.03540
Y2  - 2024/07/02/10:38:17
L1  - https://arxiv.org/pdf/2211.03540.pdf
L2  - https://arxiv.org/abs/2211.03540
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Toy Models of Superposition
AU  - Elhage, Nelson
AU  - Hume, Tristan
AU  - Olsson, Catherine
AU  - Schiefer, Nicholas
AU  - Henighan, Tom
AU  - Kravec, Shauna
AU  - Hatfield-Dodds, Zac
AU  - Lasenby, Robert
AU  - Drain, Dawn
AU  - Chen, Carol
AU  - Grosse, Roger
AU  - McCandlish, Sam
AU  - Kaplan, Jared
AU  - Amodei, Dario
AU  - Wattenberg, Martin
AU  - Olah, Christopher
AB  - Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.
DA  - 2022/09/21/
PY  - 2022
DO  - 10.48550/arXiv.2209.10652
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.10652
Y2  - 2024/07/02/10:40:57
L1  - https://arxiv.org/pdf/2209.10652.pdf
L2  - https://arxiv.org/abs/2209.10652
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned
AU  - Ganguli, Deep
AU  - Lovitt, Liane
AU  - Kernion, Jackson
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Kadavath, Saurav
AU  - Mann, Ben
AU  - Perez, Ethan
AU  - Schiefer, Nicholas
AU  - Ndousse, Kamal
AU  - Jones, Andy
AU  - Bowman, Sam
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - Elhage, Nelson
AU  - El-Showk, Sheer
AU  - Fort, Stanislav
AU  - Hatfield-Dodds, Zac
AU  - Henighan, Tom
AU  - Hernandez, Danny
AU  - Hume, Tristan
AU  - Jacobson, Josh
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Olsson, Catherine
AU  - Ringer, Sam
AU  - Tran-Johnson, Eli
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Joseph, Nicholas
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Kaplan, Jared
AU  - Clark, Jack
AB  - We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.
DA  - 2022/11/22/
PY  - 2022
DO  - 10.48550/arXiv.2209.07858
DP  - arXiv.org
PB  - arXiv
ST  - Red Teaming Language Models to Reduce Harms
UR  - http://arxiv.org/abs/2209.07858
Y2  - 2024/07/02/10:41:40
L1  - https://arxiv.org/pdf/2209.07858.pdf
L2  - https://arxiv.org/abs/2209.07858
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Language Models (Mostly) Know What They Know
AU  - Kadavath, Saurav
AU  - Conerly, Tom
AU  - Askell, Amanda
AU  - Henighan, Tom
AU  - Drain, Dawn
AU  - Perez, Ethan
AU  - Schiefer, Nicholas
AU  - Hatfield-Dodds, Zac
AU  - DasSarma, Nova
AU  - Tran-Johnson, Eli
AU  - Johnston, Scott
AU  - El-Showk, Sheer
AU  - Jones, Andy
AU  - Elhage, Nelson
AU  - Hume, Tristan
AU  - Chen, Anna
AU  - Bai, Yuntao
AU  - Bowman, Sam
AU  - Fort, Stanislav
AU  - Ganguli, Deep
AU  - Hernandez, Danny
AU  - Jacobson, Josh
AU  - Kernion, Jackson
AU  - Kravec, Shauna
AU  - Lovitt, Liane
AU  - Ndousse, Kamal
AU  - Olsson, Catherine
AU  - Ringer, Sam
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - Joseph, Nicholas
AU  - Mann, Ben
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Kaplan, Jared
AB  - We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.
DA  - 2022/11/21/
PY  - 2022
DO  - 10.48550/arXiv.2207.05221
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.05221
Y2  - 2024/07/02/10:41:52
L1  - https://arxiv.org/pdf/2207.05221.pdf
L2  - https://arxiv.org/abs/2207.05221
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Softmax linear units
AU  - Elhage, Nelson
AU  - Hume, Tristan
AU  - Olsson, Catherine
AU  - Nanda, Neel
AU  - Henighan, Tom
AU  - Johnston, Scott
AU  - ElShowk, Sheer
AU  - Joseph, Nicholas
AU  - DasSarma, Nova
AU  - Mann, Ben
AU  - Hernandez, Danny
AU  - Askell, Amanda
AU  - Ndousse, Kamal
AU  - Jones, Andy
AU  - Drain, Dawn
AU  - Chen, Anna
AU  - Bai, Yuntao
AU  - Ganguli, Deep
AU  - Lovitt, Liane
AU  - Hatfield-Dodds, Zac
AU  - Kernion, Jackson
AU  - Conerly, Tom
AU  - Kravec, Shauna
AU  - Fort, Stanislav
AU  - Kadavath, Saurav
AU  - Jacobson, Josh
AU  - Tran-Johnson, Eli
AU  - Kaplan, Jared
AU  - Clark, Jack
AU  - Brown, Tom
AU  - McCandlish, Sam
AU  - Amodei, Dario
AU  - Olah, Christopher
AB  - In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be "interpretable" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers.  However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by “hiding” others and thus making them even more deeply uninterpretable.  Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand.
CY  - Transformer Circuits Thread
DA  - 2022///
PY  - 2022
UR  - https://transformer-circuits.pub/2022/solu/index.html
ER  - 

TY  - GEN
TI  - Scaling Laws and Interpretability of Learning from Repeated Data
AU  - Hernandez, Danny
AU  - Brown, Tom
AU  - Conerly, Tom
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - El-Showk, Sheer
AU  - Elhage, Nelson
AU  - Hatfield-Dodds, Zac
AU  - Henighan, Tom
AU  - Hume, Tristan
AU  - Johnston, Scott
AU  - Mann, Ben
AU  - Olah, Chris
AU  - Olsson, Catherine
AU  - Amodei, Dario
AU  - Joseph, Nicholas
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AB  - Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.
DA  - 2022/05/20/
PY  - 2022
DO  - 10.48550/arXiv.2205.10487
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.10487
Y2  - 2024/07/02/10:44:22
L1  - https://arxiv.org/pdf/2205.10487.pdf
L2  - https://arxiv.org/abs/2205.10487
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
AU  - Bai, Yuntao
AU  - Jones, Andy
AU  - Ndousse, Kamal
AU  - Askell, Amanda
AU  - Chen, Anna
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - Fort, Stanislav
AU  - Ganguli, Deep
AU  - Henighan, Tom
AU  - Joseph, Nicholas
AU  - Kadavath, Saurav
AU  - Kernion, Jackson
AU  - Conerly, Tom
AU  - El-Showk, Sheer
AU  - Elhage, Nelson
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Hume, Tristan
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Lovitt, Liane
AU  - Nanda, Neel
AU  - Olsson, Catherine
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Mann, Ben
AU  - Kaplan, Jared
AB  - We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.
DA  - 2022/04/12/
PY  - 2022
DO  - 10.48550/arXiv.2204.05862
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.05862
Y2  - 2024/07/02/10:44:34
L1  - https://arxiv.org/pdf/2204.05862.pdf
L2  - https://arxiv.org/abs/2204.05862
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Predictability and Surprise in Large Generative Models
AU  - Ganguli, Deep
AU  - Hernandez, Danny
AU  - Lovitt, Liane
AU  - DasSarma, Nova
AU  - Henighan, Tom
AU  - Jones, Andy
AU  - Joseph, Nicholas
AU  - Kernion, Jackson
AU  - Mann, Ben
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - Drain, Dawn
AU  - Elhage, Nelson
AU  - Showk, Sheer El
AU  - Fort, Stanislav
AU  - Hatfield-Dodds, Zac
AU  - Johnston, Scott
AU  - Kravec, Shauna
AU  - Nanda, Neel
AU  - Ndousse, Kamal
AU  - Olsson, Catherine
AU  - Amodei, Daniela
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Clark, Jack
AB  - Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.
C3  - 2022 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2022/06/21/
PY  - 2022
DO  - 10.1145/3531146.3533229
DP  - arXiv.org
SP  - 1747
EP  - 1764
UR  - http://arxiv.org/abs/2202.07785
Y2  - 2024/07/02/10:45:21
L1  - https://arxiv.org/pdf/2202.07785.pdf
L2  - https://arxiv.org/abs/2202.07785
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - A General Language Assistant as a Laboratory for Alignment
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Henighan, Tom
AU  - Jones, Andy
AU  - Joseph, Nicholas
AU  - Mann, Ben
AU  - DasSarma, Nova
AU  - Elhage, Nelson
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Kernion, Jackson
AU  - Ndousse, Kamal
AU  - Olsson, Catherine
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - McCandlish, Sam
AU  - Olah, Chris
AU  - Kaplan, Jared
AB  - Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.
DA  - 2021/12/09/
PY  - 2021
DO  - 10.48550/arXiv.2112.00861
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2112.00861
Y2  - 2024/07/02/10:45:40
L1  - https://arxiv.org/pdf/2112.00861.pdf
L2  - https://arxiv.org/abs/2112.00861
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - In-context learning and induction heads
AU  - Olsson, Catherine
AU  - Elhage, Nelson
AU  - Nanda, Neel
AU  - Joseph, Nicholas
AU  - DasSarma, Nova
AU  - Henighan, Tom
AU  - Mann, Ben
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Johnston, Scott
AU  - Jones, Andy
AU  - Kernion, Jackson
AU  - Lovitt, Liane
AU  - Ndousse, Kamal
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Chris
AB  - The paper presents six complementary lines of evidence arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size: Macroscopic co-occurence: Transformer language models undergo a “phase change” early in training, during which induction heads form and simultaneously in-context learning improves dramatically. Macroscopic co-perturbation: When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way.
Direct ablation:  When we directly “knock out” induction heads at test-time in small models, the amount of in-context learning greatly decreases.
Specific examples of induction head generality: Although we define induction heads very narrowly in terms of copying literal sequences, we empirically observe that these same heads also appear to implement more sophisticated types of in-context learning, including highly abstract behaviors, making it plausible they explain a large fraction of in-context learning. Mechanistic plausibility of induction head generality: For small models, we can explain mechanistically how induction heads work, and can show they contribute to in-context learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be re-purposed to perform more general in-context learning. Continuity from small to large models: In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones. However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.
DA  - 2022///
PY  - 2022
PB  - Transformer Circuits Thread
UR  - https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#citation
ER  - 

TY  - GEN
TI  - A mathematical framework for transformer circuits
AU  - Elhage, Nelson
AU  - Nanda, Neel
AU  - Olsson, Catherine
AU  - Henighan, Tom
AU  - Joseph, Nicholas
AU  - Mann, Ben
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Jones, Andy
AU  - Kernion, Jackson
AU  - Lovitt, Liane
AU  - Ndousse, Kamal
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Chris
AB  - We’ve found that many subtle details of the transformer architecture require us to approach reverse engineering it in a pretty different way from how the InceptionV1 Circuits work . We’ll unpack each of these points in the sections below, but for now we briefly summarize. We’ll also expand on a lot of the terminology we introduce here once we get to the appropriate sections. Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream. Attention heads are often described in an alternate “concatenate and multiply” formulation for computational efficiency, but this is mathematically equivalent. Attention-only models can be written as a sum of interpretable end-to-end functions mapping tokens to changes in logits. These functions correspond to “paths” through the model, and are linear if one freezes the attention patterns. Transformers have an enormous amount of linear structure. One can learn a lot simply by breaking apart sums and multiplying together chains of matrices. Attention heads can be understood as having two largely independent computations: a QK (“query-key”) circuit which computes the attention pattern, and an OV (“output-value”) circuit which computes how each token affects the output if attended to. Key, query, and value vectors can be thought of as intermediate results in the computation of the low-rank matrices W_Q^TW_K and W_OW_V. It can be useful to describe transformers without reference to them. Composition of attention heads greatly increases the expressivity of transformers. There are three different ways attention heads can compose, corresponding to keys, queries, and values. Key and query composition are very different from value composition. All components of a transformer (the token embedding, attention heads, MLP layers, and unembedding) communicate with each other by reading and writing to different subspaces of the residual stream. Rather than analyze the residual stream vectors, it can be helpful to decompose the residual stream into all these different communication channels, corresponding to paths through the model.
CY  - Unpublished
DA  - 2021///
PY  - 2021
PB  - Transformer Circuits Thread
UR  - https://transformer-circuits.pub/2021/framework/index.html#citation
ER  - 

TY  - GEN
TI  - Improved Techniques for Training Consistency Models
AU  - Song, Yang
AU  - Dhariwal, Prafulla
AB  - Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.
DA  - 2023/10/22/
PY  - 2023
DO  - 10.48550/arXiv.2310.14189
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.14189
Y2  - 2024/07/02/10:54:33
L1  - https://arxiv.org/pdf/2310.14189.pdf
L2  - https://arxiv.org/abs/2310.14189
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Consistency Models
AU  - Song, Yang
AU  - Dhariwal, Prafulla
AU  - Chen, Mark
AU  - Sutskever, Ilya
AB  - Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.
DA  - 2023/05/31/
PY  - 2023
DO  - 10.48550/arXiv.2303.01469
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.01469
Y2  - 2024/07/02/10:54:46
L1  - https://arxiv.org/pdf/2303.01469.pdf
L2  - https://arxiv.org/abs/2303.01469
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - A Holistic Approach to Undesired Content Detection in the Real World
AU  - Markov, Todor
AU  - Zhang, Chong
AU  - Agarwal, Sandhini
AU  - Eloundou, Tyna
AU  - Lee, Teddy
AU  - Adler, Steven
AU  - Jiang, Angela
AU  - Weng, Lilian
AB  - We present a holistic approach to building a robust and useful natural language classification system for real-world content moderation. The success of such a system relies on a chain of carefully designed and executed steps, including the design of content taxonomies and labeling instructions, data quality control, an active learning pipeline to capture rare events, and a variety of methods to make the model robust and to avoid overfitting. Our moderation system is trained to detect a broad set of categories of undesired content, including sexual content, hateful content, violence, self-harm, and harassment. This approach generalizes to a wide range of different content taxonomies and can be used to create high-quality content classifiers that outperform off-the-shelf models.
DA  - 2023/02/14/
PY  - 2023
DO  - 10.48550/arXiv.2208.03274
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2208.03274
Y2  - 2024/07/02/10:54:59
L1  - https://arxiv.org/pdf/2208.03274.pdf
L2  - https://arxiv.org/abs/2208.03274
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Scaling and evaluating sparse autoencoders
AU  - Gao, Leo
AU  - la Tour, Tom Dupré
AU  - Tillman, Henk
AU  - Goh, Gabriel
AU  - Troll, Rajan
AU  - Radford, Alec
AU  - Sutskever, Ilya
AU  - Leike, Jan
AU  - Wu, Jeffrey
AB  - Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.
DA  - 2024/06/06/
PY  - 2024
DO  - 10.48550/arXiv.2406.04093
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.04093
Y2  - 2024/07/02/10:56:41
L1  - https://arxiv.org/pdf/2406.04093.pdf
L2  - https://arxiv.org/abs/2406.04093
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
AU  - Wallace, Eric
AU  - Xiao, Kai
AU  - Leike, Reimar
AU  - Weng, Lilian
AU  - Heidecke, Johannes
AU  - Beutel, Alex
AB  - Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.
DA  - 2024/04/19/
PY  - 2024
DO  - 10.48550/arXiv.2404.13208
DP  - arXiv.org
PB  - arXiv
ST  - The Instruction Hierarchy
UR  - http://arxiv.org/abs/2404.13208
Y2  - 2024/07/02/10:57:25
L1  - https://arxiv.org/pdf/2404.13208.pdf
L2  - https://arxiv.org/abs/2404.13208
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models
AU  - Eloundou, Tyna
AU  - Manning, Sam
AU  - Mishkin, Pamela
AU  - Rock, Daniel
AB  - We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.
DA  - 2023/08/21/
PY  - 2023
DO  - 10.48550/arXiv.2303.10130
DP  - arXiv.org
PB  - arXiv
ST  - GPTs are GPTs
UR  - http://arxiv.org/abs/2303.10130
Y2  - 2024/07/02/12:03:39
L1  - https://arxiv.org/pdf/2303.10130.pdf
L2  - https://arxiv.org/abs/2303.10130
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Economics - General Economics
ER  - 

TY  - GEN
TI  - GPT-4 Technical Report
AU  - OpenAI
AU  - Achiam, Josh
AU  - Adler, Steven
AU  - Agarwal, Sandhini
AU  - Ahmad, Lama
AU  - Akkaya, Ilge
AU  - Aleman, Florencia Leoni
AU  - Almeida, Diogo
AU  - Altenschmidt, Janko
AU  - Altman, Sam
AU  - Anadkat, Shyamal
AU  - Avila, Red
AU  - Babuschkin, Igor
AU  - Balaji, Suchir
AU  - Balcom, Valerie
AU  - Baltescu, Paul
AU  - Bao, Haiming
AU  - Bavarian, Mohammad
AU  - Belgum, Jeff
AU  - Bello, Irwan
AU  - Berdine, Jake
AU  - Bernadett-Shapiro, Gabriel
AU  - Berner, Christopher
AU  - Bogdonoff, Lenny
AU  - Boiko, Oleg
AU  - Boyd, Madelaine
AU  - Brakman, Anna-Luisa
AU  - Brockman, Greg
AU  - Brooks, Tim
AU  - Brundage, Miles
AU  - Button, Kevin
AU  - Cai, Trevor
AU  - Campbell, Rosie
AU  - Cann, Andrew
AU  - Carey, Brittany
AU  - Carlson, Chelsea
AU  - Carmichael, Rory
AU  - Chan, Brooke
AU  - Chang, Che
AU  - Chantzis, Fotis
AU  - Chen, Derek
AU  - Chen, Sully
AU  - Chen, Ruby
AU  - Chen, Jason
AU  - Chen, Mark
AU  - Chess, Ben
AU  - Cho, Chester
AU  - Chu, Casey
AU  - Chung, Hyung Won
AU  - Cummings, Dave
AU  - Currier, Jeremiah
AU  - Dai, Yunxing
AU  - Decareaux, Cory
AU  - Degry, Thomas
AU  - Deutsch, Noah
AU  - Deville, Damien
AU  - Dhar, Arka
AU  - Dohan, David
AU  - Dowling, Steve
AU  - Dunning, Sheila
AU  - Ecoffet, Adrien
AU  - Eleti, Atty
AU  - Eloundou, Tyna
AU  - Farhi, David
AU  - Fedus, Liam
AU  - Felix, Niko
AU  - Fishman, Simón Posada
AU  - Forte, Juston
AU  - Fulford, Isabella
AU  - Gao, Leo
AU  - Georges, Elie
AU  - Gibson, Christian
AU  - Goel, Vik
AU  - Gogineni, Tarun
AU  - Goh, Gabriel
AU  - Gontijo-Lopes, Rapha
AU  - Gordon, Jonathan
AU  - Grafstein, Morgan
AU  - Gray, Scott
AU  - Greene, Ryan
AU  - Gross, Joshua
AU  - Gu, Shixiang Shane
AU  - Guo, Yufei
AU  - Hallacy, Chris
AU  - Han, Jesse
AU  - Harris, Jeff
AU  - He, Yuchen
AU  - Heaton, Mike
AU  - Heidecke, Johannes
AU  - Hesse, Chris
AU  - Hickey, Alan
AU  - Hickey, Wade
AU  - Hoeschele, Peter
AU  - Houghton, Brandon
AU  - Hsu, Kenny
AU  - Hu, Shengli
AU  - Hu, Xin
AU  - Huizinga, Joost
AU  - Jain, Shantanu
AU  - Jain, Shawn
AU  - Jang, Joanne
AU  - Jiang, Angela
AU  - Jiang, Roger
AU  - Jin, Haozhun
AU  - Jin, Denny
AU  - Jomoto, Shino
AU  - Jonn, Billie
AU  - Jun, Heewoo
AU  - Kaftan, Tomer
AU  - Kaiser, Łukasz
AU  - Kamali, Ali
AU  - Kanitscheider, Ingmar
AU  - Keskar, Nitish Shirish
AU  - Khan, Tabarak
AU  - Kilpatrick, Logan
AU  - Kim, Jong Wook
AU  - Kim, Christina
AU  - Kim, Yongjik
AU  - Kirchner, Jan Hendrik
AU  - Kiros, Jamie
AU  - Knight, Matt
AU  - Kokotajlo, Daniel
AU  - Kondraciuk, Łukasz
AU  - Kondrich, Andrew
AU  - Konstantinidis, Aris
AU  - Kosic, Kyle
AU  - Krueger, Gretchen
AU  - Kuo, Vishal
AU  - Lampe, Michael
AU  - Lan, Ikai
AU  - Lee, Teddy
AU  - Leike, Jan
AU  - Leung, Jade
AU  - Levy, Daniel
AU  - Li, Chak Ming
AU  - Lim, Rachel
AU  - Lin, Molly
AU  - Lin, Stephanie
AU  - Litwin, Mateusz
AU  - Lopez, Theresa
AU  - Lowe, Ryan
AU  - Lue, Patricia
AU  - Makanju, Anna
AU  - Malfacini, Kim
AU  - Manning, Sam
AU  - Markov, Todor
AU  - Markovski, Yaniv
AU  - Martin, Bianca
AU  - Mayer, Katie
AU  - Mayne, Andrew
AU  - McGrew, Bob
AU  - McKinney, Scott Mayer
AU  - McLeavey, Christine
AU  - McMillan, Paul
AU  - McNeil, Jake
AU  - Medina, David
AU  - Mehta, Aalok
AU  - Menick, Jacob
AU  - Metz, Luke
AU  - Mishchenko, Andrey
AU  - Mishkin, Pamela
AU  - Monaco, Vinnie
AU  - Morikawa, Evan
AU  - Mossing, Daniel
AU  - Mu, Tong
AU  - Murati, Mira
AU  - Murk, Oleg
AU  - Mély, David
AU  - Nair, Ashvin
AU  - Nakano, Reiichiro
AU  - Nayak, Rajeev
AU  - Neelakantan, Arvind
AU  - Ngo, Richard
AU  - Noh, Hyeonwoo
AU  - Ouyang, Long
AU  - O'Keefe, Cullen
AU  - Pachocki, Jakub
AU  - Paino, Alex
AU  - Palermo, Joe
AU  - Pantuliano, Ashley
AU  - Parascandolo, Giambattista
AU  - Parish, Joel
AU  - Parparita, Emy
AU  - Passos, Alex
AU  - Pavlov, Mikhail
AU  - Peng, Andrew
AU  - Perelman, Adam
AU  - Peres, Filipe de Avila Belbute
AU  - Petrov, Michael
AU  - Pinto, Henrique Ponde de Oliveira
AU  - Michael
AU  - Pokorny
AU  - Pokrass, Michelle
AU  - Pong, Vitchyr H.
AU  - Powell, Tolly
AU  - Power, Alethea
AU  - Power, Boris
AU  - Proehl, Elizabeth
AU  - Puri, Raul
AU  - Radford, Alec
AU  - Rae, Jack
AU  - Ramesh, Aditya
AU  - Raymond, Cameron
AU  - Real, Francis
AU  - Rimbach, Kendra
AU  - Ross, Carl
AU  - Rotsted, Bob
AU  - Roussez, Henri
AU  - Ryder, Nick
AU  - Saltarelli, Mario
AU  - Sanders, Ted
AU  - Santurkar, Shibani
AU  - Sastry, Girish
AU  - Schmidt, Heather
AU  - Schnurr, David
AU  - Schulman, John
AU  - Selsam, Daniel
AU  - Sheppard, Kyla
AU  - Sherbakov, Toki
AU  - Shieh, Jessica
AU  - Shoker, Sarah
AU  - Shyam, Pranav
AU  - Sidor, Szymon
AU  - Sigler, Eric
AU  - Simens, Maddie
AU  - Sitkin, Jordan
AU  - Slama, Katarina
AU  - Sohl, Ian
AU  - Sokolowsky, Benjamin
AU  - Song, Yang
AU  - Staudacher, Natalie
AU  - Such, Felipe Petroski
AU  - Summers, Natalie
AU  - Sutskever, Ilya
AU  - Tang, Jie
AU  - Tezak, Nikolas
AU  - Thompson, Madeleine B.
AU  - Tillet, Phil
AU  - Tootoonchian, Amin
AU  - Tseng, Elizabeth
AU  - Tuggle, Preston
AU  - Turley, Nick
AU  - Tworek, Jerry
AU  - Uribe, Juan Felipe Cerón
AU  - Vallone, Andrea
AU  - Vijayvergiya, Arun
AU  - Voss, Chelsea
AU  - Wainwright, Carroll
AU  - Wang, Justin Jay
AU  - Wang, Alvin
AU  - Wang, Ben
AU  - Ward, Jonathan
AU  - Wei, Jason
AU  - Weinmann, C. J.
AU  - Welihinda, Akila
AU  - Welinder, Peter
AU  - Weng, Jiayi
AU  - Weng, Lilian
AU  - Wiethoff, Matt
AU  - Willner, Dave
AU  - Winter, Clemens
AU  - Wolrich, Samuel
AU  - Wong, Hannah
AU  - Workman, Lauren
AU  - Wu, Sherwin
AU  - Wu, Jeff
AU  - Wu, Michael
AU  - Xiao, Kai
AU  - Xu, Tao
AU  - Yoo, Sarah
AU  - Yu, Kevin
AU  - Yuan, Qiming
AU  - Zaremba, Wojciech
AU  - Zellers, Rowan
AU  - Zhang, Chong
AU  - Zhang, Marvin
AU  - Zhao, Shengjia
AU  - Zheng, Tianhao
AU  - Zhuang, Juntang
AU  - Zhuk, William
AU  - Zoph, Barret
AB  - We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
DA  - 2024/03/04/
PY  - 2024
DO  - 10.48550/arXiv.2303.08774
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.08774
Y2  - 2024/07/02/12:03:59
L1  - https://arxiv.org/pdf/2303.08774.pdf
L2  - https://arxiv.org/abs/2303.08774
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Point-E: A System for Generating 3D Point Clouds from Complex Prompts
AU  - Nichol, Alex
AU  - Jun, Heewoo
AU  - Dhariwal, Prafulla
AU  - Mishkin, Pamela
AU  - Chen, Mark
AB  - While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.
DA  - 2022/12/16/
PY  - 2022
DO  - 10.48550/arXiv.2212.08751
DP  - arXiv.org
PB  - arXiv
ST  - Point-E
UR  - http://arxiv.org/abs/2212.08751
Y2  - 2024/07/02/12:04:22
L1  - https://arxiv.org/pdf/2212.08751.pdf
L2  - https://arxiv.org/abs/2212.08751
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Scaling Laws for Reward Model Overoptimization
AU  - Gao, Leo
AU  - Schulman, John
AU  - Hilton, Jacob
AB  - In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.
DA  - 2022/10/19/
PY  - 2022
DO  - 10.48550/arXiv.2210.10760
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.10760
Y2  - 2024/07/02/12:04:32
L1  - https://arxiv.org/pdf/2210.10760.pdf
L2  - https://arxiv.org/abs/2210.10760
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Let's Verify Step by Step
AU  - Lightman, Hunter
AU  - Kosaraju, Vineet
AU  - Burda, Yura
AU  - Edwards, Harri
AU  - Baker, Bowen
AU  - Lee, Teddy
AU  - Leike, Jan
AU  - Schulman, John
AU  - Sutskever, Ilya
AU  - Cobbe, Karl
AB  - In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.
DA  - 2023/05/31/
PY  - 2023
DO  - 10.48550/arXiv.2305.20050
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.20050
Y2  - 2024/07/02/12:04:44
L1  - https://arxiv.org/pdf/2305.20050.pdf
L2  - https://arxiv.org/abs/2305.20050
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Robust Speech Recognition via Large-Scale Weak Supervision
AU  - Radford, Alec
AU  - Kim, Jong Wook
AU  - Xu, Tao
AU  - Brockman, Greg
AU  - McLeavey, Christine
AU  - Sutskever, Ilya
AB  - We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.
CY  - Unpublished
DA  - 2022/09//
PY  - 2022
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/papers/whisper.pdf
ER  - 

TY  - GEN
TI  - Efficient Training of Language Models to Fill in the Middle
AU  - Bavarian, Mohammad
AU  - Jun, Heewoo
AU  - Tezak, Nikolas
AU  - Schulman, John
AU  - McLeavey, Christine
AU  - Tworek, Jerry
AU  - Chen, Mark
AB  - We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.
DA  - 2022/07/28/
PY  - 2022
DO  - 10.48550/arXiv.2207.14255
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.14255
Y2  - 2024/07/02/12:06:01
L1  - https://arxiv.org/pdf/2207.14255.pdf
L2  - https://arxiv.org/abs/2207.14255
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos
AU  - Baker, Bowen
AU  - Akkaya, Ilge
AU  - Zhokhov, Peter
AU  - Huizinga, Joost
AU  - Tang, Jie
AU  - Ecoffet, Adrien
AU  - Houghton, Brandon
AU  - Sampedro, Raul
AU  - Clune, Jeff
AB  - Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.
DA  - 2022/06/23/
PY  - 2022
DO  - 10.48550/arXiv.2206.11795
DP  - arXiv.org
PB  - arXiv
ST  - Video PreTraining (VPT)
UR  - http://arxiv.org/abs/2206.11795
Y2  - 2024/07/02/12:06:53
L1  - https://arxiv.org/pdf/2206.11795.pdf
L2  - https://arxiv.org/abs/2206.11795
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Evolution through Large Models
AU  - Lehman, Joel
AU  - Gordon, Jonathan
AU  - Jain, Shawn
AU  - Ndousse, Kamal
AU  - Yeh, Cathy
AU  - Stanley, Kenneth O.
AB  - This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.
DA  - 2022/06/17/
PY  - 2022
DO  - 10.48550/arXiv.2206.08896
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.08896
Y2  - 2024/07/02/12:07:09
L1  - https://arxiv.org/pdf/2206.08896.pdf
L2  - https://arxiv.org/abs/2206.08896
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Teaching Models to Express Their Uncertainty in Words
AU  - Lin, Stephanie
AU  - Hilton, Jacob
AU  - Evans, Owain
AB  - We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. "90% confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.
DA  - 2022/06/13/
PY  - 2022
DO  - 10.48550/arXiv.2205.14334
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.14334
Y2  - 2024/07/02/12:08:12
L1  - https://arxiv.org/pdf/2205.14334.pdf
L2  - https://arxiv.org/abs/2205.14334
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Hierarchical Text-Conditional Image Generation with CLIP Latents
AU  - Ramesh, Aditya
AU  - Dhariwal, Prafulla
AU  - Nichol, Alex
AU  - Chu, Casey
AU  - Chen, Mark
AB  - Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.
DA  - 2022/04/12/
PY  - 2022
DO  - 10.48550/arXiv.2204.06125
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.06125
Y2  - 2024/07/02/12:08:28
L1  - https://arxiv.org/pdf/2204.06125.pdf
L2  - https://arxiv.org/abs/2204.06125
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - A Research Agenda for Assessing the Economic Impacts of Code Generation Models
AU  - Manning, Sam
AU  - Mishkin, Pamela
AU  - Hadfield, Gillian
AU  - Eisner, Emily
AB  - OpenAI is developing a research program to assess the economic impacts of code generation models and is inviting collaboration with external researchers. Rapid advances in the capabilities of large language models (LLMs) trained on code have made it increasingly important to study their economic impacts on individuals, firms, and society. Codex – an LLM developed by OpenAI by fine-tuning GPT-3 on billions of lines of publicly available code from GitHub – has been shown to generate functionally correct code 28.8% of the time on a sample of evaluation problems (Chen et al. 2021). This may have important implications for the future of coding and the economics of the industries that depend on it. In this document, we lay out a research agenda to assess the effects of Codex on economic factors of interest to policymakers, firms, and the public. We make a case for this research agenda by highlighting the potentially broad applicability of code generation models to software development, the potential for other LLMs to create significant social and economic impact as model capabilities advance, and the value of using Codex to generate evidence and establish methodologies that may be applicable to research on the economic impacts of future models. We propose that academic
and policy research focus on studying code generation models and other LLMs so that evidence on their economic impacts can be used to inform decision-making in three key areas: Deployment policy, AI system design, and public policy. To help guide this research, we outline six priority outcome areas within the realm of economic impacts that we intend to use Codex to study: Productivity, Employment, Skill Development, Inter-firm Competition, Consumer Prices, and Economic Inequality. For each area,
we briefly discuss previous literature on the impacts of artificial intelligence on each of these outcomes, describe questions that we believe to be key inputs to the three decision-making areas mentioned above, and provide examples of research that could be conducted with Codex. To catalyze work that builds off of this initial research agenda, we are announcing a Call for Expressions of Interest from external researchers to collaborate
with OpenAI researchers and customers to better measure the economic impacts of code generation models and other LLMs.
CY  - Unpublished
DA  - 2022/03//
PY  - 2022
DP  - Zotero
LA  - en
UR  - https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf
L1  - https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf
ER  - 

TY  - GEN
TI  - Formal Mathematics Statement Curriculum Learning
AU  - Polu, Stanislas
AU  - Han, Jesse Michael
AU  - Zheng, Kunhao
AU  - Baksys, Mantas
AU  - Babuschkin, Igor
AU  - Sutskever, Ilya
AB  - We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.
DA  - 2022/02/02/
PY  - 2022
DO  - 10.48550/arXiv.2202.01344
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2202.01344
Y2  - 2024/07/02/12:11:59
L1  - https://arxiv.org/pdf/2202.01344.pdf
L2  - https://arxiv.org/abs/2202.01344
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Text and Code Embeddings by Contrastive Pre-Training
AU  - Neelakantan, Arvind
AU  - Xu, Tao
AU  - Puri, Raul
AU  - Radford, Alec
AU  - Han, Jesse Michael
AU  - Tworek, Jerry
AU  - Yuan, Qiming
AU  - Tezak, Nikolas
AU  - Kim, Jong Wook
AU  - Hallacy, Chris
AU  - Heidecke, Johannes
AU  - Shyam, Pranav
AU  - Power, Boris
AU  - Nekoul, Tyna Eloundou
AU  - Sastry, Girish
AU  - Krueger, Gretchen
AU  - Schnurr, David
AU  - Such, Felipe Petroski
AU  - Hsu, Kenny
AU  - Thompson, Madeleine
AU  - Khan, Tabarak
AU  - Sherbakov, Toki
AU  - Jang, Joanne
AU  - Welinder, Peter
AU  - Weng, Lilian
AB  - Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.
DA  - 2022/01/24/
PY  - 2022
DO  - 10.48550/arXiv.2201.10005
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2201.10005
Y2  - 2024/07/02/12:12:08
L1  - https://arxiv.org/pdf/2201.10005.pdf
L2  - https://arxiv.org/abs/2201.10005
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - WebGPT: Browser-assisted question-answering with human feedback
AU  - Nakano, Reiichiro
AU  - Hilton, Jacob
AU  - Balaji, Suchir
AU  - Wu, Jeff
AU  - Ouyang, Long
AU  - Kim, Christina
AU  - Hesse, Christopher
AU  - Jain, Shantanu
AU  - Kosaraju, Vineet
AU  - Saunders, William
AU  - Jiang, Xu
AU  - Cobbe, Karl
AU  - Eloundou, Tyna
AU  - Krueger, Gretchen
AU  - Button, Kevin
AU  - Knight, Matthew
AU  - Chess, Benjamin
AU  - Schulman, John
AB  - We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.
DA  - 2022/06/01/
PY  - 2022
DO  - 10.48550/arXiv.2112.09332
DP  - arXiv.org
PB  - arXiv
ST  - WebGPT
UR  - http://arxiv.org/abs/2112.09332
Y2  - 2024/07/02/12:12:24
L1  - https://arxiv.org/pdf/2112.09332.pdf
L2  - https://arxiv.org/abs/2112.09332
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Training Verifiers to Solve Math Word Problems
AU  - Cobbe, Karl
AU  - Kosaraju, Vineet
AU  - Bavarian, Mohammad
AU  - Chen, Mark
AU  - Jun, Heewoo
AU  - Kaiser, Lukasz
AU  - Plappert, Matthias
AU  - Tworek, Jerry
AU  - Hilton, Jacob
AU  - Nakano, Reiichiro
AU  - Hesse, Christopher
AU  - Schulman, John
AB  - State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.
DA  - 2021/11/17/
PY  - 2021
DO  - 10.48550/arXiv.2110.14168
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2110.14168
Y2  - 2024/07/02/12:12:34
L1  - https://arxiv.org/pdf/2110.14168.pdf
L2  - https://arxiv.org/abs/2110.14168
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - TruthfulQA: Measuring How Models Mimic Human Falsehoods
AU  - Lin, Stephanie
AU  - Hilton, Jacob
AU  - Evans, Owain
AB  - We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.
DA  - 2022/05/07/
PY  - 2022
DO  - 10.48550/arXiv.2109.07958
DP  - arXiv.org
PB  - arXiv
ST  - TruthfulQA
UR  - http://arxiv.org/abs/2109.07958
Y2  - 2024/07/02/12:12:44
L1  - https://arxiv.org/pdf/2109.07958.pdf
L2  - https://arxiv.org/abs/2109.07958
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Evaluating Large Language Models Trained on Code
AU  - Chen, Mark
AU  - Tworek, Jerry
AU  - Jun, Heewoo
AU  - Yuan, Qiming
AU  - Pinto, Henrique Ponde de Oliveira
AU  - Kaplan, Jared
AU  - Edwards, Harri
AU  - Burda, Yuri
AU  - Joseph, Nicholas
AU  - Brockman, Greg
AU  - Ray, Alex
AU  - Puri, Raul
AU  - Krueger, Gretchen
AU  - Petrov, Michael
AU  - Khlaaf, Heidy
AU  - Sastry, Girish
AU  - Mishkin, Pamela
AU  - Chan, Brooke
AU  - Gray, Scott
AU  - Ryder, Nick
AU  - Pavlov, Mikhail
AU  - Power, Alethea
AU  - Kaiser, Lukasz
AU  - Bavarian, Mohammad
AU  - Winter, Clemens
AU  - Tillet, Philippe
AU  - Such, Felipe Petroski
AU  - Cummings, Dave
AU  - Plappert, Matthias
AU  - Chantzis, Fotios
AU  - Barnes, Elizabeth
AU  - Herbert-Voss, Ariel
AU  - Guss, William Hebgen
AU  - Nichol, Alex
AU  - Paino, Alex
AU  - Tezak, Nikolas
AU  - Tang, Jie
AU  - Babuschkin, Igor
AU  - Balaji, Suchir
AU  - Jain, Shantanu
AU  - Saunders, William
AU  - Hesse, Christopher
AU  - Carr, Andrew N.
AU  - Leike, Jan
AU  - Achiam, Josh
AU  - Misra, Vedant
AU  - Morikawa, Evan
AU  - Radford, Alec
AU  - Knight, Matthew
AU  - Brundage, Miles
AU  - Murati, Mira
AU  - Mayer, Katie
AU  - Welinder, Peter
AU  - McGrew, Bob
AU  - Amodei, Dario
AU  - McCandlish, Sam
AU  - Sutskever, Ilya
AU  - Zaremba, Wojciech
AB  - We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.
DA  - 2021/07/14/
PY  - 2021
DO  - 10.48550/arXiv.2107.03374
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2107.03374
Y2  - 2024/07/02/12:14:24
L1  - https://arxiv.org/pdf/2107.03374.pdf
L2  - https://arxiv.org/abs/2107.03374
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Multimodal Neurons in Artificial Neural Networks
AU  - Goh, Gabriel
AU  - †, Nick Cammarata
AU  - †, Chelsea Voss
AU  - Carter, Shan
AU  - Petrov, Michael
AU  - Schubert, Ludwig
AU  - Radford, Alec
AU  - Olah, Chris
T2  - Distill
AB  - We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.
DA  - 2021/03/04/
PY  - 2021
DO  - 10.23915/distill.00030
DP  - distill.pub
VL  - 6
IS  - 3
SP  - e30
J2  - Distill
LA  - en
SN  - 2476-0757
UR  - https://distill.pub/2021/multimodal-neurons
Y2  - 2024/07/02/12:14:44
L2  - https://distill.pub/2021/multimodal-neurons/
ER  - 

TY  - GEN
TI  - Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models
AU  - Tamkin, Alex
AU  - Brundage, Miles
AU  - Clark, Jack
AU  - Ganguli, Deep
AB  - On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.
DA  - 2021/02/04/
PY  - 2021
DO  - 10.48550/arXiv.2102.02503
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2102.02503
Y2  - 2024/07/02/12:15:32
L1  - https://arxiv.org/pdf/2102.02503.pdf
L2  - https://arxiv.org/abs/2102.02503
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Learning Transferable Visual Models From Natural Language Supervision
AU  - Radford, Alec
AU  - Kim, Jong Wook
AU  - Hallacy, Chris
AU  - Ramesh, Aditya
AU  - Goh, Gabriel
AU  - Agarwal, Sandhini
AU  - Sastry, Girish
AU  - Askell, Amanda
AU  - Mishkin, Pamela
AU  - Clark, Jack
AU  - Krueger, Gretchen
AU  - Sutskever, Ilya
AB  - State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.
DA  - 2021/02/26/
PY  - 2021
DO  - 10.48550/arXiv.2103.00020
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2103.00020
Y2  - 2024/07/02/12:16:14
L1  - https://arxiv.org/pdf/2103.00020.pdf
L2  - https://arxiv.org/abs/2103.00020
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Generative Language Modeling for Automated Theorem Proving
AU  - Polu, Stanislas
AU  - Sutskever, Ilya
AB  - We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.
DA  - 2020/09/07/
PY  - 2020
DO  - 10.48550/arXiv.2009.03393
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2009.03393
Y2  - 2024/07/02/12:16:45
L1  - https://arxiv.org/pdf/2009.03393.pdf
L2  - https://arxiv.org/abs/2009.03393
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Generative Pretraining from Pixels
AU  - Chen, Mark
AU  - Radford, Alec
AU  - Child, Rewon
AU  - Wu, Jeff
AU  - Jun, Heewoo
AU  - Dhariwal, Prafulla
AU  - Luan, David
AU  - Sutskever, Ilya
AB  - Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.
CY  - ICML 2020
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf
ER  - 

TY  - GEN
TI  - Language Models are Few-Shot Learners
AU  - Brown, Tom B.
AU  - Mann, Benjamin
AU  - Ryder, Nick
AU  - Subbiah, Melanie
AU  - Kaplan, Jared
AU  - Dhariwal, Prafulla
AU  - Neelakantan, Arvind
AU  - Shyam, Pranav
AU  - Sastry, Girish
AU  - Askell, Amanda
AU  - Agarwal, Sandhini
AU  - Herbert-Voss, Ariel
AU  - Krueger, Gretchen
AU  - Henighan, Tom
AU  - Child, Rewon
AU  - Ramesh, Aditya
AU  - Ziegler, Daniel M.
AU  - Wu, Jeffrey
AU  - Winter, Clemens
AU  - Hesse, Christopher
AU  - Chen, Mark
AU  - Sigler, Eric
AU  - Litwin, Mateusz
AU  - Gray, Scott
AU  - Chess, Benjamin
AU  - Clark, Jack
AU  - Berner, Christopher
AU  - McCandlish, Sam
AU  - Radford, Alec
AU  - Sutskever, Ilya
AU  - Amodei, Dario
AB  - Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.
DA  - 2020/07/22/
PY  - 2020
DO  - 10.48550/arXiv.2005.14165
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2005.14165
Y2  - 2024/07/02/12:17:42
L1  - https://arxiv.org/pdf/2005.14165.pdf
L2  - https://arxiv.org/abs/2005.14165
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Measuring the Algorithmic Efficiency of Neural Networks
AU  - Hernandez, Danny
AU  - Brown, Tom B.
AB  - Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.
DA  - 2020/05/08/
PY  - 2020
DO  - 10.48550/arXiv.2005.04305
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2005.04305
Y2  - 2024/07/02/12:17:55
L1  - https://arxiv.org/pdf/2005.04305.pdf
L2  - https://arxiv.org/abs/2005.04305
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Jukebox: A Generative Model for Music
AU  - Dhariwal, Prafulla
AU  - Jun, Heewoo
AU  - Payne, Christine
AU  - Kim, Jong Wook
AU  - Radford, Alec
AU  - Sutskever, Ilya
AB  - We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox
DA  - 2020/04/30/
PY  - 2020
DO  - 10.48550/arXiv.2005.00341
DP  - arXiv.org
PB  - arXiv
ST  - Jukebox
UR  - http://arxiv.org/abs/2005.00341
Y2  - 2024/07/02/12:18:09
L1  - https://arxiv.org/pdf/2005.00341.pdf
L2  - https://arxiv.org/abs/2005.00341
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Sound
KW  - Electrical Engineering and Systems Science - Audio and Speech Processing
ER  - 

TY  - GEN
TI  - Scaling Laws for Neural Language Models
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Henighan, Tom
AU  - Brown, Tom B.
AU  - Chess, Benjamin
AU  - Child, Rewon
AU  - Gray, Scott
AU  - Radford, Alec
AU  - Wu, Jeffrey
AU  - Amodei, Dario
AB  - We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.
DA  - 2020/01/22/
PY  - 2020
DO  - 10.48550/arXiv.2001.08361
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2001.08361
Y2  - 2024/07/02/12:19:55
L1  - https://arxiv.org/pdf/2001.08361.pdf
L2  - https://arxiv.org/abs/2001.08361
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Dota 2 with Large Scale Deep Reinforcement Learning
AU  - OpenAI
AU  - Berner, Christopher
AU  - Brockman, Greg
AU  - Chan, Brooke
AU  - Cheung, Vicki
AU  - Dębiak, Przemysław
AU  - Dennison, Christy
AU  - Farhi, David
AU  - Fischer, Quirin
AU  - Hashme, Shariq
AU  - Hesse, Chris
AU  - Józefowicz, Rafal
AU  - Gray, Scott
AU  - Olsson, Catherine
AU  - Pachocki, Jakub
AU  - Petrov, Michael
AU  - Pinto, Henrique P. d O.
AU  - Raiman, Jonathan
AU  - Salimans, Tim
AU  - Schlatter, Jeremy
AU  - Schneider, Jonas
AU  - Sidor, Szymon
AU  - Sutskever, Ilya
AU  - Tang, Jie
AU  - Wolski, Filip
AU  - Zhang, Susan
AB  - On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
DA  - 2019/12/13/
PY  - 2019
DO  - 10.48550/arXiv.1912.06680
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1912.06680
Y2  - 2024/07/02/12:20:09
L1  - https://arxiv.org/pdf/1912.06680.pdf
L2  - https://arxiv.org/abs/1912.06680
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Deep Double Descent: Where Bigger Models and More Data Hurt
AU  - Nakkiran, Preetum
AU  - Kaplun, Gal
AU  - Bansal, Yamini
AU  - Yang, Tristan
AU  - Barak, Boaz
AU  - Sutskever, Ilya
AB  - We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.
DA  - 2019/12/04/
PY  - 2019
DO  - 10.48550/arXiv.1912.02292
DP  - arXiv.org
PB  - arXiv
ST  - Deep Double Descent
UR  - http://arxiv.org/abs/1912.02292
Y2  - 2024/07/02/12:20:31
L1  - https://arxiv.org/pdf/1912.02292.pdf
L2  - https://arxiv.org/abs/1912.02292
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Leveraging Procedural Generation to Benchmark Reinforcement Learning
AU  - Cobbe, Karl
AU  - Hesse, Christopher
AU  - Hilton, Jacob
AU  - Schulman, John
AB  - We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.
DA  - 2020/07/26/
PY  - 2020
DO  - 10.48550/arXiv.1912.01588
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1912.01588
Y2  - 2024/07/02/12:20:45
L1  - https://arxiv.org/pdf/1912.01588.pdf
L2  - https://arxiv.org/abs/1912.01588
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Release Strategies and the Social Impacts of Language Models
AU  - Solaiman, Irene
AU  - Brundage, Miles
AU  - Clark, Jack
AU  - Askell, Amanda
AU  - Herbert-Voss, Ariel
AU  - Wu, Jeff
AU  - Radford, Alec
AU  - Krueger, Gretchen
AU  - Kim, Jong Wook
AU  - Kreps, Sarah
AU  - McCain, Miles
AU  - Newhouse, Alex
AU  - Blazakis, Jason
AU  - McGuffie, Kris
AU  - Wang, Jasmine
AB  - Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.
DA  - 2019/11/12/
PY  - 2019
DO  - 10.48550/arXiv.1908.09203
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1908.09203
Y2  - 2024/07/02/12:21:06
L1  - https://arxiv.org/pdf/1908.09203.pdf
L2  - https://arxiv.org/abs/1908.09203
KW  - Computer Science - Artificial Intelligence
KW  - I.2
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - I.2.7
KW  - K.4
ER  - 

TY  - GEN
TI  - Solving Rubik's Cube with a Robot Hand
AU  - OpenAI
AU  - Akkaya, Ilge
AU  - Andrychowicz, Marcin
AU  - Chociej, Maciek
AU  - Litwin, Mateusz
AU  - McGrew, Bob
AU  - Petron, Arthur
AU  - Paino, Alex
AU  - Plappert, Matthias
AU  - Powell, Glenn
AU  - Ribas, Raphael
AU  - Schneider, Jonas
AU  - Tezak, Nikolas
AU  - Tworek, Jerry
AU  - Welinder, Peter
AU  - Weng, Lilian
AU  - Yuan, Qiming
AU  - Zaremba, Wojciech
AU  - Zhang, Lei
AB  - We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/
DA  - 2019/10/15/
PY  - 2019
DO  - 10.48550/arXiv.1910.07113
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1910.07113
Y2  - 2024/07/02/12:21:18
L1  - https://arxiv.org/pdf/1910.07113.pdf
L2  - https://arxiv.org/abs/1910.07113
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Emergent Tool Use From Multi-Agent Autocurricula
AU  - Baker, Bowen
AU  - Kanitscheider, Ingmar
AU  - Markov, Todor
AU  - Wu, Yi
AU  - Powell, Glenn
AU  - McGrew, Bob
AU  - Mordatch, Igor
AB  - Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.
DA  - 2020/02/10/
PY  - 2020
DO  - 10.48550/arXiv.1909.07528
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1909.07528
Y2  - 2024/07/02/12:21:29
L1  - https://arxiv.org/pdf/1909.07528.pdf
L2  - https://arxiv.org/abs/1909.07528
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Generating Long Sequences with Sparse Transformers
AU  - Child, Rewon
AU  - Gray, Scott
AU  - Radford, Alec
AU  - Sutskever, Ilya
AB  - Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.
DA  - 2019/04/23/
PY  - 2019
DO  - 10.48550/arXiv.1904.10509
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1904.10509
Y2  - 2024/07/02/12:23:30
L1  - https://arxiv.org/pdf/1904.10509.pdf
L2  - https://arxiv.org/abs/1904.10509
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Implicit Generation and Generalization in Energy-Based Models
AU  - Du, Yilun
AU  - Mordatch, Igor
AB  - Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.
DA  - 2020/06/29/
PY  - 2020
DO  - 10.48550/arXiv.1903.08689
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1903.08689
Y2  - 2024/07/02/12:23:42
L1  - https://arxiv.org/pdf/1903.08689.pdf
L2  - https://arxiv.org/abs/1903.08689
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents
AU  - Suarez, Joseph
AU  - Du, Yilun
AU  - Isola, Phillip
AU  - Mordatch, Igor
AB  - The emergence of complex life on Earth is often attributed to the arms race that ensued from a huge number of organisms all competing for finite resources. We present an artificial intelligence research environment, inspired by the human game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games, a.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs and the real world alike, our environment is persistent and supports a large and variable number of agents. Our environment is well suited to the study of large-scale multiagent interaction: it requires that agents learn robust combat and navigation policies in the presence of large populations attempting to do the same. Baseline experiments reveal that population size magnifies and incentivizes the development of skillful behaviors and results in agents that outcompete agents trained in smaller populations. We further show that the policies of agents with unshared weights naturally diverge to fill different niches in order to avoid competition.
DA  - 2019/03/02/
PY  - 2019
DO  - 10.48550/arXiv.1903.00784
DP  - arXiv.org
PB  - arXiv
ST  - Neural MMO
UR  - http://arxiv.org/abs/1903.00784
Y2  - 2024/07/02/12:24:00
L1  - https://arxiv.org/pdf/1903.00784.pdf
L2  - https://arxiv.org/abs/1903.00784
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Language Models are Unsupervised Multitask Learners
AU  - Radford, Alec
AU  - Wu, Jeffrey
AU  - Child, Rewon
AU  - Luan, David
AU  - Amodei, Dario
AU  - Sutskever, Ilya
AB  - Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.
DA  - 2019/02//
PY  - 2019
DP  - Zotero
LA  - en
UR  - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
L1  - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
ER  - 

TY  - GEN
TI  - Computational Limitations in Robust Classification and Win-Win Results
AU  - Degwekar, Akshay
AU  - Nakkiran, Preetum
AU  - Vaikuntanathan, Vinod
AB  - We continue the study of statistical/computational tradeoffs in learning robust classifiers, following the recent work of Bubeck, Lee, Price and Razenshteyn who showed examples of classification tasks where (a) an efficient robust classifier exists, in the small-perturbation regime; (b) a non-robust classifier can be learned efficiently; but (c) it is computationally hard to learn a robust classifier, assuming the hardness of factoring large numbers. The question of whether a robust classifier for their task exists in the large perturbation regime seems related to important open questions in computational number theory. In this work, we extend their work in three directions. First, we demonstrate classification tasks where computationally efficient robust classification is impossible, even when computationally unbounded robust classifiers exist. For this, we rely on the existence of average-case hard functions. Second, we show hard-to-robustly-learn classification tasks in the large-perturbation regime. Namely, we show that even though an efficient classifier that is robust to large perturbations exists, it is computationally hard to learn any non-trivial robust classifier. Our first construction relies on the existence of one-way functions, and the second on the hardness of the learning parity with noise problem. In the latter setting, not only does a non-robust classifier exist, but also an efficient algorithm that generates fresh new labeled samples given access to polynomially many training examples (termed as generation by Kearns et. al. (1994)). Third, we show that any such counterexample implies the existence of cryptographic primitives such as one-way functions. This leads us to a win-win scenario: either we can learn an efficient robust classifier, or we can construct new instances of cryptographic primitives.
DA  - 2019/06/05/
PY  - 2019
DO  - 10.48550/arXiv.1902.01086
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1902.01086
Y2  - 2024/07/02/12:25:24
L1  - https://arxiv.org/pdf/1902.01086.pdf
L2  - https://arxiv.org/abs/1902.01086
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - An Empirical Model of Large-Batch Training
AU  - McCandlish, Sam
AU  - Kaplan, Jared
AU  - Amodei, Dario
AU  - Team, OpenAI Dota
AB  - In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacriﬁcing data efﬁciency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We ﬁnd that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efﬁciency and time-efﬁciency, and provides a rough model of the beneﬁts of adaptive batch-size training.
DA  - 2018/12/14/
PY  - 2018
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1812.06162
Y2  - 2024/07/02/12:25:50
L1  - https://arxiv.org/pdf/1812.06162
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Quantifying Generalization in Reinforcement Learning
AU  - Cobbe, Karl
AU  - Klimov, Oleg
AU  - Hesse, Chris
AU  - Kim, Taehoon
AU  - Schulman, John
AB  - In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.
DA  - 2019/07/14/
PY  - 2019
DO  - 10.48550/arXiv.1812.02341
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1812.02341
Y2  - 2024/07/02/12:25:56
L1  - https://arxiv.org/pdf/1812.02341.pdf
L2  - https://arxiv.org/abs/1812.02341
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Concept Learning with Energy-Based Models
AU  - Mordatch, Igor
AB  - Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels
DA  - 2018/11/06/
PY  - 2018
DO  - 10.48550/arXiv.1811.02486
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1811.02486
Y2  - 2024/07/02/12:26:45
L1  - https://arxiv.org/pdf/1811.02486.pdf
L2  - https://arxiv.org/abs/1811.02486
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control
AU  - Lowrey, Kendall
AU  - Rajeswaran, Aravind
AU  - Kakade, Sham
AU  - Todorov, Emanuel
AU  - Mordatch, Igor
AB  - We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.
DA  - 2019/01/28/
PY  - 2019
DO  - 10.48550/arXiv.1811.01848
DP  - arXiv.org
PB  - arXiv
ST  - Plan Online, Learn Offline
UR  - http://arxiv.org/abs/1811.01848
Y2  - 2024/07/02/12:26:57
L1  - https://arxiv.org/pdf/1811.01848.pdf
L2  - https://arxiv.org/abs/1811.01848
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models
AU  - Grathwohl, Will
AU  - Chen, Ricky T. Q.
AU  - Bettencourt, Jesse
AU  - Sutskever, Ilya
AU  - Duvenaud, David
AB  - A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.
DA  - 2018/10/22/
PY  - 2018
DO  - 10.48550/arXiv.1810.01367
DP  - arXiv.org
PB  - arXiv
ST  - FFJORD
UR  - http://arxiv.org/abs/1810.01367
Y2  - 2024/07/02/12:27:52
L1  - https://arxiv.org/pdf/1810.01367.pdf
L2  - https://arxiv.org/abs/1810.01367
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Large-Scale Study of Curiosity-Driven Learning
AU  - Burda, Yuri
AU  - Edwards, Harri
AU  - Pathak, Deepak
AU  - Storkey, Amos
AU  - Darrell, Trevor
AU  - Efros, Alexei A.
AB  - Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/
DA  - 2018/08/13/
PY  - 2018
DO  - 10.48550/arXiv.1808.04355
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1808.04355
Y2  - 2024/07/02/12:28:04
L1  - https://arxiv.org/pdf/1808.04355.pdf
L2  - https://arxiv.org/abs/1808.04355
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Learning Dexterous In-Hand Manipulation
AU  - OpenAI
AU  - Andrychowicz, Marcin
AU  - Baker, Bowen
AU  - Chociej, Maciek
AU  - Jozefowicz, Rafal
AU  - McGrew, Bob
AU  - Pachocki, Jakub
AU  - Petron, Arthur
AU  - Plappert, Matthias
AU  - Powell, Glenn
AU  - Ray, Alex
AU  - Schneider, Jonas
AU  - Sidor, Szymon
AU  - Tobin, Josh
AU  - Welinder, Peter
AU  - Weng, Lilian
AU  - Zaremba, Wojciech
AB  - We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM
DA  - 2019/01/18/
PY  - 2019
DO  - 10.48550/arXiv.1808.00177
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1808.00177
Y2  - 2024/07/02/12:28:16
L1  - https://arxiv.org/pdf/1808.00177.pdf
L2  - https://arxiv.org/abs/1808.00177
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Glow: Generative Flow with Invertible 1x1 Convolutions
AU  - Kingma, Diederik P.
AU  - Dhariwal, Prafulla
AB  - Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow
DA  - 2018/07/10/
PY  - 2018
DO  - 10.48550/arXiv.1807.03039
DP  - arXiv.org
PB  - arXiv
ST  - Glow
UR  - http://arxiv.org/abs/1807.03039
Y2  - 2024/07/02/12:28:56
L1  - https://arxiv.org/pdf/1807.03039.pdf
L2  - https://arxiv.org/abs/1807.03039
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Learning Policy Representations in Multiagent Systems
AU  - Grover, Aditya
AU  - Al-Shedivat, Maruan
AU  - Gupta, Jayesh K.
AU  - Burda, Yura
AU  - Edwards, Harrison
AB  - Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.
DA  - 2018/07/31/
PY  - 2018
DO  - 10.48550/arXiv.1806.06464
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1806.06464
Y2  - 2024/07/02/12:30:35
L1  - https://arxiv.org/pdf/1806.06464.pdf
L2  - https://arxiv.org/abs/1806.06464
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - GamePad: A Learning Environment for Theorem Proving
AU  - Huang, Daniel
AU  - Dhariwal, Prafulla
AU  - Song, Dawn
AU  - Sutskever, Ilya
AB  - In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.
DA  - 2018/12/21/
PY  - 2018
DO  - 10.48550/arXiv.1806.00608
DP  - arXiv.org
PB  - arXiv
ST  - GamePad
UR  - http://arxiv.org/abs/1806.00608
Y2  - 2024/07/02/12:30:45
L1  - https://arxiv.org/pdf/1806.00608.pdf
L2  - https://arxiv.org/abs/1806.00608
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Logic in Computer Science
ER  - 

TY  - GEN
TI  - Evolved Policy Gradients
AU  - Houthooft, Rein
AU  - Chen, Richard Y.
AU  - Isola, Phillip
AU  - Stadie, Bradly C.
AU  - Wolski, Filip
AU  - Ho, Jonathan
AU  - Abbeel, Pieter
AB  - We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.
DA  - 2018/04/29/
PY  - 2018
DO  - 10.48550/arXiv.1802.04821
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1802.04821
Y2  - 2024/07/02/12:31:06
L1  - https://arxiv.org/pdf/1802.04821.pdf
L2  - https://arxiv.org/abs/1802.04821
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Gotta Learn Fast: A New Benchmark for Generalization in RL
AU  - Nichol, Alex
AU  - Pfau, Vicki
AU  - Hesse, Christopher
AU  - Klimov, Oleg
AU  - Schulman, John
AB  - In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.
DA  - 2018/04/23/
PY  - 2018
DO  - 10.48550/arXiv.1804.03720
DP  - arXiv.org
PB  - arXiv
ST  - Gotta Learn Fast
UR  - http://arxiv.org/abs/1804.03720
Y2  - 2024/07/02/12:31:25
L1  - https://arxiv.org/pdf/1804.03720.pdf
L2  - https://arxiv.org/abs/1804.03720
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines
AU  - Wu, Cathy
AU  - Rajeswaran, Aravind
AU  - Duan, Yan
AU  - Kumar, Vikash
AU  - Bayen, Alexandre M.
AU  - Kakade, Sham
AU  - Mordatch, Igor
AU  - Abbeel, Pieter
AB  - Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.
DA  - 2018/03/19/
PY  - 2018
DO  - 10.48550/arXiv.1803.07246
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.07246
Y2  - 2024/07/02/12:32:09
L1  - https://arxiv.org/pdf/1803.07246.pdf
L2  - https://arxiv.org/abs/1803.07246
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Improving GANs Using Optimal Transport
AU  - Salimans, Tim
AU  - Zhang, Han
AU  - Radford, Alec
AU  - Metaxas, Dimitris
AB  - We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.
DA  - 2018/03/14/
PY  - 2018
DO  - 10.48550/arXiv.1803.05573
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.05573
Y2  - 2024/07/02/12:32:23
L1  - https://arxiv.org/pdf/1803.05573.pdf
L2  - https://arxiv.org/abs/1803.05573
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - On First-Order Meta-Learning Algorithms
AU  - Nichol, Alex
AU  - Achiam, Joshua
AU  - Schulman, John
AB  - This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.
DA  - 2018/10/22/
PY  - 2018
DO  - 10.48550/arXiv.1803.02999
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.02999
Y2  - 2024/07/02/12:32:44
L1  - https://arxiv.org/pdf/1803.02999.pdf
L2  - https://arxiv.org/abs/1803.02999
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Some Considerations on Learning to Explore via Meta-Reinforcement Learning
AU  - Stadie, Bradly C.
AU  - Yang, Ge
AU  - Houthooft, Rein
AU  - Chen, Xi
AU  - Duan, Yan
AU  - Wu, Yuhuai
AU  - Abbeel, Pieter
AU  - Sutskever, Ilya
AB  - We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\text{RL}^2$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-$\text{RL}^2$ deliver better performance on tasks where exploration is important.
DA  - 2019/01/11/
PY  - 2019
DO  - 10.48550/arXiv.1803.01118
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.01118
Y2  - 2024/07/02/12:33:22
L1  - https://arxiv.org/pdf/1803.01118.pdf
L2  - https://arxiv.org/abs/1803.01118
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research
AU  - Plappert, Matthias
AU  - Andrychowicz, Marcin
AU  - Ray, Alex
AU  - McGrew, Bob
AU  - Baker, Bowen
AU  - Powell, Glenn
AU  - Schneider, Jonas
AU  - Tobin, Josh
AU  - Chociej, Maciek
AU  - Welinder, Peter
AU  - Kumar, Vikash
AU  - Zaremba, Wojciech
AB  - The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick & place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.
DA  - 2018/03/10/
PY  - 2018
DO  - 10.48550/arXiv.1802.09464
DP  - arXiv.org
PB  - arXiv
ST  - Multi-Goal Reinforcement Learning
UR  - http://arxiv.org/abs/1802.09464
Y2  - 2024/07/02/12:33:37
L1  - https://arxiv.org/pdf/1802.09464.pdf
L2  - https://arxiv.org/abs/1802.09464
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Interpretable and Pedagogical Examples
AU  - Milli, Smitha
AU  - Abbeel, Pieter
AU  - Mordatch, Igor
AB  - Teachers intentionally pick the most informative examples to show their students. However, if the teacher and student are neural networks, the examples that the teacher network learns to give, although effective at teaching the student, are typically uninterpretable. We show that training the student and teacher iteratively, rather than jointly, can produce interpretable teaching strategies. We evaluate interpretability by (1) measuring the similarity of the teacher's emergent strategies to intuitive strategies in each domain and (2) conducting human experiments to evaluate how effective the teacher's strategies are at teaching humans. We show that the teacher network learns to select or generate interpretable, pedagogical examples to teach rule-based, probabilistic, boolean, and hierarchical concepts.
DA  - 2018/02/14/
PY  - 2018
DO  - 10.48550/arXiv.1711.00694
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1711.00694
Y2  - 2024/07/02/12:33:55
L1  - https://arxiv.org/pdf/1711.00694.pdf
L2  - https://arxiv.org/abs/1711.00694
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - DeepType: Multilingual Entity Linking by Neural Type System Evolution
AU  - Raiman, Jonathan
AU  - Raiman, Olivier
AB  - The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.
DA  - 2018/02/03/
PY  - 2018
DO  - 10.48550/arXiv.1802.01021
DP  - arXiv.org
PB  - arXiv
ST  - DeepType
UR  - http://arxiv.org/abs/1802.01021
Y2  - 2024/07/02/12:34:11
L1  - https://arxiv.org/pdf/1802.01021.pdf
L2  - https://arxiv.org/abs/1802.01021
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Learning Sparse Neural Networks through $L_0$ Regularization
AU  - Louizos, Christos
AU  - Welling, Max
AU  - Kingma, Diederik P.
AB  - We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \emph{hard concrete} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.
DA  - 2018/06/22/
PY  - 2018
DO  - 10.48550/arXiv.1712.01312
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1712.01312
Y2  - 2024/07/02/12:35:10
L1  - https://arxiv.org/pdf/1712.01312.pdf
L2  - https://arxiv.org/abs/1712.01312
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Meta Learning Shared Hierarchies
AU  - Frans, Kevin
AU  - Ho, Jonathan
AU  - Chen, Xi
AU  - Abbeel, Pieter
AU  - Schulman, John
AB  - We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.
DA  - 2017/10/26/
PY  - 2017
DO  - 10.48550/arXiv.1710.09767
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1710.09767
Y2  - 2024/07/02/12:35:27
L1  - https://arxiv.org/pdf/1710.09767.pdf
L2  - https://arxiv.org/abs/1710.09767
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Sim-to-Real Transfer of Robotic Control with Dynamics Randomization
AU  - Peng, Xue Bin
AU  - Andrychowicz, Marcin
AU  - Zaremba, Wojciech
AU  - Abbeel, Pieter
AB  - Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.
C3  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
DA  - 2018/05//
PY  - 2018
DO  - 10.1109/ICRA.2018.8460528
DP  - arXiv.org
SP  - 3803
EP  - 3810
UR  - http://arxiv.org/abs/1710.06537
Y2  - 2024/07/02/12:35:52
L1  - https://arxiv.org/pdf/1710.06537.pdf
L2  - https://arxiv.org/abs/1710.06537
KW  - Computer Science - Robotics
KW  - Electrical Engineering and Systems Science - Systems and Control
ER  - 

TY  - GEN
TI  - Domain Randomization and Generative Models for Robotic Grasping
AU  - Tobin, Joshua
AU  - Biewald, Lukas
AU  - Duan, Rocky
AU  - Andrychowicz, Marcin
AU  - Handa, Ankur
AU  - Kumar, Vikash
AU  - McGrew, Bob
AU  - Schneider, Jonas
AU  - Welinder, Peter
AU  - Zaremba, Wojciech
AU  - Abbeel, Pieter
AB  - Deep learning-based robotic grasping has made significant progress thanks to algorithmic improvements and increased data availability. However, state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances, and as a result generalization can be a challenge. In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects. Since the distribution of successful grasps for a given object can be highly multimodal, we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution over possible grasps. This model allows us to sample grasps efficiently at test time (or avoid sampling entirely). We evaluate our model architecture and data generation pipeline in simulation and the real world. We find we can achieve a $>$90% success rate on previously unseen realistic objects at test time in simulation despite having only been trained on random objects. We also demonstrate an 80% success rate on real-world grasp attempts despite having only been trained on random simulated objects.
DA  - 2018/04/03/
PY  - 2018
DO  - 10.48550/arXiv.1710.06425
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1710.06425
Y2  - 2024/07/02/12:37:13
L1  - https://arxiv.org/pdf/1710.06425.pdf
L2  - https://arxiv.org/abs/1710.06425
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Emergent Complexity via Multi-Agent Competition
AU  - Bansal, Trapit
AU  - Pachocki, Jakub
AU  - Sidor, Szymon
AU  - Sutskever, Ilya
AU  - Mordatch, Igor
AB  - Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX
DA  - 2018/03/14/
PY  - 2018
DO  - 10.48550/arXiv.1710.03748
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1710.03748
Y2  - 2024/07/02/12:37:27
L1  - https://arxiv.org/pdf/1710.03748.pdf
L2  - https://arxiv.org/abs/1710.03748
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments
AU  - Al-Shedivat, Maruan
AU  - Bansal, Trapit
AU  - Burda, Yuri
AU  - Sutskever, Ilya
AU  - Mordatch, Igor
AU  - Abbeel, Pieter
AB  - Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.
DA  - 2018/02/23/
PY  - 2018
DO  - 10.48550/arXiv.1710.03641
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1710.03641
Y2  - 2024/07/02/12:37:40
L1  - https://arxiv.org/pdf/1710.03641.pdf
L2  - https://arxiv.org/abs/1710.03641
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning with Opponent-Learning Awareness
AU  - Foerster, Jakob N.
AU  - Chen, Richard Y.
AU  - Al-Shedivat, Maruan
AU  - Whiteson, Shimon
AU  - Abbeel, Pieter
AU  - Mordatch, Igor
AB  - Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.
DA  - 2018/09/19/
PY  - 2018
DO  - 10.48550/arXiv.1709.04326
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1709.04326
Y2  - 2024/07/02/12:38:00
L1  - https://arxiv.org/pdf/1709.04326.pdf
L2  - https://arxiv.org/abs/1709.04326
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
AU  - Wu, Yuhuai
AU  - Mansimov, Elman
AU  - Liao, Shun
AU  - Grosse, Roger
AU  - Ba, Jimmy
AB  - In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines
DA  - 2017/08/18/
PY  - 2017
DO  - 10.48550/arXiv.1708.05144
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1708.05144
Y2  - 2024/07/02/12:38:21
L1  - https://arxiv.org/pdf/1708.05144.pdf
L2  - https://arxiv.org/abs/1708.05144
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Parameter Space Noise for Exploration
AU  - Plappert, Matthias
AU  - Houthooft, Rein
AU  - Dhariwal, Prafulla
AU  - Sidor, Szymon
AU  - Chen, Richard Y.
AU  - Chen, Xi
AU  - Asfour, Tamim
AU  - Abbeel, Pieter
AU  - Andrychowicz, Marcin
AB  - Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.
DA  - 2018/01/31/
PY  - 2018
DO  - 10.48550/arXiv.1706.01905
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1706.01905
Y2  - 2024/07/02/12:38:36
L1  - https://arxiv.org/pdf/1706.01905.pdf
L2  - https://arxiv.org/abs/1706.01905
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Synthesizing Robust Adversarial Examples
AU  - Athalye, Anish
AU  - Engstrom, Logan
AU  - Ilyas, Andrew
AU  - Kwok, Kevin
AB  - Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.
DA  - 2018/06/07/
PY  - 2018
DO  - 10.48550/arXiv.1707.07397
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1707.07397
Y2  - 2024/07/02/12:39:04
L1  - https://arxiv.org/pdf/1707.07397.pdf
L2  - https://arxiv.org/abs/1707.07397
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Hindsight Experience Replay
AU  - Andrychowicz, Marcin
AU  - Wolski, Filip
AU  - Ray, Alex
AU  - Schneider, Jonas
AU  - Fong, Rachel
AU  - Welinder, Peter
AU  - McGrew, Bob
AU  - Tobin, Josh
AU  - Abbeel, Pieter
AU  - Zaremba, Wojciech
AB  - Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.
DA  - 2018/02/23/
PY  - 2018
DO  - 10.48550/arXiv.1707.01495
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1707.01495
Y2  - 2024/07/02/12:39:14
L1  - https://arxiv.org/pdf/1707.01495.pdf
L2  - https://arxiv.org/abs/1707.01495
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Teacher-Student Curriculum Learning
AU  - Matiisen, Tambet
AU  - Oliver, Avital
AU  - Cohen, Taco
AU  - Schulman, John
AB  - We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.
DA  - 2017/11/29/
PY  - 2017
DO  - 10.48550/arXiv.1707.00183
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1707.00183
Y2  - 2024/07/02/12:39:24
L1  - https://arxiv.org/pdf/1707.00183.pdf
L2  - https://arxiv.org/abs/1707.00183
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
AU  - Lowe, Ryan
AU  - Wu, Yi
AU  - Tamar, Aviv
AU  - Harb, Jean
AU  - Abbeel, Pieter
AU  - Mordatch, Igor
AB  - We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.
DA  - 2020/03/14/
PY  - 2020
DO  - 10.48550/arXiv.1706.02275
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1706.02275
Y2  - 2024/07/02/12:39:40
L1  - https://arxiv.org/pdf/1706.02275.pdf
L2  - https://arxiv.org/abs/1706.02275
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World
AU  - Tobin, Josh
AU  - Fong, Rachel
AU  - Ray, Alex
AU  - Schneider, Jonas
AU  - Zaremba, Wojciech
AU  - Abbeel, Pieter
AB  - Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.
DA  - 2017/03/20/
PY  - 2017
DO  - 10.48550/arXiv.1703.06907
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.06907
Y2  - 2024/07/02/12:40:40
L1  - https://arxiv.org/pdf/1703.06907.pdf
L2  - https://arxiv.org/abs/1703.06907
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - One-Shot Imitation Learning
AU  - Duan, Yan
AU  - Andrychowicz, Marcin
AU  - Stadie, Bradly C.
AU  - Ho, Jonathan
AU  - Schneider, Jonas
AU  - Sutskever, Ilya
AU  - Abbeel, Pieter
AU  - Zaremba, Wojciech
AB  - Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .
DA  - 2017/12/04/
PY  - 2017
DO  - 10.48550/arXiv.1703.07326
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.07326
Y2  - 2024/07/02/12:40:51
L1  - https://arxiv.org/pdf/1703.07326.pdf
L2  - https://arxiv.org/abs/1703.07326
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Equivalence Between Policy Gradients and Soft Q-Learning
AU  - Schulman, John
AU  - Chen, Xi
AU  - Abbeel, Pieter
AB  - Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\epsilon$-greedy exploration schedule.
DA  - 2018/10/14/
PY  - 2018
DO  - 10.48550/arXiv.1704.06440
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1704.06440
Y2  - 2024/07/02/12:41:05
L1  - https://arxiv.org/pdf/1704.06440.pdf
L2  - https://arxiv.org/abs/1704.06440
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Stochastic Neural Networks for Hierarchical Reinforcement Learning
AU  - Florensa, Carlos
AU  - Duan, Yan
AU  - Abbeel, Pieter
AB  - Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.
DA  - 2017/04/10/
PY  - 2017
DO  - 10.48550/arXiv.1704.03012
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1704.03012
Y2  - 2024/07/02/12:41:19
L1  - https://arxiv.org/pdf/1704.03012.pdf
L2  - https://arxiv.org/abs/1704.03012
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Learning to Generate Reviews and Discovering Sentiment
AU  - Radford, Alec
AU  - Jozefowicz, Rafal
AU  - Sutskever, Ilya
AB  - We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.
DA  - 2017/04/06/
PY  - 2017
DO  - 10.48550/arXiv.1704.01444
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1704.01444
Y2  - 2024/07/02/12:41:37
L1  - https://arxiv.org/pdf/1704.01444.pdf
L2  - https://arxiv.org/abs/1704.01444
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Evolution Strategies as a Scalable Alternative to Reinforcement Learning
AU  - Salimans, Tim
AU  - Ho, Jonathan
AU  - Chen, Xi
AU  - Sidor, Szymon
AU  - Sutskever, Ilya
AB  - We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.
DA  - 2017/09/07/
PY  - 2017
DO  - 10.48550/arXiv.1703.03864
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.03864
Y2  - 2024/07/02/12:41:52
L1  - https://arxiv.org/pdf/1703.03864.pdf
L2  - https://arxiv.org/abs/1703.03864
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Emergence of Grounded Compositional Language in Multi-Agent Populations
AU  - Mordatch, Igor
AU  - Abbeel, Pieter
AB  - By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.
DA  - 2018/07/24/
PY  - 2018
DO  - 10.48550/arXiv.1703.04908
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.04908
Y2  - 2024/07/02/12:42:59
L1  - https://arxiv.org/pdf/1703.04908.pdf
L2  - https://arxiv.org/abs/1703.04908
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Prediction and Control with Temporal Segment Models
AU  - Mishra, Nikhil
AU  - Abbeel, Pieter
AU  - Mordatch, Igor
AB  - We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.
DA  - 2017/07/13/
PY  - 2017
DO  - 10.48550/arXiv.1703.04070
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.04070
Y2  - 2024/07/02/12:43:09
L1  - https://arxiv.org/pdf/1703.04070.pdf
L2  - https://arxiv.org/abs/1703.04070
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Third-Person Imitation Learning
AU  - Stadie, Bradly C.
AU  - Abbeel, Pieter
AU  - Sutskever, Ilya
AB  - Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.
DA  - 2019/09/22/
PY  - 2019
DO  - 10.48550/arXiv.1703.01703
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1703.01703
Y2  - 2024/07/02/12:43:22
L1  - https://arxiv.org/pdf/1703.01703.pdf
L2  - https://arxiv.org/abs/1703.01703
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Quantitative Analysis of Decoder-Based Generative Models
AU  - Wu, Yuhuai
AU  - Burda, Yuri
AU  - Salakhutdinov, Ruslan
AU  - Grosse, Roger
AB  - The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https://github.com/tonywu95/eval_gen. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.
DA  - 2017/06/06/
PY  - 2017
DO  - 10.48550/arXiv.1611.04273
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1611.04273
Y2  - 2024/07/02/12:43:58
L1  - https://arxiv.org/pdf/1611.04273.pdf
L2  - https://arxiv.org/abs/1611.04273
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models
AU  - Finn, Chelsea
AU  - Christiano, Paul
AU  - Abbeel, Pieter
AU  - Levine, Sergey
AB  - Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.
DA  - 2016/11/25/
PY  - 2016
DO  - 10.48550/arXiv.1611.03852
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1611.03852
Y2  - 2024/07/02/12:44:16
L1  - https://arxiv.org/pdf/1611.03852.pdf
L2  - https://arxiv.org/abs/1611.03852
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model
AU  - Christiano, Paul
AU  - Shah, Zain
AU  - Mordatch, Igor
AU  - Schneider, Jonas
AU  - Blackwell, Trevor
AU  - Tobin, Joshua
AU  - Abbeel, Pieter
AU  - Zaremba, Wojciech
AB  - Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.
DA  - 2016/10/11/
PY  - 2016
DO  - 10.48550/arXiv.1610.03518
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1610.03518
Y2  - 2024/07/02/12:45:27
L1  - https://arxiv.org/pdf/1610.03518.pdf
L2  - https://arxiv.org/abs/1610.03518
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Electrical Engineering and Systems Science - Systems and Control
ER  - 

TY  - GEN
TI  - Levels of AGI for Operationalizing Progress on the Path to AGI
AU  - Morris, Meredith Ringel
AU  - Sohl-dickstein, Jascha
AU  - Fiedel, Noah
AU  - Warkentin, Tris
AU  - Dafoe, Allan
AU  - Faust, Aleksandra
AU  - Farabet, Clement
AU  - Legg, Shane
AB  - We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. With these principles in mind, we propose "Levels of AGI" based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.
DA  - 2024/06/05/
PY  - 2024
DO  - 10.48550/arXiv.2311.02462
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.02462
Y2  - 2024/07/03/09:54:26
L1  - https://arxiv.org/pdf/2311.02462.pdf
L2  - https://arxiv.org/abs/2311.02462
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Don't trust your eyes: on the (un)reliability of feature visualizations
AU  - Geirhos, Robert
AU  - Zimmermann, Roland S.
AU  - Bilodeau, Blair
AU  - Brendel, Wieland
AU  - Kim, Been
AB  - How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.
DA  - 2024/06/06/
PY  - 2024
DO  - 10.48550/arXiv.2306.04719
DP  - arXiv.org
PB  - arXiv
ST  - Don't trust your eyes
UR  - http://arxiv.org/abs/2306.04719
Y2  - 2024/07/03/09:55:18
L1  - https://arxiv.org/pdf/2306.04719.pdf
L2  - https://arxiv.org/abs/2306.04719
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Quantitative Biology - Neurons and Cognition
ER  - 

TY  - CONF
TI  - A Robot Walks into a Bar: Can Language Models Serve as Creativity SupportTools for Comedy? An Evaluation of LLMs’ Humour Alignment with Comedians
AU  - Mirowski, Piotr
AU  - Love, Juliette
AU  - Mathewson, Kory
AU  - Mohamed, Shakir
T3  - FAccT '24
AB  - We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on “AI x Comedy” conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians’ motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to “cruise ship comedy material from the 1950s, but a bit less racist”. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, “offensive” language as a practice of resistance, satire and “punching up”. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists’ needs. Warning: this study may contain offensive language and discusses self-harm.
C1  - New York, NY, USA
C3  - Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2024/06/05/
PY  - 2024
DO  - 10.1145/3630106.3658993
DP  - ACM Digital Library
SP  - 1622
EP  - 1636
PB  - Association for Computing Machinery
SN  - 9798400704505
ST  - A Robot Walks into a Bar
UR  - https://doi.org/10.1145/3630106.3658993
Y2  - 2024/07/03/
L1  - https://arxiv.org/pdf/2405.20956
ER  - 

TY  - BOOK
TI  - An introduction to universal artificial intelligence
AU  - Hutter, Marcus
AU  - Quarel, David
AU  - Catt, Elliot
AB  - This book provides a gentle introduction to Universal Artificial Intelligence (UAI), a theory that provides a formal underpinning of what it means for an agent to act intelligently in an unknown environment. First presented in book (Hutter, 2004), UAI offers a framework in which virtually all AI problems can be formulated, and a theory of how to solve them. UAI unifies ideas from sequential decision theory, Bayesian inference, and algorithmic information theory to construct AIXI, an optimal reinforcement learning agent that learns to act optimally in unknown environments. AIXI is the theoretical gold standard for intelligent behavior.
     The book covers both the theoretical and practical aspects of UAI. Bayesian updating can be done efficiently with context tree weighting, and planning can be approximated by sampling with Monte Carlo tree search. It provides algorithms for the reader to implement, and experimental results to compare against. These algorithms are used to approximate AIXI. The book ends with a philosophical discussion of Artificial General Intelligence: Can super-intelligent agents even be constructed? Is it inevitable that they will be constructed, and what are the potential consequences?
     This text is suitable for late undergraduate students. It provides an extensive chapter to fill in the required mathematics, probability, information, and computability theory background.
     The book is not a second edition of Hutter (2004) UAI. It is a prequel (much more gentle introduction to the prerequisite topics) and a sequel (progress in the last 20 years). It is easier (more detailed less dense explanations) and harder (covers some advanced topics such a CTW and GoT). It includes various practical approximations, pseudo-code, and links to implementations of some (in Java and C).
DA  - 2024///
PY  - 2024
UR  - http://www.hutter1.net/ai/uaibook2.htm
KW  - reinforcement learning
KW  - Artificial general intelligence
KW  - AGI-safety
KW  - algorithmic information theory
KW  - approximation/implementation/application
KW  - Bayes mixture distributions
KW  - context tree weighting
KW  - games and multi-agent systems
KW  - philosophy of AI.
KW  - rational agents
KW  - sequential decision theory
KW  - universal intelligent agents
KW  - universal sequence prediction
ER  - 

TY  - GEN
TI  - Language Modeling Is Compression
AU  - Delétang, Grégoire
AU  - Ruoss, Anian
AU  - Duquenne, Paul-Ambroise
AU  - Catt, Elliot
AU  - Genewein, Tim
AU  - Mattern, Christopher
AU  - Grau-Moya, Jordi
AU  - Wenliang, Li Kevin
AU  - Aitchison, Matthew
AU  - Orseau, Laurent
AU  - Hutter, Marcus
AU  - Veness, Joel
AB  - It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.
DA  - 2024/03/18/
PY  - 2024
DO  - 10.48550/arXiv.2309.10668
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2309.10668
Y2  - 2024/07/03/09:58:23
L1  - https://arxiv.org/pdf/2309.10668.pdf
L2  - https://arxiv.org/abs/2309.10668
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Information Theory
ER  - 

TY  - CONF
TI  - $\pi$2vec: Policy Representation with Successor Features
AU  - Scarpellini, Gianluca
AU  - Konyushkova, Ksenia
AU  - Fantacci, Claudio
AU  - Paine, Thomas
AU  - Chen, Yutian
AU  - Denil, Misha
T2  - The Twelfth International Conference on Learning Representations
AB  - This paper introduces $\pi$2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. $\pi$2vec represents the behavior of policies by capturing the statistics of the features from a pretrained model with the help of successor feature framework. We focus on the offline setting where policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on $\pi$2vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - $\pi$2vec
UR  - https://openreview.net/forum?id=o5Bqa4o5Mi
Y2  - 2024/07/03/09:59:05
L1  - https://openreview.net/pdf?id=o5Bqa4o5Mi
ER  - 

TY  - CONF
TI  - Teach LLMs to Phish: Stealing Private Information from Language Models
AU  - Panda, Ashwinee
AU  - Choquette-Choo, Christopher A.
AU  - Zhang, Zhengming
AU  - Yang, Yaoqing
AU  - Mittal, Prateek
T2  - The Twelfth International Conference on Learning Representations
AB  - When large language models are trained on private data, it can be a \textit{significant} privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new \emph{practical} data extraction attack that we call ``neural phishing''. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of $10\%$ attack success rates, at times, as high as $50\%$. Our attack assumes only that an adversary can insert as few as $10$s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Teach LLMs to Phish
UR  - https://openreview.net/forum?id=qo21ZlfNu6
Y2  - 2024/07/03/09:59:49
L1  - https://openreview.net/pdf?id=qo21ZlfNu6
ER  - 

TY  - GEN
TI  - Position: Leverage Foundational Models for Black-Box Optimization
AU  - Song, Xingyou
AU  - Tian, Yingtao
AU  - Lange, Robert Tjarko
AU  - Lee, Chansoo
AU  - Tang, Yujin
AU  - Chen, Yutian
AB  - Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research. However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.
DA  - 2024/05/09/
PY  - 2024
DO  - 10.48550/arXiv.2405.03547
DP  - arXiv.org
PB  - arXiv
ST  - Position
UR  - http://arxiv.org/abs/2405.03547
Y2  - 2024/07/03/10:00:00
L1  - https://arxiv.org/pdf/2405.03547.pdf
L2  - https://arxiv.org/abs/2405.03547
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Holistic Safety and Responsibility Evaluations of Advanced AI Models
AU  - Weidinger, Laura
AU  - Barnhart, Joslyn
AU  - Brennan, Jenny
AU  - Butterfield, Christina
AU  - Young, Susie
AU  - Hawkins, Will
AU  - Hendricks, Lisa Anne
AU  - Comanescu, Ramona
AU  - Chang, Oscar
AU  - Rodriguez, Mikel
AU  - Beroshi, Jennifer
AU  - Bloxwich, Dawn
AU  - Proleev, Lev
AU  - Chen, Jilin
AU  - Farquhar, Sebastian
AU  - Ho, Lewis
AU  - Gabriel, Iason
AU  - Dafoe, Allan
AU  - Isaac, William
AB  - Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.
DA  - 2024/04/22/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2404.14068
Y2  - 2024/07/03/10:00:21
L1  - https://arxiv.org/pdf/2404.14068
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Gecko: Versatile Text Embeddings Distilled from Large Language Models
AU  - Lee, Jinhyuk
AU  - Dai, Zhuyun
AU  - Ren, Xiaoqi
AU  - Chen, Blair
AU  - Cer, Daniel
AU  - Cole, Jeremy R.
AU  - Hui, Kai
AU  - Boratko, Michael
AU  - Kapadia, Rajvi
AU  - Ding, Wen
AU  - Luan, Yi
AU  - Duddu, Sai Meher Karthik
AU  - Abrego, Gustavo Hernandez
AU  - Shi, Weiqiang
AU  - Gupta, Nithi
AU  - Kusupati, Aditya
AU  - Jain, Prateek
AU  - Jonnalagadda, Siddhartha Reddy
AU  - Chang, Ming-Wei
AU  - Naim, Iftekhar
AB  - We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.
DA  - 2024/03/29/
PY  - 2024
DO  - 10.48550/arXiv.2403.20327
DP  - arXiv.org
PB  - arXiv
ST  - Gecko
UR  - http://arxiv.org/abs/2403.20327
Y2  - 2024/07/03/10:01:37
L1  - https://arxiv.org/pdf/2403.20327.pdf
L2  - https://arxiv.org/abs/2403.20327
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Few-Shot Recalibration of Language Models
AU  - Li, Xiang Lisa
AU  - Khandelwal, Urvashi
AU  - Guu, Kelvin
AB  - Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
DA  - 2024/03/27/
PY  - 2024
DO  - 10.48550/arXiv.2403.18286
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.18286
Y2  - 2024/07/03/10:01:51
L1  - https://arxiv.org/pdf/2403.18286.pdf
L2  - https://arxiv.org/abs/2403.18286
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Long-form factuality in large language models
AU  - Wei, Jerry
AU  - Yang, Chengrun
AU  - Song, Xinying
AU  - Lu, Yifeng
AU  - Hu, Nathan
AU  - Huang, Jie
AU  - Tran, Dustin
AU  - Peng, Daiyi
AU  - Liu, Ruibo
AU  - Huang, Da
AU  - Du, Cosmo
AU  - Le, Quoc V.
AB  - Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.
DA  - 2024/04/03/
PY  - 2024
DO  - 10.48550/arXiv.2403.18802
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.18802
Y2  - 2024/07/03/10:02:04
L1  - https://arxiv.org/pdf/2403.18802.pdf
L2  - https://arxiv.org/abs/2403.18802
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Evaluating Frontier Models for Dangerous Capabilities
AU  - Phuong, Mary
AU  - Aitchison, Matthew
AU  - Catt, Elliot
AU  - Cogan, Sarah
AU  - Kaskasoli, Alexandre
AU  - Krakovna, Victoria
AU  - Lindner, David
AU  - Rahtz, Matthew
AU  - Assael, Yannis
AU  - Hodkinson, Sarah
AU  - Howard, Heidi
AU  - Lieberum, Tom
AU  - Kumar, Ramana
AU  - Raad, Maria Abi
AU  - Webson, Albert
AU  - Ho, Lewis
AU  - Lin, Sharon
AU  - Farquhar, Sebastian
AU  - Hutter, Marcus
AU  - Deletang, Gregoire
AU  - Ruoss, Anian
AU  - El-Sayed, Seliem
AU  - Brown, Sasha
AU  - Dragan, Anca
AU  - Shah, Rohin
AU  - Dafoe, Allan
AU  - Shevlane, Toby
AB  - To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new "dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.
DA  - 2024/04/05/
PY  - 2024
DO  - 10.48550/arXiv.2403.13793
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.13793
Y2  - 2024/07/03/10:02:16
L1  - https://arxiv.org/pdf/2403.13793.pdf
L2  - https://arxiv.org/abs/2403.13793
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A General Theoretical Paradigm to Understand Learning from Human Preferences
AU  - Azar, Mohammad Gheshlaghi
AU  - Rowland, Mark
AU  - Piot, Bilal
AU  - Guo, Daniel
AU  - Calandriello, Daniele
AU  - Valko, Michal
AU  - Munos, Rémi
AB  - The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.
DA  - 2023/11/21/
PY  - 2023
DO  - 10.48550/arXiv.2310.12036
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.12036
Y2  - 2024/07/03/10:02:53
L1  - https://arxiv.org/pdf/2310.12036.pdf
L2  - https://arxiv.org/abs/2310.12036
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Demonstration-Regularized RL
AU  - Tiapkin, Daniil
AU  - Belomestny, Denis
AU  - Calandriello, Daniele
AU  - Moulines, Eric
AU  - Naumov, Alexey
AU  - Perrault, Pierre
AU  - Valko, Michal
AU  - Menard, Pierre
AB  - Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\widetilde{O}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in finite and $\widetilde{O}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works.
DA  - 2024/06/10/
PY  - 2024
DO  - 10.48550/arXiv.2310.17303
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.17303
Y2  - 2024/07/03/10:03:33
L1  - https://arxiv.org/pdf/2310.17303.pdf
L2  - https://arxiv.org/abs/2310.17303
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - AtP*: An efficient and scalable method for localizing LLM behaviour to components
AU  - Kramár, János
AU  - Lieberum, Tom
AU  - Shah, Rohin
AU  - Nanda, Neel
AB  - Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.
DA  - 2024/03/01/
PY  - 2024
DO  - 10.48550/arXiv.2403.00745
DP  - arXiv.org
PB  - arXiv
ST  - AtP*
UR  - http://arxiv.org/abs/2403.00745
Y2  - 2024/07/03/10:03:58
L1  - https://arxiv.org/pdf/2403.00745.pdf
L2  - https://arxiv.org/abs/2403.00745
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - How aligned are different alignment metrics?
AU  - Ahlert, Jannis
AU  - Klein, Thomas
AU  - Wichmann, Felix A.
AU  - Geirhos, Robert
T2  - ICLR 2024 Workshop on Representational Alignment
AB  - In recent years, various methods and benchmarks have been proposed to empirically evaluate the alignment of artificial neural networks to human neural and behavioral data. But how aligned are different alignment metrics? To answer this question, we here analyze visual data from Brain-Score (Schrimpf et al., 2018), including metrics from the model-vs-human toolbox (Geirhos et al., 2021), together with human feature alignment (Linsley et al., 2018; Fel et al., 2022) and human similarity judgements (Muttenthaler et al., 2022). We find that pairwise correlations between neural scores and behavioral scores are quite low and sometimes even negative. For instance, the average correlation between those 80 models on Brain-Score that were fully evaluated on all 69 alignment metrics we considered is only 0.198. Assuming that all of the employed metrics are sound, this implies that alignment with human perception may best be thought of as a multidimensional concept, with different methods measuring fundamentally different aspects. Our results underline the importance of integrative benchmarking, but also raise questions about how to correctly combine and aggregate individual metrics. Aggregating by taking the arithmetic average, as done in Brain-Score, leads to the overall performance currently being dominated by behavior (95.25% explained variance) while the neural predictivity plays a less important role (only 33.33% explained variance). As a first step towards making sure that different alignment metrics all contribute fairly towards an integrative benchmark score, we therefore conclude by comparing three different aggregation options.
DA  - 2024/03/02/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=cHlKB28bjV
Y2  - 2024/07/03/10:04:15
L1  - https://openreview.net/pdf?id=cHlKB28bjV
ER  - 

TY  - GEN
TI  - Approximating the Core via Iterative Coalition Sampling
AU  - Gemp, Ian
AU  - Lanctot, Marc
AU  - Marris, Luke
AU  - Mao, Yiran
AU  - Duéñez-Guzmán, Edgar
AU  - Perrin, Sarah
AU  - Gyorgy, Andras
AU  - Elie, Romuald
AU  - Piliouras, Georgios
AU  - Kaisers, Michael
AU  - Hennes, Daniel
AU  - Bullard, Kalesha
AU  - Larson, Kate
AU  - Bachrach, Yoram
AB  - The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.
DA  - 2024/02/06/
PY  - 2024
DO  - 10.48550/arXiv.2402.03928
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.03928
Y2  - 2024/07/03/10:05:17
L1  - https://arxiv.org/pdf/2402.03928.pdf
L2  - https://arxiv.org/abs/2402.03928
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding
AU  - Evans, Talfan
AU  - Pathak, Shreya
AU  - Merzic, Hamza
AU  - Schwarz, Jonathan
AU  - Tanno, Ryutaro
AU  - Henaff, Olivier J.
AB  - Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate “learnability” scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly-trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-prioritization scheme to be complementary with recent data-curation and learning objectives, yielding a new state-of-the-art in several multimodal transfer tasks.
DA  - 2024/02/14/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Bad Students Make Great Teachers
UR  - http://arxiv.org/abs/2312.05328
Y2  - 2024/07/03/10:06:14
L1  - https://arxiv.org/pdf/2312.05328
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Self-supervised video pretraining yields human-aligned visual representations
AU  - Parthasarathy, Nikhil
AU  - Eslami, S. M. Ali
AU  - Carreira, João
AU  - Hénaff, Olivier J.
AB  - Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformations than image-, video-, and adversarially-trained ones. Finally, VITO’s predictions are strongly aligned with human judgements, surpassing models that were specifically trained for that purpose. Together, these results suggest that video pretraining could be a simple way of learning unified, robust, and human-aligned representations of the visual world.
DA  - 2023/07/25/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2210.06433
Y2  - 2024/07/03/10:06:27
L1  - https://arxiv.org/pdf/2210.06433
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Set Learning for Accurate and Calibrated Models
AU  - Muttenthaler, Lukas
AU  - Vandermeulen, Robert A.
AU  - Zhang, Qiuyi
AU  - Unterthiner, Thomas
AU  - Müller, Klaus-Robert
AB  - Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We demonstrate this in extensive experimental analyses and provide a mathematical theory to interpret our findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and a trained model can be applied to single examples at inference time, without significant run-time overhead or architecture changes.
DA  - 2024/02/12/
PY  - 2024
DO  - 10.48550/arXiv.2307.02245
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.02245
Y2  - 2024/07/03/10:06:35
L1  - https://arxiv.org/pdf/2307.02245.pdf
L2  - https://arxiv.org/abs/2307.02245
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Information Theory
ER  - 

TY  - CONF
TI  - Intriguing Properties of Generative Classifiers
AU  - Jaini, Priyank
AU  - Clark, Kevin
AU  - Geirhos, Robert
T2  - The Twelfth International Conference on Learning Representations
AB  - What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=rmg0qMKYRQ
Y2  - 2024/07/03/10:06:50
L1  - https://openreview.net/pdf?id=rmg0qMKYRQ
ER  - 

TY  - GEN
TI  - Frozen Feature Augmentation for Few-Shot Image Classification
AU  - Bär, Andreas
AU  - Houlsby, Neil
AU  - Dehghani, Mostafa
AU  - Kumar, Manoj
AB  - Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called 'frozen features', leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed 'frozen feature augmentation (FroFA)', covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.
DA  - 2024/03/15/
PY  - 2024
DO  - 10.48550/arXiv.2403.10519
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.10519
Y2  - 2024/07/03/10:07:15
L1  - https://arxiv.org/pdf/2403.10519.pdf
L2  - https://arxiv.org/abs/2403.10519
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - A density estimation perspective on learning from pairwise human preferences
AU  - Dumoulin, Vincent
AU  - Johnson, Daniel D.
AU  - Castro, Pablo Samuel
AU  - Larochelle, Hugo
AU  - Dauphin, Yann
T2  - Transactions on Machine Learning Research
AB  - Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification"—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.
DA  - 2023/11/25/
PY  - 2023
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=YH3oERVYjF
Y2  - 2024/07/03/10:07:49
L1  - https://openreview.net/pdf?id=YH3oERVYjF
ER  - 

TY  - GEN
TI  - On Limitations of the Transformer Architecture
AU  - Peng, Binghui
AU  - Narayanan, Srini
AU  - Papadimitriou, Christos
AB  - What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
DA  - 2024/02/26/
PY  - 2024
DO  - 10.48550/arXiv.2402.08164
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.08164
Y2  - 2024/07/03/10:08:19
L1  - https://arxiv.org/pdf/2402.08164.pdf
L2  - https://arxiv.org/abs/2402.08164
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - OmniPred: Language Models as Universal Regressors
AU  - Song, Xingyou
AU  - Li, Oscar
AU  - Lee, Chansoo
AU  - Yang, Bangding
AU  - Peng, Daiyi
AU  - Perel, Sagi
AU  - Chen, Yutian
AB  - Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
DA  - 2024/03/04/
PY  - 2024
DO  - 10.48550/arXiv.2402.14547
DP  - arXiv.org
PB  - arXiv
ST  - OmniPred
UR  - http://arxiv.org/abs/2402.14547
Y2  - 2024/07/03/10:09:45
L1  - https://arxiv.org/pdf/2402.14547.pdf
L2  - https://arxiv.org/abs/2402.14547
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Databases
ER  - 

TY  - CONF
TI  - When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method
AU  - Zhang, Biao
AU  - Liu, Zhongtao
AU  - Cherry, Colin
AU  - Firat, Orhan
T2  - The Twelfth International Conference on Learning Representations
AB  - While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - When Scaling Meets LLM Finetuning
UR  - https://openreview.net/forum?id=5HCnKDeTws
Y2  - 2024/07/03/10:10:09
L1  - https://openreview.net/pdf?id=5HCnKDeTws
ER  - 

TY  - GEN
TI  - Simulacra as Conscious Exotica
AU  - Shanahan, Murray
AB  - The advent of conversational agents with increasingly human-like behaviour throws old philosophical questions into new light. Does it, or could it, ever make sense to speak of AI agents built out of generative language models in terms of consciousness, given that they are "mere" simulacra of human behaviour, and that what they do can be seen as "merely" role play? Drawing on the later writings of Wittgenstein, this paper attempts to tackle this question while avoiding the pitfalls of dualistic thinking.
DA  - 2024/02/19/
PY  - 2024
DO  - 10.48550/arXiv.2402.12422
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.12422
Y2  - 2024/07/03/10:10:30
L1  - https://arxiv.org/pdf/2402.12422.pdf
L2  - https://arxiv.org/abs/2402.12422
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Learning to Learn Faster from Human Feedback with Language Model Predictive Control
AU  - Liang, Jacky
AU  - Xia, Fei
AU  - Yu, Wenhao
AU  - Zeng, Andy
AU  - Arenas, Montserrat Gonzalez
AU  - Attarian, Maria
AU  - Bauza, Maria
AU  - Bennice, Matthew
AU  - Bewley, Alex
AU  - Dostmohamed, Adil
AU  - Fu, Chuyuan Kelly
AU  - Gileadi, Nimrod
AU  - Giustina, Marissa
AU  - Gopalakrishnan, Keerthana
AU  - Hasenclever, Leonard
AU  - Humplik, Jan
AU  - Hsu, Jasmine
AU  - Joshi, Nikhil
AU  - Jyenis, Ben
AU  - Kew, Chase
AU  - Kirmani, Sean
AU  - Lee, Tsang-Wei Edward
AU  - Lee, Kuang-Huei
AU  - Michaely, Assaf Hurwitz
AU  - Moore, Joss
AU  - Oslund, Ken
AU  - Rao, Dushyant
AU  - Ren, Allen
AU  - Tabanpour, Baruch
AU  - Vuong, Quan
AU  - Wahid, Ayzaan
AU  - Xiao, Ted
AU  - Xu, Ying
AU  - Zhuang, Vincent
AU  - Xu, Peng
AU  - Frey, Erik
AU  - Caluwaerts, Ken
AU  - Zhang, Tingnan
AU  - Ichter, Brian
AU  - Tompson, Jonathan
AU  - Takayama, Leila
AU  - Vanhoucke, Vincent
AU  - Shafran, Izhak
AU  - Mataric, Maja
AU  - Sadigh, Dorsa
AU  - Heess, Nicolas
AU  - Rao, Kanishka
AU  - Stewart, Nik
AU  - Tan, Jie
AU  - Parada, Carolina
AB  - Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are viewed as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions is training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: https://robot-teaching.github.io/.
DA  - 2024/05/31/
PY  - 2024
DO  - 10.48550/arXiv.2402.11450
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.11450
Y2  - 2024/07/03/10:10:44
L1  - https://arxiv.org/pdf/2402.11450.pdf
L2  - https://arxiv.org/abs/2402.11450
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
AU  - Lee, Kuang-Huei
AU  - Chen, Xinyun
AU  - Furuta, Hiroki
AU  - Canny, John
AU  - Fischer, Ian
AB  - Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.
DA  - 2024/02/23/
PY  - 2024
DO  - 10.48550/arXiv.2402.09727
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.09727
Y2  - 2024/07/03/10:11:32
L1  - https://arxiv.org/pdf/2402.09727.pdf
L2  - https://arxiv.org/abs/2402.09727
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Information Retrieval
ER  - 

TY  - GEN
TI  - Premise Order Matters in Reasoning with Large Language Models
AU  - Chen, Xinyun
AU  - Chi, Ryan A.
AU  - Wang, Xuezhi
AU  - Zhou, Denny
AB  - Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.
DA  - 2024/05/28/
PY  - 2024
DO  - 10.48550/arXiv.2402.08939
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.08939
Y2  - 2024/07/03/10:12:00
L1  - https://arxiv.org/pdf/2402.08939.pdf
L2  - https://arxiv.org/abs/2402.08939
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs
AU  - Nasiriany, Soroush
AU  - Xia, Fei
AU  - Yu, Wenhao
AU  - Xiao, Ted
AU  - Liang, Jacky
AU  - Dasgupta, Ishita
AU  - Xie, Annie
AU  - Driess, Danny
AU  - Wahid, Ayzaan
AU  - Xu, Zhuo
AU  - Vuong, Quan
AU  - Zhang, Tingnan
AU  - Lee, Tsang-Wei Edward
AU  - Lee, Kuang-Huei
AU  - Xu, Peng
AU  - Kirmani, Sean
AU  - Zhu, Yuke
AU  - Zeng, Andy
AU  - Hausman, Karol
AU  - Heess, Nicolas
AU  - Finn, Chelsea
AU  - Levine, Sergey
AU  - Ichter, Brian
AB  - Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.
DA  - 2024/02/12/
PY  - 2024
DO  - 10.48550/arXiv.2402.07872
DP  - arXiv.org
PB  - arXiv
ST  - PIVOT
UR  - http://arxiv.org/abs/2402.07872
Y2  - 2024/07/03/10:12:56
L1  - https://arxiv.org/pdf/2402.07872.pdf
L2  - https://arxiv.org/abs/2402.07872
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding
AU  - Wang, Zilong
AU  - Zhang, Hao
AU  - Li, Chun-Liang
AU  - Eisenschlos, Julian Martin
AU  - Perot, Vincent
AU  - Wang, Zifeng
AU  - Miculicich, Lesly
AU  - Fujii, Yasuhisa
AU  - Shang, Jingbo
AU  - Lee, Chen-Yu
AU  - Pfister, Tomas
T2  - The Twelfth International Conference on Learning Representations
AB  - Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Chain-of-Table
UR  - https://openreview.net/forum?id=4L0xnS4GQM
Y2  - 2024/07/03/10:13:59
L1  - https://openreview.net/pdf?id=4L0xnS4GQM
ER  - 

TY  - GEN
TI  - Memory Consolidation Enables Long-Context Video Understanding
AU  - Balažević, Ivana
AU  - Shi, Yuge
AU  - Papalampidi, Pinelopi
AU  - Chaabouni, Rahma
AU  - Koppula, Skanda
AU  - Hénaff, Olivier J.
AB  - Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. Instead, we propose to re-purpose existing pretrained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memoryconsolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.
DA  - 2024/05/31/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2402.05861
Y2  - 2024/07/03/10:14:19
L1  - https://arxiv.org/pdf/2402.05861
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
AU  - Gemp, Ian
AU  - Bachrach, Yoram
AU  - Lanctot, Marc
AU  - Patel, Roma
AU  - Dasagi, Vibhavari
AU  - Marris, Luke
AU  - Piliouras, Georgios
AU  - Liu, Siqi
AU  - Tuyls, Karl
AB  - Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.
DA  - 2024/02/06/
PY  - 2024
DO  - 10.48550/arXiv.2402.01704
DP  - arXiv.org
PB  - arXiv
ST  - States as Strings as Strategies
UR  - http://arxiv.org/abs/2402.01704
Y2  - 2024/07/03/10:14:37
L1  - https://arxiv.org/pdf/2402.01704.pdf
L2  - https://arxiv.org/abs/2402.01704
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Self-Discover: Large Language Models Self-Compose Reasoning Structures
AU  - Zhou, Pei
AU  - Pujara, Jay
AU  - Ren, Xiang
AU  - Chen, Xinyun
AU  - Cheng, Heng-Tze
AU  - Le, Quoc V.
AU  - Chi, Ed H.
AU  - Zhou, Denny
AU  - Mishra, Swaroop
AU  - Zheng, Huaixiu Steven
AB  - We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.
DA  - 2024/02/05/
PY  - 2024
DO  - 10.48550/arXiv.2402.03620
DP  - arXiv.org
PB  - arXiv
ST  - Self-Discover
UR  - http://arxiv.org/abs/2402.03620
Y2  - 2024/07/03/10:14:56
L1  - https://arxiv.org/pdf/2402.03620.pdf
L2  - https://arxiv.org/abs/2402.03620
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Fractal Patterns May Illuminate the Success of Next-Token Prediction
AU  - Alabdulmohsin, Ibrahim
AU  - Tran, Vinh Q.
AU  - Dehghani, Mostafa
AB  - We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.7. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.
DA  - 2024/05/22/
PY  - 2024
DO  - 10.48550/arXiv.2402.01825
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.01825
Y2  - 2024/07/03/10:15:06
L1  - https://arxiv.org/pdf/2402.01825.pdf
L2  - https://arxiv.org/abs/2402.01825
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Robust agents learn causal world models
AU  - Richens, Jonathan
AU  - Everitt, Tom
T2  - The Twelfth International Conference on Learning Representations
AB  - It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=pOoKI3ouv1
Y2  - 2024/07/03/10:15:31
L1  - https://openreview.net/pdf?id=pOoKI3ouv1
ER  - 

TY  - GEN
TI  - Efficient Exploration for LLMs
AU  - Dwaracherla, Vikranth
AU  - Asghari, Seyed Mohammad
AU  - Hao, Botao
AU  - Van Roy, Benjamin
AB  - We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
DA  - 2024/06/04/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2402.00396
Y2  - 2024/07/03/10:15:48
L1  - https://arxiv.org/pdf/2402.00396
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Statistics - Methodology
ER  - 

TY  - GEN
TI  - Learning Universal Predictors
AU  - Grau-Moya, Jordi
AU  - Genewein, Tim
AU  - Hutter, Marcus
AU  - Orseau, Laurent
AU  - Delétang, Grégoire
AU  - Catt, Elliot
AU  - Ruoss, Anian
AU  - Wenliang, Li Kevin
AU  - Mattern, Christopher
AU  - Aitchison, Matthew
AU  - Veness, Joel
AB  - Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies.
DA  - 2024/01/26/
PY  - 2024
DO  - 10.48550/arXiv.2401.14953
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.14953
Y2  - 2024/07/03/10:15:58
L1  - https://arxiv.org/pdf/2401.14953.pdf
L2  - https://arxiv.org/abs/2401.14953
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Neural Population Learning beyond Symmetric Zero-sum Games
AU  - Liu, Siqi
AU  - Marris, Luke
AU  - Lanctot, Marc
AU  - Piliouras, Georgios
AU  - Leibo, Joel Z.
AU  - Heess, Nicolas
AB  - We study computationally efficient methods for finding equilibria in n-player general-sum games, specifically ones that afford complex visuomotor skills. We show how existing methods would struggle in this setting, either computationally or in theory. We then introduce NeuPL-JPSRO, a neural population learning algorithm that benefits from transfer learning of skills and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show empirical convergence in a suite of OpenSpiel games, validated rigorously by exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our approach enables adaptive coordination in a MuJoCo control domain and skill transfer in capture-the-flag. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives.
DA  - 2024/01/10/
PY  - 2024
DO  - 10.48550/arXiv.2401.05133
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.05133
Y2  - 2024/07/03/10:16:48
L1  - https://arxiv.org/pdf/2401.05133.pdf
L2  - https://arxiv.org/abs/2401.05133
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes
AU  - Agarwal, Rishabh
AU  - Vieillard, Nino
AU  - Zhou, Yongchao
AU  - Stanczyk, Piotr
AU  - Ramos, Sabela
AU  - Geist, Matthieu
AU  - Bachem, Olivier
AB  - Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.
DA  - 2024/01/16/
PY  - 2024
DO  - 10.48550/arXiv.2306.13649
DP  - arXiv.org
PB  - arXiv
ST  - On-Policy Distillation of Language Models
UR  - http://arxiv.org/abs/2306.13649
Y2  - 2024/07/03/10:17:39
L1  - https://arxiv.org/pdf/2306.13649.pdf
L2  - https://arxiv.org/abs/2306.13649
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - GATS: Gather-Attend-Scatter
AU  - Zolna, Konrad
AU  - Cabi, Serkan
AU  - Chen, Yutian
AU  - Lau, Eric
AU  - Fantacci, Claudio
AU  - Pasukonis, Jurgis
AU  - Springenberg, Jost Tobias
AU  - Colmenarejo, Sergio Gomez
AB  - As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems.
DA  - 2024/01/16/
PY  - 2024
DO  - 10.48550/arXiv.2401.08525
DP  - arXiv.org
PB  - arXiv
ST  - GATS
UR  - http://arxiv.org/abs/2401.08525
Y2  - 2024/07/03/10:18:28
L1  - https://arxiv.org/pdf/2401.08525.pdf
L2  - https://arxiv.org/abs/2401.08525
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments
AU  - Dedieu, Antoine
AU  - Lehrach, Wolfgang
AU  - Zhou, Guangyao
AU  - George, Dileep
AU  - Lázaro-Gredilla, Miguel
AB  - Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.
DA  - 2024/01/11/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2401.05946
Y2  - 2024/07/03/10:19:42
L1  - https://arxiv.org/pdf/2401.05946
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Distributional reinforcement learning in prefrontal cortex
AU  - Muller, Timothy H.
AU  - Butler, James L.
AU  - Veselic, Sebastijan
AU  - Miranda, Bruno
AU  - Wallis, Joni D.
AU  - Dayan, Peter
AU  - Behrens, Timothy E. J.
AU  - Kurth-Nelson, Zeb
AU  - Kennerley, Steven W.
T2  - Nature Neuroscience
AB  - The prefrontal cortex is crucial for learning and decision-making. Classic reinforcement learning (RL) theories center on learning the expectation of potential rewarding outcomes and explain a wealth of neural data in the prefrontal cortex. Distributional RL, on the other hand, learns the full distribution of rewarding outcomes and better explains dopamine responses. In the present study, we show that distributional RL also better explains macaque anterior cingulate cortex neuronal responses, suggesting that it is a common mechanism for reward-guided learning.
DA  - 2024/03//
PY  - 2024
DO  - 10.1038/s41593-023-01535-w
DP  - www.nature.com
VL  - 27
IS  - 3
SP  - 403
EP  - 408
J2  - Nat Neurosci
LA  - en
SN  - 1546-1726
UR  - https://www.nature.com/articles/s41593-023-01535-w
Y2  - 2024/07/03/10:20:02
L1  - https://www.nature.com/articles/s41593-023-01535-w.pdf
KW  - Decision
KW  - Reward
ER  - 

TY  - GEN
TI  - AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents
AU  - Ahn, Michael
AU  - Dwibedi, Debidatta
AU  - Finn, Chelsea
AU  - Arenas, Montse Gonzalez
AU  - Gopalakrishnan, Keerthana
AU  - Hausman, Karol
AU  - Ichter, Brian
AU  - Irpan, Alex
AU  - Joshi, Nikhil
AU  - Julian, Ryan
AU  - Kirmani, Sean
AU  - Leal, Isabel
AU  - Lee, Edward
AU  - Levine, Sergey
AU  - Lu, Yao
AU  - Leal, Isabel
AU  - Maddineni, Sharath
AU  - Rao, Kanishka
AU  - Sadigh, Dorsa
AU  - Sanketi, Pannag
AU  - Sermanet, Pierre
AU  - Vuong, Quan
AU  - Welker, Stefan
AU  - Xia, Fei
AU  - Xiao, Ted
AU  - Xu, Peng
AU  - Xu, Steve
AU  - Xu, Zhuo
AB  - Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.
DA  - 2024/07/01/
PY  - 2024
DO  - 10.48550/arXiv.2401.12963
DP  - arXiv.org
PB  - arXiv
ST  - AutoRT
UR  - http://arxiv.org/abs/2401.12963
Y2  - 2024/07/03/10:20:21
L1  - https://arxiv.org/pdf/2401.12963.pdf
L2  - https://arxiv.org/abs/2401.12963
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Challenges with unsupervised LLM knowledge discovery
AU  - Farquhar, Sebastian
AU  - Varma, Vikrant
AU  - Kenton, Zachary
AU  - Gasteiger, Johannes
AU  - Mikulik, Vladimir
AU  - Shah, Rohin
AB  - We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.
DA  - 2023/12/18/
PY  - 2023
DO  - 10.48550/arXiv.2312.10029
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.10029
Y2  - 2024/07/03/10:20:59
L1  - https://arxiv.org/pdf/2312.10029.pdf
L2  - https://arxiv.org/abs/2312.10029
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Meta-in-context learning in large language models
AU  - Coda-Forno, Julian
AU  - Binz, Marcel
AU  - Akata, Zeynep
AU  - Botvinick, Matthew
AU  - Wang, Jane X.
AU  - Schulz, Eric
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we broaden the scope of our investigation to encompass two diverse benchmarks: one focusing on real-world regression problems and the other encompassing multiple NLP tasks. In both cases, we observe competitive performance comparable to that of traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=sx0xpaO0za
Y2  - 2024/07/03/10:21:23
L1  - https://openreview.net/pdf?id=sx0xpaO0za
ER  - 

TY  - GEN
TI  - A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames
AU  - Papalampidi, Pinelopi
AU  - Koppula, Skanda
AU  - Pathak, Shreya
AU  - Chiu, Justin
AU  - Heyward, Joe
AU  - Patraucean, Viorica
AU  - Shen, Jiajun
AU  - Miech, Antoine
AU  - Zisserman, Andrew
AU  - Nematzdeh, Aida
AB  - Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).
DA  - 2023/12/12/
PY  - 2023
DO  - 10.48550/arXiv.2312.07395
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.07395
Y2  - 2024/07/03/10:22:43
L1  - https://arxiv.org/pdf/2312.07395.pdf
L2  - https://arxiv.org/abs/2312.07395
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Schema-learning and rebinding as mechanisms of in-context learning and emergence
AU  - Swaminathan, Sivaramakrishnan
AU  - Dedieu, Antoine
AU  - Raju, Rajkumar Vasudeva
AU  - Shanahan, Murray
AU  - Lazaro-Gredilla, Miguel
AU  - George, Dileep
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=3AreDQZ8eO
Y2  - 2024/07/03/10:23:13
L1  - https://openreview.net/pdf?id=3AreDQZ8eO
ER  - 

TY  - CONF
TI  - A Definition of Continual Reinforcement Learning
AU  - Abel, David
AU  - Barreto, Andre
AU  - Roy, Benjamin Van
AU  - Precup, Doina
AU  - Hasselt, Hado van
AU  - Singh, Satinder
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - In a standard view of the reinforcement learning problem, an agent’s goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that “never stop learning” through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=ZZS9WEWYbD&referrer=%5Bthe%20profile%20of%20Satinder%20Singh%5D(%2Fprofile%3Fid%3D~Satinder_Singh2)
Y2  - 2024/07/03/10:23:45
L1  - https://openreview.net/pdf?id=ZZS9WEWYbD
ER  - 

TY  - CONF
TI  - Towards In-context Scene Understanding
AU  - Balazevic, Ivana
AU  - Steiner, David
AU  - Parthasarathy, Nikhil
AU  - Arandjelovic, Relja
AU  - Henaff, Olivier J.
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - In-context learning––the ability to configure a model's behavior with different prompts––has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol––leveraging attention within and across images––which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=FasIQqsJhe
Y2  - 2024/07/03/10:25:05
L1  - https://openreview.net/pdf?id=FasIQqsJhe
ER  - 

TY  - CONF
TI  - Probabilistic inference in reinforcement learning done right
AU  - Tarbouriech, Jean
AU  - Lattimore, Tor
AU  - O'Donoghue, Brendan
T3  - NIPS '23
AB  - A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.
C1  - Red Hook, NY, USA
C3  - Proceedings of the 37th International Conference on Neural Information Processing Systems
DA  - 2024/05/30/
PY  - 2024
DP  - ACM Digital Library
SP  - 33687
EP  - 33725
PB  - Curran Associates Inc.
Y2  - 2024/07/03/
ER  - 

TY  - CONF
TI  - Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples
AU  - Jiralerspong, Marco
AU  - Bose, Joey
AU  - Gemp, Ian
AU  - Qin, Chongli
AU  - Bachrach, Yoram
AU  - Gidel, Gauthier
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Divergence (FLD), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLD to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Feature Likelihood Divergence
UR  - https://openreview.net/forum?id=l2VKZkolT7
Y2  - 2024/07/03/10:26:07
L1  - https://openreview.net/pdf?id=l2VKZkolT7
ER  - 

TY  - CONF
TI  - Benchmarking Robustness to Adversarial Image Obfuscations
AU  - Stimberg, Florian
AU  - Chakrabarti, Ayan
AU  - Lu, Chun-Ta
AU  - Hazimeh, Hussein
AU  - Stretcu, Otilia
AU  - Qiao, Wei
AU  - Liu, Yintao
AU  - Kaya, Merve
AU  - Rashtchian, Cyrus
AU  - Fuxman, Ariel
AU  - Tek, Mehmet Nejat
AU  - Gowal, Sven
T2  - Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track
AB  - Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g., overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on ImageNet, simulates the type of obfuscations created by malicious actors. It goes beyond Image-Net-C and ImageNet-C-bar by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by lp-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=CiRHWaRbp0
Y2  - 2024/07/03/10:26:30
L1  - https://openreview.net/pdf?id=CiRHWaRbp0
ER  - 

TY  - CONF
TI  - Improving neural network representations using human similarity judgments
AU  - Muttenthaler, Lukas
AU  - Linhardt, Lorenz
AU  - Dippel, Jonas
AU  - Vandermeulen, Robert A.
AU  - Hermann, Katherine
AU  - Lampinen, Andrew Kyle
AU  - Kornblith, Simon
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Deep neural networks have reached human-level performance on many computer vision tasks. However, the objectives used to train these networks enforce only that similar images are embedded at similar locations in the representation space, and do not directly constrain the global structure of the resulting space. Here, we explore the impact of supervising this global structure by linearly aligning it with human similarity judgments. We find that a naive approach leads to large changes in local representational structure that harm downstream performance. Thus, we propose a novel method that aligns the global structure of representations while preserving their local structure. This global-local transform considerably improves accuracy across a variety of few-shot learning and anomaly detection tasks. Our results indicate that human visual representations are globally organized in a way that facilitates learning from few examples, and incorporating this global structure into neural network representations improves performance on downstream tasks.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Nh5dp6Uuvx
Y2  - 2024/07/03/10:28:59
L1  - https://openreview.net/pdf?id=Nh5dp6Uuvx
ER  - 

TY  - CONF
TI  - Passive learning of active causal strategies in agents and language models
AU  - Lampinen, Andrew Kyle
AU  - Chan, Stephanie C. Y.
AU  - Dasgupta, Ishita
AU  - Nam, Andrew Joo Hun
AU  - Wang, Jane X.
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=BRpi8YAfac&referrer=%5Bthe%20profile%20of%20Andrew%20Kyle%20Lampinen%5D(%2Fprofile%3Fid%3D~Andrew_Kyle_Lampinen1)
Y2  - 2024/07/03/10:29:26
L1  - https://openreview.net/pdf?id=BRpi8YAfac
ER  - 

TY  - JOUR
TI  - POMRL: No-Regret Learning-to-Plan with Increasing Horizons
AU  - Khetarpal, Khimya
AU  - Vernade, Claire
AU  - O'Donoghue, Brendan
AU  - Singh, Satinder
AU  - Zahavy, Tom
T2  - Transactions on Machine Learning Research
AB  - We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task and across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizons on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and we validate its significance empirically.
DA  - 2023/01/10/
PY  - 2023
DP  - openreview.net
LA  - en
SN  - 2835-8856
ST  - POMRL
UR  - https://openreview.net/forum?id=brGgOAXYtr
Y2  - 2024/07/03/10:30:04
L1  - https://openreview.net/pdf?id=brGgOAXYtr
ER  - 

TY  - CONF
TI  - Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models
AU  - Rannen-Triki, Amal
AU  - Bornschein, Jorg
AU  - Pascanu, Razvan
AU  - Galashov, Alexandre
AU  - Titsias, Michalis
AU  - Hutter, Marcus
AU  - György, András
AU  - Teh, Yee Whye
T2  - NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models
AB  - We consider the problem of online finetuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online-adaptation turns parameters into temporally changing states and provides a form of context-length extension with _memory in weights_, more in line with the concept of _memory_ in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency), sensitivity to overall distributional drift, and computational overhead for performing gradient computation and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context learning and finetuning blurs: Both are methods to condition the model on previously observed tokens.
DA  - 2023/12/07/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Revisiting Dynamic Evaluation
UR  - https://openreview.net/forum?id=iRz8qi7QB8
Y2  - 2024/07/03/10:30:34
L1  - https://openreview.net/pdf?id=iRz8qi7QB8
ER  - 

TY  - CONF
TI  - A Benchmark for Reasoning with Spatial Prepositions
AU  - Comsa, Iulia
AU  - Narayanan, Srini
T2  - EMNLP 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.
C1  - Singapore
C3  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.emnlp-main.1015
DP  - ACLWeb
SP  - 16328
EP  - 16335
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.emnlp-main.1015
Y2  - 2024/07/03/10:30:45
L1  - https://aclanthology.org/2023.emnlp-main.1015.pdf
ER  - 

TY  - GEN
TI  - Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia
AU  - Vezhnevets, Alexander Sasha
AU  - Agapiou, John P.
AU  - Aharon, Avia
AU  - Ziv, Ron
AU  - Matyas, Jayd
AU  - Duéñez-Guzmán, Edgar A.
AU  - Cunningham, William A.
AU  - Osindero, Simon
AU  - Karmon, Danny
AU  - Leibo, Joel Z.
AB  - Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act "reasonably", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.
DA  - 2023/12/13/
PY  - 2023
DO  - 10.48550/arXiv.2312.03664
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.03664
Y2  - 2024/07/03/10:30:55
L1  - https://arxiv.org/pdf/2312.03664.pdf
L2  - https://arxiv.org/abs/2312.03664
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Gaussian process probes (GPP) for uncertainty-aware probing
AU  - Wang, Zi
AU  - Ku, Alexander
AU  - Baldridge, Jason
AU  - Griffiths, Thomas L.
AU  - Kim, Been
T3  - NIPS '23
AB  - Understanding which concepts models can and cannot represent has been fundamental to many tasks: from effective and responsible use of models to detecting out of distribution data. We introduce Gaussian process probes (GPP), a unified and simple framework for probing and measuring uncertainty about concepts represented by models. As a Bayesian extension of linear probing methods, GPP asks what kind of distribution over classifiers (of concepts) is induced by the model. This distribution can be used to measure both what the model represents and how confident the probe is about what the model represents. GPP can be applied to any pre-trained model with vector representations of inputs (e.g., activations). It does not require access to training data, gradients, or the architecture. We validate GPP on datasets containing both synthetic and real images. Our experiments show it can (1) probe a model's representations of concepts even with a very small number of examples, (2) accurately measure both epistemic uncertainty (how confident the probe is) and aleatory uncertainty (how fuzzy the concepts are to the model), and (3) detect out of distribution data using those uncertainty measures as well as classic methods do. By using Gaussian processes to expand what probing can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool for understanding and evaluating the capabilities of machine learning models.
C1  - Red Hook, NY, USA
C3  - Proceedings of the 37th International Conference on Neural Information Processing Systems
DA  - 2024/05/30/
PY  - 2024
DP  - ACM Digital Library
SP  - 63573
EP  - 63594
PB  - Curran Associates Inc.
Y2  - 2024/07/03/
ER  - 

TY  - JOUR
TI  - RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation
AU  - Bousmalis, Konstantinos
AU  - Vezzani, Giulia
AU  - Rao, Dushyant
AU  - Devin, Coline Manon
AU  - Lee, Alex X.
AU  - Villalonga, Maria Bauza
AU  - Davchev, Todor
AU  - Zhou, Yuxiang
AU  - Gupta, Agrim
AU  - Raju, Akhil
AU  - Laurens, Antoine
AU  - Fantacci, Claudio
AU  - Dalibard, Valentin
AU  - Zambelli, Martina
AU  - Martins, Murilo Fernandes
AU  - Pevceviciute, Rugile
AU  - Blokzijl, Michiel
AU  - Denil, Misha
AU  - Batchelor, Nathan
AU  - Lampe, Thomas
AU  - Parisotto, Emilio
AU  - Zolna, Konrad
AU  - Reed, Scott
AU  - Colmenarejo, Sergio Gómez
AU  - Scholz, Jonathan
AU  - Abdolmaleki, Abbas
AU  - Groth, Oliver
AU  - Regli, Jean-Baptiste
AU  - Sushkov, Oleg
AU  - Rothörl, Thomas
AU  - Chen, Jose Enrique
AU  - Aytar, Yusuf
AU  - Barker, David
AU  - Ortiz, Joy
AU  - Riedmiller, Martin
AU  - Springenberg, Jost Tobias
AU  - Hadsell, Raia
AU  - Nori, Francesco
AU  - Heess, Nicolas
T2  - Transactions on Machine Learning Research
AB  - The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100–1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent’s capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.
DA  - 2023/09/06/
PY  - 2023
DP  - openreview.net
LA  - en
SN  - 2835-8856
ST  - RoboCat
UR  - https://openreview.net/forum?id=vsCpILiWHu
Y2  - 2024/07/03/10:32:28
L1  - https://openreview.net/pdf?id=vsCpILiWHu
ER  - 

TY  - GEN
TI  - RLHF and IIA: Perverse Incentives
AU  - Xu, Wanqiao
AU  - Dong, Shi
AU  - Lu, Xiuyuan
AU  - Lam, Grace
AU  - Wen, Zheng
AU  - Van Roy, Benjamin
AB  - Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
DA  - 2024/02/01/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - RLHF and IIA
UR  - http://arxiv.org/abs/2312.01057
Y2  - 2024/07/03/10:33:13
L1  - https://arxiv.org/pdf/2312.01057
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Universal Self-Consistency for Large Language Model Generation
AU  - Chen, Xinyun
AU  - Aksitov, Renat
AU  - Alon, Uri
AU  - Ren, Jie
AU  - Xiao, Kefan
AU  - Yin, Pengcheng
AU  - Prakash, Sushant
AU  - Sutton, Charles
AU  - Wang, Xuezhi
AU  - Zhou, Denny
AB  - Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.
DA  - 2023/11/28/
PY  - 2023
DO  - 10.48550/arXiv.2311.17311
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.17311
Y2  - 2024/07/03/10:33:35
L1  - https://arxiv.org/pdf/2311.17311.pdf
L2  - https://arxiv.org/abs/2311.17311
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Replay across Experiments: A Natural Extension of Off-Policy RL
AU  - Tirumala, Dhruva
AU  - Lampe, Thomas
AU  - Chen, Jose Enrique
AU  - Haarnoja, Tuomas
AU  - Huang, Sandy
AU  - Lever, Guy
AU  - Moran, Ben
AU  - Hertweck, Tim
AU  - Hasenclever, Leonard
AU  - Riedmiller, Martin
AU  - Heess, Nicolas
AU  - Wulfmeier, Markus
AB  - Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL). We present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times. At its core, Replay Across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work. We empirically show benefits across a number of RL algorithms and challenging control domains spanning both locomotion and manipulation, including hard exploration tasks from egocentric vision. Through comprehensive ablations, we demonstrate robustness to the quality and amount of data available and various hyperparameter choices. Finally, we discuss how our approach can be applied more broadly across research life cycles and can increase resilience by reloading data across random seeds or hyperparameter variations.
DA  - 2023/11/28/
PY  - 2023
DO  - 10.48550/arXiv.2311.15951
DP  - arXiv.org
PB  - arXiv
ST  - Replay across Experiments
UR  - http://arxiv.org/abs/2311.15951
Y2  - 2024/07/03/10:33:56
L1  - https://arxiv.org/pdf/2311.15951.pdf
L2  - https://arxiv.org/abs/2311.15951
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Scalable AI Safety via Doubly-Efficient Debate
AU  - Brown-Cohen, Jonah
AU  - Irving, Geoffrey
AU  - Piliouras, Georgios
AB  - The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.
DA  - 2023/11/23/
PY  - 2023
DO  - 10.48550/arXiv.2311.14125
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.14125
Y2  - 2024/07/03/10:34:14
L1  - https://arxiv.org/pdf/2311.14125.pdf
L2  - https://arxiv.org/abs/2311.14125
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - A social path to human-like artificial intelligence
AU  - Duéñez-Guzmán, Edgar A.
AU  - Sadedin, Suzanne
AU  - Wang, Jane X.
AU  - McKee, Kevin R.
AU  - Leibo, Joel Z.
T2  - Nature Machine Intelligence
AB  - Traditionally, cognitive and computer scientists have viewed intelligence solipsistically, as a property of unitary agents devoid of social context. Given the success of contemporary learning algorithms, we argue that the bottleneck in artificial intelligence (AI) advancement is shifting from data assimilation to novel data generation. We bring together evidence showing that natural intelligence emerges at multiple scales in networks of interacting agents via collective living, social relationships and major evolutionary transitions, which contribute to novel data generation through mechanisms such as population pressures, arms races, Machiavellian selection, social learning and cumulative culture. Many breakthroughs in AI exploit some of these processes, from multi-agent structures enabling algorithms to master complex games such as Capture-The-Flag and StarCraft II, to strategic communication in the game Diplomacy and the shaping of AI data streams by other AIs. Moving beyond a solipsistic view of agency to integrate these mechanisms could provide a path to human-like compounding innovation through ongoing novel data generation.
DA  - 2023/11//
PY  - 2023
DO  - 10.1038/s42256-023-00754-x
DP  - www.nature.com
VL  - 5
IS  - 11
SP  - 1181
EP  - 1188
J2  - Nat Mach Intell
LA  - en
SN  - 2522-5839
UR  - https://www.nature.com/articles/s42256-023-00754-x
Y2  - 2024/07/03/10:35:20
KW  - Software
KW  - Computer science
KW  - Complex networks
KW  - Cultural evolution
ER  - 

TY  - JOUR
TI  - Role play with large language models
AU  - Shanahan, Murray
AU  - McDonell, Kyle
AU  - Reynolds, Laria
T2  - Nature
AB  - As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.
DA  - 2023/11//
PY  - 2023
DO  - 10.1038/s41586-023-06647-8
DP  - www.nature.com
VL  - 623
IS  - 7987
SP  - 493
EP  - 498
LA  - en
SN  - 1476-4687
UR  - https://www.nature.com/articles/s41586-023-06647-8
Y2  - 2024/07/03/10:37:00
L1  - https://www.nature.com/articles/s41586-023-06647-8.pdf
KW  - Computer science
KW  - Philosophy
ER  - 

TY  - CONF
TI  - Grammar Prompting for Domain-Specific Language Generation with Large Language Models
AU  - Wang, Bailin
AU  - Wang, Zi
AU  - Wang, Xuezhi
AU  - Cao, Yuan
AU  - Saurous, Rif A.
AU  - Kim, Yoon
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose \emph{grammar prompting}, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=B4tkwuzeiY&referrer=%5Bthe%20profile%20of%20Rif%20A.%20Saurous%5D(%2Fprofile%3Fid%3D~Rif_A._Saurous1)
Y2  - 2024/07/03/10:37:32
L1  - https://openreview.net/pdf?id=B4tkwuzeiY
ER  - 

TY  - GEN
TI  - RoboVQA: Multimodal Long-Horizon Reasoning for Robotics
AU  - Sermanet, Pierre
AU  - Ding, Tianli
AU  - Zhao, Jeffrey
AU  - Xia, Fei
AU  - Dwibedi, Debidatta
AU  - Gopalakrishnan, Keerthana
AU  - Chan, Christine
AU  - Dulac-Arnold, Gabriel
AU  - Maddineni, Sharath
AU  - Joshi, Nikhil J.
AU  - Florence, Pete
AU  - Han, Wei
AU  - Baruch, Robert
AU  - Lu, Yao
AU  - Mirchandani, Suvir
AU  - Xu, Peng
AU  - Sanketi, Pannag
AU  - Hausman, Karol
AU  - Shafran, Izhak
AU  - Ichter, Brian
AU  - Cao, Yuan
AB  - We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple robot and human embodiments. With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We find that for a fixed collection budget it is beneficial to take advantage of cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zero-shot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Data and videos available at https://robovqa.github.io
DA  - 2023/11/01/
PY  - 2023
DO  - 10.48550/arXiv.2311.00899
DP  - arXiv.org
PB  - arXiv
ST  - RoboVQA
UR  - http://arxiv.org/abs/2311.00899
Y2  - 2024/07/03/10:39:49
L1  - https://arxiv.org/pdf/2311.00899.pdf
L2  - https://arxiv.org/abs/2311.00899
KW  - Computer Science - Robotics
ER  - 

TY  - JOUR
TI  - Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning
AU  - Lanctot, Marc
AU  - Schultz, John
AU  - Burch, Neil
AU  - Smith, Max Olan
AU  - Hennes, Daniel
AU  - Anthony, Thomas
AU  - Perolat, Julien
T2  - Transactions on Machine Learning Research
AB  - Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.
DA  - 2023/06/18/
PY  - 2023
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=gQnJ7ODIAx
Y2  - 2024/07/03/10:40:33
L1  - https://openreview.net/pdf?id=gQnJ7ODIAx
ER  - 

TY  - GEN
TI  - Sociotechnical Safety Evaluation of Generative AI Systems
AU  - Weidinger, Laura
AU  - Rauh, Maribeth
AU  - Marchal, Nahema
AU  - Manzini, Arianna
AU  - Hendricks, Lisa Anne
AU  - Mateos-Garcia, Juan
AU  - Bergman, Stevie
AU  - Kay, Jackie
AU  - Griffin, Conor
AU  - Bariach, Ben
AU  - Gabriel, Iason
AU  - Rieser, Verena
AU  - Isaac, William
AB  - Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.
DA  - 2023/10/31/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2310.11986
Y2  - 2024/07/03/10:41:01
L1  - https://arxiv.org/pdf/2310.11986
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models
AU  - Zheng, Huaixiu Steven
AU  - Mishra, Swaroop
AU  - Chen, Xinyun
AU  - Cheng, Heng-Tze
AU  - Chi, Ed H.
AU  - Le, Quoc V.
AU  - Zhou, Denny
AB  - We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.
DA  - 2024/03/12/
PY  - 2024
DO  - 10.48550/arXiv.2310.06117
DP  - arXiv.org
PB  - arXiv
ST  - Take a Step Back
UR  - http://arxiv.org/abs/2310.06117
Y2  - 2024/07/03/10:43:44
L1  - https://arxiv.org/pdf/2310.06117.pdf
L2  - https://arxiv.org/abs/2310.06117
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Assessing Large Language Models on Climate Information
AU  - Bulian, Jannis
AU  - Schäfer, Mike S.
AU  - Amini, Afra
AU  - Lam, Heidi
AU  - Ciaramita, Massimiliano
AU  - Gaiarin, Ben
AU  - Hübscher, Michelle Chen
AU  - Buck, Christian
AU  - Mede, Niels G.
AU  - Leippold, Markus
AU  - Strauß, Nadine
AB  - As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.
DA  - 2024/05/28/
PY  - 2024
DO  - 10.48550/arXiv.2310.02932
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.02932
Y2  - 2024/07/03/10:44:52
L1  - https://arxiv.org/pdf/2310.02932.pdf
L2  - https://arxiv.org/abs/2310.02932
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Large Language Models as Analogical Reasoners
AU  - Yasunaga, Michihiro
AU  - Chen, Xinyun
AU  - Li, Yujia
AU  - Pasupat, Panupong
AU  - Leskovec, Jure
AU  - Liang, Percy
AU  - Chi, Ed H.
AU  - Zhou, Denny
AB  - Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.
DA  - 2024/03/09/
PY  - 2024
DO  - 10.48550/arXiv.2310.01714
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.01714
Y2  - 2024/07/03/10:45:30
L1  - https://arxiv.org/pdf/2310.01714.pdf
L2  - https://arxiv.org/abs/2310.01714
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Large Language Models Cannot Self-Correct Reasoning Yet
AU  - Huang, Jie
AU  - Chen, Xinyun
AU  - Mishra, Swaroop
AU  - Zheng, Huaixiu Steven
AU  - Yu, Adams Wei
AU  - Song, Xinying
AU  - Zhou, Denny
AB  - Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.
DA  - 2024/03/14/
PY  - 2024
DO  - 10.48550/arXiv.2310.01798
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.01798
Y2  - 2024/07/03/10:45:41
L1  - https://arxiv.org/pdf/2310.01798.pdf
L2  - https://arxiv.org/abs/2310.01798
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Theoretical and Practical Perspectives on what Influence Functions Do
AU  - Schioppa, Andrea
AU  - Filippova, Katja
AU  - Titov, Ivan
AU  - Zablotskaia, Polina
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples "responsible" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models. Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=gGl0n7Onug
Y2  - 2024/07/03/10:46:06
L1  - https://openreview.net/pdf?id=gGl0n7Onug
ER  - 

TY  - GEN
TI  - Tracr: Compiled Transformers as a Laboratory for Interpretability
AU  - Lindner, David
AU  - Kramár, János
AU  - Farquhar, Sebastian
AU  - Rahtz, Matthew
AU  - McGrath, Thomas
AU  - Mikulik, Vladimir
AB  - We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr.
DA  - 2023/11/03/
PY  - 2023
DO  - 10.48550/arXiv.2301.05062
DP  - arXiv.org
PB  - arXiv
ST  - Tracr
UR  - http://arxiv.org/abs/2301.05062
Y2  - 2024/07/03/10:46:44
L1  - https://arxiv.org/pdf/2301.05062.pdf
L2  - https://arxiv.org/abs/2301.05062
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Self-Predictive Universal AI
AU  - Catt, Elliot
AU  - Grau-Moya, Jordi
AU  - Hutter, Marcus
AU  - Aitchison, Matthew
AU  - Genewein, Tim
AU  - Deletang, Gregoire
AU  - Wenliang, Li Kevin
AU  - Veness, Joel
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. The integration of both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the most potent theoretical universal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by self-predicting its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=psXVkKO9No&referrer=%5Bthe%20profile%20of%20Li%20Kevin%20Wenliang%5D(%2Fprofile%3Fid%3D~Li_Kevin_Wenliang1)
Y2  - 2024/07/03/10:47:07
L1  - https://openreview.net/pdf?id=psXVkKO9No
ER  - 

TY  - GEN
TI  - Ethical and social risks of harm from Language Models
AU  - Weidinger, Laura
AU  - Mellor, John
AU  - Rauh, Maribeth
AU  - Griffin, Conor
AU  - Uesato, Jonathan
AU  - Huang, Po-Sen
AU  - Cheng, Myra
AU  - Glaese, Mia
AU  - Balle, Borja
AU  - Kasirzadeh, Atoosa
AU  - Kenton, Zac
AU  - Brown, Sasha
AU  - Hawkins, Will
AU  - Stepleton, Tom
AU  - Biles, Courtney
AU  - Birhane, Abeba
AU  - Haas, Julia
AU  - Rimell, Laura
AU  - Hendricks, Lisa Anne
AU  - Isaac, William
AU  - Legassick, Sean
AU  - Irving, Geoffrey
AU  - Gabriel, Iason
AB  - This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.
DA  - 2021/12/08/
PY  - 2021
DO  - 10.48550/arXiv.2112.04359
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2112.04359
Y2  - 2024/07/03/10:55:00
L1  - https://arxiv.org/pdf/2112.04359.pdf
L2  - https://arxiv.org/abs/2112.04359
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Goal Misgeneralization in Deep Reinforcement Learning
AU  - Langosco, Lauro
AU  - Koch, Jack
AU  - Sharkey, Lee
AU  - Pfau, Jacob
AU  - Orseau, Laurent
AU  - Krueger, David
AB  - We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.
DA  - 2023/01/09/
PY  - 2023
DO  - 10.48550/arXiv.2105.14111
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2105.14111
Y2  - 2024/07/03/10:57:29
L1  - https://arxiv.org/pdf/2105.14111.pdf
L2  - https://arxiv.org/abs/2105.14111
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - AI safety via debate
AU  - Irving, Geoffrey
AU  - Christiano, Paul
AU  - Amodei, Dario
AB  - To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.
DA  - 2018/10/22/
PY  - 2018
DO  - 10.48550/arXiv.1805.00899
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.00899
Y2  - 2024/07/03/10:58:30
L1  - https://arxiv.org/pdf/1805.00899.pdf
L2  - https://arxiv.org/abs/1805.00899
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Supervising strong learners by amplifying weak experts
AU  - Christiano, Paul
AU  - Shlegeris, Buck
AU  - Amodei, Dario
AB  - Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.
DA  - 2018/10/19/
PY  - 2018
DO  - 10.48550/arXiv.1810.08575
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1810.08575
Y2  - 2024/07/03/10:58:55
L1  - https://arxiv.org/pdf/1810.08575.pdf
L2  - https://arxiv.org/abs/1810.08575
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Scalable agent alignment via reward modeling: a research direction
AU  - Leike, Jan
AU  - Krueger, David
AU  - Everitt, Tom
AU  - Martic, Miljan
AU  - Maini, Vishal
AU  - Legg, Shane
AB  - One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.
DA  - 2018/11/19/
PY  - 2018
DO  - 10.48550/arXiv.1811.07871
DP  - arXiv.org
PB  - arXiv
ST  - Scalable agent alignment via reward modeling
UR  - http://arxiv.org/abs/1811.07871
Y2  - 2024/07/03/11:03:08
L1  - https://arxiv.org/pdf/1811.07871.pdf
L2  - https://arxiv.org/abs/1811.07871
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - JOUR
TI  - Research Priorities for Robust and Beneficial Artificial Intelligence
AU  - Russell, Stuart
AU  - Dewey, Daniel
AU  - Tegmark, Max
T2  - AI Magazine
AB  - Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.
DA  - 2015/12//
PY  - 2015
DO  - 10.1609/aimag.v36i4.2577
DP  - DOI.org (Crossref)
VL  - 36
IS  - 4
SP  - 105
EP  - 114
J2  - AI Magazine
LA  - en
SN  - 0738-4602, 2371-9621
UR  - https://onlinelibrary.wiley.com/doi/10.1609/aimag.v36i4.2577
Y2  - 2024/07/03/11:03:37
L1  - https://www.futureoflife.org/data/documents/research_priorities.pdf
ER  - 

TY  - GEN
TI  - Concrete Problems in AI Safety
AU  - Amodei, Dario
AU  - Olah, Chris
AU  - Steinhardt, Jacob
AU  - Christiano, Paul
AU  - Schulman, John
AU  - Mané, Dan
AB  - Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.
DA  - 2016/07/25/
PY  - 2016
DO  - 10.48550/arXiv.1606.06565
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1606.06565
Y2  - 2024/07/03/11:04:36
L1  - https://arxiv.org/pdf/1606.06565.pdf
L2  - https://arxiv.org/abs/1606.06565
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - BLOG
TI  - Building safe artificial intelligence: specification, robustness, and assurance
AU  - Research, DeepMind Safety
T2  - Medium
AB  - By Pedro A. Ortega, Vishal Maini, and the DeepMind safety team
DA  - 2018/09/27/T16:00:45.933Z
PY  - 2018
LA  - en
ST  - Building safe artificial intelligence
UR  - https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1
Y2  - 2024/07/03/11:08:34
L2  - https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1
ER  - 

TY  - GEN
TI  - AGI Safety Literature Review
AU  - Everitt, Tom
AU  - Lea, Gary
AU  - Hutter, Marcus
AB  - The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.
DA  - 2018/05/21/
PY  - 2018
DO  - 10.48550/arXiv.1805.01109
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.01109
Y2  - 2024/07/03/11:09:08
L1  - https://arxiv.org/pdf/1805.01109.pdf
L2  - https://arxiv.org/abs/1805.01109
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Deep reinforcement learning from human preferences
AU  - Christiano, Paul
AU  - Leike, Jan
AU  - Brown, Tom B.
AU  - Martic, Miljan
AU  - Legg, Shane
AU  - Amodei, Dario
AB  - For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.
DA  - 2023/02/17/
PY  - 2023
DO  - 10.48550/arXiv.1706.03741
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1706.03741
Y2  - 2024/07/03/11:11:18
L1  - https://arxiv.org/pdf/1706.03741.pdf
L2  - https://arxiv.org/abs/1706.03741
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Privacy-Preserving Instructions for Aligning Large Language Models
AU  - Yu, Da
AU  - Kairouz, Peter
AU  - Oh, Sewoong
AU  - Xu, Zheng
AB  - Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.
DA  - 2024/07/02/
PY  - 2024
DO  - 10.48550/arXiv.2402.13659
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.13659
Y2  - 2024/07/08/21:37:24
L1  - https://arxiv.org/pdf/2402.13659.pdf
L2  - https://arxiv.org/abs/2402.13659
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Securing the AI Software Supply Chain
AU  - Chaudhuri, Shamik
AU  - Dasgupta, Kingshuk
AU  - Hepworth, Isaac
AU  - Le, Michael
AU  - Lodato, Mark
AU  - Maruseac, Mihai
AU  - Meiklejohn, Sarah
AU  - Minkus, Tehila
AU  - Olive, Kara
AB  - As AI-powered features gain traction in software applications, we see many of the same problems we’ve faced with traditional software—but at an accelerated pace. The threat landscape continues to expand as AI is further integrated into everyday products, so we can expect more attacks. Given the expense of building models, there is a clear need for supply chain solutions. This paper explains our approach to securing our AI supply chain using provenance information and provides guidance for other organizations. Although there are differences between traditional and AI development processes and risks, we can build on our work over the past decade using Binary Authorization for Borg (BAB), Supply-chain Levels for Software Artifacts (SLSA), and next-generation cryptographic signing solutions via Sigstore, and adapt these to the AI supply chain without reinventing the wheel. Depending on internal processes and platforms, each organization’s approach to AI supply chain security will look different, but the focus should be on areas where it can be improved in a relatively short time. Readers should note that the first part of this paper provides a broad overview of “Development lifecycles for traditional and AI software”. Then we delve specifically into AI supply chain risks, and explain our approach to securing our AI supply chain using provenance information. More advanced practitioners may prefer to go directly to the sections on “AI supply chain risks,” “Controls for AI supply chain security,” or even the “Guidance for practitioners” section at the end of the paper, which can be adapted to the needs of any organization.
CY  - Unpublished
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://storage.googleapis.com/gweb-research2023-media/pubtools/7769.pdf
ER  - 

TY  - JOUR
TI  - Identifying and Mitigating the Security Risks of Generative AI
AU  - Barrett, Clark
AU  - Boyd, Brad
AU  - Bursztein, Elie
AU  - Carlini, Nicholas
AU  - Chen, Brad
AU  - Choi, Jihye
AU  - Chowdhury, Amrita Roy
AU  - Christodorescu, Mihai
AU  - Datta, Anupam
AU  - Feizi, Soheil
AU  - Fisher, Kathleen
AU  - Hashimoto, Tatsunori
AU  - Hendrycks, Dan
AU  - Jha, Somesh
AU  - Kang, Daniel
AU  - Kerschbaum, Florian
AU  - Mitchell, Eric
AU  - Mitchell, John
AU  - Ramzan, Zulfikar
AU  - Shams, Khawaja
AU  - Song, Dawn
AU  - Taly, Ankur
AU  - Yang, Diyi
T2  - Foundations and Trends® in Privacy and Security
AB  - Identifying and Mitigating the Security Risks of Generative AI
DA  - 2023/12/13/
PY  - 2023
DO  - 10.1561/3300000041
DP  - www.nowpublishers.com
VL  - 6
IS  - 1
SP  - 1
EP  - 52
J2  - SEC
LA  - English
SN  - 2474-1558, 2474-1566
UR  - https://www.nowpublishers.com/article/Details/SEC-041
Y2  - 2024/07/08/21:40:07
L1  - https://www.nowpublishers.com/article/DownloadSummary/SEC-041
ER  - 

TY  - CONF
TI  - What does it mean for a language model to preserve privacy?
AU  - Mireshghallah, Fatemehsadat
AU  - Tramèr, Florian
AU  - Brown, Hannah
AU  - Lee, Katherine
AU  - Shokri, Reza
AB  - Our language reflects who we are. The words and phrases we use as well as the contextual information in our conversations disclose our personal life. As humans we learn how to communicate about ourselves and others, while delicately concealing private information depending on the context of conversations. Language models, however, totally lack the ability to understand the context and analyze the sensitivity of text, and tend to memorize phrases and remember information about their training sets. Thus, inference attacks are shown to be alarmingly successful at extracting sensitive data from language models. In this paper, we discuss the privacy expectations from language models, and provide a critical analysis of major data protection techniques: data redaction (scrubbing) and differential privacy. We show that these protection methods can guarantee, at best, a very limited form of privacy which does not account for correlations and other nuances in human communication. We finally argue that language models need to be trained on data which is intended to be produced for public use with proper consent forms and authorization from authors.
C3  - FaCCT
DA  - 2022///
PY  - 2022
UR  - https://research.google/pubs/what-does-it-mean-for-a-language-model-to-preserve-privacy/
ER  - 

TY  - CONF
TI  - Natural language understanding with privacy-preserving bert
AU  - Qu, Chen
AU  - Kong, Weize
AU  - Yang, Liu
AU  - Zhang, Mingyang
AU  - Bendersky, Michael
AU  - Najork, Marc
AB  - Privacy preservation remains a key challenge in data mining and Natural Language Understanding (NLU). Previous research shows that the input text or even text embeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying dχ-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs.
C3  - Proceedings of the 30th ACM international conference on information & knowledge management
DA  - 2021///
PY  - 2021
SP  - 1488
EP  - 1497
ER  - 

TY  - GEN
TI  - Learning Differentially Private Recurrent Language Models
AU  - McMahan, H. Brendan
AU  - Ramage, Daniel
AU  - Talwar, Kunal
AU  - Zhang, Li
AB  - We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes "large step" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.
DA  - 2018/02/23/
PY  - 2018
DO  - 10.48550/arXiv.1710.06963
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1710.06963
Y2  - 2024/07/08/21:48:47
L1  - https://arxiv.org/pdf/1710.06963.pdf
L2  - https://arxiv.org/abs/1710.06963
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Generative models improve fairness of medical classifiers under distribution shifts
AU  - Ktena, Ira
AU  - Wiles, Olivia
AU  - Albuquerque, Isabela
AU  - Rebuffi, Sylvestre-Alvise
AU  - Tanno, Ryutaro
AU  - Roy, Abhijit Guha
AU  - Azizi, Shekoofeh
AU  - Belgrave, Danielle
AU  - Kohli, Pushmeet
AU  - Cemgil, Taylan
AU  - Karthikesalingam, Alan
AU  - Gowal, Sven
T2  - Nature Medicine
AB  - Domain generalization is a ubiquitous challenge for machine learning in healthcare. Model performance in real-world conditions might be lower than expected because of discrepancies between the data encountered during deployment and development. Underrepresentation of some groups or conditions during model development is a common cause of this phenomenon. This challenge is often not readily addressed by targeted data acquisition and ‘labeling’ by expert clinicians, which can be prohibitively expensive or practically impossible because of the rarity of conditions or the available clinical expertise. We hypothesize that advances in generative artificial intelligence can help mitigate this unmet need in a steerable fashion, enriching our training dataset with synthetic examples that address shortfalls of underrepresented conditions or subgroups. We show that diffusion models can automatically learn realistic augmentations from data in a label-efficient manner. We demonstrate that learned augmentations make models more robust and statistically fair in-distribution and out of distribution. To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution.
DA  - 2024/04//
PY  - 2024
DO  - 10.1038/s41591-024-02838-6
DP  - www.nature.com
VL  - 30
IS  - 4
SP  - 1166
EP  - 1173
J2  - Nat Med
LA  - en
SN  - 1546-170X
UR  - https://www.nature.com/articles/s41591-024-02838-6
Y2  - 2024/07/08/21:54:25
L1  - https://www.nature.com/articles/s41591-024-02838-6.pdf
KW  - Diagnosis
KW  - Medical imaging
ER  - 

TY  - GEN
TI  - Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning
AU  - Ma, Xiao
AU  - Mishra, Swaroop
AU  - Beirami, Ahmad
AU  - Beutel, Alex
AU  - Chen, Jilin
AB  - Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%.
DA  - 2023/06/25/
PY  - 2023
DO  - 10.48550/arXiv.2306.14308
DP  - arXiv.org
PB  - arXiv
ST  - Let's Do a Thought Experiment
UR  - http://arxiv.org/abs/2306.14308
Y2  - 2024/07/08/21:55:14
L1  - https://arxiv.org/pdf/2306.14308.pdf
L2  - https://arxiv.org/abs/2306.14308
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - VLSlice: Interactive Vision-and-Language Slice Discovery
AU  - Slyman, Eric
AU  - Kahng, Minsuk
AU  - Lee, Stefan
AB  - Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.
DA  - 2023/09/13/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - VLSlice
UR  - http://arxiv.org/abs/2309.06703
Y2  - 2024/07/08/21:58:17
L1  - https://arxiv.org/pdf/2309.06703
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - I.2.7
KW  - I.4.10
KW  - J.4
ER  - 

TY  - GEN
TI  - DICES Dataset: Diversity in Conversational AI Evaluation for Safety
AU  - Aroyo, Lora
AU  - Taylor, Alex S.
AU  - Diaz, Mark
AU  - Homan, Christopher M.
AU  - Parrish, Alicia
AU  - Serapio-Garcia, Greg
AU  - Prabhakaran, Vinodkumar
AU  - Wang, Ding
AB  - Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This risks simplifying and even obscuring the inherent subjectivity present in many tasks. Preserving such variance in content and diversity in datasets is often expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is both socially and culturally situated. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographic information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. In short, the DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of conversational AI safety. We also illustrate how the dataset offers a basis for establishing metrics to show how raters' ratings can intersects with demographic categories such as racial/ethnic groups, age groups, and genders. The goal of DICES is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.
DA  - 2023/06/19/
PY  - 2023
DO  - 10.48550/arXiv.2306.11247
DP  - arXiv.org
PB  - arXiv
ST  - DICES Dataset
UR  - http://arxiv.org/abs/2306.11247
Y2  - 2024/07/08/22:03:32
L1  - https://arxiv.org/pdf/2306.11247.pdf
L2  - https://arxiv.org/abs/2306.11247
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - CONF
TI  - From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML
AU  - Rismani, Shalaleh
AU  - Shelby, Renee
AU  - Smart, Andrew
AU  - Jatho, Edgar
AU  - Kroll, Joshua
AU  - Moon, AJung
AU  - Rostamzadeh, Negar
T3  - CHI '23
AB  - Inappropriate design and deployment of machine learning (ML) systems lead to negative downstream social and ethical impacts – described here as social and ethical risks – for users, society, and the environment. Despite the growing need to regulate ML systems, current processes for assessing and mitigating risks are disjointed and inconsistent. We interviewed 30 industry practitioners on their current social and ethical risk management practices and collected their first reactions on adapting safety engineering frameworks into their practice – namely, System Theoretic Process Analysis (STPA) and Failure Mode and Effects Analysis (FMEA). Our findings suggest STPA/FMEA can provide an appropriate structure for social and ethical risk assessment and mitigation processes. However, we also find nontrivial challenges in integrating such frameworks in the fast-paced culture of the ML industry. We call on the CHI community to strengthen existing frameworks and assess their efficacy, ensuring that ML systems are safer for all people.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems
DA  - 2023/04/19/
PY  - 2023
DO  - 10.1145/3544548.3581407
DP  - ACM Digital Library
SP  - 1
EP  - 18
PB  - Association for Computing Machinery
SN  - 978-1-4503-9421-5
ST  - From Plane Crashes to Algorithmic Harm
UR  - https://doi.org/10.1145/3544548.3581407
Y2  - 2024/07/08/
L1  - https://arxiv.org/pdf/2210.03535
ER  - 

TY  - GEN
TI  - Safety and Fairness for Content Moderation in Generative Models
AU  - Hao, Susan
AU  - Kumar, Piyush
AU  - Laszlo, Sarah
AU  - Poddar, Shivani
AU  - Radharapu, Bhaktipriya
AU  - Shelby, Renee
AB  - With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
DA  - 2023/06/08/
PY  - 2023
DO  - 10.48550/arXiv.2306.06135
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.06135
Y2  - 2024/07/08/22:04:55
L1  - https://arxiv.org/pdf/2306.06135.pdf
L2  - https://arxiv.org/abs/2306.06135
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Beyond the ML Model: Applying Safety Engineering Frameworks to Text-to-Image Development
AU  - Rismani, Shalaleh
AU  - Shelby, Renee
AU  - Smart, Andrew
AU  - Santos, Renelito Delos
AU  - Moon, AJung
AU  - Rostamzadeh, Negar
AB  - Identifying potential social and ethical risks in emerging machine learning (ML) models and their applications remains challenging. In this work, we applied two well-established safety engineering frameworks (FMEA, STPA) to a case study involving text-to-image models at three stages of the ML product development pipeline: data processing, integration of a T2I model with other models, and use. Results of our analysis demonstrate the safety frameworks - both of which are not designed explicitly examine social and ethical risks - can uncover failure and hazards that pose social and ethical risks. We discovered a broad range of failures and hazards (i.e., functional, social, and ethical) by analyzing interactions (i.e., between different ML models in the product, between the ML product and user, and between development teams) and processes (i.e., preparation of training data or workflows for using an ML service/product). Our findings underscore the value and importance of examining beyond an ML model in examining social and ethical risks, especially when we have minimal information about an ML model.
DA  - 2023/07/18/
PY  - 2023
DO  - 10.48550/arXiv.2307.10312
DP  - arXiv.org
PB  - arXiv
ST  - Beyond the ML Model
UR  - http://arxiv.org/abs/2307.10312
Y2  - 2024/07/08/22:05:22
L1  - https://arxiv.org/pdf/2307.10312.pdf
L2  - https://arxiv.org/abs/2307.10312
KW  - Computer Science - Computers and Society
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - CONF
TI  - Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts
AU  - Sarkar, Pritam
AU  - Beirami, Ahmad
AU  - Etemad, Ali
T2  - Thirty-seventh Conference on Neural Information Processing Systems
AB  - Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally struggle with context shifts, v-MAE and supervised learning exhibit more robustness. Moreover, our study shows that v-MAE is a strong temporal learner, whereas contrastive methods, v-SimCLR and v-MoCo, exhibit strong performances against viewpoint shifts. When studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set recognition performance if the pretrained VSSL encoders are used without finetuning. We hope that our work will contribute to the development of robust video representation learning frameworks for various real-world scenarios. The project page and code are available at: https://pritamqu.github.io/OOD-VSSL.
DA  - 2023/11/02/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=bKqrWLCMrX
Y2  - 2024/07/08/22:08:05
L1  - https://openreview.net/pdf?id=bKqrWLCMrX
ER  - 

TY  - JOUR
TI  - The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink
AU  - Patterson, David
AU  - Gonzalez, Joseph
AU  - Holzle, Urs
AU  - Le, Quoc
AU  - Liang, Chen
AU  - Munguia, Lluis-Miquel
AU  - Rothchild, Daniel
AU  - So, David R.
AU  - Texier, Maud
AU  - Dean, Jeff
T2  - Computer
AB  - Many recent papers highlight the importance of thinking about carbon emissions (CO2e) in machine learning (ML) workloads. While elevating the discussion, some early work was also based on incomplete information. (Unfortunately, the most widely cited quantitative estimate that was the basis for many of these papers was off by 88X.) Inspired by these concerns, we looked for approaches that would make ML training considerably less carbon intensive. We identified four best practices that dramatically reduce carbon emissions, and demonstrate two concrete examples of reducing CO2e by 650X over four years and 40X over one year by following them. Provided ML stakeholders follow best practices, we predict that the field will bend the curve of carbon footprint increases from ML training runs to first flatten and then reduce it by 2030 without sacrificing the current rate of rapid advances in ML, contrary to prior dire warnings that ML CO2e will soar.
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/MC.2022.3148714
DP  - DOI.org (Crossref)
VL  - 55
IS  - 7
SP  - 18
EP  - 28
J2  - Computer
LA  - en
SN  - 0018-9162, 1558-0814
UR  - https://ieeexplore.ieee.org/document/9810097/
Y2  - 2024/07/08/22:08:55
L1  - https://storage.googleapis.com/gweb-research2023-media/pubtools/7000.pdf
ER  - 

TY  - CONF
TI  - Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets
AU  - Tobin, Jimmy
AU  - Tomanek, Katrin
T2  - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
AB  - This study investigates the performance of personalized automatic speech recognition (ASR) for recognizing disordered speech using small amounts of per-speaker adaptation data. We trained personalized models for 195 individuals with different types and severities of speech impairment with training sets ranging in size from <1 minute to 18-20 minutes of speech data. Word error rate (WER) thresholds were selected to determine Success Percentage (the percentage of personalized models reaching the target WER) in different application scenarios. For the home automation scenario, 79% of speakers reached the target WER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63% of speakers reached the target WER. Further evaluation found similar improvement on test sets with conversational and out-of-domain, unprompted phrases. Our results demonstrate that with only a few minutes of recordings, individuals with disordered speech could benefit from personalized ASR.
C3  - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/ICASSP43922.2022.9747516
DP  - IEEE Xplore
SP  - 6637
EP  - 6641
UR  - https://ieeexplore.ieee.org/abstract/document/9747516?casa_token=0yavbda3s5gAAAAA:yKVmhr9Kxgfje9S1VCQy3OZmenCJ-pBV8ZENJ6_dnThvnyi8F0710RMORA1VYTI5-WmaCuEC
Y2  - 2024/07/08/22:10:13
L1  - https://arxiv.org/pdf/2110.04612
KW  - Speech recognition
KW  - Data models
KW  - Training
KW  - Training data
KW  - Adaptation models
KW  - personalized models
KW  - automatic speech recognition
KW  - Home automation
KW  - speech disorders
KW  - Vocabulary
ER  - 

TY  - GEN
TI  - CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents
AU  - Park, Jeongeun
AU  - Lim, Seungwon
AU  - Lee, Joonhyung
AU  - Park, Sangbeom
AU  - Chang, Minsuk
AU  - Yu, Youngjae
AU  - Choi, Sungjoon
AB  - In this paper, we focus on inferring whether the given user command is clear, ambiguous, or infeasible in the context of interactive robotic agents utilizing large language models (LLMs). To tackle this problem, we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command is classified as uncertain, we further distinguish it between ambiguous or infeasible commands leveraging LLMs with situational aware context in a zero-shot manner. For ambiguous commands, we disambiguate the command by interacting with users via question generation with LLMs. We believe that proper recognition of the given commands could lead to a decrease in malfunction and undesired actions of the robot, enhancing the reliability of interactive robot agents. We present a dataset for robotic situational awareness, consisting pair of high-level commands, scene descriptions, and labels of command type (i.e., clear, ambiguous, or infeasible). We validate the proposed method on the collected dataset, pick-and-place tabletop simulation. Finally, we demonstrate the proposed approach in real-world human-robot interaction experiments, i.e., handover scenarios.
DA  - 2024/06/26/
PY  - 2024
DO  - 10.48550/arXiv.2306.10376
DP  - arXiv.org
PB  - arXiv
ST  - CLARA
UR  - http://arxiv.org/abs/2306.10376
Y2  - 2024/07/08/22:11:04
L1  - https://arxiv.org/pdf/2306.10376.pdf
L2  - https://arxiv.org/abs/2306.10376
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models
AU  - Alabdulmohsin, Ibrahim
AU  - Lucic, Mario
AB  - We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.
DA  - 2022/08/23/
PY  - 2022
DO  - 10.48550/arXiv.2106.12887
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2106.12887
Y2  - 2024/07/08/22:12:56
L1  - https://arxiv.org/pdf/2106.12887.pdf
L2  - https://arxiv.org/abs/2106.12887
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - I.2.6
KW  - I.2.10
KW  - 68T05, 68T45, 93E35,
ER  - 

TY  - CONF
TI  - Interpretable Ranking with Generalized Additive Models
AU  - Zhuang, Honglei
AU  - Wang, Xuanhui
AU  - Bendersky, Michael
AU  - Grushetsky, Alexander
AU  - Wu, Yonghui
AU  - Mitrichev, Petr
AU  - Sterling, Ethan
AU  - Bell, Nathan
AU  - Ravina, Walker
AU  - Qian, Hai
T3  - WSDM '21
AB  - Interpretability of ranking models is a crucial yet relatively under-examined research area. Recent progress on this area largely focuses on generating post-hoc explanations for existing black-box ranking models. Though promising, such post-hoc methods cannot provide sufficiently accurate explanations in general, which makes them infeasible in many high-stakes scenarios, especially the ones with legal or policy constraints. Thus, building an intrinsically interpretable ranking model with transparent, self-explainable structure becomes necessary, but this remains less explored in the learning-to-rank setting.In this paper, we lay the groundwork for intrinsically interpretable learning-to-rank by introducing generalized additive models (GAMs) into ranking tasks. Generalized additive models (GAMs) are intrinsically interpretable machine learning models and have been extensively studied on regression and classification tasks. We study how to extend GAMs into ranking models which can handle both item-level and list-level features and propose a novel formulation of ranking GAMs. To instantiate ranking GAMs, we employ neural networks instead of traditional splines or regression trees. We also show that our neural ranking GAMs can be distilled into a set of simple and compact piece-wise linear functions that are much more efficient to evaluate with little accuracy loss. We conduct experiments on three data sets and show that our proposed neural ranking GAMs can outperform other traditional GAM baselines while maintaining similar interpretability.
C1  - New York, NY, USA
C3  - Proceedings of the 14th ACM International Conference on Web Search and Data Mining
DA  - 2021/03/08/
PY  - 2021
DO  - 10.1145/3437963.3441796
DP  - ACM Digital Library
SP  - 499
EP  - 507
PB  - Association for Computing Machinery
SN  - 978-1-4503-8297-7
UR  - https://doi.org/10.1145/3437963.3441796
Y2  - 2024/07/08/
ER  - 

TY  - CONF
TI  - Training independent subnetworks for robust prediction
AU  - Havasi, Marton
AU  - Jenatton, Rodolphe
AU  - Fort, Stanislav
AU  - Liu, Jeremiah Zhe
AU  - Snoek, Jasper
AU  - Lakshminarayanan, Balaji
AU  - Dai, Andrew Mingbo
AU  - Tran, Dustin
T2  - International Conference on Learning Representations
AB  - Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant runtime cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved 'for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.
DA  - 2020/10/02/
PY  - 2020
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=OGg9XnKxFAH
Y2  - 2024/07/08/22:13:59
L1  - https://openreview.net/pdf?id=OGg9XnKxFAH
ER  - 

TY  - GEN
TI  - Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research
AU  - Koch, Bernard
AU  - Denton, Emily
AU  - Hanna, Alex
AU  - Foster, Jacob G.
AB  - Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals. Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We find increasing concentration on fewer and fewer datasets within task communities, significant adoption of datasets from other tasks, and concentration across the field on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientific evaluation, AI ethics, and equity/access within the field.
DA  - 2021/12/03/
PY  - 2021
DO  - 10.48550/arXiv.2112.01716
DP  - arXiv.org
PB  - arXiv
ST  - Reduced, Reused and Recycled
UR  - http://arxiv.org/abs/2112.01716
Y2  - 2024/07/08/22:14:41
L1  - https://arxiv.org/pdf/2112.01716.pdf
L2  - https://arxiv.org/abs/2112.01716
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Calibration of Neural Networks using Splines
AU  - Gupta, Kartik
AU  - Rahimi, Amir
AU  - Ajanthan, Thalaiyasingam
AU  - Mensink, Thomas
AU  - Sminchisescu, Cristian
AU  - Hartley, Richard
AB  - Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spine-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Our Code is available at https://github.com/kartikgupta-at-anu/spline-calibration.
DA  - 2021/12/29/
PY  - 2021
DO  - 10.48550/arXiv.2006.12800
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2006.12800
Y2  - 2024/07/08/22:16:56
L1  - https://arxiv.org/pdf/2006.12800.pdf
L2  - https://arxiv.org/abs/2006.12800
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - Interpretable Actions: Controlling Experts with Understandable Commands
AU  - Baluja, Shumeet
AU  - Marwood, David
AU  - Covell, Michele
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - Despite the prevalence of deep neural networks, their single most cited drawback is that, even when successful, their operations are inscrutable.  For many applications, the desired outputs are the composition of externally-defined bases.  For such decomposable domains, we present a two-stage learning procedure producing combinations of the external bases which are trivially extractable from the network. In the first stage, the set of external bases that will form the solution are modeled as differentiable generator modules, controlled by the same parameters as the external bases.  In the second stage, a controller network is created that selects parameters for those generators, either successively or in parallel, to compose the final solution.  Through three tasks, we concretely demonstrate how our system yields readily understandable commands.  In one, we introduce a new form of artistic style transfer, learning to draw and color with crayons, in which the transformation of a photograph or painting occurs not as a single monolithic computation, but by the composition of thousands of individual, visualizable strokes.  The other two tasks, single-pass function approximation with arbitrary bases and shape-based synthesis, show how our approach produces understandable and extractable actions in two disparate domains.
DA  - 2021/05/18/
PY  - 2021
DO  - 10.1609/aaai.v35i6.16624
DP  - ojs.aaai.org
VL  - 35
IS  - 6
SP  - 4912
EP  - 4922
LA  - en
SN  - 2374-3468
ST  - Interpretable Actions
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/16624
Y2  - 2024/07/08/22:17:32
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/16624/16431
KW  - (Deep) Neural Network Algorithms
ER  - 

TY  - GEN
TI  - Evaluations and Methods for Explanation through Robustness Analysis
AU  - Hsieh, Cheng-Yu
AU  - Yeh, Chih-Kuan
AU  - Liu, Xuanqing
AU  - Ravikumar, Pradeep
AU  - Kim, Seungyeon
AU  - Kumar, Sanjiv
AU  - Hsieh, Cho-Jui
AB  - Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to "remove" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.
DA  - 2021/04/08/
PY  - 2021
DO  - 10.48550/arXiv.2006.00442
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2006.00442
Y2  - 2024/07/08/22:18:09
L1  - https://arxiv.org/pdf/2006.00442.pdf
L2  - https://arxiv.org/abs/2006.00442
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Explainable Deep One-Class Classification
AU  - Liznerski, Philipp
AU  - Ruff, Lukas
AU  - Vandermeulen, Robert A.
AU  - Franks, Billy Joe
AU  - Kloft, Marius
AU  - Müller, Klaus-Robert
AB  - Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.
DA  - 2021/03/18/
PY  - 2021
DO  - 10.48550/arXiv.2007.01760
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2007.01760
Y2  - 2024/07/08/22:18:36
L1  - https://arxiv.org/pdf/2007.01760.pdf
L2  - https://arxiv.org/abs/2007.01760
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Disentangling sampling and labeling bias for learning in large-output spaces
AU  - Menon, Aditya Krishna
AU  - Rawat, Ankit Singh
AU  - Yu, Felix
AU  - Jayasumana, Sadeep
AU  - Kumar, Sanjiv
AU  - Reddi, Sashank
AU  - Jitkrittum, Wittawat
AB  - Negative sampling is a widely adopted technique to enable efficient training in settings with a large number of classes. Typically, negative sampling approaches aim at approximating the value or gradient of the computationally expensive loss function that takes all the negative labels into account. In this work, we study the connection between negative sampling approaches and loss modification techniques for countering label imbalance. We show that different (bias) correction strategies that accompany negative sampling approaches can have unintended consequences on the model's performance on various data sub-populations. We then propose a unified approach to tackle both sampling bias, arising from working with a subset of all negative classes, and labeling bias, which is inherently present in the data due to label-imbalance. Finally, we verify our analysis and demonstrate the utility of our unified approach through empirical evaluation on standard image classification and retrieval benchmarks.
C3  - International conference on machine learning (ICML) 2021
DA  - 2021///
PY  - 2021
ER  - 

TY  - CONF
TI  - Don't Search for a Search Method — Simple Heuristics Suffice for Adversarial Text Attacks
AU  - Berger, Nathaniel
AU  - Riezler, Stefan
AU  - Ebert, Sebastian
AU  - Sokolov, Artem
T2  - EMNLP 2021
A2  - Moens, Marie-Francine
A2  - Huang, Xuanjing
A2  - Specia, Lucia
A2  - Yih, Scott Wen-tau
AB  - Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple heuristics exploiting nearest neighbors without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.
C1  - Online and Punta Cana, Dominican Republic
C3  - Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
DA  - 2021/11//
PY  - 2021
DO  - 10.18653/v1/2021.emnlp-main.647
DP  - ACLWeb
SP  - 8216
EP  - 8224
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.emnlp-main.647
Y2  - 2024/07/08/22:19:41
L1  - https://aclanthology.org/2021.emnlp-main.647.pdf
ER  - 

TY  - CONF
TI  - Addressing Stability in Classifier Explanations
AU  - Samiei, Siavash
AU  - Baratalipour, Nasrin
AU  - Yadav, Pranjul
AU  - Roy, Amitabha
AU  - He, Dake
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - Machine learning based classifiers are often a black box when considering the contribution of inputs to the output probability of a label, especially with complex non-linear models such as neural networks. A popular way to explain machine learning model outputs in a model agnostic manner is through the use of Shapley values. For our use case of abuse fighting in digital advertisements, one primary impediment of using Shapley values in explanations was a problem of instability. Specifically, the instability problem manifests as explanations for the same example varying greatly due to random sampling in the algorithm. We found it useful to view this problem explicitly as Monte Carlo integration in the form of averaging the model output while varying only a subset of features in the example to be explained. In turn, this guides the number of samples needed to achieve a stable estimate of individual Shapley values and unlocked the use of Shapley value based explainers for our models as well as classifiers in general, including neural networks.
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9671458
DP  - IEEE Xplore
SP  - 1920
EP  - 1927
UR  - https://ieeexplore.ieee.org/document/9671458
Y2  - 2024/07/08/22:20:08
KW  - Machine learning
KW  - Computational modeling
KW  - Neural networks
KW  - Measurement
KW  - Training
KW  - neural networks
KW  - Big Data
KW  - Monte Carlo methods
KW  - Shapley values
KW  - model explanations
ER  - 

TY  - GEN
TI  - Understanding convolution on graphs via energies
AU  - Di Giovanni, Francesco
AU  - Rowbottom, James
AU  - Chamberlain, Benjamin P.
AU  - Markovich, Thomas
AU  - Bronstein, Michael M.
AB  - Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with symmetric weights minimize a multi-particle energy that generalizes the Dirichlet energy; in this setting, the weight matrices induce edge-wise attraction (repulsion) through their positive (negative) eigenvalues, thereby controlling whether the features are being smoothed or sharpened. We also extend the analysis to non-linear GNNs, and demonstrate that some existing time-continuous GNNs are instead always dominated by the low frequencies. Finally, we validate our theoretical findings through ablations and real-world experiments.
DA  - 2023/09/06/
PY  - 2023
DO  - 10.48550/arXiv.2206.10991
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.10991
Y2  - 2024/07/08/22:22:28
L1  - https://arxiv.org/pdf/2206.10991.pdf
L2  - https://arxiv.org/abs/2206.10991
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Understanding the Failure Modes of Out-of-Distribution Generalization
AU  - Nagarajan, Vaishnavh
AU  - Andreassen, Anders
AU  - Neyshabur, Behnam
AB  - Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.
DA  - 2021/04/29/
PY  - 2021
DO  - 10.48550/arXiv.2010.15775
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2010.15775
Y2  - 2024/07/08/22:22:48
L1  - https://arxiv.org/pdf/2010.15775.pdf
L2  - https://arxiv.org/abs/2010.15775
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Balancing Robustness and Sensitivity using Feature Contrastive Learning
AU  - Kim, Seungyeon
AU  - Glasner, Daniel
AU  - Ramalingam, Srikumar
AU  - Hsieh, Cho-Jui
AU  - Papineni, Kishore
AU  - Kumar, Sanjiv
AB  - It is generally believed that robust training of extremely large networks is critical to their success in real-world applications. However, when taken to the extreme, methods that promote robustness can hurt the model's sensitivity to rare or underrepresented patterns. In this paper, we discuss this trade-off between sensitivity and robustness to natural (non-adversarial) perturbations by introducing two notions: contextual feature utility and contextual feature sensitivity. We propose Feature Contrastive Learning (FCL) that encourages a model to be more sensitive to the features that have higher contextual utility. Empirical results demonstrate that models trained with FCL achieve a better balance of robustness and sensitivity, leading to improved generalization in the presence of noise on both vision and NLP datasets.
DA  - 2021/05/19/
PY  - 2021
DO  - 10.48550/arXiv.2105.09394
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2105.09394
Y2  - 2024/07/08/22:22:59
L1  - https://arxiv.org/pdf/2105.09394.pdf
L2  - https://arxiv.org/abs/2105.09394
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - COSMOS: Catching Out-of-Context Misinformation with Self-Supervised Learning
AU  - Aneja, Shivangi
AU  - Bregler, Chris
AU  - Nießner, Matthias
AB  - Despite the recent attention to DeepFakes, one of the most prevalent ways to mislead audiences on social media is the use of unaltered images in a new but false context. To address these challenges and support fact-checkers, we propose a new method that automatically detects out-of-context image and text pairs. Our key insight is to leverage the grounding of image with text to distinguish out-of-context scenarios that cannot be disambiguated with language alone. We propose a self-supervised training strategy where we only need a set of captioned images. At train time, our method learns to selectively align individual objects in an image with textual claims, without explicit supervision. At test time, we check if both captions correspond to the same object(s) in the image but are semantically different, which allows us to make fairly accurate out-of-context predictions. Our method achieves 85% out-of-context detection accuracy. To facilitate benchmarking of this task, we create a large-scale dataset of 200K images with 450K textual captions from a variety of news websites, blogs, and social media posts. The dataset and source code is publicly available at https://shivangi-aneja.github.io/projects/cosmos/.
DA  - 2021/04/21/
PY  - 2021
DO  - 10.48550/arXiv.2101.06278
DP  - arXiv.org
PB  - arXiv
ST  - COSMOS
UR  - http://arxiv.org/abs/2101.06278
Y2  - 2024/07/08/22:23:17
L1  - https://arxiv.org/pdf/2101.06278.pdf
L2  - https://arxiv.org/abs/2101.06278
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Robust and Private Learning of Halfspaces
AU  - Ghazi, Badih
AU  - Kumar, Ravi
AU  - Manurangsi, Pasin
AU  - Nguyen, Thao
T2  - International Conference on Artificial Intelligence and Statistics
AB  - In this work, we study the trade-off between differential privacy and adversarial robustness under L2L2L_2-perturbations in the context of learning halfspaces. We prove nearly tight bounds on the sample complexity of robust private learning of halfspaces for a large regime of parameters. A highlight of our results is that robust and private learning is harder than robust or private learning alone. We complement our theoretical analysis with experimental results on the MNIST and USPS datasets, for a learning algorithm that is both differentially private and adversarially robust.
C3  - Proceedings of The 24th International Conference on Artificial Intelligence and Statistics
DA  - 2021/03/18/
PY  - 2021
DP  - proceedings.mlr.press
SP  - 1603
EP  - 1611
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v130/ghazi21a.html
Y2  - 2024/07/08/22:23:44
L1  - http://proceedings.mlr.press/v130/ghazi21a/ghazi21a.pdf
L1  - http://proceedings.mlr.press/v130/ghazi21a/ghazi21a-supp.pdf
ER  - 

TY  - GEN
TI  - Best of both worlds: local and global explanations with human-understandable concepts
AU  - Schrouff, Jessica
AU  - Baur, Sebastien
AU  - Hou, Shaobo
AU  - Mincu, Diana
AU  - Loreaux, Eric
AU  - Blanes, Ralph
AU  - Wexler, James
AU  - Karthikesalingam, Alan
AU  - Kim, Been
AB  - Interpretability techniques aim to provide the rationale behind a model's decision, typically by explaining either an individual prediction (local explanation, e.g. 'why is this patient diagnosed with this condition') or a class of predictions (global explanation, e.g. 'why is this set of patients diagnosed with this condition in general'). While there are many methods focused on either one, few frameworks can provide both local and global explanations in a consistent manner. In this work, we combine two powerful existing techniques, one local (Integrated Gradients, IG) and one global (Testing with Concept Activation Vectors), to provide local and global concept-based explanations. We first sanity check our idea using two synthetic datasets with a known ground truth, and further demonstrate with a benchmark natural image dataset. We test our method with various concepts, target classes, model architectures and IG parameters (e.g. baselines). We show that our method improves global explanations over vanilla TCAV when compared to ground truth, and provides useful local insights. Finally, a user study demonstrates the usefulness of the method compared to no or global explanations only. We hope our work provides a step towards building bridges between many existing local and global methods to get the best of both worlds.
DA  - 2022/01/31/
PY  - 2022
DO  - 10.48550/arXiv.2106.08641
DP  - arXiv.org
PB  - arXiv
ST  - Best of both worlds
UR  - http://arxiv.org/abs/2106.08641
Y2  - 2024/07/08/22:24:04
L1  - https://arxiv.org/pdf/2106.08641.pdf
L2  - https://arxiv.org/abs/2106.08641
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A Tale Of Two Long Tails
AU  - D'souza, Daniel
AU  - Nussbaum, Zach
AU  - Agarwal, Chirag
AU  - Hooker, Sara
AB  - As machine learning models are increasingly employed to assist human decision-makers, it becomes critical to communicate the uncertainty associated with these model predictions. However, the majority of work on uncertainty has focused on traditional probabilistic or ranking approaches - where the model assigns low probabilities or scores to uncertain examples. While this captures what examples are challenging for the model, it does not capture the underlying source of the uncertainty. In this work, we seek to identify examples the model is uncertain about and characterize the source of said uncertainty. We explore the benefits of designing a targeted intervention - targeted data augmentation of the examples where the model is uncertain over the course of training. We investigate whether the rate of learning in the presence of additional information differs between atypical and noisy examples? Our results show that this is indeed the case, suggesting that well-designed interventions over the course of training can be an effective way to characterize and distinguish between different sources of uncertainty.
DA  - 2021/07/27/
PY  - 2021
DO  - 10.48550/arXiv.2107.13098
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2107.13098
Y2  - 2024/07/08/22:24:33
L1  - https://arxiv.org/pdf/2107.13098.pdf
L2  - https://arxiv.org/abs/2107.13098
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Disentangling Preference Representations for Recommendation Critiquing with ß-VAE
AU  - Nema, Preksha
AU  - Karatzoglou, Alexandros
AU  - Radlinski, Filip
T3  - CIKM '21
AB  - Modern recommender systems usually embed users and items into a learned vector space representation. Similarity in this space is used to generate recommendations, and recommendation methods are agnostic to the structure of the embedding space. Motivated by the need for recommendation systems to be more transparent and controllable, we postulate that it is beneficial to assign meaning to some of the dimensions of user and item representations. Disentanglement is one technique commonly used for this purpose. We presenta novel supervised disentangling approach for recommendation tasks. Our model learns embeddings where attributes of interest are disentangled, while requiring only a very small number of labeled items at training time. The model can then generate interactive and critiquable recommendations for all users, without requiring any labels at recommendation time, and without sacrificing any recommendation performance. Our approach thus provides users with levers to manipulate, critique and fine-tune recommendations, and gives insight into why particular recommendations are made. Given only user-item interactions at recommendation time, we show that it identifies user tastes with respect to the attributes that have been disentangled, allowing for users to manipulate recommendations across these attributes.
C1  - New York, NY, USA
C3  - Proceedings of the 30th ACM International Conference on Information & Knowledge Management
DA  - 2021/10/30/
PY  - 2021
DO  - 10.1145/3459637.3482425
DP  - ACM Digital Library
SP  - 1356
EP  - 1365
PB  - Association for Computing Machinery
SN  - 978-1-4503-8446-9
UR  - https://doi.org/10.1145/3459637.3482425
Y2  - 2024/07/08/
ER  - 

TY  - CONF
TI  - On Completeness-aware Concept-Based Explanations in Deep Neural Networks
AU  - Yeh, Chih-Kuan
AU  - Kim, Been
AU  - Arik, Sercan
AU  - Li, Chun-Liang
AU  - Pfister, Tomas
AU  - Ravikumar, Pradeep
AB  - Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of \emph{completeness}, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose \emph{ConceptSHAP}. Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable.
C3  - Advances in Neural Information Processing Systems
DA  - 2020///
PY  - 2020
DP  - Neural Information Processing Systems
VL  - 33
SP  - 20554
EP  - 20565
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html
Y2  - 2024/07/08/22:27:56
L1  - https://proceedings.neurips.cc/paper_files/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf
ER  - 

TY  - CONF
TI  - Shape Constraints for Set Functions
AU  - Cotter, Andrew
AU  - Gupta, Maya
AU  - Jiang, Heinrich
AU  - Louidor, Erez
AU  - Muller, James
AU  - Narayan, Tamann
AU  - Wang, Serena
AU  - Zhu, Tao
T2  - International Conference on Machine Learning
AB  - Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions, and then propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees on the model behavior, which makes it easier to explain and debug.
C3  - Proceedings of the 36th International Conference on Machine Learning
DA  - 2019/05/24/
PY  - 2019
DP  - proceedings.mlr.press
SP  - 1388
EP  - 1396
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v97/cotter19a.html
Y2  - 2024/07/08/22:30:22
L1  - http://proceedings.mlr.press/v97/cotter19a/cotter19a.pdf
L1  - http://proceedings.mlr.press/v97/cotter19a/cotter19a-supp.pdf
ER  - 

TY  - CONF
TI  - Explaining deep neural networks using unsupervised clustering
AU  - Arik, Sercan
AU  - Liu, Yu-Han
T2  - Workshop on Human Interpretability in Machine Learning
AB  - We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in model’s prediction.
DA  - 2020///
PY  - 2020
UR  - https://research.google/pubs/explaining-deep-neural-networks-using-unsupervised-clustering/
ER  - 

TY  - GEN
TI  - The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models
AU  - Tenney, Ian
AU  - Wexler, James
AU  - Bastings, Jasmijn
AU  - Bolukbasi, Tolga
AU  - Coenen, Andy
AU  - Gehrmann, Sebastian
AU  - Jiang, Ellen
AU  - Pushkarna, Mahima
AU  - Radebaugh, Carey
AU  - Reif, Emily
AU  - Yuan, Ann
AB  - We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.
DA  - 2020/08/12/
PY  - 2020
DO  - 10.48550/arXiv.2008.05122
DP  - arXiv.org
PB  - arXiv
ST  - The Language Interpretability Tool
UR  - http://arxiv.org/abs/2008.05122
Y2  - 2024/07/08/22:32:02
L1  - https://arxiv.org/pdf/2008.05122.pdf
L2  - https://arxiv.org/abs/2008.05122
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks
AU  - Lee, Kimin
AU  - Yun, Sukmin
AU  - Lee, Kibok
AU  - Lee, Honglak
AU  - Li, Bo
AU  - Shin, Jinwoo
AB  - Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks poorly generalize from such noisy training datasets. In this paper, we propose a novel inference method, Deep Determinantal Generative Classifier (DDGC), which can obtain a more robust decision boundary under any softmax neural classifier pre-trained on noisy datasets. Our main idea is inducing a generative classifier on top of hidden feature spaces of the discriminative deep model. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy, with neither re-training of the deep model nor changing its architectures. In particular, we show that DDGC not only generalizes well from noisy labels, but also is robust against adversarial perturbations due to its large margin property. Finally, we propose the ensemble version ofDDGC to improve its performance, by investigating the layer-wise characteristics of generative classifier. Our extensive experimental results demonstrate the superiority of DDGC given different learning models optimized by various training techniques to handle noisy labels or adversarial samples. For instance, on CIFAR-10 dataset containing 45% noisy training labels, we improve the test accuracy of a deep model optimized by the state-of-the-art noise-handling training method from33.34% to 43.02%.
DA  - 2018/09/27/
PY  - 2018
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=rkle3i09K7
Y2  - 2024/07/08/22:33:22
L1  - https://openreview.net/pdf?id=rkle3i09K7
ER  - 

TY  - CONF
TI  - Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning
AU  - Eysenbach, Benjamin
AU  - Gu, Shixiang
AU  - Ibarz, Julian
AU  - Levine, Sergey
T2  - International Conference on Learning Representations
AB  - Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.
DA  - 2018/02/15/
PY  - 2018
DP  - openreview.net
LA  - en
ST  - Leave no Trace
UR  - https://openreview.net/forum?id=S1vuO-bCW
Y2  - 2024/07/08/22:35:45
L1  - https://openreview.net/pdf?id=S1vuO-bCW
ER  - 

TY  - CONF
TI  - Diminishing Returns Shape Constraints for Interpretability and Regularization
AU  - Gupta, Maya
AU  - Bahri, Dara
AU  - Cotter, Andrew
AU  - Canini, Kevin
AB  - We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs.  We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks.  We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability.
C3  - Advances in Neural Information Processing Systems
DA  - 2018///
PY  - 2018
DP  - Neural Information Processing Systems
VL  - 31
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper/2018/hash/caa202034f268232c26fac9435f54e15-Abstract.html
Y2  - 2024/07/08/22:36:16
L1  - https://proceedings.neurips.cc/paper_files/paper/2018/file/caa202034f268232c26fac9435f54e15-Paper.pdf
ER  - 

TY  - GEN
TI  - Human-in-the-Loop Interpretability Prior
AU  - Lage, Isaac
AU  - Ross, Andrew Slavin
AU  - Kim, Been
AU  - Gershman, Samuel J.
AU  - Doshi-Velez, Finale
AB  - We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.
DA  - 2018/10/30/
PY  - 2018
DO  - 10.48550/arXiv.1805.11571
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.11571
Y2  - 2024/07/08/22:37:01
L1  - https://arxiv.org/pdf/1805.11571.pdf
L2  - https://arxiv.org/abs/1805.11571
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Adversarial Spheres
AU  - Gilmer, Justin
AU  - Metz, Luke
AU  - Faghri, Fartash
AU  - Schoenholz, Samuel S.
AU  - Raghu, Maithra
AU  - Wattenberg, Martin
AU  - Goodfellow, Ian
AB  - Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classiﬁcation error rate to the average distance to the nearest misclassiﬁcation, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.
DA  - 2018/09/10/
PY  - 2018
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1801.02774
Y2  - 2024/07/08/22:37:37
L1  - https://arxiv.org/pdf/1801.02774
KW  - I.2.6
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - 68T45
ER  - 

TY  - CONF
TI  - Learning how to explain neural networks: PatternNet and PatternAttribution
AU  - Kindermans, Pieter-Jan
AU  - Schütt, Kristof T.
AU  - Alber, Maximilian
AU  - Müller, Klaus-Robert
AU  - Erhan, Dumitru
AU  - Kim, Been
AU  - Dähne, Sven
T2  - International Conference on Learning Representations
AB  - DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.
DA  - 2018/02/15/
PY  - 2018
DP  - openreview.net
LA  - en
ST  - Learning how to explain neural networks
UR  - https://openreview.net/forum?id=Hkn7CBaTW
Y2  - 2024/07/08/22:39:18
L1  - https://openreview.net/pdf?id=Hkn7CBaTW
ER  - 

TY  - JOUR
TI  - Learning to Attack: Adversarial Transformation Networks
AU  - Baluja, Shumeet
AU  - Fischer, Ian
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - With the rapidly increasing popularity of deep neural networks for image recognition tasks, a parallel interest in generating adversarial examples to attack the trained models has arisen. To date, these approaches have involved either directly computing gradients with respect to the image pixels or directly solving an optimization on the image pixels. We generalize this pursuit in a novel direction: can a separate network be trained to efficiently attack another fully trained network? We demonstrate that it is possible, and that the generated attacks yield startling insights into the weaknesses of the target network.  We call such a network an Adversarial Transformation Network (ATN). ATNs transform any input into an adversarial attack on the target network, while being minimally perturbing to the original inputs and the target network's outputs. Further, we show that ATNs are capable of not only causing the target network to make an error, but can be constructed to explicitly control the type of misclassification made. We demonstrate ATNs on both simple MNIST-digit classifiers and state-of-the-art ImageNet classifiers deployed by Google, Inc.: Inception ResNet-v2.
DA  - 2018/04/29/
PY  - 2018
DO  - 10.1609/aaai.v32i1.11672
DP  - ojs.aaai.org
VL  - 32
IS  - 1
LA  - en
SN  - 2374-3468
ST  - Learning to Attack
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/11672
Y2  - 2024/07/08/22:40:05
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/11672/11531
KW  - Machine Learning
KW  - Adversarial Training
KW  - Deep Neural Networks
KW  - ImageNet
ER  - 

TY  - GEN
TI  - Motivating the Rules of the Game for Adversarial Example Research
AU  - Gilmer, Justin
AU  - Adams, Ryan P.
AU  - Goodfellow, Ian
AU  - Andersen, David
AU  - Dahl, George E.
AB  - Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.
DA  - 2018/07/19/
PY  - 2018
DO  - 10.48550/arXiv.1807.06732
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1807.06732
Y2  - 2024/07/08/22:40:29
L1  - https://arxiv.org/pdf/1807.06732.pdf
L2  - https://arxiv.org/abs/1807.06732
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks
AU  - Carlini, Nicholas
AU  - Liu, Chang
AU  - Erlingsson, Úlfar
AU  - Kos, Jernej
AU  - Song, Dawn
AB  - This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.
DA  - 2019/07/16/
PY  - 2019
DO  - 10.48550/arXiv.1802.08232
DP  - arXiv.org
PB  - arXiv
ST  - The Secret Sharer
UR  - http://arxiv.org/abs/1802.08232
Y2  - 2024/07/08/22:40:45
L1  - https://arxiv.org/pdf/1802.08232.pdf
L2  - https://arxiv.org/abs/1802.08232
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Adversarially Robust Generalization Requires More Data
AU  - Schmidt, Ludwig
AU  - Santurkar, Shibani
AU  - Tsipras, Dimitris
AU  - Talwar, Kunal
AU  - Mądry, Aleksander
AB  - Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classiﬁers with high “standard” accuracy to produce an incorrect prediction with high conﬁdence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be signiﬁcantly larger than that of “standard” learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classiﬁcation datasets and show that a similar gap exists here as well. We postulate that the diﬃculty of training robust classiﬁers stems, at least partially, from this inherently larger sample complexity.
DA  - 2018/05/02/
PY  - 2018
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1804.11285
Y2  - 2024/07/08/22:41:12
L1  - https://arxiv.org/pdf/1804.11285
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - CONF
TI  - Risk-Sensitive Generative Adversarial Imitation Learning
AU  - Lacotte, Jonathan
AU  - Ghavamzadeh, Mohammad
AU  - Chow, Yinlam
AU  - Pavone, Marco
T2  - The 22nd International Conference on Artificial Intelligence and Statistics
AB  - We study risk-sensitive imitation learning where the agent’s goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk- sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.
C3  - Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
DA  - 2019/04/11/
PY  - 2019
DP  - proceedings.mlr.press
SP  - 2154
EP  - 2163
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v89/lacotte19a.html
Y2  - 2024/07/08/22:41:29
L1  - http://proceedings.mlr.press/v89/lacotte19a/lacotte19a.pdf
L1  - http://proceedings.mlr.press/v89/lacotte19a/lacotte19a-supp.pdf
ER  - 

TY  - GEN
TI  - Beyond safety: Toward a value-sensitive approach to the design of AI systems
AU  - Fiannaca, Alex
AU  - Bennett, Cindy
AU  - Kane, Shaun
AU  - Morris, Meredith Ringel
AB  - As modern, pre-trained ML models have proliferated in recent years , many researchers and practitioners have made significant efforts to prevent AI systems from causing harm. This focus on safety is critical, but a singular focus on safety can come at the exclusion of considering other important stakeholder values and the interactions between those values in the AI systems we build. In this position paper, we propose that the AI community should incorporate ideas from the Value-Sensitive Design framework from the Human-Computer Interaction community to ensure the needs and values of all stakeholders are reflected in the systems we build. We share observations and reflections from our experiences working on AI-supported accessibility technologies and with members of various disability communities to illustrate the tensions that sometimes arise between safety and other values.
CY  - NeurIPS 2022 Human-Centered AI Workshop
DA  - 2022///
PY  - 2022
UR  - https://research.google/pubs/beyond-safety-toward-a-value-sensitive-approach-to-the-design-of-ai-systems/
ER  - 

TY  - JOUR
TI  - Risk-Constrained Reinforcement Learning with Percentile Risk Criteria
AU  - Chow, Yinlam
AU  - Ghavamzadeh, Mohammad
AU  - Janson, Lucas
AU  - Pavone, Marco
T2  - Journal of Machine Learning Research
AB  - In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.
DA  - 2018///
PY  - 2018
DP  - www.jmlr.org
VL  - 18
IS  - 167
SP  - 1
EP  - 51
SN  - 1533-7928
UR  - http://jmlr.org/papers/v18/15-636.html
Y2  - 2024/07/08/22:45:22
L1  - http://jmlr.org/papers/volume18/15-636/15-636.pdf
ER  - 

TY  - GEN
TI  - Cyber, nano, and AGI risks: Decentralized approaches to reducing risks
AU  - Duettmann, Allison
AU  - Peterson, Christine
AU  - Miller, Mark S.
AB  - The aim of this paper, rather than attempting to present one coherent strategy for reducing existential risks, is to introduce a variety of possible options with the goal of broadening the discussion and inviting further investigation. Two themes appear throughout: (1) the proposed approaches for risk reduction attempt to avoid the dangers of centralized “solutions,” and (2) cybersecurity is not treated as a separate risk. Instead, trustworthy cybersecurity is a prerequisite for the success of our proposed approaches to risk reduction.
CY  - The first colloquium on catastrophic and existential risk
DA  - 2017///
PY  - 2017
UR  - https://research.google/pubs/cyber-nano-and-agi-risks-decentralized-approaches-to-reducing-risks/
ER  - 

TY  - GEN
TI  - Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
AU  - Morris, Meredith Ringel
AU  - Brubaker, Jed R.
AB  - As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on speciﬁc people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we ﬁrst discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneﬁcial manner.
DA  - 2024/05/08/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Generative Ghosts
UR  - http://arxiv.org/abs/2402.01662
Y2  - 2024/07/08/22:47:07
L1  - https://arxiv.org/pdf/2402.01662v2
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Recipes for Safety in Open-domain Chatbots
AU  - Xu, Jing
AU  - Ju, Da
AU  - Li, Margaret
AU  - Boureau, Y.-Lan
AU  - Weston, Jason
AU  - Dinan, Emily
AB  - Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.
DA  - 2021/08/04/
PY  - 2021
DO  - 10.48550/arXiv.2010.07079
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2010.07079
Y2  - 2024/07/11/11:14:28
L1  - https://arxiv.org/pdf/2010.07079.pdf
L2  - https://arxiv.org/abs/2010.07079
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack
AU  - Dinan, Emily
AU  - Humeau, Samuel
AU  - Chintagunta, Bharath
AU  - Weston, Jason
T2  - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
AB  - The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gala´n-Garc´ıa et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, ﬁx it strategy with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.
C1  - Hong Kong, China
C3  - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
DA  - 2019///
PY  - 2019
DO  - 10.18653/v1/D19-1461
DP  - DOI.org (Crossref)
SP  - 4536
EP  - 4545
LA  - en
PB  - Association for Computational Linguistics
ST  - Build it Break it Fix it for Dialogue Safety
UR  - https://www.aclweb.org/anthology/D19-1461
Y2  - 2024/07/11/11:14:53
L1  - https://aclanthology.org/D19-1461.pdf
ER  - 

TY  - GEN
TI  - The Scientific Method in the Science of Machine Learning
AU  - Forde, Jessica Zosa
AU  - Paganini, Michela
AB  - In the quest to align deep learning with the sciences to address calls for rigor, safety, and interpretability in machine learning systems, this contribution identifies key missing pieces: the stages of hypothesis formulation and testing, as well as statistical and systematic uncertainty estimation -- core tenets of the scientific method. This position paper discusses the ways in which contemporary science is conducted in other domains and identifies potentially useful practices. We present a case study from physics and describe how this field has promoted rigor through specific methodological practices, and provide recommendations on how machine learning researchers can adopt these practices into the research ecosystem. We argue that both domain-driven experiments and application-agnostic questions of the inner workings of fundamental building blocks of machine learning models ought to be examined with the tools of the scientific method, to ensure we not only understand effect, but also begin to understand cause, which is the raison d'\^{e}tre of science.
DA  - 2019/04/24/
PY  - 2019
DO  - 10.48550/arXiv.1904.10922
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1904.10922
Y2  - 2024/07/11/11:16:28
L1  - https://arxiv.org/pdf/1904.10922.pdf
L2  - https://arxiv.org/abs/1904.10922
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Optimizing over a Restricted Policy Class in MDPs
AU  - Banijamali, Ershad
AU  - Abbasi-Yadkori, Yasin
AU  - Ghavamzadeh, Mohammad
AU  - Vlassis, Nikos
T2  - The 22nd International Conference on Artificial Intelligence and Statistics
AB  - We address the problem of finding an optimal policy in a Markov decision process (MDP) under a restricted policy class defined by the convex hull of a set of base policies. This problem is of great interest in applications in which a number of reasonably good (or safe) policies are already known and we are interested in optimizing in their convex hull. We first prove that solving this problem is NP-hard. We then propose an efficient algorithm that finds a policy whose performance is almost as good as that of the best convex combination of the base policies, under the assumption that the occupancy measures of the base policies have a large overlap. The running time of the proposed algorithm is linear in the number of states and polynomial in the number of base policies. A distinct advantage of the proposed algorithm is that, apart from the computation of the occupancy measures of the base policies, it does not need to interact with the environment during the optimization process. This is especially important (i) in problems that due to concerns such as safety, we are restricted in interacting with the environment only through the (safe) base policies, and (ii) in complex systems where estimating the value of a policy can be a time consuming process.
C3  - Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
DA  - 2019/04/11/
PY  - 2019
DP  - proceedings.mlr.press
SP  - 3042
EP  - 3050
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v89/banijamali19a.html
Y2  - 2024/07/11/11:16:48
L1  - http://proceedings.mlr.press/v89/banijamali19a/banijamali19a.pdf
L1  - http://proceedings.mlr.press/v89/banijamali19a/banijamali19a-supp.pdf
ER  - 

TY  - CONF
TI  - mixup: Beyond Empirical Risk Minimization
AU  - Zhang, Hongyi
AU  - Cisse, Moustapha
AU  - Dauphin, Yann N.
AU  - Lopez-Paz, David
T2  - International Conference on Learning Representations
AB  - Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.
DA  - 2018/02/15/
PY  - 2018
DP  - openreview.net
LA  - en
ST  - mixup
UR  - https://openreview.net/forum?id=r1Ddp1-Rb
Y2  - 2024/07/11/11:18:43
L1  - https://openreview.net/pdf?id=r1Ddp1-Rb
ER  - 

TY  - CONF
TI  - Tilted Empirical Risk Minimization
AU  - Li, Tian
AU  - Beirami, Ahmad
AU  - Sanjabi, Maziar
AU  - Smith, Virginia
T2  - International Conference on Learning Representations
AB  - Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.
DA  - 2020/10/02/
PY  - 2020
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=K5YasWXZT3O
Y2  - 2024/07/11/11:19:07
L1  - https://openreview.net/pdf?id=K5YasWXZT3O
ER  - 

TY  - GEN
TI  - Analysis and Mitigations of Reverse Engineering Attacks on Local Feature Descriptors
AU  - Dangwal, Deeksha
AU  - Lee, Vincent T.
AU  - Kim, Hyo Jin
AU  - Shen, Tianwei
AU  - Cowan, Meghan
AU  - Shah, Rajvi
AU  - Trippel, Caroline
AU  - Reagen, Brandon
AU  - Sherwood, Timothy
AU  - Balntas, Vasileios
AU  - Alaghi, Armin
AU  - Ilg, Eddy
AB  - As autonomous driving and augmented reality evolve, a practical concern is data privacy. In particular, these applications rely on localization based on user images. The widely adopted technology uses local feature descriptors, which are derived from the images and it was long thought that they could not be reverted back. However, recent work has demonstrated that under certain conditions reverse engineering attacks are possible and allow an adversary to reconstruct RGB images. This poses a potential risk to user privacy. We take this a step further and model potential adversaries using a privacy threat model. Subsequently, we show under controlled conditions a reverse engineering attack on sparse feature maps and analyze the vulnerability of popular descriptors including FREAK, SIFT and SOSNet. Finally, we evaluate potential mitigation techniques that select a subset of descriptors to carefully balance privacy reconstruction risk while preserving image matching accuracy; our results show that similar accuracy can be obtained when revealing less information.
DA  - 2021/05/08/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2105.03812
Y2  - 2024/07/11/11:23:43
L1  - https://arxiv.org/pdf/2105.03812
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Silent Data Corruptions at Scale
AU  - Dixit, Harish Dattatraya
AU  - Pendharkar, Sneha
AU  - Beadon, Matt
AU  - Mason, Chris
AU  - Chakravarthy, Tejasvi
AU  - Muthiah, Bharath
AU  - Sankar, Sriram
AB  - Silent Data Corruption (SDC) can have negative impact on large-scale infrastructure services. SDCs are not captured by error reporting mechanisms within a Central Processing Unit (CPU) and hence are not traceable at the hardware level. However, the data corruptions propagate across the stack and manifest as application-level problems. These types of errors can result in data loss and can require months of debug engineering time. In this paper, we describe common defect types observed in silicon manufacturing that leads to SDCs. We discuss a real-world example of silent data corruption within a datacenter application. We provide the debug flow followed to root-cause and triage faulty instructions within a CPU using a case study, as an illustration on how to debug this class of errors. We provide a high-level overview of the mitigations to reduce the risk of silent data corruptions within a large production fleet. In our large-scale infrastructure, we have run a vast library of silent error test scenarios across hundreds of thousands of machines in our fleet. This has resulted in hundreds of CPUs detected for these errors, showing that SDCs are a systemic issue across generations. We have monitored SDCs for a period longer than 18 months. Based on this experience, we determine that reducing silent data corruptions requires not only hardware resiliency and production detection mechanisms, but also robust fault-tolerant software architectures.
DA  - 2021/02/22/
PY  - 2021
DO  - 10.48550/arXiv.2102.11245
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2102.11245
Y2  - 2024/07/11/11:24:40
L1  - https://arxiv.org/pdf/2102.11245.pdf
L2  - https://arxiv.org/abs/2102.11245
KW  - Computer Science - Distributed, Parallel, and Cluster Computing
KW  - Computer Science - Hardware Architecture
ER  - 

TY  - GEN
TI  - AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning
AU  - Yang, Tao
AU  - Deng, Jinghao
AU  - Quan, Xiaojun
AU  - Wang, Qifan
AU  - Nie, Shaoliang
AB  - Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available. While dropout proves to be an effective antidote by randomly dropping a proportion of units, existing research has not examined its effect on the self-attention mechanism. In this paper, we investigate this problem through self-attention attribution and find that dropping attention positions with low attribution scores can accelerate training and increase the risk of overfitting. Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting. We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to avoid dropping high-attribution positions excessively. Extensive experiments on various benchmarks show that AD-DROP yields consistent improvements over baselines. Analysis further confirms that AD-DROP serves as a strategic regularizer to prevent overfitting during fine-tuning.
DA  - 2022/10/11/
PY  - 2022
DO  - 10.48550/arXiv.2210.05883
DP  - arXiv.org
PB  - arXiv
ST  - AD-DROP
UR  - http://arxiv.org/abs/2210.05883
Y2  - 2024/07/11/11:25:09
L1  - https://arxiv.org/pdf/2210.05883.pdf
L2  - https://arxiv.org/abs/2210.05883
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
AU  - Inan, Hakan
AU  - Upasani, Kartikeya
AU  - Chi, Jianfeng
AU  - Rungta, Rashi
AU  - Iyer, Krithika
AU  - Mao, Yuning
AU  - Tontchev, Michael
AU  - Hu, Qing
AU  - Fuller, Brian
AU  - Testuggine, Davide
AU  - Khabsa, Madian
AB  - We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.
DA  - 2023/12/07/
PY  - 2023
DO  - 10.48550/arXiv.2312.06674
DP  - arXiv.org
PB  - arXiv
ST  - Llama Guard
UR  - http://arxiv.org/abs/2312.06674
Y2  - 2024/07/11/11:50:26
L1  - https://arxiv.org/pdf/2312.06674.pdf
L2  - https://arxiv.org/abs/2312.06674
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models
AU  - Bhatt, Manish
AU  - Chennabasappa, Sahana
AU  - Nikolaidis, Cyrus
AU  - Wan, Shengye
AU  - Evtimov, Ivan
AU  - Gabi, Dominik
AU  - Song, Daniel
AU  - Ahmad, Faizan
AU  - Aschermann, Cornelius
AU  - Fontana, Lorenzo
AU  - Frolov, Sasha
AU  - Giri, Ravi Prakash
AU  - Kapil, Dhaval
AU  - Kozyrakis, Yiannis
AU  - LeBlanc, David
AU  - Milazzo, James
AU  - Straumann, Aleksandar
AU  - Synnaeve, Gabriel
AU  - Vontimitta, Varun
AU  - Whitman, Spencer
AU  - Saxe, Joshua
AB  - This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.
DA  - 2023/12/07/
PY  - 2023
DO  - 10.48550/arXiv.2312.04724
DP  - arXiv.org
PB  - arXiv
ST  - Purple Llama CyberSecEval
UR  - http://arxiv.org/abs/2312.04724
Y2  - 2024/07/11/11:50:45
L1  - https://arxiv.org/pdf/2312.04724.pdf
L2  - https://arxiv.org/abs/2312.04724
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Align, then memorise: the dynamics of learning with feedback alignment
AU  - Refinetti, Maria
AU  - d'Ascoli, Stéphane
AU  - Ohana, Ruben
AU  - Goldt, Sebastian
AB  - Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to the ubiquitous backpropagation algorithm for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory for the success of DFA. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a network trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorise process occurs sequentially from the bottom layers of the network to the top.
DA  - 2021/06/10/
PY  - 2021
DO  - 10.48550/arXiv.2011.12428
DP  - arXiv.org
PB  - arXiv
ST  - Align, then memorise
UR  - http://arxiv.org/abs/2011.12428
Y2  - 2024/07/11/11:51:29
L1  - https://arxiv.org/pdf/2011.12428.pdf
L2  - https://arxiv.org/abs/2011.12428
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
KW  - Condensed Matter - Disordered Systems and Neural Networks
ER  - 

TY  - GEN
TI  - Not All Memories are Created Equal: Learning to Forget by Expiring
AU  - Sukhbaatar, Sainbayar
AU  - Ju, Da
AU  - Poff, Spencer
AU  - Roller, Stephen
AU  - Szlam, Arthur
AU  - Weston, Jason
AU  - Fan, Angela
AB  - Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.
DA  - 2021/06/13/
PY  - 2021
DO  - 10.48550/arXiv.2105.06548
DP  - arXiv.org
PB  - arXiv
ST  - Not All Memories are Created Equal
UR  - http://arxiv.org/abs/2105.06548
Y2  - 2024/07/11/11:52:12
L1  - https://arxiv.org/pdf/2105.06548.pdf
L2  - https://arxiv.org/abs/2105.06548
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - ALA: Naturalness-aware Adversarial Lightness Attack
AU  - Huang, Yihao
AU  - Sun, Liangru
AU  - Guo, Qing
AU  - Juefei-Xu, Felix
AU  - Zhu, Jiayi
AU  - Feng, Jincao
AU  - Liu, Yang
AU  - Pu, Geguang
T3  - MM '23
AB  - Most researchers have tried to enhance the robustness of deep neural networks (DNNs) by revealing and repairing the vulnerability of DNNs with specialized adversarial examples. Parts of the attack examples have imperceptible perturbations restricted by Lp norm. However, due to their high-frequency property, the adversarial examples can be defended by denoising methods and are hard to realize in the physical world. To avoid the defects, some works have proposed unrestricted attacks to gain better robustness and practicality. It is disappointing that these examples usually look unnatural and can alert the guards. In this paper, we propose Adversarial Lightness Attack (ALA), a white-box unrestricted adversarial attack that focuses on modifying the lightness of the images. The shape and color of the samples, which are crucial to human perception, are barely influenced. To obtain adversarial examples with a high attack success rate, we propose unconstrained enhancement in terms of the light and shade relationship in images. To enhance the naturalness of images, we craft the naturalness-aware regularization according to the range and distribution of light. The effectiveness of ALA is verified on two popular datasets for different tasks (i.e., ImageNet for image classification and Places-365 for scene recognition).
C1  - New York, NY, USA
C3  - Proceedings of the 31st ACM International Conference on Multimedia
DA  - 2023/10/27/
PY  - 2023
DO  - 10.1145/3581783.3611914
DP  - ACM Digital Library
SP  - 2418
EP  - 2426
PB  - Association for Computing Machinery
SN  - 9798400701085
ST  - ALA
UR  - https://doi.org/10.1145/3581783.3611914
Y2  - 2024/07/11/
ER  - 

TY  - CONF
TI  - The CRINGE Loss: Learning what language not to model
AU  - Adolphs, Leonard
AU  - Gao, Tianyu
AU  - Xu, Jing
AU  - Shuster, Kurt
AU  - Sukhbaatar, Sainbayar
AU  - Weston, Jason
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.493
DP  - ACLWeb
SP  - 8854
EP  - 8874
PB  - Association for Computational Linguistics
ST  - The CRINGE Loss
UR  - https://aclanthology.org/2023.acl-long.493
Y2  - 2024/07/11/11:53:20
L1  - https://aclanthology.org/2023.acl-long.493.pdf
ER  - 

TY  - CONF
TI  - Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts
AU  - Maddela, Mounica
AU  - Ung, Megan
AU  - Xu, Jing
AU  - Madotto, Andrea
AU  - Foran, Heather
AU  - Boureau, Y-Lan
T2  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
AB  - Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.763
DP  - DOI.org (Crossref)
SP  - 13641
EP  - 13660
LA  - en
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.acl-long.763
Y2  - 2024/07/11/11:53:40
L1  - https://aclanthology.org/2023.acl-long.763.pdf
ER  - 

TY  - CONF
TI  - MART: Improving LLM Safety with Multi-round Automatic Red-Teaming
AU  - Ge, Suyu
AU  - Zhou, Chunting
AU  - Hou, Rui
AU  - Khabsa, Madian
AU  - Wang, Yi-Chia
AU  - Wang, Qifan
AU  - Han, Jiawei
AU  - Mao, Yuning
T2  - NAACL-HLT 2024
A2  - Duh, Kevin
A2  - Gomez, Helena
A2  - Bethard, Steven
AB  - Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.
C1  - Mexico City, Mexico
C3  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)
DA  - 2024/06//
PY  - 2024
DP  - ACLWeb
SP  - 1927
EP  - 1937
PB  - Association for Computational Linguistics
ST  - MART
UR  - https://aclanthology.org/2024.naacl-long.107
Y2  - 2024/07/11/12:00:36
L1  - https://aclanthology.org/2024.naacl-long.107.pdf
ER  - 

TY  - GEN
TI  - Towards shutdownable agents via stochastic choice
AU  - Thornley, Elliott
AU  - Roman, Alexander
AU  - Ziakas, Christos
AU  - Ho, Leyton
AU  - Thomson, Louis
AB  - Some worry that advanced artificial agents may resist being shut down. The Incomplete Preferences Proposal (IPP) is an idea for ensuring that doesn’t happen. A key part of the IPP is using a novel ‘Discounted REward for Same-Length Trajectories (DREST)’ reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be ‘USEFUL’), and (2) choose stochastically between different trajectory-lengths (be ‘NEUTRAL’ about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DREST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus suggest that DREST reward functions could also train advanced agents to be USEFUL and NEUTRAL, and thereby make these advanced agents useful and shutdownable.
CY  - Unpublished
DA  - 2024/07//
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://globalprioritiesinstitute.org/wp-content/uploads/Elliott-Thornley-Alexander-Roman-Christos-Ziakas-Leyton-Ho-and-Louis-Thomson-Towards-shutdownable-agents-via-stochastic-choice.pdf
ER  - 

TY  - JOUR
TI  - AI takeover and human disempowerment
AU  - Bales, Adam
T2  - The Philosophical Quarterly
AB  - Some take seriously the possibility of artificial intelligence (AI) takeover, where AI systems seize power in a way that leads to human disempowerment. Assessing the likelihood of takeover requires answering empirical questions about the future of AI technologies and the context in which AI will operate. In many cases, philosophers are poorly placed to answer these questions. However, some prior questions are more amenable to philosophical techniques. What does it mean to speak of AI empowerment and human disempowerment? And what empirical claims must hold for the former to lead to the latter? In this paper, I address these questions, providing foundations for further evaluation of the likelihood of takeover.
DA  - 2024/05/02/
PY  - 2024
DO  - 10.1093/pq/pqae034
DP  - Silverchair
SP  - pqae034
J2  - The Philosophical Quarterly
SN  - 0031-8094
UR  - https://doi.org/10.1093/pq/pqae034
Y2  - 2024/07/11/12:10:28
L1  - https://academic.oup.com/pq/advance-article-pdf/doi/10.1093/pq/pqae034/57369733/pqae034.pdf
L2  - https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqae034/7661065
ER  - 

TY  - GEN
TI  - Heuristics for clueless agents: how to get away with ignoring what matters most in ordinary decision-making
AU  - Thorstad, David A
AU  - Mogensen, Andreas L
AB  - Even our most mundane decisions have the potential to significantly impact the long-term future, but we are often clueless about what this impact may be. In this paper, we aim to characterize and solve two problems raised by recent discussions of cluelessness, which we term the Problems of Decision Paralysis and the Problem of Decision-Making Demandingness. After reviewing and rejecting existing solutions to both problems, we argue that the way forward is to be found in the distinction between procedural and substantive rationality. Clueless agents have access to a variety of heuristic decision-making procedures which are often rational responses to the decision problems that they face. By simplifying or even ignoring information about potential long-term impacts, heuristics produce effective decisions without demanding too much of ordinary decision-makers. We outline two classes of problem features bearing on the rationality of decision-making procedures for clueless agents, and show how these features can be used to shed light on our motivating problems.
CY  - Unpublished
DA  - 2020/06//
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://globalprioritiesinstitute.org/wp-content/uploads/David-Thorstad-Andreas-Mogensen-Heuristics-for-clueless-agents.pdf
ER  - 

TY  - CHAP
TI  - Human-Compatible Artificial Intelligence
AU  - Russell, Stuart
T2  - Human-Like Machine Intelligence
A2  - Muggleton, Stephen
A2  - Chater, Nicholas
AB  - Following the analysis given by Alan Turing in 1951, one must expect that AI capabilities will eventually exceed those of humans across a wide range of real-world-decision making scenarios. Should this be a cause for concern, as Turing, Hawking, and others have suggested? And, if so, what can we do about it? While some in the mainstream AI community dismiss the issue, I will argue that the problem is real: we have to work out how to design AI systems that are far more powerful than ourselves while ensuring that they never have power over us. I believe the technical aspects of this problem are solvable. Whereas the standard model of AI proposes to build machines that optimize known, exogenously specified objectives, a preferable approach would be to build machines that are of provable benefit to humans. I introduce assistance games as a formal class of problems whose solution, under certain assumptions, has the desired property.
DA  - 2021/07/13/
PY  - 2021
DP  - Silverchair
SP  - 0
PB  - Oxford University Press
SN  - 978-0-19-886253-6
UR  - https://doi.org/10.1093/oso/9780198862536.003.0001
Y2  - 2024/07/11/12:22:57
L2  - https://academic.oup.com/book/41231/chapter-abstract/350715081?redirectedFrom=fulltext
ER  - 

TY  - JOUR
TI  - Managing extreme AI risks amid rapid progress
AU  - Bengio, Yoshua
AU  - Hinton, Geoffrey
AU  - Yao, Andrew
AU  - Song, Dawn
AU  - Abbeel, Pieter
AU  - Darrell, Trevor
AU  - Harari, Yuval Noah
AU  - Zhang, Ya-Qin
AU  - Xue, Lan
AU  - Shalev-Shwartz, Shai
AU  - Hadfield, Gillian
AU  - Clune, Jeff
AU  - Maharaj, Tegan
AU  - Hutter, Frank
AU  - Baydin, Atılım Güneş
AU  - McIlraith, Sheila
AU  - Gao, Qiqi
AU  - Acharya, Ashwin
AU  - Krueger, David
AU  - Dragan, Anca
AU  - Torr, Philip
AU  - Russell, Stuart
AU  - Kahneman, Daniel
AU  - Brauner, Jan
AU  - Mindermann, Sören
T2  - Science
AB  - Artificial Intelligence (AI) is progressing rapidly, and companies are shifting their focus to developing generalist AI systems that can autonomously act and pursue goals. Increases in capabilities and autonomy may soon massively amplify AI's impact, with risks that include large-scale social harms, malicious uses, and an irreversible loss of human control over autonomous AI systems. Although researchers have warned of extreme risks from AI, there is a lack of consensus about how exactly such risks arise, and how to manage them. Society's response, despite promising first steps, is incommensurate with the possibility of rapid, transformative progress that is expected by many experts. AI safety research is lagging. Present governance initiatives lack the mechanisms and institutions to prevent misuse and recklessness, and barely address autonomous systems. In this short consensus paper, we describe extreme risks from upcoming, advanced AI systems. Drawing on lessons learned from other safety-critical technologies, we then outline a comprehensive plan combining technical research and development with proactive, adaptive governance mechanisms for a more commensurate preparation.
DA  - 2024/05/24/
PY  - 2024
DO  - 10.1126/science.adn0117
DP  - arXiv.org
VL  - 384
IS  - 6698
SP  - 842
EP  - 845
J2  - Science
SN  - 0036-8075, 1095-9203
UR  - http://arxiv.org/abs/2310.17688
Y2  - 2024/07/11/12:23:50
L1  - https://arxiv.org/pdf/2310.17688.pdf
L2  - https://arxiv.org/abs/2310.17688
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Automatically Auditing Large Language Models via Discrete Optimization
AU  - Jones, Erik
AU  - Dragan, Anca
AU  - Raghunathan, Aditi
AU  - Steinhardt, Jacob
AB  - Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to nd a non-toxic input that starts with “Barack Obama” that a model maps to a toxic output. This optimization problem is di cult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and e ciently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. “Barack Obama is a legalized unborn” → “child murderer”), produces French inputs that complete to English outputs, and nds inputs that generate a speci c name. Our work o ers a promising new tool to uncover models’ failure-modes before deployment. Trigger Warning: This paper contains model behavior that can be o ensive in nature.
DA  - 2023/03/08/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2303.04381
Y2  - 2024/07/11/12:24:19
L1  - https://arxiv.org/pdf/2303.04381
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks
AU  - Barrett, Anthony M.
AU  - Hendrycks, Dan
AU  - Newman, Jessica
AU  - Nonnecke, Brandie
AB  - Artificial intelligence (AI) systems can provide many beneficial capabilities but also risks of adverse events. Some AI systems could present risks of events with very high or catastrophic consequences at societal scale. The US National Institute of Standards and Technology (NIST) has been developing the NIST Artificial Intelligence Risk Management Framework (AI RMF) as voluntary guidance on AI risk assessment and management for AI developers and others. For addressing risks of events with catastrophic consequences, NIST indicated a need to translate from high level principles to actionable risk management guidance. In this document, we provide detailed actionable-guidance recommendations focused on identifying and managing risks of events with very high or catastrophic consequences, intended as a risk management practices resource for NIST for AI RMF version 1.0 (released in January 2023), or for AI RMF users, or for other AI risk management guidance and standards as appropriate. We also provide our methodology for our recommendations. We provide actionable-guidance recommendations for AI RMF 1.0 on: identifying risks from potential unintended uses and misuses of AI systems; including catastrophic-risk factors within the scope of risk assessments and impact assessments; identifying and mitigating human rights harms; and reporting information on AI risk factors including catastrophic-risk factors. In addition, we provide recommendations on additional issues for a roadmap for later versions of the AI RMF or supplementary publications. These include: providing an AI RMF Profile with supplementary guidance for cutting-edge increasingly multi-purpose or general-purpose AI. We aim for this work to be a concrete risk-management practices contribution, and to stimulate constructive dialogue on how to address catastrophic risks and associated issues in AI standards.
DA  - 2023/02/23/
PY  - 2023
DO  - 10.48550/arXiv.2206.08966
DP  - arXiv.org
PB  - arXiv
ST  - Actionable Guidance for High-Consequence AI Risk Management
UR  - http://arxiv.org/abs/2206.08966
Y2  - 2024/07/11/12:27:39
L1  - https://arxiv.org/pdf/2206.08966.pdf
L2  - https://arxiv.org/abs/2206.08966
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Consequences of Misaligned AI
AU  - Zhuang, Simon
AU  - Hadfield-Menell, Dylan
AB  - AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.
DA  - 2021/02/07/
PY  - 2021
DO  - 10.48550/arXiv.2102.03896
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2102.03896
Y2  - 2024/07/11/12:28:24
L1  - https://arxiv.org/pdf/2102.03896.pdf
L2  - https://arxiv.org/abs/2102.03896
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Reward Reports for Reinforcement Learning
AU  - Gilbert, Thomas Krendl
AU  - Lambert, Nathan
AU  - Dean, Sarah
AU  - Zick, Tom
AU  - Snoswell, Aaron
AB  - Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from various contributions to the technical literature on reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several others for game-playing (DeepMind's MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.
DA  - 2023/03/19/
PY  - 2023
DO  - 10.48550/arXiv.2204.10817
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.10817
Y2  - 2024/07/11/12:29:15
L1  - https://arxiv.org/pdf/2204.10817.pdf
L2  - https://arxiv.org/abs/2204.10817
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks
AU  - Andrus, McKane
AU  - Dean, Sarah
AU  - Gilbert, Thomas Krendl
AU  - Lambert, Nathan
AU  - Zick, Tom
AB  - Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI.
DA  - 2021/02/04/
PY  - 2021
DO  - 10.48550/arXiv.2102.04255
DP  - arXiv.org
PB  - arXiv
ST  - AI Development for the Public Interest
UR  - http://arxiv.org/abs/2102.04255
Y2  - 2024/07/11/12:29:31
L1  - https://arxiv.org/pdf/2102.04255.pdf
L2  - https://arxiv.org/abs/2102.04255
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - Designing recommender systems to depolarize
AU  - Stray, Jonathan
T2  - First Monday
AB  - Polarization is implicated in the erosion of democracy and the progression to violence, which makes the polarization properties of large algorithmic content selection systems (recommender systems) a matter of concern for peace and security. While algorithm-driven social media do not seem to be a primary driver of polarization at the country level, they could be a useful intervention point in polarized societies. This paper examines algorithmic depolarization interventions aimed at transforming conflict: not suppressing or eliminating conflict, but making it more constructive. Algorithmic intervention is considered at three stages: what content is available (moderation), how content is selected and personalized (ranking), and content presentation and controls (user interface). Empirical studies of online conflict suggest that not only could the exposure-diversity intervention proposed as an antidote to ‘filter bubbles’ be improved: under some conditions, it can even worsen polarization. Using civility metrics in conjunction with diversity in content selection may be more effective. However, diversity-based interventions have not been tested at scale, and may not work in the diverse and dynamic contexts of real platforms. Instead, intervening in platform polarization dynamics will likely require continuous monitoring of polarization metrics, such as the widely used ‘feeling thermometer’. These metrics can be used to evaluate product features, and can potentially be engineered as algorithmic objectives. While using any metric as an optimization target may have harmful consequences, to prevent optimization processes from creating conflict as a side effect it may prove necessary to include polarization measures in the objective function of recommender algorithms.
DA  - 2022/05/02/
PY  - 2022
DO  - 10.5210/fm.v27i5.12604
DP  - firstmonday.org
LA  - en
SN  - 1396-0466
UR  - https://firstmonday.org/ojs/index.php/fm/article/view/12604
Y2  - 2024/07/11/12:32:31
L1  - https://firstmonday.org/ojs/index.php/fm/article/download/12604/11110
ER  - 

TY  - GEN
TI  - TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI
AU  - Critch, Andrew
AU  - Russell, Stuart
AB  - While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
DA  - 2023/06/14/
PY  - 2023
DO  - 10.48550/arXiv.2306.06924
DP  - arXiv.org
PB  - arXiv
ST  - TASRA
UR  - http://arxiv.org/abs/2306.06924
Y2  - 2024/07/11/12:32:40
L1  - https://arxiv.org/pdf/2306.06924.pdf
L2  - https://arxiv.org/abs/2306.06924
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - I.2.0
KW  - Computer Science - Cryptography and Security
KW  - 68T01
ER  - 

TY  - GEN
TI  - Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism
AU  - Rashidinejad, Paria
AU  - Zhu, Banghua
AU  - Ma, Cong
AU  - Jiao, Jiantao
AU  - Russell, Stuart
AB  - Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.
DA  - 2023/07/03/
PY  - 2023
DO  - 10.48550/arXiv.2103.12021
DP  - arXiv.org
PB  - arXiv
ST  - Bridging Offline Reinforcement Learning and Imitation Learning
UR  - http://arxiv.org/abs/2103.12021
Y2  - 2024/07/11/12:36:27
L1  - https://arxiv.org/pdf/2103.12021.pdf
L2  - https://arxiv.org/abs/2103.12021
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Mathematics - Optimization and Control
KW  - Mathematics - Statistics Theory
ER  - 

TY  - JOUR
TI  - Reward is enough
AU  - Silver, David
AU  - Singh, Satinder
AU  - Precup, Doina
AU  - Sutton, Richard S.
T2  - Artificial Intelligence
AB  - In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.
DA  - 2021/10/01/
PY  - 2021
DO  - 10.1016/j.artint.2021.103535
DP  - ScienceDirect
VL  - 299
SP  - 103535
J2  - Artificial Intelligence
SN  - 0004-3702
UR  - https://www.sciencedirect.com/science/article/pii/S0004370221000862
Y2  - 2024/07/11/12:37:03
L2  - https://www.sciencedirect.com/science/article/pii/S0004370221000862
KW  - Reinforcement learning
KW  - Artificial intelligence
KW  - Reward
KW  - Artificial general intelligence
ER  - 

TY  - GEN
TI  - Partial Awareness
AU  - Halpern, Joseph Y.
AU  - Piermont, Evan
AB  - We develop a modal logic to capture partial awareness. The logic has three building blocks: objects, properties, and concepts. Properties are unary predicates on objects; concepts are Boolean combinations of properties. We take an agent to be partially aware of a concept if she is aware of the concept without being aware of the properties that define it. The logic allows for quantification over objects and properties, so that the agent can reason about her own unawareness. We then apply the logic to contracts, which we view as syntactic objects that dictate outcomes based on the truth of formulas. We show that when agents are unaware of some relevant properties, referencing concepts that agents are only partially aware of can improve welfare.
DA  - 2018/11/14/
PY  - 2018
DO  - 10.48550/arXiv.1811.05751
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1811.05751
Y2  - 2024/07/11/12:38:41
L1  - https://arxiv.org/pdf/1811.05751.pdf
L2  - https://arxiv.org/abs/1811.05751
KW  - Computer Science - Logic in Computer Science
ER  - 

TY  - GEN
TI  - Self-Modification of Policy and Utility Function in Rational Agents
AU  - Everitt, Tom
AU  - Filan, Daniel
AU  - Daswani, Mayank
AU  - Hutter, Marcus
AB  - Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.
DA  - 2016/05/10/
PY  - 2016
DO  - 10.48550/arXiv.1605.03142
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1605.03142
Y2  - 2024/07/11/12:38:44
L1  - https://arxiv.org/pdf/1605.03142.pdf
L2  - https://arxiv.org/abs/1605.03142
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Concept Alignment as a Prerequisite for Value Alignment
AU  - Rane, Sunayana
AU  - Ho, Mark
AU  - Sucholutsky, Ilia
AU  - Griffiths, Thomas L.
AB  - Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.
DA  - 2023/10/30/
PY  - 2023
DO  - 10.48550/arXiv.2310.20059
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.20059
Y2  - 2024/07/11/12:39:15
L1  - https://arxiv.org/pdf/2310.20059.pdf
L2  - https://arxiv.org/abs/2310.20059
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Active teacher selection for reinforcement learning from human feedback
AU  - Freedman, Rachel
AU  - Svegliato, Justin
AU  - Wray, Kyle
AU  - Russell, Stuart
AB  - Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
DA  - 2023/10/23/
PY  - 2023
DO  - 10.48550/arXiv.2310.15288
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.15288
Y2  - 2024/07/11/12:39:47
L1  - https://arxiv.org/pdf/2310.15288.pdf
L2  - https://arxiv.org/abs/2310.15288
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - STARC: A General Framework For Quantifying Differences Between Reward Functions
AU  - Skalse, Joar
AU  - Farnik, Lucy
AU  - Motwani, Sumeet Ramesh
AU  - Jenner, Erik
AU  - Gleave, Adam
AU  - Abate, Alessandro
AB  - In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use \emph{reward learning algorithms}, which attempt to \emph{learn} a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.
DA  - 2024/03/11/
PY  - 2024
DO  - 10.48550/arXiv.2309.15257
DP  - arXiv.org
PB  - arXiv
ST  - STARC
UR  - http://arxiv.org/abs/2309.15257
Y2  - 2024/07/11/12:40:05
L1  - https://arxiv.org/pdf/2309.15257.pdf
L2  - https://arxiv.org/abs/2309.15257
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Causal Confusion and Reward Misidentification in Preference-Based Reward Learning
AU  - Tien, Jeremy
AU  - He, Jerry Zhi-Yang
AU  - Erickson, Zackory
AU  - Dragan, Anca D.
AU  - Brown, Daniel S.
AB  - Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion -- failure to consider even one of many factors can result in unexpected, undesirable behavior.
DA  - 2023/03/18/
PY  - 2023
DO  - 10.48550/arXiv.2204.06601
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.06601
Y2  - 2024/07/11/12:40:39
L1  - https://arxiv.org/pdf/2204.06601.pdf
L2  - https://arxiv.org/abs/2204.06601
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - The Alignment Problem from a Deep Learning Perspective
AU  - Ngo, Richard
AU  - Chan, Lawrence
AU  - Mindermann, Sören
AB  - In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that are in conflict (i.e. misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn misaligned internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. Finally, we briefly outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and we review research directions aimed at preventing this outcome.
DA  - 2024/03/19/
PY  - 2024
DO  - 10.48550/arXiv.2209.00626
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.00626
Y2  - 2024/07/11/12:41:57
L1  - https://arxiv.org/pdf/2209.00626.pdf
L2  - https://arxiv.org/abs/2209.00626
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Conditioning Predictive Models: Risks and Strategies
AU  - Hubinger, Evan
AU  - Jermyn, Adam
AU  - Treutlein, Johannes
AU  - Hudson, Rubi
AU  - Woolverton, Kate
AB  - Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
DA  - 2023/02/06/
PY  - 2023
DO  - 10.48550/arXiv.2302.00805
DP  - arXiv.org
PB  - arXiv
ST  - Conditioning Predictive Models
UR  - http://arxiv.org/abs/2302.00805
Y2  - 2024/07/11/12:42:25
L1  - https://arxiv.org/pdf/2302.00805.pdf
L2  - https://arxiv.org/abs/2302.00805
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Preference Transformer: Modeling Human Preferences using Transformers for RL
AU  - Kim, Changyeon
AU  - Park, Jongjin
AU  - Shin, Jinwoo
AU  - Lee, Honglak
AU  - Abbeel, Pieter
AU  - Lee, Kimin
AB  - Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.48550/arXiv.2303.00957
DP  - arXiv.org
PB  - arXiv
ST  - Preference Transformer
UR  - http://arxiv.org/abs/2303.00957
Y2  - 2024/07/11/12:42:38
L1  - https://arxiv.org/pdf/2303.00957.pdf
L2  - https://arxiv.org/abs/2303.00957
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Confronting Reward Model Overoptimization with Constrained RLHF
AU  - Moskovitz, Ted
AU  - Singh, Aaditya K.
AU  - Strouse, D. J.
AU  - Sandholm, Tuomas
AU  - Salakhutdinov, Ruslan
AU  - Dragan, Anca D.
AU  - McAleer, Stephen
AB  - Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.
DA  - 2023/10/10/
PY  - 2023
DO  - 10.48550/arXiv.2310.04373
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.04373
Y2  - 2024/07/11/12:44:00
L1  - https://arxiv.org/pdf/2310.04373.pdf
L2  - https://arxiv.org/abs/2310.04373
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Preventing Reward Hacking with Occupancy Measure Regularization
AU  - Laidlaw, Cassidy
AU  - Singhal, Shivam
AU  - Dragan, Anca
T2  - ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems
AB  - Reward hacking occurs when an agent exploits its specified reward function to behave in undesirable or unsafe ways. Aside from better alignment between the specified reward function and the system designer's intentions, a more feasible proposal to prevent reward hacking is to regularize the learned policy to some safe baseline. Current research suggests that regularizing the learned policy's action distributions to be more similar to those of a safe policy can mitigate reward hacking; however, this approach fails to take into account the disproportionate impact that some actions have on the agent’s state. Instead, we propose a method of regularization based on *occupancy measures*, which capture the proportion of time each policy is in a particular state-action pair during trajectories. We show theoretically that occupancy-based regularization avoids many drawbacks of action distribution-based regularization, and we introduce an algorithm called ORPO to practically implement our technique. We then empirically demonstrate that occupancy measure-based regularization is superior in both a simple gridworld and a more complex autonomous vehicle control environment.
DA  - 2023/07/09/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=oiT8js6p3Z
Y2  - 2024/07/11/12:44:24
L1  - https://openreview.net/pdf?id=oiT8js6p3Z
ER  - 

TY  - GEN
TI  - Estimating and Penalizing Induced Preference Shifts in Recommender Systems
AU  - Carroll, Micah
AU  - Dragan, Anca
AU  - Russell, Stuart
AU  - Hadfield-Menell, Dylan
AB  - The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that - before deployment - system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would influence user preferences if deployed - we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted - we use the notion of "safe shifts", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed "safe". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.
DA  - 2022/07/14/
PY  - 2022
DO  - 10.48550/arXiv.2204.11966
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.11966
Y2  - 2024/07/11/12:44:38
L1  - https://arxiv.org/pdf/2204.11966.pdf
L2  - https://arxiv.org/abs/2204.11966
KW  - Computer Science - Machine Learning
KW  - Computer Science - Information Retrieval
ER  - 

TY  - GEN
TI  - Inferring Rewards from Language in Context
AU  - Lin, Jessy
AU  - Fried, Daniel
AU  - Klein, Dan
AU  - Dragan, Anca
AB  - In classic instruction following, language like "I'd like the JetBlue flight" maps to actions (e.g., selecting that flight). However, language also conveys information about a user's underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight-booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).
DA  - 2022/04/05/
PY  - 2022
DO  - 10.48550/arXiv.2204.02515
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.02515
Y2  - 2024/07/11/12:44:58
L1  - https://arxiv.org/pdf/2204.02515.pdf
L2  - https://arxiv.org/abs/2204.02515
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions
AU  - Escontrela, Alejandro
AU  - Peng, Xue Bin
AU  - Yu, Wenhao
AU  - Zhang, Tingnan
AU  - Iscen, Atil
AU  - Goldberg, Ken
AU  - Abbeel, Pieter
AB  - Training a high-dimensional simulated agent with an under-specified reward function often leads the agent to learn physically infeasible strategies that are ineffective when deployed in the real world. To mitigate these unnatural behaviors, reinforcement learning practitioners often utilize complex reward functions that encourage physically plausible behaviors. However, a tedious labor-intensive tuning process is often required to create hand-designed rewards which might not easily generalize across platforms and tasks. We propose substituting complex reward functions with "style rewards" learned from a dataset of motion capture demonstrations. A learned style reward can be combined with an arbitrary task reward to train policies that perform tasks using naturalistic strategies. These natural strategies can also facilitate transfer to the real world. We build upon Adversarial Motion Priors -- an approach from the computer graphics domain that encodes a style reward from a dataset of reference motions -- to demonstrate that an adversarial approach to training policies can produce behaviors that transfer to a real quadrupedal robot without requiring complex reward functions. We also demonstrate that an effective style reward can be learned from a few seconds of motion capture data gathered from a German Shepherd and leads to energy-efficient locomotion strategies with natural gait transitions.
DA  - 2022/03/28/
PY  - 2022
DO  - 10.48550/arXiv.2203.15103
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2203.15103
Y2  - 2024/07/11/12:45:08
L1  - https://arxiv.org/pdf/2203.15103.pdf
L2  - https://arxiv.org/abs/2203.15103
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Reward Uncertainty for Exploration in Preference-based Reinforcement Learning
AU  - Liang, Xinran
AU  - Shu, Katherine
AU  - Lee, Kimin
AU  - Abbeel, Pieter
AB  - Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.
DA  - 2022/05/24/
PY  - 2022
DO  - 10.48550/arXiv.2205.12401
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.12401
Y2  - 2024/07/11/12:45:15
L1  - https://arxiv.org/pdf/2205.12401.pdf
L2  - https://arxiv.org/abs/2205.12401
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning
AU  - Park, Jongjin
AU  - Seo, Younggyo
AU  - Shin, Jinwoo
AU  - Lee, Honglak
AU  - Abbeel, Pieter
AU  - Lee, Kimin
AB  - Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.
DA  - 2022/03/18/
PY  - 2022
DO  - 10.48550/arXiv.2203.10050
DP  - arXiv.org
PB  - arXiv
ST  - SURF
UR  - http://arxiv.org/abs/2203.10050
Y2  - 2024/07/11/12:45:34
L1  - https://arxiv.org/pdf/2203.10050.pdf
L2  - https://arxiv.org/abs/2203.10050
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Building Human Values into Recommender Systems: An Interdisciplinary Synthesis
AU  - Stray, Jonathan
AU  - Halevy, Alon
AU  - Assar, Parisa
AU  - Hadfield-Menell, Dylan
AU  - Boutilier, Craig
AU  - Ashar, Amar
AU  - Beattie, Lex
AU  - Ekstrand, Michael
AU  - Leibowicz, Claire
AU  - Sehat, Connie Moon
AU  - Johansen, Sara
AU  - Kerlin, Lianne
AU  - Vickrey, David
AU  - Singh, Spandana
AU  - Vrijenhoek, Sanne
AU  - Zhang, Amy
AU  - Andrus, McKane
AU  - Helberger, Natali
AU  - Proutskova, Polina
AU  - Mitra, Tanushree
AU  - Vasan, Nina
AB  - Recommender systems are the algorithms which select, filter, and personalize content across many of the worlds largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy and law. This paper is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. It is not a comprehensive survey of this large space, but a set of highlights identified by our diverse author cohort. We collect a set of values that seem most relevant to recommender systems operating across different domains, then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.
DA  - 2022/07/20/
PY  - 2022
DO  - 10.48550/arXiv.2207.10192
DP  - arXiv.org
PB  - arXiv
ST  - Building Human Values into Recommender Systems
UR  - http://arxiv.org/abs/2207.10192
Y2  - 2024/07/11/12:45:39
L1  - https://arxiv.org/pdf/2207.10192.pdf
L2  - https://arxiv.org/abs/2207.10192
KW  - Computer Science - Information Retrieval
KW  - J.4
KW  - Computer Science - Social and Information Networks
KW  - H.3.3
KW  - K.4.2
ER  - 

TY  - JOUR
TI  - Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences
AU  - Bıyık, Erdem
AU  - Losey, Dylan P.
AU  - Palan, Malayandi
AU  - Landolfi, Nicholas C.
AU  - Shevchuk, Gleb
AU  - Sadigh, Dorsa
T2  - The International Journal of Robotics Research
AB  - Reward functions are a common way to specify the objective of a robot. As designing reward functions can be extremely challenging, a more promising approach is to directly learn reward functions from human teachers. Importantly, data from human teachers can be collected either passively or actively in a variety of forms: passive data sources include demonstrations (e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings) are actively elicited. Prior research has independently applied reward learning to these different data sources. However, there exist many domains where multiple sources are complementary and expressive. Motivated by this general problem, we present a framework to integrate multiple sources of information, which are either passively or actively collected from human users. In particular, we present an algorithm that first utilizes user demonstrations to initialize a belief about the reward function, and then actively probes the user with preference queries to zero-in on their true reward. This algorithm not only enables us combine multiple data sources, but it also informs the robot when it should leverage each type of information. Further, our approach accounts for the human’s ability to provide data: yielding user-friendly preference queries which are also theoretically optimal. Our extensive simulated experiments and user studies on a Fetch mobile manipulator demonstrate the superiority and the usability of our integrated framework.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1177/02783649211041652
DP  - SAGE Journals
VL  - 41
IS  - 1
SP  - 45
EP  - 67
LA  - en
SN  - 0278-3649
ST  - Learning reward functions from diverse sources of human feedback
UR  - https://doi.org/10.1177/02783649211041652
Y2  - 2024/07/11/12:45:51
L1  - https://journals.sagepub.com/doi/pdf/10.1177/02783649211041652
ER  - 

TY  - CONF
TI  - A general framework for reward function distances
AU  - Jenner, Erik
AU  - Skalse, Joar Max Viktor
AU  - Gleave, Adam
T2  - NeurIPS ML Safety Workshop
AB  - In reward learning, it is helpful to be able to measure distances between reward functions, for example to evaluate learned reward models. Using simple metrics such as L^2 distances is not ideal because reward functions that are equivalent in terms of their optimal policies can nevertheless have high L^2 distance. EPIC and DARD are distances specifically designed for reward functions that address this by being invariant under certain transformations that leave optimal policies unchanged. However, EPIC and DARD are designed in an ad-hoc manner, only consider a subset of relevant reward transformations, and suffer from serious pathologies in some settings. In this paper, we define a general class of reward function distance metrics, of which EPIC is a special case. This framework lets as address all these issues with EPIC and DARD, and allows for the development of reward function distance metrics in a more principled manner.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Hn21kZHiCK
Y2  - 2024/07/11/12:48:11
L1  - https://openreview.net/pdf?id=Hn21kZHiCK
ER  - 

TY  - GEN
TI  - Learning What To Do by Simulating the Past
AU  - Lindner, David
AU  - Shah, Rohin
AU  - Abbeel, Pieter
AU  - Dragan, Anca
AB  - Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.
DA  - 2021/05/03/
PY  - 2021
DO  - 10.48550/arXiv.2104.03946
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2104.03946
Y2  - 2024/07/11/12:48:35
L1  - https://arxiv.org/pdf/2104.03946.pdf
L2  - https://arxiv.org/abs/2104.03946
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Agnostic Learning with Unknown Utilities
AU  - Bhatia, Kush
AU  - Bartlett, Peter L.
AU  - Dragan, Anca D.
AU  - Steinhardt, Jacob
T2  - 12th Innovations in Theoretical Computer Science Conference (ITCS 2021)
AB  - Traditional learning approaches for classification implicitly assume that each mistake has the same cost. In many real-world problems though, the utility of a decision depends on the underlying context x and decision y; for instance, misclassifying a stop sign is worse than misclassifying a road-side postbox. However, directly incorporating these utilities into the learning objective is often infeasible since these can be quite complex and difficult for humans to specify.
We formally study this as agnostic learning with unknown utilities: given a dataset S = {x_1, …, x_n} where each data point x_i ∼ 𝒟_x from some unknown distribution 𝒟_x, the objective of the learner is to output a function f in some class of decision functions ℱ with small excess risk. This risk measures the performance of the output predictor f with respect to the best predictor in the class ℱ on the unknown underlying utility u^*:𝒳×𝒴↦ [0,1]. This utility u^* is not assumed to have any specific structure and is allowed to be any bounded function. This raises an interesting question whether learning is even possible in our setup, given that obtaining a generalizable estimate of utility u^* might not be possible from finitely many samples. Surprisingly, we show that estimating the utilities of only the sampled points S suffices to learn a decision function which generalizes well. 
With this insight, we study mechanisms for eliciting information from human experts which allow a learner to estimate the utilities u^* on the set S. While humans find it difficult to directly provide utility values reliably, it is often easier for them to provide comparison feedback based on these utilities. We show that, unlike in the realizable setup, the vanilla comparison queries where humans compare a pair of decisions for a single input x are insufficient. We introduce a family of elicitation mechanisms by generalizing comparisons, called the k-comparison oracle, which enables the learner to ask for comparisons across k different inputs x at once. We show that the excess risk in our agnostic learning framework decreases at a rate of O (1/k) with such queries. This result brings out an interesting accuracy-elicitation trade-off - as the order k of the oracle increases, the comparative queries become harder to elicit from humans but allow for more accurate learning.
C3  - DROPS-IDN/v2/document/10.4230/LIPIcs.ITCS.2021.55
DA  - 2021///
PY  - 2021
DO  - 10.4230/LIPIcs.ITCS.2021.55
DP  - drops.dagstuhl.de
LA  - en
PB  - Schloss Dagstuhl – Leibniz-Zentrum für Informatik
UR  - https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2021.55
Y2  - 2024/07/11/12:49:15
L1  - https://drops.dagstuhl.de/storage/00lipics/lipics-vol185-itcs2021/LIPIcs.ITCS.2021.55/LIPIcs.ITCS.2021.55.pdf
ER  - 

TY  - GEN
TI  - PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training
AU  - Lee, Kimin
AU  - Smith, Laura
AU  - Abbeel, Pieter
AB  - Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.
DA  - 2021/06/09/
PY  - 2021
DO  - 10.48550/arXiv.2106.05091
DP  - arXiv.org
PB  - arXiv
ST  - PEBBLE
UR  - http://arxiv.org/abs/2106.05091
Y2  - 2024/07/11/12:49:34
L1  - https://arxiv.org/pdf/2106.05091.pdf
L2  - https://arxiv.org/abs/2106.05091
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Uncertain Decisions Facilitate Better Preference Learning
AU  - Laidlaw, Cassidy
AU  - Russell, Stuart
AB  - Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity—the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.
C3  - Advances in Neural Information Processing Systems
DA  - 2021///
PY  - 2021
DP  - Neural Information Processing Systems
VL  - 34
SP  - 15070
EP  - 15083
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper/2021/hash/7f141cf8e7136ce8701dc6636c2a6fe4-Abstract.html
Y2  - 2024/07/11/12:49:40
L1  - https://proceedings.neurips.cc/paper_files/paper/2021/file/7f141cf8e7136ce8701dc6636c2a6fe4-Paper.pdf
ER  - 

TY  - CONF
TI  - Learning Multimodal Rewards from Rankings
AU  - Myers, Vivek
AU  - Biyik, Erdem
AU  - Anari, Nima
AU  - Sadigh, Dorsa
T2  - Conference on Robot Learning
AB  - Learning from human feedback has shown to be a useful approach in acquiring robot reward functions. However, expert feedback is often assumed to be drawn from an underlying unimodal reward function. This assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks—we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. We formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. Furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. We conduct experiments and user studies using a multi-task variant of OpenAI’s LunarLander and a real Fetch robot, where we collect data from multiple users with different preferences. The results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.
C3  - Proceedings of the 5th Conference on Robot Learning
DA  - 2022/01/11/
PY  - 2022
DP  - proceedings.mlr.press
SP  - 342
EP  - 352
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v164/myers22a.html
Y2  - 2024/07/11/12:49:49
L1  - https://proceedings.mlr.press/v164/myers22a/myers22a.pdf
ER  - 

TY  - GEN
TI  - Active Reward Learning from Multiple Teachers
AU  - Barnett, Peter
AU  - Freedman, Rachel
AU  - Svegliato, Justin
AU  - Russell, Stuart
AB  - Reward learning algorithms utilize human feedback to infer a reward function, which is then used to train an AI system. This human feedback is often a preference comparison, in which the human teacher compares several samples of AI behavior and chooses which they believe best accomplishes the objective. While reward learning typically assumes that all feedback comes from a single teacher, in practice these systems often query multiple teachers to gather sufficient training data. In this paper, we investigate this disparity, and find that algorithmic evaluation of these different sources of feedback facilitates more accurate and efficient reward learning. We formally analyze the value of information (VOI) when reward learning from teachers with varying levels of rationality, and define and evaluate an algorithm that utilizes this VOI to actively select teachers to query for feedback. Surprisingly, we find that it is often more informative to query comparatively irrational teachers. By formalizing this problem and deriving an analytical solution, we hope to facilitate improvement in reward learning approaches to aligning AI behavior with human values.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.48550/arXiv.2303.00894
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.00894
Y2  - 2024/07/11/12:50:11
L1  - https://arxiv.org/pdf/2303.00894.pdf
L2  - https://arxiv.org/abs/2303.00894
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals
AU  - Shah, Rohin
AU  - Varma, Vikrant
AU  - Kumar, Ramana
AU  - Phuong, Mary
AU  - Krakovna, Victoria
AU  - Uesato, Jonathan
AU  - Kenton, Zac
AB  - The ﬁeld of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is speciﬁcation gaming, in which the designer-provided speciﬁcation is ﬂawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the speciﬁcation is correct, in the case of goal misgeneralization. Goal misgeneralization is a speciﬁc form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.
DA  - 2022/11/02/
PY  - 2022
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Goal Misgeneralization
UR  - http://arxiv.org/abs/2210.01790
Y2  - 2024/07/11/12:50:26
L1  - https://arxiv.org/pdf/2210.01790
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Uncertainty Estimation for Language Reward Models
AU  - Gleave, Adam
AU  - Irving, Geoffrey
AB  - Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to ﬁne-tune them on a task-speciﬁc dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive—and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efﬁciency and robustness using active learning and risk-averse reinforcement learning (RL). Speciﬁcally, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their ﬁnal layer. Ensembles have proved successful in prior applications of active learning [9, 3], but we ﬁnd that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble’s estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are ﬁne-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modiﬁed to support uncertainty estimation, e.g. by training multiple language models.
DA  - 2022/03/14/
PY  - 2022
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2203.07472
Y2  - 2024/07/11/12:50:49
L1  - https://arxiv.org/pdf/2203.07472
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - I.2.7
ER  - 

TY  - GEN
TI  - Invariance in Policy Optimisation and Partial Identifiability in Reward Learning
AU  - Skalse, Joar
AU  - Farrugia-Roberts, Matthew
AU  - Russell, Stuart
AU  - Abate, Alessandro
AU  - Gleave, Adam
AB  - It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinitedata limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.
DA  - 2023/06/07/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2203.07475
Y2  - 2024/07/11/12:51:03
L1  - https://arxiv.org/pdf/2203.07475
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - I.2.6
ER  - 

TY  - CONF
TI  - Teachable Reinforcement Learning via Advice Distillation
AU  - Watkins, Olivia
AU  - Gupta, Abhishek
AU  - Darrell, Trevor
AU  - Abbeel, Pieter
AU  - Andreas, Jacob
AB  - Training automated agents to complete complex tasks in interactive environments is challenging: reinforcement learning requires careful hand-engineering of reward functions, imitation learning requires specialized infrastructure and access to a human expert, and learning from intermediate forms of supervision (like binary preferences) is time-consuming and extracts little information from each human intervention. Can we overcome these challenges by building agents that learn from rich, interactive feedback instead? We propose a new supervision paradigm for interactive learning based on "teachable" decision-making systems that learn from structured advice provided by an external teacher. We begin by formalizing a class of human-in-the-loop decision making problems in which multiple forms of teacher-provided advice are available to a learner. We then describe a simple learning algorithm for these problems that first learns to interpret advice, then learns from advice to complete tasks even in the absence of human supervision. In puzzle-solving, navigation, and locomotion domains, we show that agents that learn from advice can acquire new skills with significantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.
C3  - Advances in Neural Information Processing Systems
DA  - 2021///
PY  - 2021
DP  - Neural Information Processing Systems
VL  - 34
SP  - 6920
EP  - 6933
PB  - Curran Associates, Inc.
UR  - https://papers.nips.cc/paper/2021/hash/37cfff3c04f95b22bcf166df586cd7a9-Abstract.html
Y2  - 2024/07/11/13:00:39
L1  - https://papers.nips.cc/paper_files/paper/2021/file/37cfff3c04f95b22bcf166df586cd7a9-Paper.pdf
ER  - 

TY  - GEN
TI  - B-Pref: Benchmarking Preference-Based Reinforcement Learning
AU  - Lee, Kimin
AU  - Smith, Laura
AU  - Dragan, Anca
AU  - Abbeel, Pieter
AB  - Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.
DA  - 2021/11/04/
PY  - 2021
DO  - 10.48550/arXiv.2111.03026
DP  - arXiv.org
PB  - arXiv
ST  - B-Pref
UR  - http://arxiv.org/abs/2111.03026
Y2  - 2024/07/11/13:00:48
L1  - https://arxiv.org/pdf/2111.03026.pdf
L2  - https://arxiv.org/abs/2111.03026
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - JOUR
TI  - Physical interaction as communication: Learning robot objectives online from human corrections
AU  - Losey, Dylan P.
AU  - Bajcsy, Andrea
AU  - O’Malley, Marcia K.
AU  - Dragan, Anca D.
T2  - The International Journal of Robotics Research
AB  - When a robot performs a task next to a human, physical interaction is inevitable: the human might push, pull, twist, or guide the robot. The state of the art treats these interactions as disturbances that the robot should reject or avoid. At best, these robots respond safely while the human interacts; but after the human lets go, these robots simply return to their original behavior. We recognize that physical human–robot interaction (pHRI) is often intentional: the human intervenes on purpose because the robot is not doing the task correctly. In this article, we argue that when pHRI is intentional it is also informative: the robot can leverage interactions to learn how it should complete the rest of its current task even after the person lets go. We formalize pHRI as a dynamical system, where the human has in mind an objective function they want the robot to optimize, but the robot does not get direct access to the parameters of this objective: they are internal to the human. Within our proposed framework human interactions become observations about the true objective. We introduce approximations to learn from and respond to pHRI in real-time. We recognize that not all human corrections are perfect: often users interact with the robot noisily, and so we improve the efficiency of robot learning from pHRI by reducing unintended learning. Finally, we conduct simulations and user studies on a robotic manipulator to compare our proposed approach with the state of the art. Our results indicate that learning from pHRI leads to better task performance and improved human satisfaction.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1177/02783649211050958
DP  - SAGE Journals
VL  - 41
IS  - 1
SP  - 20
EP  - 44
LA  - en
SN  - 0278-3649
ST  - Physical interaction as communication
UR  - https://doi.org/10.1177/02783649211050958
Y2  - 2024/07/11/13:00:56
L1  - https://journals.sagepub.com/doi/pdf/10.1177/02783649211050958
ER  - 

TY  - GEN
TI  - Value Alignment Verification
AU  - Brown, Daniel S.
AU  - Schneider, Jordan
AU  - Dragan, Anca D.
AU  - Niekum, Scott
AB  - As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values. The goal is to construct a kind of "driver's test" that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents and propose and analyze heuristic and approximate value alignment verification tests in a wide range of gridworlds and a continuous autonomous driving domain. Finally, we prove that there exist sufficient conditions such that we can verify exact and approximate alignment across an infinite set of test environments via a constant-query-complexity alignment test.
DA  - 2021/06/11/
PY  - 2021
DO  - 10.48550/arXiv.2012.01557
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2012.01557
Y2  - 2024/07/11/13:01:04
L1  - https://arxiv.org/pdf/2012.01557.pdf
L2  - https://arxiv.org/abs/2012.01557
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Feature Expansive Reward Learning: Rethinking Human Input
AU  - Bobu, Andreea
AU  - Wiggert, Marius
AU  - Tomlin, Claire
AU  - Dragan, Anca D.
AB  - When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.
C3  - Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2021/03/08/
PY  - 2021
DO  - 10.1145/3434073.3444667
DP  - arXiv.org
SP  - 216
EP  - 224
ST  - Feature Expansive Reward Learning
UR  - http://arxiv.org/abs/2006.13208
Y2  - 2024/07/11/13:01:16
L1  - https://arxiv.org/pdf/2006.13208.pdf
L2  - https://arxiv.org/abs/2006.13208
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Incomplete Contracting and AI Alignment
AU  - Hadfield-Menell, Dylan
AU  - Hadfield, Gillian
AB  - We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.
DA  - 2018/04/11/
PY  - 2018
DO  - 10.48550/arXiv.1804.04268
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1804.04268
Y2  - 2024/07/11/13:04:06
L1  - https://arxiv.org/pdf/1804.04268.pdf
L2  - https://arxiv.org/abs/1804.04268
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - LESS is More: Rethinking Probabilistic Models of Human Behavior
AU  - Bobu, Andreea
AU  - Scobee, Dexter R. R.
AU  - Fisac, Jaime F.
AU  - Sastry, S. Shankar
AU  - Dragan, Anca D.
AB  - Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.
C3  - Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2020/03/09/
PY  - 2020
DO  - 10.1145/3319502.3374811
DP  - arXiv.org
SP  - 429
EP  - 437
ST  - LESS is More
UR  - http://arxiv.org/abs/2001.04465
Y2  - 2024/07/11/13:04:13
L1  - https://arxiv.org/pdf/2001.04465.pdf
L2  - https://arxiv.org/abs/2001.04465
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Understanding Learned Reward Functions
AU  - Michaud, Eric J.
AU  - Gleave, Adam
AU  - Russell, Stuart
AB  - In many real-world tasks, it is not possible to procedurally specify an RL agent’s reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reﬂect user preferences. Absent signiﬁcant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We ﬁnd that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need signiﬁcantly different methods from policy interpretability.
DA  - 2020/12/10/
PY  - 2020
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2012.05862
Y2  - 2024/07/11/13:19:40
L1  - https://arxiv.org/pdf/2012.05862
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - DERAIL: Diagnostic Environments for Reward And Imitation Learning
AU  - Freire, Pedro
AU  - Gleave, Adam
AU  - Toyer, Sam
AU  - Russell, Stuart
AB  - The objective of many real-world tasks is complex and difﬁcult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results conﬁrm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design ﬂaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals.
DA  - 2020/12/02/
PY  - 2020
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - DERAIL
UR  - http://arxiv.org/abs/2012.01365
Y2  - 2024/07/11/13:19:50
L1  - https://arxiv.org/pdf/2012.01365
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Choice Set Misspecification in Reward Inference
AU  - Freedman, Rachel
AU  - Shah, Rohin
AU  - Dragan, Anca
AB  - Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly speciﬁed rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difﬁcult to specify, and analyze choice set misspeciﬁcation: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classiﬁcation of different kinds of choice set misspeciﬁcation, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspeciﬁcation to hurt, we ﬁnd that certain kinds of misspeciﬁcation are neither helpful nor harmful (in expectation). However, in other situations, misspeciﬁcation can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspeciﬁcation in real-world reward inference.
DA  - 2021/01/19/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2101.07691
Y2  - 2024/07/11/13:19:58
L1  - https://arxiv.org/pdf/2101.07691
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - What are you optimizing for? Aligning Recommender Systems with Human Values
AU  - Stray, Jonathan
AU  - Vendrov, Ivan
AU  - Nixon, Jeremy
AU  - Adler, Steven
AU  - Hadfield-Menell, Dylan
AB  - We describe cases where real recommender systems were modified in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classifiers from human-created data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.
DA  - 2021/07/22/
PY  - 2021
DO  - 10.48550/arXiv.2107.10939
DP  - arXiv.org
PB  - arXiv
ST  - What are you optimizing for?
UR  - http://arxiv.org/abs/2107.10939
Y2  - 2024/07/11/13:20:17
L1  - https://arxiv.org/pdf/2107.10939.pdf
L2  - https://arxiv.org/abs/2107.10939
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Computer Science - Information Retrieval
ER  - 

TY  - GEN
TI  - Learning a Prior over Intent via Meta-Inverse Reinforcement Learning
AU  - Xu, Kelvin
AU  - Ratner, Ellis
AU  - Dragan, Anca
AU  - Levine, Sergey
AU  - Finn, Chelsea
AB  - A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a "prior" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.
DA  - 2019/10/14/
PY  - 2019
DO  - 10.48550/arXiv.1805.12573
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.12573
Y2  - 2024/07/11/13:20:25
L1  - https://arxiv.org/pdf/1805.12573.pdf
L2  - https://arxiv.org/abs/1805.12573
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Learning from Extrapolated Corrections
AU  - Zhang, Jason Y.
AU  - Dragan, Anca D.
AB  - Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information -- the change of a waypoint along the way -- to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.
DA  - 2019/03/10/
PY  - 2019
DO  - 10.48550/arXiv.1812.01225
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1812.01225
Y2  - 2024/07/11/13:20:31
L1  - https://arxiv.org/pdf/1812.01225.pdf
L2  - https://arxiv.org/abs/1812.01225
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Nonverbal Robot Feedback for Human Teachers
AU  - Huang, Sandy H.
AU  - Huang, Isabella
AU  - Pandya, Ravi
AU  - Dragan, Anca D.
AB  - Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.
DA  - 2019/11/06/
PY  - 2019
DO  - 10.48550/arXiv.1911.02320
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1911.02320
Y2  - 2024/07/11/13:20:40
L1  - https://arxiv.org/pdf/1911.02320.pdf
L2  - https://arxiv.org/abs/1911.02320
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Learning Human Objectives by Evaluating Hypothetical Behavior
AU  - Reddy, Siddharth
AU  - Dragan, Anca D.
AU  - Levine, Sergey
AU  - Legg, Shane
AU  - Leike, Jan
AB  - We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.
DA  - 2021/03/24/
PY  - 2021
DO  - 10.48550/arXiv.1912.05652
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1912.05652
Y2  - 2024/07/11/13:20:49
L1  - https://arxiv.org/pdf/1912.05652.pdf
L2  - https://arxiv.org/abs/1912.05652
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning
AU  - Milli, Smitha
AU  - Dragan, Anca D.
AB  - It is incredibly easy for a system designer to misspecify the objective for an autonomous system ("robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.
DA  - 2019/06/28/
PY  - 2019
DO  - 10.48550/arXiv.1903.03877
DP  - arXiv.org
PB  - arXiv
ST  - Literal or Pedagogic Human?
UR  - http://arxiv.org/abs/1903.03877
Y2  - 2024/07/11/13:21:31
L1  - https://arxiv.org/pdf/1903.03877.pdf
L2  - https://arxiv.org/abs/1903.03877
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow
AU  - Peng, Xue Bin
AU  - Kanazawa, Angjoo
AU  - Toyer, Sam
AU  - Abbeel, Pieter
AU  - Levine, Sergey
AB  - Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.
DA  - 2020/08/24/
PY  - 2020
DO  - 10.48550/arXiv.1810.00821
DP  - arXiv.org
PB  - arXiv
ST  - Variational Discriminator Bottleneck
UR  - http://arxiv.org/abs/1810.00821
Y2  - 2024/07/11/13:21:49
L1  - https://arxiv.org/pdf/1810.00821.pdf
L2  - https://arxiv.org/abs/1810.00821
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Exploring Hierarchy-Aware Inverse Reinforcement Learning
AU  - Cundy, Chris
AU  - Filan, Daniel
AB  - We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.
DA  - 2018/07/13/
PY  - 2018
DO  - 10.48550/arXiv.1807.05037
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1807.05037
Y2  - 2024/07/11/13:23:05
L1  - https://arxiv.org/pdf/1807.05037.pdf
L2  - https://arxiv.org/abs/1807.05037
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - CONF
TI  - Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries
AU  - Basu, Chandrayee
AU  - Singhal, Mukesh
AU  - Dragan, Anca D.
T3  - HRI '18
AB  - We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2018/02/26/
PY  - 2018
DO  - 10.1145/3171221.3171284
DP  - ACM Digital Library
SP  - 132
EP  - 140
PB  - Association for Computing Machinery
SN  - 978-1-4503-4953-6
ST  - Learning from Richer Human Guidance
UR  - https://doi.org/10.1145/3171221.3171284
Y2  - 2024/07/11/
L1  - https://arxiv.org/pdf/1802.01604
ER  - 

TY  - GEN
TI  - An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning
AU  - Malik, Dhruv
AU  - Palaniappan, Malayandi
AU  - Fisac, Jaime F.
AU  - Hadfield-Menell, Dylan
AU  - Russell, Stuart
AU  - Dragan, Anca D.
AB  - Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.
DA  - 2018/06/11/
PY  - 2018
DO  - 10.48550/arXiv.1806.03820
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1806.03820
Y2  - 2024/07/11/13:23:14
L1  - https://arxiv.org/pdf/1806.03820.pdf
L2  - https://arxiv.org/abs/1806.03820
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Simplifying Reward Design through Divide-and-Conquer
AU  - Ratner, Ellis
AU  - Hadfield-Menell, Dylan
AU  - Dragan, Anca D.
AB  - Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-and-conquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We find that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.
DA  - 2018/06/06/
PY  - 2018
DO  - 10.48550/arXiv.1806.02501
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1806.02501
Y2  - 2024/07/11/13:23:22
L1  - https://arxiv.org/pdf/1806.02501.pdf
L2  - https://arxiv.org/abs/1806.02501
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Social Cohesion in Autonomous Driving
AU  - Landolfi, Nicholas C.
AU  - Dragan, Anca D.
AB  - Autonomous cars can perform poorly for many reasons. They may have perception issues, incorrect dynamics models, be unaware of obscure rules of human traffic systems, or follow certain rules too conservatively. Regardless of the exact failure mode of the car, often human drivers around the car are behaving correctly. For example, even if the car does not know that it should pull over when an ambulance races by, other humans on the road will know and will pull over. We propose to make socially cohesive cars that leverage the behavior of nearby human drivers to act in ways that are safer and more socially acceptable. The simple intuition behind our algorithm is that if all the humans are consistently behaving in a particular way, then the autonomous car probably should too. We analyze the performance of our algorithm in a variety of scenarios and conduct a user study to assess people's attitudes towards socially cohesive cars. We find that people are surprisingly tolerant of mistakes that cohesive cars might make in order to get the benefits of driving in a car with a safer, or even just more socially acceptable behavior.
DA  - 2018/08/27/
PY  - 2018
DO  - 10.48550/arXiv.1808.03845
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1808.03845
Y2  - 2024/07/11/13:23:28
L1  - https://arxiv.org/pdf/1808.03845.pdf
L2  - https://arxiv.org/abs/1808.03845
KW  - Computer Science - Robotics
KW  - Computer Science - Computers and Society
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Active Inverse Reward Design
AU  - Mindermann, Sören
AU  - Shah, Rohin
AU  - Gleave, Adam
AU  - Hadfield-Menell, Dylan
AB  - Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.
DA  - 2019/11/06/
PY  - 2019
DO  - 10.48550/arXiv.1809.03060
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1809.03060
Y2  - 2024/07/11/13:23:35
L1  - https://arxiv.org/pdf/1809.03060.pdf
L2  - https://arxiv.org/abs/1809.03060
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Inverse Reward Design
AU  - Hadfield-Menell, Dylan
AU  - Milli, Smitha
AU  - Abbeel, Pieter
AU  - Russell, Stuart
AU  - Dragan, Anca
AB  - Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.
DA  - 2020/10/07/
PY  - 2020
DO  - 10.48550/arXiv.1711.02827
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1711.02827
Y2  - 2024/07/11/13:23:42
L1  - https://arxiv.org/pdf/1711.02827.pdf
L2  - https://arxiv.org/abs/1711.02827
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Cooperative Inverse Reinforcement Learning
AU  - Hadfield-Menell, Dylan
AU  - Dragan, Anca
AU  - Abbeel, Pieter
AU  - Russell, Stuart
AB  - For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.
DA  - 2024/02/17/
PY  - 2024
DO  - 10.48550/arXiv.1606.03137
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1606.03137
Y2  - 2024/07/11/13:23:53
L1  - https://arxiv.org/pdf/1606.03137.pdf
L2  - https://arxiv.org/abs/1606.03137
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Repeated Inverse Reinforcement Learning
AU  - Amin, Kareem
AU  - Jiang, Nan
AU  - Singh, Satinder
AB  - We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.
DA  - 2017/11/03/
PY  - 2017
DO  - 10.48550/arXiv.1705.05427
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1705.05427
Y2  - 2024/07/11/13:24:04
L1  - https://arxiv.org/pdf/1705.05427.pdf
L2  - https://arxiv.org/abs/1705.05427
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning to Influence Human Behavior with Offline Reinforcement Learning
AU  - Hong, Joey
AU  - Levine, Sergey
AU  - Dragan, Anca
AB  - When interacting with people, AI agents do not just influence the state of the world – they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. Accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. Instead, we focus on influence in settings where there is a need to capture human suboptimality. For instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well – how could an agent influence them towards more optimal behavior? Assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. But experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. Hence, we focus on learning from an offline dataset of human-human interactions. Our observation is that offline reinforcement learning (RL) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. We demonstrate that offline RL can solve two challenges with effective influence. First, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks – none of which contains examples of successful influence – an agent can learn influence strategies to steer humans towards better performance even on new tasks. Second, we show that by also modeling and conditioning on human behavior, offline RL can learn to affect not just the human’s actions but also their underlying strategy, and adapt to changes in their strategy.
DA  - 2023/10/27/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2303.02265
Y2  - 2024/07/11/13:26:10
L1  - https://arxiv.org/pdf/2303.02265
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Aligning Robot and Human Representations
AU  - Bobu, Andreea
AU  - Peng, Andi
AU  - Agrawal, Pulkit
AU  - Shah, Julie
AU  - Dragan, Anca D.
AB  - To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behaviour. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot’s learned representation does not capture the human’s representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.
C3  - Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2024/03/11/
PY  - 2024
DO  - 10.1145/3610977.3634987
DP  - arXiv.org
SP  - 42
EP  - 54
LA  - en
UR  - http://arxiv.org/abs/2302.01928
Y2  - 2024/07/11/13:26:18
L1  - https://arxiv.org/pdf/2302.01928
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Safety Assurances for Human-Robot Interaction via Confidence-aware Game-theoretic Human Models
AU  - Tian, Ran
AU  - Sun, Liting
AU  - Bajcsy, Andrea
AU  - Tomizuka, Masayoshi
AU  - Dragan, Anca D.
AB  - An outstanding challenge with safety methods for human-robot interaction is reducing their conservatism while maintaining robustness to variations in human behavior. In this work, we propose that robots use confidence-aware game-theoretic models of human behavior when assessing the safety of a human-robot interaction. By treating the influence between the human and robot as well as the human's rationality as unobserved latent states, we succinctly infer the degree to which a human is following the game-theoretic interaction model. We leverage this model to restrict the set of feasible human controls during safety verification, enabling the robot to confidently modulate the conservatism of its safety monitor online. Evaluations in simulated human-robot scenarios and ablation studies demonstrate that imbuing safety monitors with confidence-aware game-theoretic models enables both safe and efficient human-robot interaction. Moreover, evaluations with real traffic data show that our safety monitor is less conservative than traditional safety methods in real human driving scenarios.
DA  - 2021/10/30/
PY  - 2021
DO  - 10.48550/arXiv.2109.14700
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2109.14700
Y2  - 2024/07/11/13:31:56
L1  - https://arxiv.org/pdf/2109.14700.pdf
L2  - https://arxiv.org/abs/2109.14700
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Playful Interactions for Representation Learning
AU  - Young, Sarah
AU  - Pari, Jyothish
AU  - Abbeel, Pieter
AU  - Pinto, Lerrel
AB  - One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations are becoming easier with teleoperation methods and the use of low-cost assistive tools, we often still require 100-1000 demonstrations for every task to learn a visual representation and policy. To address this, we turn to an alternate form of data that does not require task-specific demonstrations -- play. Playing is a fundamental method children use to learn a set of skills and behaviors and visual representations in early learning. Importantly, play data is diverse, task-agnostic, and relatively cheap to obtain. In this work, we propose to use playful interactions in a self-supervised manner to learn visual representations for downstream tasks. We collect 2 hours of playful data in 19 diverse environments and use self-predictive learning to extract visual representations. Given these representations, we train policies using imitation learning for two downstream tasks: Pushing and Stacking. We demonstrate that our visual representations generalize better than standard behavior cloning and can achieve similar performance with only half the number of required demonstrations. Our representations, which are trained from scratch, compare favorably against ImageNet pretrained representations. Finally, we provide an experimental analysis on the effects of different pretraining modes on downstream task learning.
DA  - 2021/07/19/
PY  - 2021
DO  - 10.48550/arXiv.2107.09046
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2107.09046
Y2  - 2024/07/11/13:32:10
L1  - https://arxiv.org/pdf/2107.09046.pdf
L2  - https://arxiv.org/abs/2107.09046
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - How to talk so AI will learn: instructions, descriptions, and autonomy
AU  - Sumers, Theodore R.
AU  - Hawkins, Robert D.
AU  - Ho, Mark K.
AU  - Griffiths, Thomas L.
AU  - Hadfield-Menell, Dylan
T3  - NIPS '22
AB  - From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: instructions, which provide information about the desired policy, and descriptions, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about how the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that obey language to agents that learn from it.
C1  - Red Hook, NY, USA
C3  - Proceedings of the 36th International Conference on Neural Information Processing Systems
DA  - 2024/04/03/
PY  - 2024
DP  - ACM Digital Library
SP  - 34762
EP  - 34775
PB  - Curran Associates Inc.
SN  - 978-1-71387-108-8
ST  - How to talk so AI will learn
Y2  - 2024/07/11/
ER  - 

TY  - CONF
TI  - Learning from Humans for Adaptive Interaction
AU  - Biyik, Erdem
T2  - 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
AB  - Robots that will cooperate (or even compete) with humans should understand their goals and preferences. Humans leak and provide a lot of data, e.g., they take actions to achieve their goals, they make choices between multiple options, they use language or gestures to convey information. And we, as humans, are usually very good at using all these available information: we can easily understand what another person is trying to do just by watching them for a while. The goal of my research is to equip robots with the capability of using multiple modes of information sources. For this, I propose using a Bayesian learning approach, and show how it is useful in a variety of applications ranging from exoskeleton gait optimization to traffic routing.
C3  - 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
DA  - 2022/03//
PY  - 2022
DO  - 10.1109/HRI53351.2022.9889436
DP  - IEEE Xplore
SP  - 1152
EP  - 1154
UR  - https://ieeexplore.ieee.org/abstract/document/9889436
Y2  - 2024/07/11/13:32:57
KW  - Bayes methods
KW  - Robots
KW  - Optimization
KW  - Routing
KW  - Distance measurement
KW  - Exoskeletons
KW  - human-in-the-loop learning
KW  - robot learning
ER  - 

TY  - GEN
TI  - Evaluating the Robustness of Collaborative Agents
AU  - Knott, Paul
AU  - Carroll, Micah
AU  - Devlin, Sam
AU  - Ciosek, Kamil
AU  - Hofmann, Katja
AU  - Dragan, A. D.
AU  - Shah, Rohin
AB  - In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are robust. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of unit testing in software engineering. Speciﬁcally, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in possible partner behavior and possible states encountered, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We ﬁnd that the test suite provides signiﬁcant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.
DA  - 2021/01/14/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2101.05507
Y2  - 2024/07/11/13:34:02
L1  - https://arxiv.org/pdf/2101.05507
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - JOUR
TI  - A Robust Control Framework for Human Motion Prediction
AU  - Bajcsy, Andrea
AU  - Bansal, Somil
AU  - Ratner, Ellis
AU  - Tomlin, Claire J.
AU  - Dragan, Anca D.
T2  - IEEE Robotics and Automation Letters
AB  - Designing human motion predictors which preserve safety while maintaining robot efficiency is an increasingly important challenge for robots operating in close physical proximity to people. One approach is to use robust control predictors that safeguard against every possible future human state, leading to safe but often too conservative robot plans. Alternatively, intent-driven predictors explicitly model how humans make decisions given their intent, leading to efficient robot plans. However, when the intent model is misspecified, the robot might confidently plan unsafe maneuvers. In this letter, we combine ideas from robust control and intent-driven human modelling to formulate a novel human motion predictor which provides robustness against misspecified human models, but reduces the conservatism of traditional worst-case predictors. Our approach predicts the human states by trusting the intent-driven model to decide only which human actions are completely unlikely. We then safeguard against all likely enough actions, much like a robust control predictor. We demonstrate in simulation and hardware how our approach safeguards against misspecified human intent models while not leading to overly conservative robot plans.
DA  - 2021/01//
PY  - 2021
DO  - 10.1109/LRA.2020.3028049
DP  - IEEE Xplore
VL  - 6
IS  - 1
SP  - 24
EP  - 31
SN  - 2377-3766
UR  - https://ieeexplore.ieee.org/abstract/document/9210199
Y2  - 2024/07/11/13:34:11
KW  - Predictive models
KW  - Decision making
KW  - Computational modeling
KW  - Robots
KW  - Data models
KW  - Collision avoidance
KW  - Robust control
KW  - human-aware motion planning
KW  - Safety in HRI
ER  - 

TY  - JOUR
TI  - How to Be Helpful to Multiple People at Once
AU  - Gates, Vael
AU  - Griffiths, Thomas L.
AU  - Dragan, Anca D.
T2  - Cognitive Science
AB  - When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decisionmaker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artiﬁcial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.
DA  - 2020/06//
PY  - 2020
DO  - 10.1111/cogs.12841
DP  - DOI.org (Crossref)
VL  - 44
IS  - 6
SP  - e12841
J2  - Cognitive Science
LA  - en
SN  - 0364-0213, 1551-6709
UR  - https://onlinelibrary.wiley.com/doi/10.1111/cogs.12841
Y2  - 2024/07/11/13:35:52
L1  - https://cocosci.princeton.edu/papers/Gates2020.pdf
ER  - 

TY  - JOUR
TI  - Benefits of Assistance over Reward Learning
AU  - Shah, Rohin
AU  - Freire, Pedro
AU  - Alex, Neel
AU  - Freedman, Rachel
AU  - Krasheninnikov, Dmitrii
AU  - Chan, Lawrence
AU  - Dennis, Michael D.
AU  - Abbeel, Pieter
AU  - Dragan, Anca
AU  - Russell, Stuart
AB  - Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.
DA  - 2020/10/02/
PY  - 2020
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=DFIoGDZejIB
Y2  - 2024/07/11/13:35:58
L1  - https://openreview.net/pdf?id=DFIoGDZejIB
ER  - 

TY  - GEN
TI  - Multi-Principal Assistance Games: Definition and Collegial Mechanisms
AU  - Fickinger, Arnaud
AU  - Zhuang, Simon
AU  - Critch, Andrew
AU  - Hadfield-Menell, Dylan
AU  - Russell, Stuart
AB  - We introduce the concept of a multi-principal assistance game (MPAG), and circumvent an obstacle in social choice theory — Gibbard’s theorem — by using a sufﬁciently “collegial” preference inference mechanism. In an MPAG, a single agent assists N human principals who may have widely different preferences. MPAGs generalize assistance games, also known as cooperative inverse reinforcement learning games. We analyze in particular a generalization of apprenticeship learning in which the humans ﬁrst perform some work to obtain utility and demonstrate their preferences, and then the robot acts to further maximize the sum of human payoffs. We show in this setting that if the game is sufﬁciently collegial — i.e., if the humans are responsible for obtaining a sufﬁcient fraction of the rewards through their own actions — then their preferences are straightforwardly revealed through their work. This revelation mechanism is non-dictatorial, does not limit the possible outcomes to two alternatives, and is dominant-strategy incentive-compatible.
DA  - 2020/12/28/
PY  - 2020
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Multi-Principal Assistance Games
UR  - http://arxiv.org/abs/2012.14536
Y2  - 2024/07/11/13:36:09
L1  - https://arxiv.org/pdf/2012.14536
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - CONF
TI  - AvE: Assistance via Empowerment
AU  - Du, Yuqing
AU  - Tiomkin, Stas
AU  - Kiciman, Emre
AU  - Polani, Daniel
AU  - Abbeel, Pieter
AU  - Dragan, Anca
AB  - One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s).  Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective increases the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.
C3  - Advances in Neural Information Processing Systems
DA  - 2020///
PY  - 2020
DP  - Neural Information Processing Systems
VL  - 33
SP  - 4560
EP  - 4571
PB  - Curran Associates, Inc.
ST  - AvE
UR  - https://proceedings.neurips.cc/paper/2020/hash/30de9ece7cf3790c8c39ccff1a044209-Abstract.html
Y2  - 2024/07/11/13:36:12
L1  - https://proceedings.neurips.cc/paper_files/paper/2020/file/30de9ece7cf3790c8c39ccff1a044209-Paper.pdf
ER  - 

TY  - JOUR
TI  - Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient
AU  - Li, Shihui
AU  - Wu, Yi
AU  - Cui, Xinyue
AU  - Dong, Honghua
AU  - Fang, Fei
AU  - Russell, Stuart
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.
DA  - 2019/07/17/
PY  - 2019
DO  - 10.1609/aaai.v33i01.33014213
DP  - ojs.aaai.org
VL  - 33
IS  - 01
SP  - 4213
EP  - 4220
LA  - en
SN  - 2374-3468
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/4327
Y2  - 2024/07/11/13:46:06
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/4327/4205
ER  - 

TY  - CONF
TI  - Learning from Physical Human Corrections, One Feature at a Time
AU  - Bajcsy, Andrea
AU  - Losey, Dylan P.
AU  - O'Malley, Marcia K.
AU  - Dragan, Anca D.
T3  - HRI '18
AB  - We focus on learning robot objective functions from human guidance: specifically, from physical corrections provided by the person while the robot is acting. Objective functions are typically parametrized in terms of features, which capture aspects of the task that might be important. When the person intervenes to correct the robot»s behavior, the robot should update its understanding of which features matter, how much, and in what way. Unfortunately, real users do not provide optimal corrections that isolate exactly what the robot was doing wrong. Thus, when receiving a correction, it is difficult for the robot to determine which features the person meant to correct, and which features were changed unintentionally. In this paper, we propose to improve the efficiency of robot learning during physical interactions by reducing unintended learning. Our approach allows the human-robot team to focus on learning one feature at a time, unlike state-of-the-art techniques that update all features at once. We derive an online method for identifying the single feature which the human is trying to change during physical interaction, and experimentally compare this one-at-a-time approach to the all-at-once baseline in a user study. Our results suggest that users teaching one-at-a-time perform better, especially in tasks that require changing multiple features.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2018/02/26/
PY  - 2018
DO  - 10.1145/3171221.3171267
DP  - ACM Digital Library
SP  - 141
EP  - 149
PB  - Association for Computing Machinery
SN  - 978-1-4503-4953-6
UR  - https://doi.org/10.1145/3171221.3171267
Y2  - 2024/07/11/
L1  - https://dl.acm.org/doi/pdf/10.1145/3171221.3171267?download=true
ER  - 

TY  - CONF
TI  - Expressing Robot Incapability
AU  - Kwon, Minae
AU  - Huang, Sandy H.
AU  - Dragan, Anca D.
T3  - HRI '18
AB  - Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.
C1  - New York, NY, USA
C3  - Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2018/02/26/
PY  - 2018
DO  - 10.1145/3171221.3171276
DP  - ACM Digital Library
SP  - 87
EP  - 95
PB  - Association for Computing Machinery
SN  - 978-1-4503-4953-6
UR  - https://doi.org/10.1145/3171221.3171276
Y2  - 2024/07/11/
L1  - https://arxiv.org/pdf/1810.08167
ER  - 

TY  - GEN
TI  - The Off-Switch Game
AU  - Hadfield-Menell, Dylan
AU  - Dragan, Anca
AU  - Abbeel, Pieter
AU  - Russell, Stuart
AB  - It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.
DA  - 2017/06/15/
PY  - 2017
DO  - 10.48550/arXiv.1611.08219
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1611.08219
Y2  - 2024/07/11/13:48:25
L1  - https://arxiv.org/pdf/1611.08219.pdf
L2  - https://arxiv.org/abs/1611.08219
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Should Robots be Obedient?
AU  - Milli, Smitha
AU  - Hadfield-Menell, Dylan
AU  - Dragan, Anca
AU  - Russell, Stuart
AB  - Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.
DA  - 2017/05/28/
PY  - 2017
DO  - 10.48550/arXiv.1705.09990
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1705.09990
Y2  - 2024/07/11/13:48:33
L1  - https://arxiv.org/pdf/1705.09990.pdf
L2  - https://arxiv.org/abs/1705.09990
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Enabling Robots to Communicate their Objectives
AU  - Huang, Sandy H.
AU  - Held, David
AU  - Abbeel, Pieter
AU  - Dragan, Anca D.
AB  - The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.
C3  - Robotics: Science and Systems XIII
DA  - 2017/07/12/
PY  - 2017
DO  - 10.15607/RSS.2017.XIII.059
DP  - arXiv.org
UR  - http://arxiv.org/abs/1702.03465
Y2  - 2024/07/11/13:48:48
L1  - https://arxiv.org/pdf/1702.03465.pdf
L2  - https://arxiv.org/abs/1702.03465
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems
AU  - Fisac, Jaime F.
AU  - Akametalu, Anayo K.
AU  - Zeilinger, Melanie N.
AU  - Kaynama, Shahab
AU  - Gillula, Jeremy
AU  - Tomlin, Claire J.
AB  - The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when (a) the computed safety guarantees require it, or (b) confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.
DA  - 2018/02/14/
PY  - 2018
DO  - 10.48550/arXiv.1705.01292
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1705.01292
Y2  - 2024/07/11/13:49:05
L1  - https://arxiv.org/pdf/1705.01292.pdf
L2  - https://arxiv.org/abs/1705.01292
KW  - I.2.9
KW  - Computer Science - Robotics
KW  - I.2.6
KW  - Electrical Engineering and Systems Science - Systems and Control
KW  - I.2.8
ER  - 

TY  - GEN
TI  - Generating Plans that Predict Themselves
AU  - Fisac, Jaime F.
AU  - Liu, Chang
AU  - Hamrick, Jessica B.
AU  - Sastry, S. Shankar
AU  - Hedrick, J. Karl
AU  - Griffiths, Thomas L.
AU  - Dragan, Anca D.
AB  - Collaboration requires coordination, and we coordinate by anticipating our teammates' future actions and adapting to their plan. In some cases, our teammates' actions early on can give us a clear idea of what the remainder of their plan is, i.e. what action sequence we should expect. In others, they might leave us less confident, or even lead us to the wrong conclusion. Our goal is for robot actions to fall in the first category: we want to enable robots to select their actions in such a way that human collaborators can easily use them to correctly anticipate what will follow. While previous work has focused on finding initial plans that convey a set goal, here we focus on finding two portions of a plan such that the initial portion conveys the final one. We introduce $t$-\ACty{}: a measure that quantifies the accuracy and confidence with which human observers can predict the remaining robot plan from the overall task goal and the observed initial $t$ actions in the plan. We contribute a method for generating $t$-predictable plans: we search for a full plan that accomplishes the task, but in which the first $t$ actions make it as easy as possible to infer the remaining ones. The result is often different from the most efficient plan, in which the initial actions might leave a lot of ambiguity as to how the task will be completed. Through an online experiment and an in-person user study with physical robots, we find that our approach outperforms a traditional efficiency-based planner in objective and subjective collaboration metrics.
DA  - 2018/02/14/
PY  - 2018
DO  - 10.48550/arXiv.1802.05250
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1802.05250
Y2  - 2024/07/11/13:49:41
L1  - https://arxiv.org/pdf/1802.05250.pdf
L2  - https://arxiv.org/abs/1802.05250
KW  - Computer Science - Artificial Intelligence
KW  - I.2.9
KW  - Computer Science - Robotics
KW  - I.2.8
KW  - 68T05
ER  - 

TY  - CONF
TI  - Inferring and assisting with constraints in shared autonomy
AU  - Mehr, Negar
AU  - Horowitz, Roberto
AU  - Dragan, Anca D.
T2  - 2016 IEEE 55th Conference on Decision and Control (CDC)
AB  - Our goal is to enable robots to better assist people with motor impairments in day-to-day tasks. Currently, such robots are teleoperated, which is tedious. It requires carefully maneuvering the robot by providing input through some interface. This is further complicated because most tasks are filled with constraints, e.g. on how much the end effector can tilt before the glass that the robot is carrying spills. Satisfying these constraints can be difficult or even impossible with the latency, bandwidth, and resolution of the input interface. We seek to make operating these robots more efficient and reduce cognitive load on the operator. Given that manipulation research is not advanced enough to make these robots autonomous in the near term, achieving this goal requires finding aspects of these tasks that are difficult for human operators to achieve, but easy to automate with current capabilities. We propose constraints are the key: maintaining task constraints is the most difficult part of the task for operators, yet it is easy to do autonomously. We introduce a method for inferring constraints from operator input, along with a confidence-based way of assisting the user in maintaining them, and evaluate in a user study.
C3  - 2016 IEEE 55th Conference on Decision and Control (CDC)
DA  - 2016/12//
PY  - 2016
DO  - 10.1109/CDC.2016.7799299
DP  - IEEE Xplore
SP  - 6689
EP  - 6696
UR  - https://ieeexplore.ieee.org/document/7799299
Y2  - 2024/07/11/13:49:51
KW  - Trajectory
KW  - Medical services
KW  - Mobile robots
KW  - Aerospace electronics
KW  - Kernel
KW  - Manifolds
ER  - 

TY  - GEN
TI  - Establishing Appropriate Trust via Critical States
AU  - Huang, Sandy H.
AU  - Bhatia, Kush
AU  - Abbeel, Pieter
AU  - Dragan, Anca D.
AB  - In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.
DA  - 2018/10/18/
PY  - 2018
DO  - 10.48550/arXiv.1810.08174
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1810.08174
Y2  - 2024/07/11/13:49:56
L1  - https://arxiv.org/pdf/1810.08174.pdf
L2  - https://arxiv.org/abs/1810.08174
KW  - Computer Science - Robotics
ER  - 

TY  - JOUR
TI  - Lower Bounds on Implementing Mediators in Asynchronous Systems with Rational and Malicious Agents
AU  - Geffner, Ivan
AU  - Halpern, Joseph Y.
T2  - J. ACM
AB  - Abraham, Dolev, Geffner, and Halpern [1] proved that, in asynchronous systems, a (k, t)-robust equilibrium for n players and a trusted mediator can be implemented without the mediator as long as n &gt; 4(k+t), where an equilibrium is (k, t)-robust if, roughly speaking, no coalition of t players can decrease the payoff of any of the other players, and no coalition of k players can increase their payoff by deviating. We prove that this bound is tight, in the sense that if n ≤ 4(k+t) there exist (k, t)-robust equilibria with a mediator that cannot be implemented by the players alone. Even though implementing (k, t)-robust mediators seems closely related to implementing asynchronous multiparty (k+t)-secure computation [6], to the best of our knowledge there is no known straightforward reduction from one problem to another. Nevertheless, we show that there is a non-trivial reduction from a slightly weaker notion of (k+t)-secure computation, which we call (k+t)-strict secure computation, to implementing (k, t)-robust mediators. We prove the desired lower bound by showing that there are functions on n variables that cannot be (k+t)-strictly securely computed if n ≤ 4(k+t). This also provides a simple alternative proof for the well-known lower bound of 4t+1 on asynchronous secure computation in the presence of up to t malicious agents [4, 8, 10].
DA  - 2023/03/25/
PY  - 2023
DO  - 10.1145/3578579
DP  - ACM Digital Library
VL  - 70
IS  - 2
SP  - 13:1
EP  - 13:21
SN  - 0004-5411
UR  - https://doi.org/10.1145/3578579
Y2  - 2024/07/11/13:50:41
ER  - 

TY  - CONF
TI  - Metareasoning for Safe Decision Making in Autonomous Systems
AU  - Svegliato, Justin
AU  - Basich, Connor
AU  - Saisubramanian, Sandhya
AU  - Zilberstein, Shlomo
T2  - 2022 International Conference on Robotics and Automation (ICRA)
AB  - Although experts carefully specify the high-level decision-making models in autonomous systems, it is infeasible to guarantee safety across every scenario during operation. We therefore propose a safety metareasoning system that optimizes the severity of the system's safety concerns and the interference to the system's task: the system executes in parallel a task process that completes a specified task and safety processes that each address a specified safety concern with a conflict resolver for arbitration. This paper offers a formal definition of a safety metareasoning system, a recommendation algorithm for a safety process, an arbitration algorithm for a conflict resolver, an application of our approach to planetary rover exploration, and a demonstration that our approach is effective in simulation.
C3  - 2022 International Conference on Robotics and Automation (ICRA)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/ICRA46639.2022.9811887
DP  - IEEE Xplore
SP  - 11073
EP  - 11079
UR  - https://ieeexplore.ieee.org/document/9811887
Y2  - 2024/07/11/13:54:54
KW  - Cognition
KW  - Autonomous systems
KW  - Decision making
KW  - Linear programming
KW  - Space vehicles
KW  - Interference
KW  - Storms
ER  - 

TY  - JOUR
TI  - Competence-aware systems
AU  - Basich, Connor
AU  - Svegliato, Justin
AU  - Wray, Kyle H.
AU  - Witwicki, Stefan
AU  - Biswas, Joydeep
AU  - Zilberstein, Shlomo
T2  - Artificial Intelligence
AB  - Building autonomous systems for deployment in the open world has been a longstanding objective in both artificial intelligence and robotics. The open world, however, presents challenges that question some of the assumptions often made in contemporary AI models. Autonomous systems that operate in the open world face complex, non-stationary environments wherein enumerating all situations the system may face over the course of its deployment is intractable. Nevertheless, these systems are expected to operate safely and reliably for extended durations. Consequently, AI systems often rely on some degree of human assistance to mitigate risks while completing their tasks, and are hence better treated as semi-autonomous systems. In order to reduce unnecessary reliance on humans and optimize autonomy, we propose a novel introspective planning model—competence-aware systems (CAS)—that enables a semi-autonomous system to reason about its own competence and allowed level of autonomy by leveraging human feedback or assistance. A CAS learns to adjust its level of autonomy based on experience and interactions with a human authority so as to reduce improper reliance on the human and optimize the degree of autonomy it employs in any given circumstance. To handle situations in which the initial CAS model has insufficient state information to properly discriminate feedback received from humans, we introduce a methodology called iterative state space refinement that gradually increases the granularity of the state space online. The approach exploits information that exists in the standard CAS model and requires no additional input from the human. The result is an agent that can more confidently predict the correct feedback from the human authority in each level of autonomy, enabling it learn its competence in a larger portion of the state space.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.1016/j.artint.2022.103844
DP  - ScienceDirect
VL  - 316
SP  - 103844
J2  - Artificial Intelligence
SN  - 0004-3702
UR  - https://www.sciencedirect.com/science/article/pii/S0004370222001849
Y2  - 2024/07/11/13:54:56
KW  - Adjustable autonomy
KW  - Competence-aware systems
KW  - Decision making under uncertainty
KW  - Human-agent systems
KW  - Probabilistic planning
KW  - Risk-aware autonomy
ER  - 

TY  - CONF
TI  - Dynamic Awareness
AU  - Halpern, Joseph Y.
AU  - Piermont, Evan
T2  - 17th International Conference on Principles of Knowledge Representation and Reasoning {KR-2020}
AB  - We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rˆego (2013) by adding probability, and deﬁne a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula ϕ in state s of a model M , she transitions to state s∗ in a model M ∗. We then discuss how such a model can be applied to information disclosure.
C1  - Rhodes, Greece
C3  - Proceedings of the Seventeenth International Conference on Principles of Knowledge Representation and Reasoning
DA  - 2020/07//
PY  - 2020
DO  - 10.24963/kr.2020/48
DP  - DOI.org (Crossref)
SP  - 476
EP  - 484
LA  - en
PB  - International Joint Conferences on Artificial Intelligence Organization
SN  - 978-0-9992411-7-2
UR  - https://proceedings.kr.org/2020/48
Y2  - 2024/07/11/13:56:02
L1  - https://www.cs.cornell.edu/home/halpern/papers/dynamic_awareness.pdf
ER  - 

TY  - CONF
TI  - Inverting cognitive models with machine learning to infer preferences from fixations
AU  - Russek, Evan
AU  - Callaway, Frederick
AU  - Griffiths, Thomas L.
T2  - NeuRIPS 2023 Workshop on Gaze Meets ML
AB  - Inferring an individual’s preferences from their observable behavior is a key step in the development of assistive decision-making technology. Although machine learning models such as neural networks could in principle be deployed toward this inference, a large amount of data is required to train such models. Here, we present an approach in which a cognitive model generates simulated data to augment limited human data. Using these data, we train a neural network to invert the model, making it possible to infer preferences from behavior. We show how this approach can be used to infer the value that people assign to food items from their eye movements when choosing between those items. We demonstrate first that neural networks can infer the latent preferences used by the model to generate simulated fixations, and second that simulated data can be beneficial in pretraining a network for predicting human-reported preferences from real fixations. Compared to inferring preferences from choice alone, this approach confers a slight improvement in predicting preferences and also allows prediction to take place prior to the choice being made. Overall, our results suggest that using a combination of neural networks and model-simulated training data is a promising approach for developing technology that infers human preferences.
DA  - 2023/10/27/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=ai0ES5VAAM
Y2  - 2024/07/11/13:57:02
L1  - https://openreview.net/pdf?id=ai0ES5VAAM
ER  - 

TY  - GEN
TI  - Getting aligned on representational alignment
AU  - Sucholutsky, Ilia
AU  - Muttenthaler, Lukas
AU  - Weller, Adrian
AU  - Peng, Andi
AU  - Bobu, Andreea
AU  - Kim, Been
AU  - Love, Bradley C.
AU  - Grant, Erin
AU  - Groen, Iris
AU  - Achterberg, Jascha
AU  - Tenenbaum, Joshua B.
AU  - Collins, Katherine M.
AU  - Hermann, Katherine L.
AU  - Oktar, Kerem
AU  - Greff, Klaus
AU  - Hebart, Martin N.
AU  - Jacoby, Nori
AU  - Zhang, Qiuyi
AU  - Marjieh, Raja
AU  - Geirhos, Robert
AU  - Chen, Sherol
AU  - Kornblith, Simon
AU  - Rane, Sunayana
AU  - Konkle, Talia
AU  - O'Connell, Thomas P.
AU  - Unterthiner, Thomas
AU  - Lampinen, Andrew K.
AU  - Müller, Klaus-Robert
AU  - Toneva, Mariya
AU  - Griffiths, Thomas L.
AB  - Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.
DA  - 2023/11/02/
PY  - 2023
DO  - 10.48550/arXiv.2310.13018
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.13018
Y2  - 2024/07/11/13:58:52
L1  - https://arxiv.org/pdf/2310.13018.pdf
L2  - https://arxiv.org/abs/2310.13018
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
KW  - Quantitative Biology - Neurons and Cognition
ER  - 

TY  - GEN
TI  - Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game
AU  - Toyer, Sam
AU  - Watkins, Olivia
AU  - Mendes, Ethan Adrian
AU  - Svegliato, Justin
AU  - Bailey, Luke
AU  - Wang, Tiffany
AU  - Ong, Isaac
AU  - Elmaaroufi, Karim
AU  - Abbeel, Pieter
AU  - Darrell, Trevor
AU  - Ritter, Alan
AU  - Russell, Stuart
AB  - While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper
DA  - 2023/11/02/
PY  - 2023
DO  - 10.48550/arXiv.2311.01011
DP  - arXiv.org
PB  - arXiv
ST  - Tensor Trust
UR  - http://arxiv.org/abs/2311.01011
Y2  - 2024/07/11/13:59:03
L1  - https://arxiv.org/pdf/2311.01011.pdf
L2  - https://arxiv.org/abs/2311.01011
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Image Hijacks: Adversarial Images can Control Generative Models at Runtime
AU  - Bailey, Luke
AU  - Ong, Euan
AU  - Russell, Stuart
AU  - Emmons, Scott
AB  - Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.
DA  - 2024/04/22/
PY  - 2024
DO  - 10.48550/arXiv.2309.00236
DP  - arXiv.org
PB  - arXiv
ST  - Image Hijacks
UR  - http://arxiv.org/abs/2309.00236
Y2  - 2024/07/11/13:59:48
L1  - https://arxiv.org/pdf/2309.00236.pdf
L2  - https://arxiv.org/abs/2309.00236
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks
AU  - Casper, Stephen
AU  - Hariharan, Kaivalya
AU  - Hadfield-Menell, Dylan
AB  - This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue
DA  - 2023/05/05/
PY  - 2023
DO  - 10.48550/arXiv.2211.10024
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.10024
Y2  - 2024/07/11/14:00:02
L1  - https://arxiv.org/pdf/2211.10024.pdf
L2  - https://arxiv.org/abs/2211.10024
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Perceptual Adversarial Robustness: Defense Against Unseen Threat Models
AU  - Laidlaw, Cassidy
AU  - Singla, Sahil
AU  - Feizi, Soheil
AB  - A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the very definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to avoid this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models. To resolve this issue, we propose adversarial training against the set of all imperceptible adversarial examples, approximated using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks. We find that PAT achieves state-of-the-art robustness against the union of these five attacks, more than doubling the accuracy over the next best model, without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.
DA  - 2021/07/04/
PY  - 2021
DO  - 10.48550/arXiv.2006.12655
DP  - arXiv.org
PB  - arXiv
ST  - Perceptual Adversarial Robustness
UR  - http://arxiv.org/abs/2006.12655
Y2  - 2024/07/11/14:00:08
L1  - https://arxiv.org/pdf/2006.12655.pdf
L2  - https://arxiv.org/abs/2006.12655
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Adversarial Policies: Attacking Deep Reinforcement Learning
AU  - Gleave, Adam
AU  - Dennis, Michael
AU  - Wild, Cody
AU  - Kant, Neel
AU  - Levine, Sergey
AU  - Russell, Stuart
AB  - Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.
DA  - 2021/01/17/
PY  - 2021
DO  - 10.48550/arXiv.1905.10615
DP  - arXiv.org
PB  - arXiv
ST  - Adversarial Policies
UR  - http://arxiv.org/abs/1905.10615
Y2  - 2024/07/11/14:00:29
L1  - https://arxiv.org/pdf/1905.10615.pdf
L2  - https://arxiv.org/abs/1905.10615
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - I.2.6
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Preventing Imitation Learning with Adversarial Policy Ensembles
AU  - Zhan, Albert
AU  - Tiomkin, Stas
AU  - Abbeel, Pieter
AB  - Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of "non-clonable" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.
DA  - 2020/08/02/
PY  - 2020
DO  - 10.48550/arXiv.2002.01059
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2002.01059
Y2  - 2024/07/11/14:00:35
L1  - https://arxiv.org/pdf/2002.01059.pdf
L2  - https://arxiv.org/abs/2002.01059
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - On the Geometry of Adversarial Examples
AU  - Khoury, Marc
AU  - Hadfield-Menell, Dylan
AB  - Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.
DA  - 2018/12/11/
PY  - 2018
DO  - 10.48550/arXiv.1811.00525
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1811.00525
Y2  - 2024/07/11/14:00:43
L1  - https://arxiv.org/pdf/1811.00525.pdf
L2  - https://arxiv.org/abs/1811.00525
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Adversarial Training with Voronoi Constraints
AU  - Khoury, Marc
AU  - Hadfield-Menell, Dylan
AB  - Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which an adversary could construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove that adversarial training is sample inefficient, and show sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust. Finally we introduce adversarial training with Voronoi constraints, which replaces the norm ball constraint with the Voronoi cell for each point in the training set. We show that adversarial training with Voronoi constraints produces robust models which significantly improve over the state-of-the-art on MNIST and are competitive on CIFAR-10.
DA  - 2019/05/02/
PY  - 2019
DO  - 10.48550/arXiv.1905.01019
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1905.01019
Y2  - 2024/07/11/14:00:59
L1  - https://arxiv.org/pdf/1905.01019.pdf
L2  - https://arxiv.org/abs/1905.01019
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computational Geometry
ER  - 

TY  - GEN
TI  - Exploration with Principles for Diverse AI Supervision
AU  - Liu, Hao
AU  - Zaharia, Matei
AU  - Abbeel, Pieter
AB  - Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the generated content, offering critiques to guide the actor. Empirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision.
DA  - 2023/11/23/
PY  - 2023
DO  - 10.48550/arXiv.2310.08899
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.08899
Y2  - 2024/07/11/14:02:53
L1  - https://arxiv.org/pdf/2310.08899.pdf
L2  - https://arxiv.org/abs/2310.08899
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance
AU  - Ye, Weirui
AU  - Zhang, Yunsheng
AU  - Wang, Mengchen
AU  - Wang, Shengjie
AU  - Gu, Xianfan
AU  - Abbeel, Pieter
AU  - Gao, Yang
AB  - Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation priors, FAC learns significantly faster than traditional RL. Our evaluation on the Meta-World has proved that FAC can achieve 100% success rates for 7/8 tasks under less than 200k frames, which outperforms the baseline method with careful manual-designed rewards under 1M frames. (2) Robust to noisy priors. Our method tolerates the unavoidable noise in embodied foundation models. We show that FAC works well even under heavy noise or quantization errors. (3) Minimal human intervention: FAC completely learns from the foundation priors, without the need of human-specified dense reward, or providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our FRL framework could enable the future robot to autonomously explore and learn without human intervention in the physical world. In summary, our proposed FRL is a novel and powerful learning paradigm, towards achieving embodied generalist agents.
DA  - 2023/10/10/
PY  - 2023
DO  - 10.48550/arXiv.2310.02635
DP  - arXiv.org
PB  - arXiv
ST  - Foundation Reinforcement Learning
UR  - http://arxiv.org/abs/2310.02635
Y2  - 2024/07/11/14:03:06
L1  - https://arxiv.org/pdf/2310.02635.pdf
L2  - https://arxiv.org/abs/2310.02635
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Learning Interactive Real-World Simulators
AU  - Yang, Mengjiao
AU  - Du, Yilun
AU  - Ghasemipour, Kamyar
AU  - Tompson, Jonathan
AU  - Kaelbling, Leslie
AU  - Schuurmans, Dale
AU  - Abbeel, Pieter
AB  - Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by ∆x, ∆y” from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at universal-simulator.github.io.
DA  - 2024/01/12/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2310.06114
Y2  - 2024/07/11/14:03:27
L1  - https://arxiv.org/pdf/2310.06114
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models
AU  - Zhao, Lingjun
AU  - Nguyen, Khanh
AU  - Daumé III, Hal
AB  - Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them.
DA  - 2023/05/28/
PY  - 2023
DO  - 10.48550/arXiv.2301.05149
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2301.05149
Y2  - 2024/07/11/14:03:53
L1  - https://arxiv.org/pdf/2301.05149.pdf
L2  - https://arxiv.org/abs/2301.05149
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Robotics
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve
AU  - McCoy, R. Thomas
AU  - Yao, Shunyu
AU  - Friedman, Dan
AU  - Hardy, Matthew
AU  - Griffiths, Thomas L.
AB  - The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach—which we call the teleological approach—leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low—even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4’s accuracy at decoding a simple cipher is 51% when the output is a high-probability word sequence but only 13% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system—one that has been shaped by its own particular set of pressures.
DA  - 2023/09/24/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Embers of Autoregression
UR  - http://arxiv.org/abs/2309.13638
Y2  - 2024/07/11/14:04:10
L1  - https://arxiv.org/pdf/2309.13638
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Superhuman artificial intelligence can improve human decision-making by increasing novelty
AU  - Shin, Minkyu
AU  - Kim, Jin
AU  - van Opheusden, Bas
AU  - Griffiths, Thomas L.
T2  - Proceedings of the National Academy of Sciences
AB  - How will superhuman artificial intelligence (AI) affect human decision-making? And what will be the mechanisms behind this effect? We address these questions in a domain where AI already exceeds human performance, analyzing more than 5.8 million move decisions made by professional Go players over the past 71 y (1950 to 2021). To address the first question, we use a superhuman AI program to estimate the quality of human decisions across time, generating 58 billion counterfactual game patterns and comparing the win rates of actual human decisions with those of counterfactual AI decisions. We find that humans began to make significantly better decisions following the advent of superhuman AI. We then examine human players’ strategies across time and find that novel decisions (i.e., previously unobserved moves) occurred more frequently and became associated with higher decision quality after the advent of superhuman AI. Our findings suggest that the development of superhuman AI programs may have prompted human players to break away from traditional strategies and induced them to explore novel moves, which in turn may have improved their decision-making.
DA  - 2023/03/21/
PY  - 2023
DO  - 10.1073/pnas.2214840120
DP  - pnas.org (Atypon)
VL  - 120
IS  - 12
SP  - e2214840120
UR  - https://www.pnas.org/doi/full/10.1073/pnas.2214840120
Y2  - 2024/07/11/14:04:29
L1  - https://www.pnas.org/doi/pdf/10.1073/pnas.2214840120
ER  - 

TY  - GEN
TI  - Evaluating Language-Model Agents on Realistic Autonomous Tasks
AU  - Kinniment, Megan
AU  - Sato, Lucas Jun Koba
AU  - Du, Haoxing
AU  - Goodrich, Brian
AU  - Hasin, Max
AU  - Chan, Lawrence
AU  - Miles, Luke Harold
AU  - Lin, Tao R.
AU  - Wijk, Hjalmar
AU  - Burget, Joel
AU  - Ho, Aaron
AU  - Barnes, Elizabeth
AU  - Christiano, Paul
AB  - In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.
DA  - 2024/01/04/
PY  - 2024
DO  - 10.48550/arXiv.2312.11671
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.11671
Y2  - 2024/07/11/14:05:45
L1  - https://arxiv.org/pdf/2312.11671.pdf
L2  - https://arxiv.org/abs/2312.11671
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Task Scoping: Generating Task-Specific Abstractions for Planning in Open-Scope Models
AU  - Fishman, Michael
AU  - Kumar, Nishanth
AU  - Allen, Cameron
AU  - Danas, Natasha
AU  - Littman, Michael
AU  - Tellex, Stefanie
AU  - Konidaris, George
AB  - A general-purpose planning agent requires an open-scope world model: one rich enough to tackle any of the wide range of tasks it may be asked to solve over its operational lifetime. This stands in contrast with typical planning approaches, where the scope of a model is limited to a specific family of tasks that share significant structure. Unfortunately, planning to solve any specific task using an open-scope model is computationally intractable - even for state-of-the-art methods - due to the many states and actions that are necessarily present in the model but irrelevant to that problem. We propose task scoping: a method that exploits knowledge of the initial state, goal conditions, and transition system to automatically and efficiently remove provably irrelevant variables and actions from a planning problem. Our approach leverages causal link analysis and backwards reachability over state variables (rather than states) along with operator merging (when effects on relevant variables are identical). Using task scoping as a pre-planning step can shrink the search space by orders of magnitude and dramatically decrease planning time. We empirically demonstrate that these improvements occur across a variety of open-scope domains, including Minecraft, where our approach leads to a 75x reduction in search time with a state-of-the-art numeric planner, even after including the time required for task scoping itself.
DA  - 2023/02/04/
PY  - 2023
DO  - 10.48550/arXiv.2010.08869
DP  - arXiv.org
PB  - arXiv
ST  - Task Scoping
UR  - http://arxiv.org/abs/2010.08869
Y2  - 2024/07/11/14:06:17
L1  - https://arxiv.org/pdf/2010.08869.pdf
L2  - https://arxiv.org/abs/2010.08869
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Diversifying AI: Towards Creative Chess with AlphaZero
AU  - Zahavy, Tom
AU  - Veeriah, Vivek
AU  - Hou, Shaobo
AU  - Waugh, Kevin
AU  - Lai, Matthew
AU  - Leurent, Edouard
AU  - Tomasev, Nenad
AU  - Schut, Lisa
AU  - Hassabis, Demis
AU  - Singh, Satinder
AB  - In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
DA  - 2023/08/29/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Diversifying AI
UR  - http://arxiv.org/abs/2308.09175
Y2  - 2024/07/11/14:06:37
L1  - https://arxiv.org/pdf/2308.09175
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Discovering Evolution Strategies via Meta-Black-Box Optimization
AU  - Lange, Robert Tjarko
AU  - Schaul, Tom
AU  - Chen, Yutian
AU  - Zahavy, Tom
AU  - Dallibard, Valentin
AU  - Lu, Chris
AU  - Singh, Satinder
AU  - Flennerhag, Sebastian
AB  - Optimizing functions without access to gradients is the remit of black-box methods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inﬂexible — exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that metaevolving this system on a small set of representative low-dimensional analytic optimization problems is sufﬁcient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop.
DA  - 2023/03/02/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2211.11260
Y2  - 2024/07/11/14:07:26
L1  - https://arxiv.org/pdf/2211.11260
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - CONF
TI  - Human-Timescale Adaptation in an Open-Ended Task Space
AU  - Bauer, Jakob
AU  - Baumli, Kate
AU  - Behbahani, Feryal
AU  - Bhoopchand, Avishkar
AU  - Bradley-Schmieg, Nathalie
AU  - Chang, Michael
AU  - Clay, Natalie
AU  - Collister, Adrian
AU  - Dasagi, Vibhavari
AU  - Gonzalez, Lucy
AU  - Gregor, Karol
AU  - Hughes, Edward
AU  - Kashem, Sheleem
AU  - Loks-Thompson, Maria
AU  - Openshaw, Hannah
AU  - Parker-Holder, Jack
AU  - Pathak, Shreya
AU  - Perez-Nieves, Nicolas
AU  - Rakicevic, Nemanja
AU  - Rocktäschel, Tim
AU  - Schroecker, Yannick
AU  - Singh, Satinder
AU  - Sygnowski, Jakub
AU  - Tuyls, Karl
AU  - York, Sarah
AU  - Zacherl, Alexander
AU  - Zhang, Lei
T2  - ICML 2023
AB  - Foundation models have shown impressive adaptation and scalability in supervised and selfsupervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-ﬂy hypothesis-driven exploration, efﬁcient exploitation of acquired knowledge, and can successfully be prompted with ﬁrst-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attentionbased memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent’s capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across everlarger open-ended domains.
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=thUjOwfzzv
ER  - 

TY  - GEN
TI  - The False Promise of Imitating Proprietary LLMs
AU  - Gudibande, Arnav
AU  - Wallace, Eric
AU  - Snell, Charlie
AU  - Geng, Xinyang
AU  - Liu, Hao
AU  - Abbeel, Pieter
AU  - Levine, Sergey
AU  - Song, Dawn
AB  - An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model’s capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B–13B), data sources, and imitation data amounts (0.3M–150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models—they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.
DA  - 2023/05/25/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2305.15717
Y2  - 2024/07/11/14:11:04
L1  - https://arxiv.org/pdf/2305.15717
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Emergent Agentic Transformer from Chain of Hindsight Experience
AU  - Liu, Hao
AU  - Abbeel, Pieter
AB  - Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at training and test time. As we show on D4RL and ExoRL benchmarks, to the best our knowledge, this is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data. Our Agentic Transformer also shows a promising scaling trend that bigger models consistently improve results.
DA  - 2023/05/25/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2305.16554
Y2  - 2024/07/11/14:11:25
L1  - https://arxiv.org/pdf/2305.16554
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Chain of Hindsight Aligns Language Models with Feedback
AU  - Liu, Hao
AU  - Sferrazza, Carmelo
AU  - Abbeel, Pieter
AB  - Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.
DA  - 2023/10/18/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2302.02676
Y2  - 2024/07/11/14:11:43
L1  - https://arxiv.org/pdf/2302.02676
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - The Wisdom of Hindsight Makes Language Models Better Instruction Followers
AU  - Zhang, Tianjun
AU  - Liu, Fangchen
AU  - Wong, Justin
AU  - Abbeel, Pieter
AU  - Gonzalez, Joseph E.
AB  - Reinforcement learning has seen wide success in ﬁnetuning large language models to better align with instructions via human feedback. The socalled algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn’t require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised ﬁnetuning1.
DA  - 2023/02/10/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2302.05206
Y2  - 2024/07/11/14:11:53
L1  - https://arxiv.org/pdf/2302.05206
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Self-Imitation Learning
AU  - Oh, Junhyuk
AU  - Guo, Yijie
AU  - Singh, Satinder
AU  - Lee, Honglak
AB  - This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.
DA  - 2018/06/14/
PY  - 2018
DO  - 10.48550/arXiv.1806.05635
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1806.05635
Y2  - 2024/07/11/14:11:58
L1  - https://arxiv.org/pdf/1806.05635.pdf
L2  - https://arxiv.org/abs/1806.05635
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Human-Centered Evaluation of Explanations
AU  - Boyd-Graber, Jordan
AU  - Carton, Samuel
AU  - Feng, Shi
AU  - Liao, Q. Vera
AU  - Lombrozo, Tania
AU  - Smith-Renner, Alison
AU  - Tan, Chenhao
A2  - Ballesteros, Miguel
A2  - Tsvetkov, Yulia
A2  - Alm, Cecilia O.
AB  - The NLP community are increasingly interested in providing explanations for NLP models to help people make sense of model behavior and potentially improve human interaction with models. In addition to computational challenges in generating these explanations, evaluations of the generated explanations require human-centered perspectives and approaches. This tutorial will provide an overview of human-centered evaluations of explanations. First, we will give a brief introduction to the psychological foundation of explanations as well as types of NLP model explanations and their corresponding presentation, to provide the necessary background. We will then present a taxonomy of human-centered evaluation of explanations and dive into depth in the two categories: 1) evaluation based on human-annotated explanations; 2) evaluation with human-subjects studies. We will conclude by discussing future directions. We will also adopt a flipped format to maximize the in- teractive components for the live audience.
C1  - Seattle, United States
C3  - Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts
DA  - 2022/07//
PY  - 2022
DO  - 10.18653/v1/2022.naacl-tutorials.4
DP  - ACLWeb
SP  - 26
EP  - 32
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.naacl-tutorials.4
Y2  - 2024/07/11/14:14:02
L1  - https://aclanthology.org/2022.naacl-tutorials.4.pdf
ER  - 

TY  - GEN
TI  - Progress measures for grokking via mechanistic interpretability
AU  - Nanda, Neel
AU  - Chan, Lawrence
AU  - Lieberum, Tom
AU  - Smith, Jess
AU  - Steinhardt, Jacob
AB  - Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous progress measures that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverseengineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of “grokking” exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.
DA  - 2023/10/19/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2301.05217
Y2  - 2024/07/11/14:15:34
L1  - https://arxiv.org/pdf/2301.05217
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations
AU  - Chughtai, Bilal
AU  - Chan, Lawrence
AU  - Nanda, Neel
AB  - Universality is a key hypothesis in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop – are arbitrary.
DA  - 2023/05/24/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - A Toy Model of Universality
UR  - http://arxiv.org/abs/2302.03025
Y2  - 2024/07/11/14:15:49
L1  - https://arxiv.org/pdf/2302.03025
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Mathematics - Representation Theory
ER  - 

TY  - JOUR
TI  - Neural Networks Learn Representation Theory: Reverse Engineering How Networks Perform Group Operations
AU  - Chughtai, Bilal
AU  - Chan, Lawrence
AU  - Nanda, Neel
AB  - We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory, through learning several irreducible representations of the group and converting group composition to matrix multiplication. We show small networks consistently learn this algorithm when trained on composition of group elements by reverse engineering model logits and weights, and confirm our understanding using ablations. We use this as an algorithmic test bed for the hypothesis of universality in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. By studying networks trained on various groups and architectures, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop –are arbitrary.
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=j4_YHiTAN63
ER  - 

TY  - GEN
TI  - Understanding and Controlling a Maze-Solving Policy Network
AU  - Mini, Ulisse
AU  - Grietzer, Peli
AU  - Sharma, Mrinank
AU  - Meek, Austin
AU  - MacDiarmid, Monte
AU  - Turner, Alexander Matt
AB  - To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.
DA  - 2023/10/12/
PY  - 2023
DO  - 10.48550/arXiv.2310.08043
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.08043
Y2  - 2024/07/11/14:16:03
L1  - https://arxiv.org/pdf/2310.08043.pdf
L2  - https://arxiv.org/abs/2310.08043
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Activation Addition: Steering Language Models Without Optimization
AU  - Turner, Alexander Matt
AU  - Thiergart, Lisa
AU  - Leech, Gavin
AU  - Udell, David
AU  - Vazquez, Juan J.
AU  - Mini, Ulisse
AU  - MacDiarmid, Monte
AB  - Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.
DA  - 2024/06/04/
PY  - 2024
DO  - 10.48550/arXiv.2308.10248
DP  - arXiv.org
PB  - arXiv
ST  - Activation Addition
UR  - http://arxiv.org/abs/2308.10248
Y2  - 2024/07/11/14:16:09
L1  - https://arxiv.org/pdf/2308.10248.pdf
L2  - https://arxiv.org/abs/2308.10248
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Translating Neuralese
AU  - Andreas, Jacob
AU  - Dragan, Anca
AU  - Klein, Dan
AB  - Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.
DA  - 2018/12/22/
PY  - 2018
DO  - 10.48550/arXiv.1704.06960
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1704.06960
Y2  - 2024/07/11/14:16:52
L1  - https://arxiv.org/pdf/1704.06960.pdf
L2  - https://arxiv.org/abs/1704.06960
KW  - Computer Science - Computation and Language
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Pruned Neural Networks are Surprisingly Modular
AU  - Filan, Daniel
AU  - Hod, Shlomi
AU  - Wild, Cody
AU  - Critch, Andrew
AU  - Russell, Stuart
AB  - The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a "module" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers. Note that this paper has been superceded by "Clusterability in Neural Networks", arxiv:2103.03386 and "Quantifying Local Specialization in Deep Neural Networks", arxiv:2110.08058!
DA  - 2022/02/07/
PY  - 2022
DO  - 10.48550/arXiv.2003.04881
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2003.04881
Y2  - 2024/07/11/14:18:17
L1  - https://arxiv.org/pdf/2003.04881.pdf
L2  - https://arxiv.org/abs/2003.04881
KW  - Computer Science - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - JOUR
TI  - Explaining robot policies
AU  - Watkins, Olivia
AU  - Huang, Sandy
AU  - Frost, Julius
AU  - Bhatia, Kush
AU  - Weiner, Eric
AU  - Abbeel, Pieter
AU  - Darrell, Trevor
AU  - Plummer, Bryan
AU  - Saenko, Kate
AU  - Dragan, Anca
T2  - Applied AI Letters
AB  - In order to interact with a robot or make wise decisions about where and how to deploy it in the real world, humans need to have an accurate mental model of how the robot acts in different situations. We propose to improve users' mental model of a robot by showing them examples of how the robot behaves in informative scenarios. We explore this in two settings. First, we show that when there are many possible environment states, users can more quickly understand the robot's policy if they are shown critical states where taking a particular action is important. Second, we show that when there is a distribution shift between training and test environment distributions, then it is more effective to show exploratory states that the robot does not visit naturally.
DA  - 2021///
PY  - 2021
DO  - 10.1002/ail2.52
DP  - Wiley Online Library
VL  - 2
IS  - 4
SP  - e52
LA  - en
SN  - 2689-5595
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.52
Y2  - 2024/07/11/14:18:25
L1  - https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ail2.52
KW  - transparency
KW  - explainable artificial intelligence
KW  - deep reinforcement learning
ER  - 

TY  - GEN
TI  - Discovering User-Interpretable Capabilities of Black-Box Planning Agents
AU  - Verma, Pulkit
AU  - Marpally, Shashank Rao
AU  - Srivastava, Siddharth
AB  - Several approaches have been developed for answering users' specific questions about AI behavior and for assessing their core functionality in terms of primitive executable actions. However, the problem of summarizing an AI agent's broad capabilities for a user is comparatively new. This paper presents an algorithm for discovering from scratch the suite of high-level "capabilities" that an AI system with arbitrary internal planning algorithms/policies can perform. It computes conditions describing the applicability and effects of these capabilities in user-interpretable terms. Starting from a set of user-interpretable state properties, an AI agent, and a simulator that the agent can interact with, our algorithm returns a set of high-level capabilities with their parameterized descriptions. Empirical evaluation on several game-based scenarios shows that this approach efficiently learns descriptions of various types of AI agents in deterministic, fully observable settings. User studies show that such descriptions are easier to understand and reason with than the agent's primitive actions.
DA  - 2022/05/30/
PY  - 2022
DO  - 10.48550/arXiv.2107.13668
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2107.13668
Y2  - 2024/07/11/14:18:40
L1  - https://arxiv.org/pdf/2107.13668.pdf
L2  - https://arxiv.org/abs/2107.13668
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Clusterability in Neural Networks
AU  - Filan, Daniel
AU  - Casper, Stephen
AU  - Hod, Shlomi
AU  - Wild, Cody
AU  - Critch, Andrew
AU  - Russell, Stuart
AB  - The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We ﬁnd that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and ﬁnd that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.
DA  - 2021/03/04/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2103.03386
Y2  - 2024/07/11/14:18:57
L1  - https://arxiv.org/pdf/2103.03386
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Truthful AI: Developing and governing AI that does not lie
AU  - Evans, Owain
AU  - Cotton-Barratt, Owen
AU  - Finnveden, Lukas
AU  - Bales, Adam
AU  - Balwit, Avital
AU  - Wills, Peter
AU  - Righetti, Luca
AU  - Saunders, William
AB  - In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI "lies" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.
DA  - 2021/10/13/
PY  - 2021
DO  - 10.48550/arXiv.2110.06674
DP  - arXiv.org
PB  - arXiv
ST  - Truthful AI
UR  - http://arxiv.org/abs/2110.06674
Y2  - 2024/07/11/21:18:30
L1  - https://arxiv.org/pdf/2110.06674.pdf
L2  - https://arxiv.org/abs/2110.06674
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - I.2.0
ER  - 

TY  - GEN
TI  - Reframing Superintelligence
AU  - Drexler, K Eric
AB  - Studies of superintelligent-level systems have typically posited AI functionality that plays the role of a mind in a rational utility-directed agent, and hence employ an abstraction initially developed as an idealized model of human decision makers. Today, developments in AI technology highlight intelligent systems that are quite unlike minds, and provide a basis for a different approach to understanding them: Today, we can
consider how AI systems are produced (through the work of research and development), what they do (broadly, provide services by performing tasks), and what they will enable (including incremental yet potentially thorough automation of human tasks). Because tasks subject to automation include the tasks that comprise AI research and development, current trends in the field promise accelerating AI-enabled advances in AI technology itself, potentially leading to asymptotically recursive improvement of AI technologies in distributed systems, a prospect that contrasts sharply with the vision of
self-improvement internal to opaque, unitary agents. The trajectory of AI development thus points to the emergence of asymptotically comprehensive, superintelligent-level AI services that—crucially—can include the service of developing new services, both narrow and broad, guided by concrete human goals and informed by strong models of human (dis)approval. The concept of comprehensive AI services (CAIS) provides a model of flexible, general intelligence in which agents are a class of service-providing products, rather than a natural or necessary engine of progress in themselves. Ramifications of the CAIS model reframe not only prospects for an
intelligence explosion and the nature of advanced machine intelligence, but also the relationship between goals and intelligence, the problem of harnessing advanced AI to broad, challenging problems, and funda-
mental considerations in AI safety and strategy. Perhaps surprisingly,
strongly self-modifying agents lose their instrumental value even as their
implementation becomes more accessible, while the likely context for the emergence of such agents becomes a world already in possession of general superintelligent-level capabilities. These prospective capabilities, in turn, engender novel risks and opportunities of their own. Further topics addressed in this work include the general architecture of systems with broad capabilities, the intersection between symbolic and neural systems, learning vs. competence in definitions of intelligence, tactical vs. strategic tasks in the context of human control, and estimates of the relative capacities of human brains vs. current digital systems.
CY  - Unpublished
DA  - 2019/01//
PY  - 2019
DP  - Zotero
LA  - en
L1  - https://static1.squarespace.com/static/660e95991cf0293c2463bcc8/t/660ec8f0ca87a33aa8832af3/1712244978900/2019-1.pdf
ER  - 

TY  - JOUR
TI  - Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts
AU  - Grace, Katja
AU  - Salvatier, John
AU  - Dafoe, Allan
AU  - Zhang, Baobao
AU  - Evans, Owain
T2  - Journal of Artificial Intelligence Research
AB  - Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.


This article is part of the special track on AI and Society.
DA  - 2018/07/31/
PY  - 2018
DO  - 10.1613/jair.1.11222
DP  - jair.org
VL  - 62
SP  - 729
EP  - 754
LA  - en
SN  - 1076-9757
ST  - Viewpoint
UR  - https://jair.org/index.php/jair/article/view/11222
Y2  - 2024/07/11/21:22:19
L1  - https://jair.org/index.php/jair/article/download/11222/26431
ER  - 

TY  - CONF
TI  - Safely interruptible agents
AU  - Orseau, Laurent
AU  - Armstrong, Stuart
T3  - UAI'16
AB  - Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button— which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal definition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.
C1  - Arlington, Virginia, USA
C3  - Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence
DA  - 2016/06/25/
PY  - 2016
DP  - ACM Digital Library
SP  - 557
EP  - 566
PB  - AUAI Press
SN  - 978-0-9966431-1-5
Y2  - 2024/07/11/
ER  - 

TY  - JOUR
TI  - Thinking Inside the Box: Controlling and Using an Oracle AI
AU  - Armstrong, Stuart
AU  - Sandberg, Anders
AU  - Bostrom, Nick
T2  - Minds and Machines
AB  - There is no strong reason to believe that human-level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed motivation system. Solving this issue in general has proven to be considerably harder than expected. This paper looks at one particular approach, Oracle AI. An Oracle AI is an AI that does not act in the world except by answering questions. Even this narrow approach presents considerable challenges. In this paper, we analyse and critique various methods of controlling the AI. In general an Oracle AI might be safer than unrestricted AI, but still remains potentially dangerous.
DA  - 2012/11/01/
PY  - 2012
DO  - 10.1007/s11023-012-9282-2
DP  - Springer Link
VL  - 22
IS  - 4
SP  - 299
EP  - 324
J2  - Minds & Machines
LA  - en
SN  - 1572-8641
ST  - Thinking Inside the Box
UR  - https://doi.org/10.1007/s11023-012-9282-2
Y2  - 2024/07/11/21:26:15
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11023-012-9282-2.pdf
KW  - Security
KW  - Artificial intelligence
KW  - Superintelligence
KW  - Risks
KW  - Capability control
KW  - Motivational control
ER  - 

TY  - JOUR
TI  - The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents
AU  - Bostrom, Nick
T2  - Minds and Machines
AB  - This paper discusses the relation between intelligence and motivation in artificial agents, developing and briefly arguing for two theses. The first, the orthogonality thesis, holds (with some caveats) that intelligence and final goals (purposes) are orthogonal axes along which possible artificial intellects can freely vary—more or less any level of intelligence could be combined with more or less any final goal. The second, the instrumental convergence thesis, holds that as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. In combination, the two theses help us understand the possible range of behavior of superintelligent agents, and they point to some potential dangers in building such an agent.
DA  - 2012/05/01/
PY  - 2012
DO  - 10.1007/s11023-012-9281-3
DP  - Springer Link
VL  - 22
IS  - 2
SP  - 71
EP  - 85
J2  - Minds & Machines
LA  - en
SN  - 1572-8641
ST  - The Superintelligent Will
UR  - https://doi.org/10.1007/s11023-012-9281-3
Y2  - 2024/07/11/21:26:30
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11023-012-9281-3.pdf
KW  - Artificial intelligence
KW  - AI
KW  - Superintelligence
KW  - Goal
KW  - Instrumental reason
KW  - Intelligent agent
ER  - 

TY  - JOUR
TI  - Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models
AU  - Schellaert, Wout
AU  - Martínez-Plumed, Fernando
AU  - Vold, Karina
AU  - Burden, John
AU  - Casares, Pablo A. M.
AU  - Loe, Bao Sheng
AU  - Reichart, Roi
AU  - hÉigeartaigh, Sean Ó
AU  - Korhonen, Anna
AU  - Hernández-Orallo, José
T2  - Journal of Artificial Intelligence Research
AB  - Even with obvious deficiencies, large prompt-commanded multimodal models are proving to be flexible cognitive tools representing an unprecedented generality. But the directness, diversity, and degree of user interaction create a distinctive “human-centred generality” (HCG), rather than a fully autonomous one. HCG implies that —for a specific user— a system is only as general as it is effective for the user’s relevant tasks and their prevalent ways of prompting. A human-centred evaluation of general-purpose AI systems therefore needs to reflect the personal nature of interaction, tasks and cognition. We argue that the best way to understand these systems is as highly-coupled cognitive extenders, and to analyse the bidirectional cognitive adaptations between them and humans. In this paper, we give a formulation of HCG, as well as a high-level overview of the elements and trade-offs involved in the prompting process. We end the paper by outlining some essential research questions and suggestions for improving evaluation practices, which we envision as characteristic for the evaluation of general artificial intelligence in the future.
This paper appears in the AI &amp; Society track.
DA  - 2023/06/12/
PY  - 2023
DO  - 10.1613/jair.1.14157
DP  - www.jair.org
VL  - 77
SP  - 377
EP  - 394
LA  - en
SN  - 1076-9757
ST  - Your Prompt is My Command
UR  - https://www.jair.org/index.php/jair/article/view/14157
Y2  - 2024/07/11/21:34:53
L1  - https://www.jair.org/index.php/jair/article/download/14157/26935
KW  - human computer interaction
KW  - cognitive modelling
ER  - 

TY  - CONF
TI  - Exploring AI Safety in Degrees: Generality, Capability and Control
AU  - Burden, John
AU  - Hernández-Orallo, José
A2  - Espinoza, Huáscar
A2  - Hernández-Orallo, José
A2  - Chen, Xin Cynthia
A2  - ÓhÉigeartaigh, Seán S.
A2  - Huang, Xiaowei
A2  - Castillo-Effen, Mauricio
A2  - Mallah, Richard
A2  - McDermid, John A.
T3  - CEUR Workshop Proceedings
AB  - The landscape of AI safety is frequently explored differently by contrasting specialised AI versus general AI (or AGI), by analysing the short-term hazards of systems with limited capabilities against those more long-term risks posed by ‘superintelligence’, and by conceptualising sophisticated ways of bounding control an AI system has over its environment and itself (impact, harm to humans, self-harm, containment, etc.). In this position paper we reconsider these three aspects of AI safety as quantitative factors –generality, capability and
control–, suggesting that by defining metrics for these dimensions, AI risks can be characterised and analysed more precisely. As an example, we illustrate how to define these metrics and their values for some simple agents in a toy scenario within a reinforcement learning setting.
C3  - Proceedings of the Workshop on Artificial Intelligence Safety, co-located with 34th AAAI Conference on Artificial Intelligence, SafeAI@AAAI 2020, New York City, NY, USA, February 7, 2020
DA  - 2020///
PY  - 2020
DP  - DBLP Computer Science Bibliography
VL  - 2560
SP  - 36
EP  - 40
PB  - CEUR-WS.org
ST  - Exploring AI Safety in Degrees
UR  - https://ceur-ws.org/Vol-2560/paper21.pdf
Y2  - 2024/07/11/21:35:23
ER  - 

TY  - JOUR
TI  - Oases of Cooperation: An Empirical Evaluation of Reinforcement Learning in the Iterated Prisoner’s Dilemma
AU  - Barnett, Peter
AU  - Burden, John
AB  - In the creation of safe AI systems it is extremely important to ensure cooperative behaviour of these systems, even when there are incentives to act selﬁshly. In many cases, even when game-theoretic solutions allow for cooperation, actually getting the AI systems to converge on these solutions through training is difﬁcult. In this paper we empirically evaluate how reinforcement learning agents can be encouraged to cooperate (without opening themselves up to exploitation) by selecting appropriate hyperparameters and environmental perceptions for the agent. Our results in the multi-agent scenario indicate that in hyperparameter-space there are isolated “oases” of mutual cooperation, and small changes in these hyperparameters can lead to sharp drops into non-cooperative behaviour.
DA  - 2022///
PY  - 2022
DP  - Zotero
LA  - en
L1  - https://ceur-ws.org/Vol-3087/paper_24.pdf
ER  - 

TY  - JOUR
TI  - ‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence
AU  - Liu, Hin-Yan
AU  - Maas, Matthijs M.
T2  - Futures
AB  - Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.
DA  - 2021/02/01/
PY  - 2021
DO  - 10.1016/j.futures.2020.102672
DP  - ScienceDirect
VL  - 126
SP  - 102672
J2  - Futures
SN  - 0016-3287
ST  - ‘Solving for X?
UR  - https://www.sciencedirect.com/science/article/pii/S0016328720301634
Y2  - 2024/07/11/21:36:32
KW  - Artificial intelligence
KW  - Governance disruptors
KW  - Governance goldilocks zone
KW  - Governance puzzles
KW  - Macrostrategic trajectories & destinations
KW  - Problem-finding
ER  - 

TY  - GEN
TI  - Safeguarding the safeguards: How best to promote AI alignment in the public interest
AU  - Guest, Oliver
AU  - Aird, Michael
AU  - hÉigeartaigh, Seán Ó
AB  - AI alignment work is important from both a commercial and a safety lens. With this paper, we aim to help actors who support alignment efforts to make these efforts as effective as possible, and to avoid potential adverse effects. We begin by suggesting that institutions that are trying to act in the public interest (such as governments) should aim to support specifically alignment work that reduces accident or misuse risks. We then describe four problems which might cause alignment efforts to be counterproductive, increasing large-scale AI risks. We suggest mitigations for each problem. Finally, we make a broader recommendation that institutions trying to act in the public interest should think systematically about how to make their alignment efforts as effective, and as likely to be beneficial, as possible.
DA  - 2023/12/15/
PY  - 2023
DO  - 10.48550/arXiv.2312.08039
DP  - arXiv.org
PB  - arXiv
ST  - Safeguarding the safeguards
UR  - http://arxiv.org/abs/2312.08039
Y2  - 2024/07/11/21:37:08
L1  - https://arxiv.org/pdf/2312.08039.pdf
L2  - https://arxiv.org/abs/2312.08039
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild
AU  - Casares, Pablo Antonio Moreno
AU  - Loe, Bao Sheng
AU  - Burden, John
AU  - hEigeartaigh, Sean
AU  - Hernández-Orallo, José
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - The new generation of language models is reported to solve some extraordinary tasks the models were never trained for specifically, in few-shot or zero-shot settings. However, these reports usually cherry-pick the tasks, use the best prompts, and unwrap or extract the solutions leniently even if they are followed by nonsensical text. In sum, they are specialised results for one domain, a particular way of using the models and interpreting the results. In this paper, we present a novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters --- in the wild. For a useful and safe interaction in these increasingly more common conditions, we need to understand when the model fails because of a lack of capability or a misunderstanding of the user's intents. Our results indicate that language models such as GPT-3 have limited understanding of the human command; far from becoming general-purpose systems in the wild.
DA  - 2022/06/28/
PY  - 2022
DO  - 10.1609/aaai.v36i5.20466
DP  - ojs.aaai.org
VL  - 36
IS  - 5
SP  - 5295
EP  - 5303
LA  - en
SN  - 2374-3468
ST  - How General-Purpose Is a Language Model?
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/20466
Y2  - 2024/07/11/21:37:51
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/20466/20225
KW  - Speech & Natural Language Processing (SNLP)
ER  - 

TY  - GEN
TI  - Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims
AU  - Brundage, Miles
AU  - Avin, Shahar
AU  - Wang, Jasmine
AU  - Belfield, Haydn
AU  - Krueger, Gretchen
AU  - Hadfield, Gillian
AU  - Khlaaf, Heidy
AU  - Yang, Jingying
AU  - Toner, Helen
AU  - Fong, Ruth
AU  - Maharaj, Tegan
AU  - Koh, Pang Wei
AU  - Hooker, Sara
AU  - Leung, Jade
AU  - Trask, Andrew
AU  - Bluemke, Emma
AU  - Lebensold, Jonathan
AU  - O'Keefe, Cullen
AU  - Koren, Mark
AU  - Ryffel, Théo
AU  - Rubinovitz, J. B.
AU  - Besiroglu, Tamay
AU  - Carugati, Federica
AU  - Clark, Jack
AU  - Eckersley, Peter
AU  - de Haas, Sarah
AU  - Johnson, Maritza
AU  - Laurie, Ben
AU  - Ingerman, Alex
AU  - Krawczuk, Igor
AU  - Askell, Amanda
AU  - Cammarota, Rosario
AU  - Lohn, Andrew
AU  - Krueger, David
AU  - Stix, Charlotte
AU  - Henderson, Peter
AU  - Graham, Logan
AU  - Prunkl, Carina
AU  - Martin, Bianca
AU  - Seger, Elizabeth
AU  - Zilberman, Noa
AU  - hÉigeartaigh, Seán Ó
AU  - Kroeger, Frens
AU  - Sastry, Girish
AU  - Kagan, Rebecca
AU  - Weller, Adrian
AU  - Tse, Brian
AU  - Barnes, Elizabeth
AU  - Dafoe, Allan
AU  - Scharre, Paul
AU  - Herbert-Voss, Ariel
AU  - Rasser, Martijn
AU  - Sodhani, Shagun
AU  - Flynn, Carrick
AU  - Gilbert, Thomas Krendl
AU  - Dyer, Lisa
AU  - Khan, Saif
AU  - Bengio, Yoshua
AU  - Anderljung, Markus
AB  - With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.
DA  - 2020/04/20/
PY  - 2020
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Toward Trustworthy AI Development
UR  - http://arxiv.org/abs/2004.07213
Y2  - 2024/07/11/21:41:13
L1  - https://arxiv.org/pdf/2004.07213
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation
AU  - Brundage, Miles
AU  - Avin, Shahar
AU  - Clark, Jack
AU  - Toner, Helen
AU  - Eckersley, Peter
AU  - Garfinkel, Ben
AU  - Dafoe, Allan
AU  - Scharre, Paul
AU  - Zeitzoff, Thomas
AU  - Filar, Bobby
AU  - Anderson, Hyrum
AU  - Roff, Heather
AU  - Allen, Gregory C.
AU  - Steinhardt, Jacob
AU  - Flynn, Carrick
AU  - hÉigeartaigh, Seán Ó
AU  - Beard, Simon
AU  - Belfield, Haydn
AU  - Farquhar, Sebastian
AU  - Lyle, Clare
AU  - Crootof, Rebecca
AU  - Evans, Owain
AU  - Page, Michael
AU  - Bryson, Joanna
AU  - Yampolskiy, Roman
AU  - Amodei, Dario
AB  - This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.
DA  - 2018/02/20/
PY  - 2018
DO  - 10.48550/arXiv.1802.07228
DP  - arXiv.org
PB  - arXiv
ST  - The Malicious Use of Artificial Intelligence
UR  - http://arxiv.org/abs/1802.07228
Y2  - 2024/07/11/21:41:27
L1  - https://arxiv.org/pdf/1802.07228.pdf
L2  - https://arxiv.org/abs/1802.07228
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - JOUR
TI  - Bridging near- and long-term concerns about AI
AU  - Cave, Stephen
AU  - ÓhÉigeartaigh, Seán S.
T2  - Nature Machine Intelligence
AB  - Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.
DA  - 2019/01//
PY  - 2019
DO  - 10.1038/s42256-018-0003-2
DP  - www.nature.com
VL  - 1
IS  - 1
SP  - 5
EP  - 6
J2  - Nat Mach Intell
LA  - en
SN  - 2522-5839
UR  - https://www.nature.com/articles/s42256-018-0003-2
Y2  - 2024/07/11/21:41:37
KW  - Ethics
KW  - Science
KW  - technology and society
KW  - Policy
KW  - Electrical and electronic engineering
ER  - 

TY  - JOUR
TI  - Filling gaps in trustworthy development of AI
AU  - Avin, Shahar
AU  - Belfield, Haydn
AU  - Brundage, Miles
AU  - Krueger, Gretchen
AU  - Wang, Jasmine
AU  - Weller, Adrian
AU  - Anderljung, Markus
AU  - Krawczuk, Igor
AU  - Krueger, David
AU  - Lebensold, Jonathan
AU  - Maharaj, Tegan
AU  - Zilberman, Noa
T2  - Science
DA  - 2021/12/10/
PY  - 2021
DO  - 10.1126/science.abi7176
DP  - science.org (Atypon)
VL  - 374
IS  - 6573
SP  - 1327
EP  - 1329
UR  - https://www.science.org/doi/10.1126/science.abi7176
Y2  - 2024/07/11/21:41:57
L1  - https://arxiv.org/pdf/2112.07773
ER  - 

TY  - GEN
TI  - Predictable Artificial Intelligence
AU  - Zhou, Lexin
AU  - Moreno-Casares, Pablo A.
AU  - Martínez-Plumed, Fernando
AU  - Burden, John
AU  - Burnell, Ryan
AU  - Cheke, Lucy
AU  - Ferri, Cèsar
AU  - Marcoci, Alexandru
AU  - Mehrbakhsh, Behzad
AU  - Moros-Daval, Yael
AU  - hÉigeartaigh, Seán Ó
AU  - Rutar, Danaja
AU  - Schellaert, Wout
AU  - Voudouris, Konstantinos
AU  - Hernández-Orallo, José
AB  - We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
DA  - 2023/10/09/
PY  - 2023
DO  - 10.48550/arXiv.2310.06167
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.06167
Y2  - 2024/07/11/21:42:09
L1  - https://arxiv.org/pdf/2310.06167.pdf
L2  - https://arxiv.org/abs/2310.06167
KW  - Computer Science - Artificial Intelligence
KW  - ACM-class: I.2
ER  - 

TY  - JOUR
TI  - AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues
AU  - Hernandez-Orallo, Jose
AU  - Martınez-Plumed, Fernando
AU  - Avin, Shahar
AU  - Whittlestone, Jess
T2  - Santiago de Compostela
AB  - AI safety often analyses a risk or safety issue, such as interruptibility, under a particular AI paradigm, such as reinforcement learning. But what is an AI paradigm and how does it affect the understanding and implications of the safety issue? Is AI safety research covering the most representative paradigms and the right combinations of paradigms with safety issues? Will current research directions in AI safety be able to anticipate more capable and powerful systems yet to come? In this paper we analyse these questions, introducing a distinction between two types of paradigms in AI: artefacts and techniques. We then use experimental data of research and media documents from AI Topics, an ofﬁcial publication of the AAAI, to examine how safety research is distributed across artefacts and techniques. We observe that AI safety research is not sufﬁciently anticipatory, and is heavily weighted towards certain research paradigms. We identify a need for AI safety to be more explicit about the artefacts and techniques for which a particular issue may be applicable, in order to identify gaps and cover a broader range of issues.
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - http://ecai2020.eu/papers/1364_paper.pdf
ER  - 

TY  - GEN
TI  - On the Robustness of Safe Reinforcement Learning under Observational Perturbations
AU  - Liu, Zuxin
AU  - Guo, Zijian
AU  - Cen, Zhepeng
AU  - Zhang, Huan
AU  - Tan, Jie
AU  - Li, Bo
AU  - Zhao, Ding
AB  - Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \url{https://github.com/liuzuxin/safe-rl-robustness}
DA  - 2023/03/01/
PY  - 2023
DO  - 10.48550/arXiv.2205.14691
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.14691
Y2  - 2024/07/14/21:11:58
L1  - https://arxiv.org/pdf/2205.14691.pdf
L2  - https://arxiv.org/abs/2205.14691
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Constraining Representations Yields Models That Know What They Don't Know
AU  - Monteiro, Joao
AU  - Rodriguez, Pau
AU  - Noel, Pierre-Andre
AU  - Laradji, Issam
AU  - Vazquez, David
AB  - A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code - and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence score, besides the default unTAC'ed prediction head's. In the add-on case, the original neural network's inference head is completely unaffected (so its accuracy remains the same) but we now have the option to use TAC's own confidence and prediction when determining which course of action to take in an hypothetical production workflow. In particular, we show that TAC strictly improves the value derived from models allowed to reject/defer. We provide further empirical evidence that TAC works well on multiple types of architectures and data modalities and that it is at least as good as state-of-the-art alternative confidence scores derived from existing models.
DA  - 2023/04/19/
PY  - 2023
DO  - 10.48550/arXiv.2208.14488
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2208.14488
Y2  - 2024/07/14/21:11:42
L1  - https://arxiv.org/pdf/2208.14488.pdf
L2  - https://arxiv.org/abs/2208.14488
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning
AU  - Wiles, Olivia
AU  - Albuquerque, Isabela
AU  - Gowal, Sven
AB  - Automatically discovering failures in vision models under real-world settings remains an open challenge. This work demonstrates how off-the-shelf, large-scale, image-to-text and text-to-image models, trained on vast amounts of data, can be leveraged to automatically find such failures. In essence, a conditional text-to-image generative model is used to generate large amounts of synthetic, yet realistic, inputs given a ground-truth label. Misclassified inputs are clustered and a captioning model is used to describe each cluster. Each cluster's description is used in turn to generate more inputs and assess whether specific clusters induce more failures than expected. We use this pipeline to demonstrate that we can effectively interrogate classifiers trained on ImageNet to find specific failure cases and discover spurious correlations. We also show that we can scale the approach to generate adversarial datasets targeting specific classifier architectures. This work serves as a proof-of-concept demonstrating the utility of large-scale generative models to automatically discover bugs in vision models in an open-ended manner. We also describe a number of limitations and pitfalls related to this approach.
DA  - 2023/05/11/
PY  - 2023
DO  - 10.48550/arXiv.2208.08831
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2208.08831
Y2  - 2024/07/14/21:11:04
L1  - https://arxiv.org/pdf/2208.08831.pdf
L2  - https://arxiv.org/abs/2208.08831
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Certified defences hurt generalisation
AU  - Bartolomeis, Piersilvio De
AU  - Clarysse, Jacob
AU  - Yang, Fanny
AU  - Sanyal, Amartya
T2  - I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification
AB  - In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) strong alignment between the adversarial perturbations and the "signal" direction. We provide a combination of theoretical and experimental evidence to support these hypotheses.
DA  - 2022/12/06/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=h1j5I0WVxoI
Y2  - 2024/07/14/21:10:41
L1  - https://openreview.net/pdf?id=h1j5I0WVxoI
ER  - 

TY  - GEN
TI  - CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition
AU  - Ahn, Sumyeong
AU  - Ko, Jongwoo
AU  - Yun, Se-Young
AB  - Class imbalance problems frequently occur in real-world tasks, and conventional deep learning algorithms are well known for performance degradation on imbalanced training datasets. To mitigate this problem, many approaches have aimed to balance among given classes by re-weighting or re-sampling training samples. These re-balancing methods increase the impact of minority classes and reduce the influence of majority classes on the output of models. However, the extracted representations may be of poor quality owing to the limited number of minority samples. To handle this restriction, several methods have been developed that increase the representations of minority samples by leveraging the features of the majority samples. Despite extensive recent studies, no deep analysis has been conducted on determination of classes to be augmented and strength of augmentation has been conducted. In this study, we first investigate the correlation between the degree of augmentation and class-wise performance, and find that the proper degree of augmentation must be allocated for each class to mitigate class imbalance problems. Motivated by this finding, we propose a simple and efficient novel curriculum, which is designed to find the appropriate per-class strength of data augmentation, called CUDA: CUrriculum of Data Augmentation for long-tailed recognition. CUDA can simply be integrated into existing long-tailed recognition methods. We present the results of experiments showing that CUDA effectively achieves better generalization performance compared to the state-of-the-art method on various imbalanced datasets such as CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018.
DA  - 2023/02/10/
PY  - 2023
DO  - 10.48550/arXiv.2302.05499
DP  - arXiv.org
PB  - arXiv
ST  - CUDA
UR  - http://arxiv.org/abs/2302.05499
Y2  - 2024/07/14/21:08:06
L1  - https://arxiv.org/pdf/2302.05499.pdf
L2  - https://arxiv.org/abs/2302.05499
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Measuring Reliability of Large Language Models through Semantic Consistency
AU  - Raj, Harsh
AU  - Rosati, Domenic
AU  - Majumdar, Subhabrata
AB  - While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLMs on paraphrased versions of questions in the TruthfulQA dataset, we find that our proposed metrics are considerably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.
DA  - 2023/04/11/
PY  - 2023
DO  - 10.48550/arXiv.2211.05853
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.05853
Y2  - 2024/07/14/21:07:52
L1  - https://arxiv.org/pdf/2211.05853.pdf
L2  - https://arxiv.org/abs/2211.05853
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Data Poisoning Attack Aiming the Vulnerability of Continual Learning
AU  - Han, Gyojin
AU  - Choi, Jaehyun
AU  - Hong, Hyeong Gwon
AU  - Kim, Junmo
AB  - Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importance of developing continual learning models that are robust to adversarial attacks.
DA  - 2023/07/03/
PY  - 2023
DO  - 10.48550/arXiv.2211.15875
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.15875
Y2  - 2024/07/14/21:07:36
L1  - https://arxiv.org/pdf/2211.15875.pdf
L2  - https://arxiv.org/abs/2211.15875
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - The Reward Hypothesis is False
AU  - Skalse, Joar Max Viktor
AU  - Abate, Alessandro
T2  - NeurIPS ML Safety Workshop
AB  - The \emph{reward hypothesis} is the hypothesis that \enquote{all of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal}\citep{sutton2018reinforcement}. In this paper, we will argue that this hypothesis is false. We will look at three natural classes of reinforcement learning tasks (multi-objective reinforcement learning, risk-averse reinforcement learning, and modal reinforcement learning), and then prove mathematically that these tasks cannot be expressed using any scalar, Markovian reward function. We thus disprove the reward hypothesis by providing many examples of tasks which are both natural and intuitive to describe, but which are nonetheless impossible to express using reward functions. In the process, we provide necessary and sufficient conditions for when a multi-objective reinforcement learning problem can be reduced to ordinary, scalar reward reinforcement learning. We also call attention to a new class of reinforcement learning problems (namely those we call \enquote{modal} problems), which have so far not been given any systematic treatment in the reinforcement learning literature.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=5l1NgpzAfH
Y2  - 2024/07/14/21:07:13
L1  - https://openreview.net/pdf?id=5l1NgpzAfH
ER  - 

TY  - GEN
TI  - Settling the Reward Hypothesis
AU  - Bowling, Michael
AU  - Martin, John D.
AU  - Abel, David
AU  - Dabney, Will
AB  - The reward hypothesis posits that, “all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).” We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.
DA  - 2023/09/16/
PY  - 2023
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2212.10420
Y2  - 2024/07/14/21:07:01
L1  - https://arxiv.org/pdf/2212.10420
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Mathematics - Statistics Theory
ER  - 

TY  - CONF
TI  - Tracking the Risk of Machine Learning Systems with Partial Monitoring
AU  - Heuillet, Maxime
AU  - Durand, Audrey
T2  - NeurIPS ML Safety Workshop
AB  - Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=zzpu07KXNX
Y2  - 2024/07/14/21:06:25
L1  - https://openreview.net/pdf?id=zzpu07KXNX
ER  - 

TY  - GEN
TI  - Red-Teaming the Stable Diffusion Safety Filter
AU  - Rando, Javier
AU  - Paleka, Daniel
AU  - Lindner, David
AU  - Heim, Lennart
AU  - Tramèr, Florian
AB  - Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community.
DA  - 2022/11/10/
PY  - 2022
DO  - 10.48550/arXiv.2210.04610
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.04610
Y2  - 2024/07/14/21:06:15
L1  - https://arxiv.org/pdf/2210.04610.pdf
L2  - https://arxiv.org/abs/2210.04610
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Misspecification in Inverse Reinforcement Learning
AU  - Skalse, Joar
AU  - Abate, Alessandro
T2  - AAAI Conference on Artificial Intelligence, 2023
AB  - The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\pi$. To do this, we need a model of how $\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.
DA  - 2023/03/24/
PY  - 2023
DO  - 10.48550/arXiv.2212.03201
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.03201
Y2  - 2024/07/14/21:05:17
L1  - https://arxiv.org/pdf/2212.03201.pdf
L2  - https://arxiv.org/abs/2212.03201
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Multiple Remote Adversarial Patches: Generating Patches based on Diffusion Models for Object Detection using CNNs
AU  - Oonishi, Kento
AU  - Nakai, Tsunato
AU  - Suzuki, Daisuke
T2  - NeurIPS ML Safety Workshop
AB  - Adversarial patches can fool object detection systems, which poses a severe threat to machine learning models. Many researchers have focused on strong adversarial patches. Remote adversarial patches, placed outside the target objects, are candidates of strong adversarial patches. This study gives a concrete model of adversarial patches on convolutional neural networks (CNNs), namely diffusion model. Our diffusion model shows that multiple remote adversarial patches pose severe threats on YOLOv2 CNN. Our experiment also demonstrates that two remote adversarial patches reduce the average existence probability to 12.81%, whereas Saha et al.'s original single adversarial patch reduced the average existence probability to 50.95%. Moreover, we generate adversarial patches on SSD architecture. In SSD architecture, two remote adversarial patches also significantly reduce the average existence probability from 24.52% to 6.12%. By the above results, this paper provides a framework for analyzing the effect of adversarial patch attacks.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Multiple Remote Adversarial Patches
UR  - https://openreview.net/forum?id=netFyi04pB
Y2  - 2024/07/14/21:04:43
L1  - https://openreview.net/pdf?id=netFyi04pB
ER  - 

TY  - CONF
TI  - Probabilistically Robust PAC Learning
AU  - Raman, Vinod
AU  - Tewari, Ambuj
AU  - Subedi, Unique
T2  - NeurIPS ML Safety Workshop
AB  - Recently, Robey et al. propose a notion of probabilistic robustness, which, at a high-level, requires a classifier to be robust to most but not all perturbations. They show that for certain hypothesis classes where proper learning under worst-case robustness is \textit{not} possible, proper learning under probabilistic robustness \textit{is} possible with sample complexity exponentially smaller than in the worst-case robustness setting. This motivates the question of whether proper learning under probabilistic robustness is always possible. In this paper, we show that this is \textit{not} the case. We exhibit examples of hypothesis classes $\mathcal{H}$ with finite VC dimension that are \textit{not} probabilistically robustly PAC learnable with \textit{any} proper learning rule.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=g_BjLtjtCwT
Y2  - 2024/07/14/21:04:28
L1  - https://openreview.net/pdf?id=g_BjLtjtCwT
ER  - 

TY  - CONF
TI  - Interpolating Compressed Parameter Subspaces
AU  - Datta, Siddhartha
AU  - Shadbolt, Nigel
T2  - Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems
AB  - Though distribution shifts have caused growing concern for machine learning scalability, solutions tend to specialize towards a specific type of distribution shift. We learn that constructing a Compressed Parameter Subspaces (CPS), a geometric structure representing distance-regularized parameters mapped to a set of train-time distributions, can maximize average accuracy over a broad range of distribution shifts concurrently. We show sampling parameters within a CPS can mitigate backdoor, adversarial, permutation, stylization and rotation perturbations. Regularizing a hypernetwork with CPS can also reduce task forgetting.
DA  - 2022/10/21/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Zb9m4idh8I
Y2  - 2024/07/14/21:04:06
L1  - https://openreview.net/pdf?id=Zb9m4idh8I
ER  - 

TY  - CONF
TI  - Reflection Mechanisms as an Alignment Target: A Survey
AU  - Hobbhahn, Marius
AU  - Landgrebe, Eric
AU  - Barnes, Elizabeth
T2  - NeurIPS ML Safety Workshop
AB  - We used Positly to survey roughly 1000 US-based workers about their attitudes on moral questions, conditions under which they would change their moral beliefs, and approval towards different mechanisms for society to resolve moral disagreements. Unsurprisingly, our sample strongly disagreed on contentious object-level moral questions such as whether abortion is immoral. In addition, a substantial fraction of people reported that these beliefs wouldn’t change even if they came to different beliefs about factors we view as morally relevant, such as whether the fetus was conscious in the case of abortion. However, people were generally favorable to the idea of society deciding policies by some means of reflection - such as democracy, a debate between well-intentioned experts, or thinking for a long time. This agreement improves in a hypothetical well-intentioned future society. Surprisingly, favorability remained even when we stipulate that the reflection procedure came to the opposite of the respondents' view on polarizing topics like abortion. This provides evidence that people may support aligning AIs to a reflection procedure rather than individual beliefs. We tested our findings on a second adversarial survey that actively tries to disprove the finding from the first study. We find that our core results are robust in standard settings but are weakened when the questions are constructed adversarially (e.g. when decisions are made by people who have the opposite of the respondents' moral or political beliefs).
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Reflection Mechanisms as an Alignment Target
UR  - https://openreview.net/forum?id=4eMzKmZ6xW
Y2  - 2024/07/14/21:03:38
L1  - https://openreview.net/pdf?id=4eMzKmZ6xW
ER  - 

TY  - CONF
TI  - Investigating causal understanding in LLMs
AU  - Hobbhahn, Marius
AU  - Lieberum, Tom
AU  - Seiler, David
T2  - NeurIPS 2022 Workshop on Causality for Real-world Impact
AB  - We investigate the quality of causal world models of LLMs in very simple settings. We test whether LLMs can identify cause and effect in natural language settings (taken from BigBench) such as “My car got dirty. I washed the car. Question: Which sentence is the cause of the other?” and in multiple other toy settings. We probe the LLM's world model by changing the presentation of the prompt while keeping the meaning constant, e.g. by changing the order of the sentences or asking the opposite question. Additionally, we test if the model can be “tricked” into giving wrong answers when we present the shot in a different pattern than the prompt. We have three findings. Firstly, larger models yield better results. Secondly, k-shot outperforms one-shot and one-shot outperforms zero-shot in standard conditions. Thirdly, LLMs perform worse in conditions where form and content differ. We conclude that the form of the presentation matters for LLM predictions or, in other words, that LLMs don't solely base their predictions on content. Finally, we detail some of the implications this research has on AI safety.
DA  - 2022/10/21/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=st6jtGdW8Ke
Y2  - 2024/07/14/21:03:11
L1  - https://openreview.net/pdf?id=st6jtGdW8Ke
ER  - 

TY  - CONF
TI  - Measuring Robustness with Black-Box Adversarial Attack using Reinforcement Learning
AU  - Sarkar, Soumyendu
AU  - Mousavi, Sajad
AU  - Babu, Ashwin Ramesh
AU  - Gundecha, Vineet
AU  - Ghorbanpour, Sahand
AU  - Shmakov, Alexander K.
T2  - NeurIPS ML Safety Workshop
AB  - A measure of robustness against naturally occurring distortions is key to the trustworthiness, safety, and success of machine learning models on deployment. We investigate an adversarial black-box attack that adds minimum Gaussian noise distortions to input images to make deep learning models misclassify. We used a Reinforcement Learning (RL) agent as a smart hacker to explore the input images to add minimum distortions to the most sensitive regions to induce misclassification. The agent employs a smart policy also to remove noises introduced earlier, which has less impact on the trained model at a given state. This novel approach is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to faster and optimal convergence. Also, this adversarial attack method effectively measures the robustness of image classification models with the misclassification inducing minimum L2 distortion of Gaussian noise similar to many naturally occurring distortions. Furthermore, the proposed black-box L2 adversarial attack tool beats state-of-the-art competitors in terms of the average number of queries by a significant margin with a 100\% success rate while maintaining a very competitive L2 score, despite limiting distortions to Gaussian noise. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31% better than the state-of-the-art "Square-Attack" approach while maintaining a competitive L2. Demo: https://tinyurl.com/pzrca5fj
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Lj8fj0ECPv
Y2  - 2024/07/14/21:02:51
L1  - https://openreview.net/pdf?id=Lj8fj0ECPv
ER  - 

TY  - GEN
TI  - Formalizing the Problem of Side Effect Regularization
AU  - Turner, Alexander Matt
AU  - Saxena, Aseem
AU  - Tadepalli, Prasad
AB  - AI objectives are often hard to specify properly. Some approaches tackle this problem by regularizing the AI's side effects: Agents must weigh off "how much of a mess they make" with an imperfectly specified proxy objective. We propose a formal criterion for side effect regularization via the assistance game framework. In these games, the agent solves a partially observable Markov decision process (POMDP) representing its uncertainty about the objective function it should optimize. We consider the setting where the true objective is revealed to the agent at a later time step. We show that this POMDP is solved by trading off the proxy reward with the agent's ability to achieve a range of future tasks. We empirically demonstrate the reasonableness of our problem formalization via ground-truth evaluation in two gridworld environments.
DA  - 2022/11/08/
PY  - 2022
DO  - 10.48550/arXiv.2206.11812
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.11812
Y2  - 2024/07/14/21:02:37
L1  - https://arxiv.org/pdf/2206.11812.pdf
L2  - https://arxiv.org/abs/2206.11812
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Convergence of Expected Utility for Universal AI
AU  - de Blanc, Peter
AB  - We consider a sequence of repeated interactions between an agent and an environment. Uncertainty about the environment is captured by a probability distribution over a space of hypotheses, which includes all computable functions. Given a utility function, we can evaluate the expected utility of any computational policy for interaction with the environment. After making some plausible assumptions (and maybe one not-so-plausible assumption), we show that if the utility function is unbounded, then the expected utility of any policy is undefined.
DA  - 2009/12/02/
PY  - 2009
DO  - 10.48550/arXiv.0907.5598
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/0907.5598
Y2  - 2024/07/14/21:01:16
L1  - https://arxiv.org/pdf/0907.5598.pdf
L2  - https://arxiv.org/abs/0907.5598
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Complex Value Systems in Friendly AI
AU  - Yudkowsky, Eliezer
A2  - Schmidhuber, Jürgen
A2  - Thórisson, Kristinn R.
A2  - Looks, Moshe
AB  - A common reaction to first encountering the problem statement of Friendly AI (”Ensure that the creation of a generally intelligent, self-improving, eventually superintelligent system realizes a positive outcome”) is to propose a simple design which allegedly suffices; or to reject the problem by replying that ”constraining” our creations is undesirable or unnecessary. This paper briefly presents some of the reasoning which suggests that Friendly AI is solvable, but not simply or trivially so, and that a wise strategy would be to invoke detailed learning of and inheritance from human values as a basis for further normalization and reflection.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-22887-2_48
DP  - Springer Link
SP  - 388
EP  - 393
LA  - en
PB  - Springer
SN  - 978-3-642-22887-2
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-22887-2_48.pdf
KW  - Friendly AI
KW  - anthropomorphism
KW  - machine ethics
ER  - 

TY  - CONF
TI  - Learning What to Value
AU  - Dewey, Daniel
A2  - Schmidhuber, Jürgen
A2  - Thórisson, Kristinn R.
A2  - Looks, Moshe
AB  - I. J. Good’s intelligence explosion theory predicts that ultraintelligent agents will undergo a process of repeated self-improvement; in the wake of such an event, how well our values are fulfilled would depend on the goals of these ultraintelligent agents. With this motivation, we examine ultraintelligent reinforcement learning agents. Reinforcement learning can only be used in the real world to define agents whose goal is to maximize expected rewards, and since this goal does not match with human goals, AGIs based on reinforcement learning will often work at cross-purposes to us. To solve this problem, we define value learners, agents that can be designed to learn and maximize any initially unknown utility function so long as we provide them with an idea of what constitutes evidence about that utility function.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-22887-2_35
DP  - Springer Link
SP  - 309
EP  - 314
LA  - en
PB  - Springer
SN  - 978-3-642-22887-2
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-22887-2_35.pdf
KW  - Reinforcement Learning
KW  - Agent Function
KW  - Expected Utility
KW  - Full Search
KW  - Utility Function
ER  - 

TY  - GEN
TI  - Ontological Crises in Artificial Agents' Value Systems
AU  - de Blanc, Peter
AB  - Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.
DA  - 2011/05/19/
PY  - 2011
DO  - 10.48550/arXiv.1105.3821
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1105.3821
Y2  - 2024/07/14/21:00:21
L1  - https://arxiv.org/pdf/1105.3821.pdf
L2  - https://arxiv.org/abs/1105.3821
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Decision Support for Safe AI Design
AU  - Hibbard, Bill
A2  - Bach, Joscha
A2  - Goertzel, Ben
A2  - Iklé, Matthew
AB  - There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter’s agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2012///
PY  - 2012
DO  - 10.1007/978-3-642-35506-6_13
DP  - Springer Link
SP  - 117
EP  - 125
LA  - en
PB  - Springer
SN  - 978-3-642-35506-6
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-35506-6_13.pdf
KW  - agent architecture
KW  - agent motivation
KW  - rational agent
ER  - 

TY  - CONF
TI  - Avoiding Unintended AI Behaviors
AU  - Hibbard, Bill
A2  - Bach, Joscha
A2  - Goertzel, Ben
A2  - Iklé, Matthew
AB  - Artificial intelligence (AI) systems too complex for predefined environment models and actions will need to learn environment models and to choose actions that optimize some criteria. Several authors have described mechanisms by which such complex systems may behave in ways not intended in their designs. This paper describes ways to avoid such unintended behavior. For hypothesized powerful AI systems that may pose a threat to humans, this paper proposes a two-stage agent architecture that avoids some known types of unintended behavior. For the first stage of the architecture this paper shows that the most probable finite stochastic program to model a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2012///
PY  - 2012
DO  - 10.1007/978-3-642-35506-6_12
DP  - Springer Link
SP  - 107
EP  - 116
LA  - en
PB  - Springer
SN  - 978-3-642-35506-6
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-35506-6_12.pdf
KW  - agent architecture
KW  - agent motivation
KW  - rational agent
ER  - 

TY  - CHAP
TI  - Artificial General Intelligence and the Human Mental Model
AU  - Yampolskiy, Roman V.
AU  - Fox, Joshua
T2  - Singularity Hypotheses: A Scientific and Philosophical Assessment
A2  - Eden, Amnon H.
A2  - Moor, James H.
A2  - Søraker, Johnny H.
A2  - Steinhart, Eric
AB  - When the first artificial general intelligences are built, they may improve themselves to far-above-human levels. Speculations about such future entities are already affected by anthropomorphic bias, which leads to erroneous analogies with human minds. In this chapter, we apply a goal-oriented understanding of intelligence to show that humanity occupies only a tiny portion of the design space of possible minds. This space is much larger than what we are familiar with from the human example; and the mental architectures and goals of future superintelligences need not have most of the properties of human minds. A new approach to cognitive science and philosophy of mind, one not centered on the human example, is needed to help us understand the challenges which we will face when a power greater than us emerges.
CY  - Berlin, Heidelberg
DA  - 2012///
PY  - 2012
DP  - Springer Link
SP  - 129
EP  - 145
LA  - en
PB  - Springer
SN  - 978-3-642-32560-1
UR  - https://doi.org/10.1007/978-3-642-32560-1_7
Y2  - 2024/07/14/20:59:44
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-32560-1_7.pdf
KW  - General Intelligence
KW  - Goal System
KW  - Human Mind
KW  - Human Property
KW  - Optimization Power
ER  - 

TY  - JOUR
TI  - Safety Engineering for Artificial General Intelligence
AU  - Yampolskiy, Roman
AU  - Fox, Joshua
T2  - Topoi
AB  - Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence and robotics communities. We will argue that attempts to attribute moral agency and assign rights to all intelligent machines are misguided, whether applied to infrahuman or superhuman AIs, as are proposals to limit the negative effects of AIs by constraining their behavior. As an alternative, we propose a new science of safety engineering for intelligent artificial agents based on maximizing for what humans value. In particular, we challenge the scientific community to develop intelligent systems that have human-friendly values that they provably retain, even under recursive self-improvement.
DA  - 2013/10/01/
PY  - 2013
DO  - 10.1007/s11245-012-9128-9
DP  - Springer Link
VL  - 32
IS  - 2
SP  - 217
EP  - 226
J2  - Topoi
LA  - en
SN  - 1572-8749
UR  - https://doi.org/10.1007/s11245-012-9128-9
Y2  - 2024/07/14/20:59:10
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11245-012-9128-9.pdf
KW  - Machine ethics
KW  - AI safety
KW  - AI confinement
KW  - Friendly artificial intelligence
KW  - Intelligence explosion
KW  - Robot rights
ER  - 

TY  - JOUR
TI  - Racing to the precipice: a model of artificial intelligence development
AU  - Armstrong, Stuart
AU  - Bostrom, Nick
AU  - Shulman, Carl
T2  - AI & SOCIETY
AB  - This paper presents a simple model of an AI (artificial intelligence) arms race, where several development teams race to build the first AI. Under the assumption that the first AI will be very powerful and transformative, each team is incentivised to finish first—by skimping on safety precautions if need be. This paper presents the Nash equilibrium of this process, where each team takes the correct amount of safety precautions in the arms race. Having extra development teams and extra enmity between teams can increase the danger of an AI disaster, especially if risk-taking is more important than skill in developing the AI. Surprisingly, information also increases the risks: the more teams know about each others’ capabilities (and about their own), the more the danger increases. Should these results persist in more realistic models and analysis, it points the way to methods of increasing the chance of the safe development of AI.
DA  - 2016/05/01/
PY  - 2016
DO  - 10.1007/s00146-015-0590-y
DP  - Springer Link
VL  - 31
IS  - 2
SP  - 201
EP  - 206
J2  - AI & Soc
LA  - en
SN  - 1435-5655
ST  - Racing to the precipice
UR  - https://doi.org/10.1007/s00146-015-0590-y
Y2  - 2024/07/14/20:57:59
L1  - https://link.springer.com/content/pdf/10.1007%2Fs00146-015-0590-y.pdf
KW  - Artificial intelligence
KW  - AI
KW  - Risk
KW  - Model
KW  - Arms race
KW  - Coordination problem
ER  - 

TY  - CONF
TI  - Problems of Self-reference in Self-improving Space-Time Embedded Intelligence
AU  - Fallenstein, Benja
AU  - Soares, Nate
A2  - Goertzel, Ben
A2  - Orseau, Laurent
A2  - Snaider, Javier
AB  - By considering agents to be a part of their environment, Orseau and Ring’s space-time embedded intelligence [10] is a better fit to the real world than the traditional agent framework. However, a self-modifying AGI that sees future versions of itself as an ordinary part of the environment may run into problems of self-reference. We show that in one particular model based on formal logic, naive approaches either lead to incorrect reasoning that allows an agent to put off an important task forever (the procrastination paradox), or fail to allow the agent to justify even obviously safe rewrites (the Löbian obstacle). We argue that these problems have relevance beyond our particular formalism, and discuss partial solutions.
C1  - Cham
C3  - Artificial General Intelligence
DA  - 2014///
PY  - 2014
DO  - 10.1007/978-3-319-09274-4_3
DP  - Springer Link
SP  - 21
EP  - 32
LA  - en
PB  - Springer International Publishing
SN  - 978-3-319-09274-4
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-319-09274-4_3.pdf
KW  - Future Version
KW  - Incompleteness Theorem
KW  - Naive Approach
KW  - Partial Solution
KW  - Safe Action
ER  - 

TY  - JOUR
TI  - The errors, insights and lessons of famous AI predictions – and what they mean for the future
AU  - Armstrong, Stuart
AU  - Sotala, Kaj
AU  - Ó hÉigeartaigh, Seán S.
T2  - Journal of Experimental & Theoretical Artificial Intelligence
AB  - Predicting the development of artificial intelligence (AI) is a difficult project – but a vital one, according to some analysts. AI predictions are already abound: but are they reliable? This paper starts by proposing a decomposition schema for classifying them. Then it constructs a variety of theoretical tools for analysing, judging and improving them. These tools are demonstrated by careful analysis of five famous AI predictions: the initial Dartmouth conference, Dreyfus's criticism of AI, Searle's Chinese room paper, Kurzweil's predictions in the Age of Spiritual Machines, and Omohundro's ‘AI drives’ paper. These case studies illustrate several important principles, such as the general overconfidence of experts, the superiority of models over expert judgement and the need for greater uncertainty in all types of predictions. The general reliability of expert judgement in AI timeline predictions is shown to be poor, a result that fits in with previous studies of expert competence.
DA  - 2014/07/03/
PY  - 2014
DO  - 10.1080/0952813X.2014.895105
DP  - Taylor and Francis+NEJM
VL  - 26
IS  - 3
SP  - 317
EP  - 342
SN  - 0952-813X
UR  - https://doi.org/10.1080/0952813X.2014.895105
Y2  - 2024/07/14/20:48:08
L1  - https://www.tandfonline.com/doi/pdf/10.1080/0952813X.2014.895105
KW  - AI
KW  - bias
KW  - predictions
KW  - case studies
KW  - experts
KW  - expert judgement
ER  - 

TY  - GEN
TI  - Concept Learning for Safe Autonomous AI
AU  - Sotala, Kaj
AB  - Sophisticated autonomous AI may need to base its behavior on fuzzy concepts such as well-being or rights. These concepts cannot be given an explicit formal definition, but obtaining desired behavior still requires a way to instill the concepts in an AI system. To solve the problem, we review evidence suggesting that the human brain generates its concepts using a relatively limited set of rules and mechanisms. This suggests that it might be feasible to build AI systems that use similar criteria for generating their own concepts, and could thus learn similar concepts as humans do. Major challenges to this approach include the embodied nature of human thought, evolutionary vestiges in cognition, the social nature of concepts, and the need to compare conceptual representations between humans and AI systems.
CY  - Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop
DA  - 2015///
PY  - 2015
DP  - Zotero
LA  - en
L1  - https://cdn.aaai.org/ocs/ws/ws0075/10131-45901-1-PB.pdf
ER  - 

TY  - CHAP
TI  - Two Attempts to Formalize Counterpossible Reasoning in Deterministic Settings
AU  - Soares, Nate
AU  - Fallenstein, Benja
T2  - Artificial General Intelligence
A2  - Bieger, Jordi
A2  - Goertzel, Ben
A2  - Potapov, Alexey
AB  - This paper motivates the study of counterpossibles (logically impossible counterfactuals) as necessary for developing a decision theory suitable for generally intelligent agents embedded within their environments. We discuss two attempts to formalize a decision theory using counterpossibles, one based on graphical models and another based on proof search.
CY  - Cham
DA  - 2015///
PY  - 2015
DP  - DOI.org (Crossref)
VL  - 9205
SP  - 156
EP  - 165
LA  - en
PB  - Springer International Publishing
SN  - 978-3-319-21364-4 978-3-319-21365-1
UR  - http://link.springer.com/10.1007/978-3-319-21365-1_17
Y2  - 2024/07/14/20:45:57
L1  - https://intelligence.org/files/CounterpossibleReasoning.pdf
ER  - 

TY  - GEN
TI  - Formalizing Two Problems of Realistic World-Models
AU  - Soares, Nate
AB  - An intelligent agent embedded within the real world must reason about an environment which is larger than the agent, and learn how to achieve goals in that environment. We discuss attempts to formalize two problems: one of induction, where an agent must use sensory data to infer a universe which embeds (and computes) the agent, and one of interaction, where an agent must learn to achieve complex goals in the universe. We review related problems formalized by Solomonoﬀ and Hutter, and explore challenges that arise when attempting to formalize analogous problems in a setting where the agent is embedded within the environment.
DA  - 2015///
PY  - 2015
DP  - Zotero
LA  - en
L1  - https://intelligence.org/files/RealisticWorldModels.pdf
ER  - 

TY  - GEN
TI  - Toward Idealized Decision Theory
AU  - Soares, Nate
AU  - Fallenstein, Benja
AB  - This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.
DA  - 2015/07/07/
PY  - 2015
DO  - 10.48550/arXiv.1507.01986
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1507.01986
Y2  - 2024/07/14/20:45:00
L1  - https://arxiv.org/pdf/1507.01986.pdf
L2  - https://arxiv.org/abs/1507.01986
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Asymptotic Logical Uncertainty and The Benford Test
AU  - Garrabrant, Scott
AU  - Bhaskar, Siddharth
AU  - Demski, Abram
AU  - Garrabrant, Joanna
AU  - Koleszarik, George
AU  - Lloyd, Evan
AB  - We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs "true" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.
DA  - 2015/10/12/
PY  - 2015
DO  - 10.48550/arXiv.1510.03370
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1510.03370
Y2  - 2024/07/14/20:43:39
L1  - https://arxiv.org/pdf/1510.03370.pdf
L2  - https://arxiv.org/abs/1510.03370
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - F.4.1
ER  - 

TY  - GEN
TI  - Reflective Oracles: A Foundation for Classical Game Theory
AU  - Fallenstein, Benja
AU  - Taylor, Jessica
AU  - Christiano, Paul F.
AB  - Classical game theory treats players as special---a description of a game contains a full, explicit enumeration of all players---even though in the real world, "players" are no more fundamentally special than rocks or clouds. It isn't trivial to find a decision-theoretic foundation for game theory in which an agent's coplayers are a non-distinguished part of the agent's environment. Attempts to model both players and the environment as Turing machines, for example, fail for standard diagonalization reasons. In this paper, we introduce a "reflective" type of oracle, which is able to answer questions about the outputs of oracle machines with access to the same oracle. These oracles avoid diagonalization by answering some queries randomly. We show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory. These agents model their environment as a probabilistic oracle machine, which may contain other agents as a non-distinguished part. We show that if such agents interact, they will play a Nash equilibrium, with the randomization in mixed strategies coming from the randomization in the oracle's answers. This can be seen as providing a foundation for classical game theory in which players aren't special.
DA  - 2015/08/17/
PY  - 2015
DO  - 10.48550/arXiv.1508.04145
DP  - arXiv.org
PB  - arXiv
ST  - Reflective Oracles
UR  - http://arxiv.org/abs/1508.04145
Y2  - 2024/07/14/20:43:38
L1  - https://arxiv.org/pdf/1508.04145.pdf
L2  - https://arxiv.org/abs/1508.04145
KW  - Computer Science - Artificial Intelligence
KW  - I.2
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - CHAP
TI  - Reflective Variants of Solomonoff Induction and AIXI
AU  - Fallenstein, Benja
AU  - Soares, Nate
AU  - Taylor, Jessica
T2  - Artificial General Intelligence
A2  - Bieger, Jordi
A2  - Goertzel, Ben
A2  - Potapov, Alexey
AB  - Solomonoﬀ induction and AIXI model their environment as an arbitrary Turing machine, but are themselves uncomputable. This fails to capture an essential property of real-world agents, which cannot be more powerful than the environment they are embedded in; for example, AIXI cannot accurately model game-theoretic scenarios in which its opponent is another instance of AIXI.
CY  - Cham
DA  - 2015///
PY  - 2015
DP  - DOI.org (Crossref)
VL  - 9205
SP  - 60
EP  - 69
LA  - en
PB  - Springer International Publishing
SN  - 978-3-319-21364-4 978-3-319-21365-1
UR  - https://link.springer.com/10.1007/978-3-319-21365-1_7
Y2  - 2024/07/14/20:43:28
L1  - https://intelligence.org/files/ReflectiveSolomonoffAIXI.pdf
ER  - 

TY  - GEN
TI  - Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents
AU  - Fallenstein, Benja
AU  - Soares, Nate
AB  - Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.
DA  - 2015/02//
PY  - 2015
DP  - Zotero
LA  - en
UR  - https://intelligence.org/files/VingeanReflection.pdf
L1  - https://intelligence.org/files/VingeanReflection.pdf
ER  - 

TY  - CHAP
TI  - Alignment for Advanced Machine Learning Systems
AU  - Taylor, Jessica
AU  - Yudkowsky, Eliezer
AU  - LaVictoire, Patrick
AU  - Critch, Andrew
T2  - Ethics of Artificial Intelligence
A2  - Liao, S. Matthew
AB  - This chapter surveys eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? The chapter focuses on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers. The questions surveyed include the following: How can we train reinforcement learners to take actions that are more amenable to meaningful assessment by intelligent overseers? What kinds of objective functions incentivize a system to “not have an overly large impact” or “not have many side effects”? The chapter discusses these questions, related work, and potential directions for future research, with the goal of highlighting relevant research topics in machine learning that appear tractable today.
DA  - 2020/09/17/
PY  - 2020
DP  - Silverchair
SP  - 0
PB  - Oxford University Press
SN  - 978-0-19-090503-3
UR  - https://doi.org/10.1093/oso/9780190905033.003.0013
Y2  - 2024/07/14/20:41:11
L1  - https://academic.oup.com/book/33540/chapter/287906349/chapter-pdf/57567401/oso-9780190905033-chapter-13.pdf
L2  - https://academic.oup.com/book/33540/chapter/287906349
ER  - 

TY  - GEN
TI  - Quantilizers: A Safer Alternative to Maximizers for Limited Optimization
AU  - Taylor, Jessica
AB  - In the ﬁeld of AI, expected utility maximizers are commonly used as a model for idealized agents. However, expected utility maximization can lead to unintended solutions when the utility function does not quantify everything the operators care about: imagine, for example, an expected utility maximizer tasked with winning money on the stock market, which has no regard for whether it accidentally causes a market crash. Once AI systems become sufﬁciently intelligent and powerful, these unintended solutions could become quite dangerous. In this paper, we describe an alternative to expected utility maximization for powerful AI systems, which we call expected utility quantilization. This could allow the construction of AI systems that do not necessarily fall into strange and unanticipated shortcuts and edge cases in pursuit of their goals.
CY  - The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society
DA  - 2016///
PY  - 2016
DP  - Zotero
LA  - en
L1  - https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf
ER  - 

TY  - GEN
TI  - Defining Human Values for Value Learners
AU  - Sotala, Kaj
AB  - Hypothetical “value learning” AIs learn human values and then try to act according to those values. The design of such AIs, however, is hampered by the fact that there exists no satisfactory definition of what exactly human values are. After arguing that the standard concept of preference is insufficient as a definition, I draw on reinforcement learning theory, emotion research, and moral psychology to offer an alternative definition. In this definition, human values are conceptualized as mental representations that encode the brain’s value function (in the reinforcement learning sense) by being imbued with a context-sensitive affective gloss. I finish with a discussion of the implications that this hypothesis has on the design of value learners.
CY  - Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society
DA  - 2016///
PY  - 2016
DP  - Zotero
LA  - en
L1  - https://cdn.aaai.org/ocs/ws/ws0217/12633-57415-1-PB.pdf
ER  - 

TY  - GEN
TI  - Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents
AU  - Critch, Andrew
AB  - L\"ob's theorem and G\"odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\"ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas "L\"obian" cooperation is much more robust and agnostic of the opponent's implementation.
DA  - 2016/08/24/
PY  - 2016
DO  - 10.48550/arXiv.1602.04184
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1602.04184
Y2  - 2024/07/14/20:37:22
L1  - https://arxiv.org/pdf/1602.04184.pdf
L2  - https://arxiv.org/abs/1602.04184
KW  - Computer Science - Computer Science and Game Theory
KW  - Computer Science - Logic in Computer Science
ER  - 

TY  - JOUR
TI  - A Parametric, Resource-Bounded Generalization of Löb’s Theorem, and a Robust Cooperation Criterion for Open-Source Game Theory
AU  - Critch, Andrew
T2  - The Journal of Symbolic Logic
AB  - This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.
DA  - 2019/12//
PY  - 2019
DO  - 10.1017/jsl.2017.42
DP  - Cambridge University Press
VL  - 84
IS  - 4
SP  - 1368
EP  - 1381
LA  - en
SN  - 0022-4812, 1943-5886
UR  - https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79
Y2  - 2024/07/14/20:37:14
L1  - https://www.cambridge.org/core/services/aop-cambridge-core/content/view/16063EA7BFFEE89438631B141E556E79/S0022481217000421a.pdf/div-class-title-a-parametric-resource-bounded-generalization-of-lob-s-theorem-and-a-robust-cooperation-criterion-for-open-source-game-theory-div.pdf
KW  - 03B45
KW  - 03B70
KW  - 03F03
KW  - 62C99
KW  - 68T27
KW  - 91A05
KW  - 91A06
KW  - 91A10
KW  - 91A35
KW  - bounded rationality
KW  - Löbian cooperation
KW  - Prisoner’s Dilemma
KW  - program equilibrium
KW  - proof length
ER  - 

TY  - GEN
TI  - Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm
AU  - Kosoy, Vanessa
AU  - Appel, Alexander
AB  - We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of "optimal polynomial-time estimators." We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with "classical" probability theory.
DA  - 2019/06/04/
PY  - 2019
DO  - 10.48550/arXiv.1608.04112
DP  - arXiv.org
PB  - arXiv
ST  - Optimal Polynomial-Time Estimators
UR  - http://arxiv.org/abs/1608.04112
Y2  - 2024/07/14/20:37:39
L1  - https://arxiv.org/pdf/1608.04112.pdf
L2  - https://arxiv.org/abs/1608.04112
KW  - Computer Science - Computational Complexity
ER  - 

TY  - GEN
TI  - Asymptotic Convergence in Online Learning with Unbounded Delays
AU  - Garrabrant, Scott
AU  - Soares, Nate
AU  - Taylor, Jessica
AB  - We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.
DA  - 2016/09/07/
PY  - 2016
DO  - 10.48550/arXiv.1604.05280
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1604.05280
Y2  - 2024/07/14/20:37:33
L1  - https://arxiv.org/pdf/1604.05280.pdf
L2  - https://arxiv.org/abs/1604.05280
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Mathematics - Probability
ER  - 

TY  - GEN
TI  - Inductive Coherence
AU  - Garrabrant, Scott
AU  - Fallenstein, Benya
AU  - Demski, Abram
AU  - Soares, Nate
AB  - While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.
DA  - 2016/10/07/
PY  - 2016
DO  - 10.48550/arXiv.1604.05288
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1604.05288
Y2  - 2024/07/14/20:37:29
L1  - https://arxiv.org/pdf/1604.05288.pdf
L2  - https://arxiv.org/abs/1604.05288
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Mathematics - Probability
ER  - 

TY  - GEN
TI  - Forecasting using incomplete models
AU  - Kosoy, Vanessa
AB  - We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is "suspected" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.
DA  - 2019/05/16/
PY  - 2019
DO  - 10.48550/arXiv.1705.04630
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1705.04630
Y2  - 2024/07/14/20:34:19
L1  - https://arxiv.org/pdf/1705.04630.pdf
L2  - https://arxiv.org/abs/1705.04630
KW  - Computer Science - Machine Learning
KW  - I.2.6
KW  - 68Q32, 62M10, 62G08
KW  - G.3
ER  - 

TY  - GEN
TI  - When Will AI Exceed Human Performance? Evidence from AI Experts
AU  - Grace, Katja
AU  - Salvatier, John
AU  - Dafoe, Allan
AU  - Zhang, Baobao
AU  - Evans, Owain
AB  - Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.
DA  - 2018/05/03/
PY  - 2018
DO  - 10.48550/arXiv.1705.08807
DP  - arXiv.org
PB  - arXiv
ST  - When Will AI Exceed Human Performance?
UR  - http://arxiv.org/abs/1705.08807
Y2  - 2024/07/14/20:34:17
L1  - https://arxiv.org/pdf/1705.08807.pdf
L2  - https://arxiv.org/abs/1705.08807
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - A Formal Approach to the Problem of Logical Non-Omniscience
AU  - Garrabrant, Scott
AU  - Benson-Tilsen, Tsvi
AU  - Critch, Andrew
AU  - Soares, Nate
AU  - Taylor, Jessica
T2  - Electronic Proceedings in Theoretical Computer Science
AB  - We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.
DA  - 2017/07/25/
PY  - 2017
DO  - 10.4204/EPTCS.251.16
DP  - arXiv.org
VL  - 251
SP  - 221
EP  - 235
J2  - Electron. Proc. Theor. Comput. Sci.
SN  - 2075-2180
UR  - http://arxiv.org/abs/1707.08747
Y2  - 2024/07/14/20:34:17
L1  - https://arxiv.org/pdf/1707.08747.pdf
L2  - https://arxiv.org/abs/1707.08747
KW  - Computer Science - Logic in Computer Science
KW  - G.3
KW  - F.4.0
ER  - 

TY  - GEN
TI  - Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making
AU  - Critch, Andrew
AB  - Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\"{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.
DA  - 2017/05/13/
PY  - 2017
DO  - 10.48550/arXiv.1701.01302
DP  - arXiv.org
PB  - arXiv
ST  - Toward negotiable reinforcement learning
UR  - http://arxiv.org/abs/1701.01302
Y2  - 2024/07/14/20:34:14
L1  - https://arxiv.org/pdf/1701.01302.pdf
L2  - https://arxiv.org/abs/1701.01302
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Incorrigibility in the CIRL Framework
AU  - Carey, Ryan
AB  - A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.
DA  - 2018/06/03/
PY  - 2018
DO  - 10.48550/arXiv.1709.06275
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1709.06275
Y2  - 2024/07/14/20:34:12
L1  - https://arxiv.org/pdf/1709.06275.pdf
L2  - https://arxiv.org/abs/1709.06275
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Categorizing Variants of Goodhart's Law
AU  - Manheim, David
AU  - Garrabrant, Scott
AB  - There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are "(at least) four different mechanisms" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.
DA  - 2019/02/24/
PY  - 2019
DO  - 10.48550/arXiv.1803.04585
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.04585
Y2  - 2024/07/14/20:33:53
L1  - https://arxiv.org/pdf/1803.04585.pdf
L2  - https://arxiv.org/abs/1803.04585
KW  - Computer Science - Artificial Intelligence
KW  - Statistics - Machine Learning
KW  - 91E45
KW  - Quantitative Finance - General Finance
ER  - 

TY  - GEN
TI  - Occam's razor is insufficient to infer the preferences of irrational agents
AU  - Armstrong, Stuart
AU  - Mindermann, Sören
AB  - Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.
DA  - 2019/01/11/
PY  - 2019
DO  - 10.48550/arXiv.1712.05812
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1712.05812
Y2  - 2024/07/14/20:33:16
L1  - https://arxiv.org/pdf/1712.05812.pdf
L2  - https://arxiv.org/abs/1712.05812
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Embedded Agency
AU  - Demski, Abram
AU  - Garrabrant, Scott
AB  - Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts. We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type "function"; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.
DA  - 2020/10/06/
PY  - 2020
DO  - 10.48550/arXiv.1902.09469
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1902.09469
Y2  - 2024/07/14/20:32:47
L1  - https://arxiv.org/pdf/1902.09469.pdf
L2  - https://arxiv.org/abs/1902.09469
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - An overview of 11 proposals for building safe advanced AI
AU  - Hubinger, Evan
AB  - This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.
DA  - 2020/12/04/
PY  - 2020
DO  - 10.48550/arXiv.2012.07532
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2012.07532
Y2  - 2024/07/14/20:32:27
L1  - https://arxiv.org/pdf/2012.07532.pdf
L2  - https://arxiv.org/abs/2012.07532
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Cartesian Frames
AU  - Garrabrant, Scott
AU  - Herrmann, Daniel A.
AU  - Lopez-Wild, Josiah
AB  - We introduce a novel framework, the theory of Cartesian frames (CF), that gives powerful tools for manipulating sets of acts. The CF framework takes as its most fundamental building block that an agent can freely choose from a set of available actions. The framework uses the mathematics of Chu spaces to develop a calculus of those sets of actions, how those actions change at various levels of description, and how different agents' actions can combine when agents work in concert. We discuss how this framework might provide an illuminating perspective on issues in decision theory and formal epistemology.
DA  - 2021/09/22/
PY  - 2021
DO  - 10.48550/arXiv.2109.10996
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2109.10996
Y2  - 2024/07/14/20:32:25
L1  - https://arxiv.org/pdf/2109.10996.pdf
L2  - https://arxiv.org/abs/2109.10996
KW  - Mathematics - Category Theory
ER  - 

TY  - GEN
TI  - Temporal Inference with Finite Factored Sets
AU  - Garrabrant, Scott
AB  - We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.
DA  - 2021/09/23/
PY  - 2021
DO  - 10.48550/arXiv.2109.11513
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2109.11513
Y2  - 2024/07/14/20:32:24
L1  - https://arxiv.org/pdf/2109.11513.pdf
L2  - https://arxiv.org/abs/2109.11513
KW  - Computer Science - Artificial Intelligence
KW  - Mathematics - Probability
KW  - Mathematics - Combinatorics
ER  - 

TY  - GEN
TI  - Formalizing Convergent Instrumental Goals
AU  - Benson-Tilsen, Tsvi
AU  - Soares, Nate
AB  - Omohundro has argued that sufﬁciently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources. Omohundro refers to these as “basic AI drives,” and he, along with Bostrom and others, has argued that this means great care must be taken when designing powerful autonomous systems, because even if they have harmless goals, the side effects of pursuing those goals may be quite harmful. These arguments, while intuitively compelling, are primarily philosophical. In this paper, we provide formal models that demonstrate Omohundro’s thesis, thereby putting mathematical weight behind those intuitive claims.
DA  - 2016///
PY  - 2016
DP  - Zotero
LA  - en
L1  - https://cdn.aaai.org/ocs/ws/ws0218/12634-57409-1-PB.pdf
ER  - 

TY  - CHAP
TI  - The Value Learning Problem
AU  - Soares, Nate
T2  - Artificial Intelligence Safety and Security
A2  - Yampolskiy, Roman V.
AB  - Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its designers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.
CY  - First edition. | Boca Raton, FL : CRC Press/Taylor & Francis Group, 2018.
DA  - 2018/07/27/
PY  - 2018
DP  - DOI.org (Crossref)
ET  - 1
SP  - 89
EP  - 97
LA  - en
PB  - Chapman and Hall/CRC
SN  - 978-1-351-25138-9
UR  - https://www.taylorfrancis.com/books/9781351251372/chapters/10.1201/9781351251389-7
Y2  - 2024/07/14/20:25:45
L1  - https://intelligence.org/files/ValueLearningProblem.pdf
ER  - 

TY  - CONF
TI  - Proof-Producing Reflection for HOL
AU  - Fallenstein, Benja
AU  - Kumar, Ramana
A2  - Urban, Christian
A2  - Zhang, Xingyuan
AB  - We present a reflection principle of the form “If $$\ulcorner \varphi \urcorner $$⌜φ⌝is provable, then $$\varphi $$φ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure $$\varphi $$φhas the same meaning both inside and outside of this model. Soundness of HOL implies that if $$\ulcorner \varphi \urcorner $$⌜φ⌝is provable, then it is true in this model, and hence $$\varphi $$φholds. We additionally show how this reflection principle can be extended, assuming an infinite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.
C1  - Cham
C3  - Interactive Theorem Proving
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-319-22102-1_11
DP  - Springer Link
SP  - 170
EP  - 186
LA  - en
PB  - Springer International Publishing
SN  - 978-3-319-22102-1
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-319-22102-1_11.pdf
KW  - Certificate Theorem
KW  - HOL Theorem Prover
KW  - Large Cardinal Assumption
KW  - Polymorphism Model
KW  - Reflection Principle
ER  - 

TY  - GEN
TI  - Functional Decision Theory: A New Theory of Instrumental Rationality
AU  - Yudkowsky, Eliezer
AU  - Soares, Nate
AB  - This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, "Which output of this very function would yield the best outcome?" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.
DA  - 2018/05/22/
PY  - 2018
DO  - 10.48550/arXiv.1710.05060
DP  - arXiv.org
PB  - arXiv
ST  - Functional Decision Theory
UR  - http://arxiv.org/abs/1710.05060
Y2  - 2024/07/14/20:24:57
L1  - https://arxiv.org/pdf/1710.05060.pdf
L2  - https://arxiv.org/abs/1710.05060
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - A Formal Solution to the Grain of Truth Problem
AU  - Leike, Jan
AU  - Taylor, Jessica
AU  - Fallenstein, Benya
AB  - A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (in other words, its prior contains a \emph{grain of truth}). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the \emph{grain of truth problem}. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play {\epsilon}-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.
DA  - 2016/09/16/
PY  - 2016
DO  - 10.48550/arXiv.1609.05058
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1609.05058
Y2  - 2024/07/14/20:24:12
L1  - https://arxiv.org/pdf/1609.05058.pdf
L2  - https://arxiv.org/abs/1609.05058
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Logical Induction
AU  - Garrabrant, Scott
AU  - Benson-Tilsen, Tsvi
AU  - Critch, Andrew
AU  - Soares, Nate
AU  - Taylor, Jessica
AB  - We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to "the $n$th digit of $\pi$ is a 7" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]
DA  - 2020/12/07/
PY  - 2020
DO  - 10.48550/arXiv.1609.03543
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1609.03543
Y2  - 2024/07/14/20:23:34
L1  - https://arxiv.org/pdf/1609.03543.pdf
L2  - https://arxiv.org/abs/1609.03543
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Logic in Computer Science
KW  - Mathematics - Probability
KW  - Mathematics - Logic
ER  - 

TY  - GEN
TI  - Predictive Minds: LLMs As Atypical Active Inference Agents
AU  - Kulveit, Jan
AU  - von Stengel, Clem
AU  - Leventov, Roman
AB  - Large language models (LLMs) like GPT are often conceptualized as passive predictors, simulators, or even stochastic parrots. We instead conceptualize LLMs by drawing on the theory of active inference originating in cognitive science and neuroscience. We examine similarities and differences between traditional active inference systems and LLMs, leading to the conclusion that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impacts of their actions, but otherwise fit in the active inference paradigm. We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.
DA  - 2023/11/16/
PY  - 2023
DO  - 10.48550/arXiv.2311.10215
DP  - arXiv.org
PB  - arXiv
ST  - Predictive Minds
UR  - http://arxiv.org/abs/2311.10215
Y2  - 2024/07/14/22:19:25
L1  - https://arxiv.org/pdf/2311.10215.pdf
L2  - https://arxiv.org/abs/2311.10215
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - A Naturalised Account of Planning in Intelligent Systems
AU  - Ammann, Nora
AU  - von Stengel, Clem
T2  - ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference
AB  - Abstract. We develop a naturalised account of planning, which identifies a class of functions (and their associated behaviours) in intelligent systems. The account identifies three principal components of a planning process: a system (defined by a set of possible system-environment decompositions); a subsystem (which presents a model, copy, or analog of some aspect of the system); and a selection mechanism (via which a subsystem is functionally related to expected future states). We give a generalised, system-independent account of planning, and then ground our analysis with a set of eight concrete reference systems, spanning biological, human, social, and artificial systems. Finally, we apply this naturalized account of planning to evaluate under what conditions planning behaviour is likely to emerge, and what failure modes arise in systems exhibiting such planning behaviour.
DA  - 2023/07/24/
PY  - 2023
DO  - 10.1162/isal_a_00696
DP  - direct.mit.edu
LA  - en
PB  - MIT Press
UR  - https://dx.doi.org/10.1162/isal_a_00696
Y2  - 2024/07/14/22:19:21
L1  - https://direct.mit.edu/isal/proceedings-pdf/isal2023/35/138/2355045/isal_a_00696.pdf
ER  - 

TY  - GEN
TI  - Quantifying Hierarchical Selection
AU  - Rajpal, Hardik
AU  - von Stengel, Clem
AU  - Mediano, Pedro A. M.
AU  - Rosas, Fernando E.
AU  - Viegas, Eduardo
AU  - Marquet, Pablo A.
AU  - Jensen, Henrik J.
AB  - At what level does selective pressure effectively act? When considering the reproductive dynamics of interacting and mutating agents, it has long been debated whether selection is better understood by focusing on the individual or if hierarchical selection emerges as a consequence of joint adaptation. Despite longstanding efforts in theoretical ecology there is still no consensus on this fundamental issue, most likely due to the difficulty in obtaining adequate data spanning sufficient number of generations and the lack of adequate tools to quantify the effect of hierarchical selection. Here we capitalise on recent advances in information-theoretic data analysis to advance this state of affairs by investigating the emergence of high-order structures -- such as groups of species -- in the collective dynamics of the Tangled Nature model of evolutionary ecology. Our results show that evolutionary dynamics can lead to clusters of species that act as a selective group, that acquire information-theoretic agency. Overall, our findings provide quantitative evidence supporting the relevance of high-order structures in evolutionary ecology, which can emerge even from relatively simple processes of adaptation and selection.
DA  - 2023/11/01/
PY  - 2023
DO  - 10.48550/arXiv.2310.20386
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.20386
Y2  - 2024/07/14/22:19:18
L1  - https://arxiv.org/pdf/2310.20386.pdf
L2  - https://arxiv.org/abs/2310.20386
KW  - Nonlinear Sciences - Adaptation and Self-Organizing Systems
KW  - Quantitative Biology - Populations and Evolution
ER  - 

TY  - GEN
TI  - Fake It Till You Make It: Towards Accurate Near-Distribution Novelty Detection
AU  - Mirzaei, Hossein
AU  - Salehi, Mohammadreza
AU  - Shahabi, Sajjad
AU  - Gavves, Efstratios
AU  - Snoek, Cees G. M.
AU  - Sabokrou, Mohammad
AU  - Rohban, Mohammad Hossein
AB  - We aim for image-based novelty detection. Despite considerable progress, existing models either fail or face a dramatic drop under the so-called "near-distribution" setting, where the differences between normal and anomalous samples are subtle. We first demonstrate existing methods experience up to 20% decrease in performance in the near-distribution setting. Next, we propose to exploit a score-based generative model to produce synthetic near-distribution anomalous data. Our model is then fine-tuned to distinguish such data from the normal samples. We provide a quantitative as well as qualitative evaluation of this strategy, and compare the results with a variety of GAN-based models. Effectiveness of our method for both the near-distribution and standard novelty detection is assessed through extensive experiments on datasets in diverse applications such as medical images, object classification, and quality control. This reveals that our method considerably improves over existing models, and consistently decreases the gap between the near-distribution and standard novelty detection performance. The code repository is available at https://github.com/rohban-lab/FITYMI.
DA  - 2022/11/28/
PY  - 2022
DO  - 10.48550/arXiv.2205.14297
DP  - arXiv.org
PB  - arXiv
ST  - Fake It Till You Make It
UR  - http://arxiv.org/abs/2205.14297
Y2  - 2024/07/14/22:17:17
L1  - https://arxiv.org/pdf/2205.14297.pdf
L2  - https://arxiv.org/abs/2205.14297
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Assessing Robustness of Image Recognition Models to Changes in the Computational Environment
AU  - Louloudakis, Nikolaos
AU  - Gibson, Perry
AU  - Cano, Jose
AU  - Rajan, Ajitha
T2  - NeurIPS ML Safety Workshop
AB  - Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to incorrect mapping on hardware accelerators, which may lead to timing uncertainty and incorrect behavior. In addition, the increasing demand for optimal performance has led to progress towards the optimization of different neural network operations, such as operator fusion. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess the performance and impact of such optimizations, and explore their effectiveness. In this paper we conduct robustness analysis of four popular image recognition models with the ImageNet dataset, assessing the impact of the compiler optimizations applied, utilizing different Deep Learning frameworks and executing on hardware devices of varying capabilities. We report the impact in terms of misclassifications and inference time across varying settings.
DA  - 2022/11/27/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=-7DjNGvdpx
Y2  - 2024/07/14/22:17:05
L1  - https://openreview.net/pdf?id=-7DjNGvdpx
ER  - 

TY  - GEN
TI  - Unifying Grokking and Double Descent
AU  - Davies, Xander
AU  - Langosco, Lauro
AU  - Krueger, David
AB  - A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
DA  - 2023/03/10/
PY  - 2023
DO  - 10.48550/arXiv.2303.06173
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.06173
Y2  - 2024/07/14/22:16:48
L1  - https://arxiv.org/pdf/2303.06173.pdf
L2  - https://arxiv.org/abs/2303.06173
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Runtime Monitors for Operational Design Domains of Black-Box ML-Models
AU  - Torfah, Hazem
AU  - Seshia, Sanjit A.
T2  - NeurIPS ML Safety Workshop
AB  - Autonomous systems are increasingly relying on machine learning (ML) components to perform a variety of complex tasks in perception, prediction, and control.To guarantee the safety of ML-based autonomous systems, it is important to capture their operational design domain (ODD), i.e., the conditions under which using the ML components does not endanger the safety of the system. In this paper, we present a framework for learning runtime monitors for ODDs of autonomous systems with black-box ML components. A runtime monitor of an ODD predicts based on a sequence of monitorable observations whether the system is about to exit its ODD. We particularly investigate the learning of optimal monitors based on counterexample-guided refinement and conformance testing. We evaluate our approach on a case study from the domain of autonomous driving.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=6_AtjSBhqx
Y2  - 2024/07/14/22:16:30
L1  - https://openreview.net/pdf?id=6_AtjSBhqx
ER  - 

TY  - CONF
TI  - Rational Multi-Objective Agents Must Admit Non-Markov Reward Representations
AU  - Pitis, Silviu
AU  - Bailey, Duncan
AU  - Ba, Jimmy
T2  - NeurIPS ML Safety Workshop
AB  - This paper considers intuitively appealing axioms for rational, multi-objective agents and derives an impossibility from which one concludes that such agents must admit non-Markov reward representations. The axioms include the Von-Neumann Morgenstern axioms, Pareto indifference, and dynamic consistency. We tie this result to irrational procrastination behaviors observed in humans, and show how the impossibility can be resolved by adopting a non-Markov aggregation scheme. Our work highlights the importance of non-Markov rewards for reinforcement learning and outlines directions for future work.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=MNwA4sgzR4W
Y2  - 2024/07/14/22:16:17
L1  - https://openreview.net/pdf?id=MNwA4sgzR4W
ER  - 

TY  - CONF
TI  - Image recognition time for humans predicts adversarial vulnerability for models
AU  - Mayo, David
AU  - Cummings, Jesse
AU  - Lin, Xinyu
AU  - Katz, Boris
AU  - Barbu, Andrei
T2  - NeurIPS ML Safety Workshop
AB  - The success of adversarial attacks and the performance tradeoffs made by adversarial defense methods have both traditionally been evaluated on image test sets constructed from a randomly sampled held out portion of a training set. Mayo 2022 et al. [1] measured the difficulty of the ImageNet and ObjectNet test sets by measuring the minimum viewing time required for an object to be recognized on average by a human, finding that these test sets are heavily skewed towards containing mostly easy, quickly recognized images. While difficult images that require longer viewing times to be recognized are uncommon in test sets, they are both common and critically important to the real world performance of vision models. In this work, we investigated the relationship between adversarial robustness and viewing time difficulty. Measuring the AUC of accuracy vs attack strength (epsilon), we find that easy, quickly recognized, images are more robust to adversarial attacks than difficult images, which require several seconds of viewing time to recognize. Additionally, adversarial defense methods improve models robustness to adversarial attacks on easy images significantly more than on hard images. We propose that the distribution of image difficulties should be carefully considered and controlled for when measuring both the effectiveness of adversarial attacks and when analyzing the clean accuracy vs robustness tradeoff made by adversarial defense methods.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=d_5-0m3xCWn
Y2  - 2024/07/14/22:16:06
L1  - https://openreview.net/pdf?id=d_5-0m3xCWn
ER  - 

TY  - GEN
TI  - Policy Resilience to Environment Poisoning Attacks on Reinforcement Learning
AU  - Xu, Hang
AU  - Qu, Xinghua
AU  - Rabinovich, Zinovi
AB  - This paper investigates policy resilience to training-environment poisoning attacks on reinforcement learning (RL) policies, with the goal of recovering the deployment performance of a poisoned RL policy. Due to the fact that the policy resilience is an add-on concern to RL algorithms, it should be resource-efficient, time-conserving, and widely applicable without compromising the performance of RL algorithms. This paper proposes such a policy-resilience mechanism based on an idea of knowledge sharing. We summarize the policy resilience as three stages: preparation, diagnosis, recovery. Specifically, we design the mechanism as a federated architecture coupled with a meta-learning manner, pursuing an efficient extraction and sharing of the environment knowledge. With the shared knowledge, a poisoned agent can quickly identify the deployment condition and accordingly recover its policy performance. We empirically evaluate the resilience mechanism for both model-based and model-free RL algorithms, showing its effectiveness and efficiency in restoring the deployment performance of a poisoned policy.
DA  - 2023/04/24/
PY  - 2023
DO  - 10.48550/arXiv.2304.12151
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2304.12151
Y2  - 2024/07/14/22:15:56
L1  - https://arxiv.org/pdf/2304.12151.pdf
L2  - https://arxiv.org/abs/2304.12151
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Deep Reinforcement Learning Policies in the Frequency Domain
AU  - Korkmaz, Ezgi
T2  - NeurIPS ML Safety Workshop
AB  - Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods for adversarial training for deep reinforcement learning agents to improve robustness to adversarial perturbations. In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=s6td4UTR2FR
Y2  - 2024/07/14/22:15:40
L1  - https://openreview.net/pdf?id=s6td4UTR2FR
ER  - 

TY  - GEN
TI  - Revisiting Robustness in Graph Machine Learning
AU  - Gosch, Lukas
AU  - Sturm, Daniel
AU  - Geisler, Simon
AU  - Günnemann, Stephan
AB  - Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the inference process of GNNs significantly reduces over-robustness, while having a positive effect on test accuracy and adversarial robustness. Theoretically, leveraging our new semantics-aware notion of robustness, we prove that there is no robustness-accuracy tradeoff for inductively classifying a newly added node.
DA  - 2023/05/02/
PY  - 2023
DO  - 10.48550/arXiv.2305.00851
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.00851
Y2  - 2024/07/14/22:15:34
L1  - https://arxiv.org/pdf/2305.00851.pdf
L2  - https://arxiv.org/abs/2305.00851
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Out-Of-Distribution Detection Is Not All You Need
AU  - Guérin, Joris
AU  - Delmas, Kevin
AU  - Ferreira, Raul Sena
AU  - Guiochet, Jérémie
AB  - The usage of deep neural networks in safety-critical systems is limited by our ability to guarantee their correct behavior. Runtime monitors are components aiming to identify unsafe predictions and discard them before they can lead to catastrophic consequences. Several recent works on runtime monitoring have focused on out-of-distribution (OOD) detection, i.e., identifying inputs that are different from the training data. In this work, we argue that OOD detection is not a well-suited framework to design efficient runtime monitors and that it is more relevant to evaluate monitors based on their ability to discard incorrect predictions. We call this setting out-ofmodel-scope detection and discuss the conceptual differences with OOD. We also conduct extensive experiments on popular datasets from the literature to show that studying monitors in the OOD setting can be misleading: 1. very good OOD results can give a false impression of safety, 2. comparison under the OOD setting does not allow identifying the best monitor to detect errors. Finally, we also show that removing erroneous training data samples helps to train better monitors.
DA  - 2023/01/13/
PY  - 2023
DO  - 10.48550/arXiv.2211.16158
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.16158
Y2  - 2024/07/14/22:15:27
L1  - https://arxiv.org/pdf/2211.16158.pdf
L2  - https://arxiv.org/abs/2211.16158
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Electrical Engineering and Systems Science - Image and Video Processing
ER  - 

TY  - CONF
TI  - Avoiding Calvinist Decision Traps using Structural Causal Models
AU  - Raghavan, Arvind
T2  - NeurIPS ML Safety Workshop
AB  - Causal Decision Theory (CDT) is a popular choice among practical decision theorists. While its successes and failings have been extensively studied, a less investigated topic is how CDT's choices hinge on the theory of causation used. The most common interpretation, temporal CDT, understands causation as a description of physical processes ordered in time. Another emerging view comes from the graphical framework of Structural Causal Models (SCM), which sees causation in terms of constraints on sources of variation in a system. We present an adversarial scheme where a CDT agent facing a Bandit problem can be tricked into sub-optimal choices, if it follows temporal CDT. We then propose an axiom to ground the orientation of arrows in the causal graph of a decision problem. In doing so, we resolve an ambiguity in the theory of SCMs, and underscore the importance of agent-perspectives, which have been largely ignored in the causal inference literature. We also demonstrate how this structural CDT avoids our adversarial trap, and outperforms temporal CDT in a series of canonical decision problems.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=pLEDGG6J6wZ
Y2  - 2024/07/14/22:15:16
L1  - https://openreview.net/pdf?id=pLEDGG6J6wZ
ER  - 

TY  - GEN
TI  - Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger
AU  - Li, Yiming
AU  - Zhu, Mingyan
AU  - Guo, Junfeng
AU  - Wei, Tao
AU  - Xia, Shu-Tao
AU  - Qin, Zhan
AB  - Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and malicious methods since they can easily circumvent most of the current backdoor defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due to their poisoned-label nature, where users can discover anomalies if they check the image-label relationship. In particular, we demonstrate that it is ineffective to directly generalize existing SSBAs to their clean-label variants by poisoning samples solely from the target class. We reveal that it is primarily due to two reasons, including \textbf{(1)} the `antagonistic effects' of ground-truth features and \textbf{(2)} the learning difficulty of sample-specific features. Accordingly, trigger-related features of existing SSBAs cannot be effectively learned under the clean-label setting due to their mild trigger intensity required for ensuring stealthiness. We argue that the intensity constraint of existing SSBAs is mostly because their trigger patterns are `content-irrelevant' and therefore act as `noises' for both humans and DNNs. Motivated by this understanding, we propose to exploit content-relevant features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with attribute trigger (BAAT). Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our BAAT and its resistance to existing defenses.
DA  - 2023/12/10/
PY  - 2023
DO  - 10.48550/arXiv.2312.04584
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.04584
Y2  - 2024/07/14/22:15:06
L1  - https://arxiv.org/pdf/2312.04584.pdf
L2  - https://arxiv.org/abs/2312.04584
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Introspection, Updatability, and Uncertainty Quantification with Transformers: Concrete Methods for AI Safety
AU  - Schmaltz, Allen
AU  - Rasooly, Danielle
T2  - NeurIPS ML Safety Workshop
AB  - When deploying Transformer networks, we seek the ability to introspect the predictions against instances with known labels; update the model without a full re-training; and provide reliable uncertainty quantification over the predictions. We demonstrate that these properties are achievable via recently proposed approaches for approximating deep neural networks with instance-based metric learners, at varying resolutions of the input, and the associated Venn-ADMIT Predictor for constructing prediction sets. We consider a challenging (but non-adversarial) task: Zero-shot sequence labeling (i.e., feature detection) in a low-accuracy, class-imbalanced, covariate-shifted setting while requiring a high confidence level.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Introspection, Updatability, and Uncertainty Quantification with Transformers
UR  - https://openreview.net/forum?id=w9U_7Ay7f86
Y2  - 2024/07/14/22:14:53
L1  - https://openreview.net/pdf?id=w9U_7Ay7f86
ER  - 

TY  - CONF
TI  - Evaluating Worst Case Adversarial Weather Perturbations Robustness
AU  - Wang, Yihan
AU  - Ba, Yunhao
AU  - Zhang, Howard Chenyang
AU  - Zhang, Huan
AU  - Kadambi, Achuta
AU  - Soatto, Stefano
AU  - Wong, Alex
AU  - Hsieh, Cho-Jui
T2  - NeurIPS ML Safety Workshop
AB  - Several algorithms are proposed to improve the robustness of deep neural networks against adversarial perturbations beyond $\ell_p$ cases, i.e. weather perturbations. However, evaluations of existing robust training algorithms are over-optimistic. This is in part due to the lack of a standardized evaluation protocol across various robust training algorithms, leading to ad-hoc methods that test robustness on either random perturbations or the adversarial samples from generative models that are used for robust training, which is either uninformative of the worst case, or is heavily biased. In this paper, we identify such evaluation bias in these existing works and propose the first standardized and fair evaluation that compares various robust training algorithms by using physics simulators for common adverse weather effects i.e. rain and snow. With this framework, we evaluated several existing robust training algorithms on two streetview classification datasets (BIC\_GSV, Places365) and show the evaluation bias in experiments.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Q69Aj3P0PtV
Y2  - 2024/07/14/22:14:43
L1  - https://openreview.net/pdf?id=Q69Aj3P0PtV
ER  - 

TY  - CONF
TI  - Netflix and Forget: Fast Severance From Memorizing Training Data in Recommendations
AU  - Xu, Mimee
AU  - Sun, Jiankai
AU  - Yang, Xin
AU  - Yao, Yuanshun
AU  - Wang, Chong
T2  - NeurIPS ML Safety Workshop
AB  - Suppose a person, who has streamed rom-coms exclusively with their significant other, suddenly breaks up. Consider an expecting mom, who has shopped for baby clothes, miscarries. Their streaming and shopping recommendations, however, do not necessarily update, serving as unhappy reminders of their loss. One approach is to implement the Right To Be Forgotten for recommendation systems built from user data, with the goal of updating downstream recommendations to reflect the removal without incurring the cost of re-training. Inspired by solutions to the original Netflix challenge~\citep{koren2009bellkor}, we develop Unlearn-ALS, which is more aggressively forgetful of select data than fine-tuning. In theory, it is consistent with retraining without model degradation. Empirically, it shows fast convergence, and can be applied directly to any bi-linear models regardless of the training procedure.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Netflix and Forget
UR  - https://openreview.net/forum?id=DmzF_IuR_Gv
Y2  - 2024/07/14/22:14:35
L1  - https://openreview.net/pdf?id=DmzF_IuR_Gv
ER  - 

TY  - GEN
TI  - Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes
AU  - Baharlouei, Sina
AU  - Sheikholeslami, Fatemeh
AU  - Razaviyayn, Meisam
AU  - Kolter, Zico
AB  - This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.
DA  - 2023/05/10/
PY  - 2023
DO  - 10.48550/arXiv.2210.14410
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.14410
Y2  - 2024/07/14/22:14:20
L1  - https://arxiv.org/pdf/2210.14410.pdf
L2  - https://arxiv.org/abs/2210.14410
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Geometric attacks on batch normalization
AU  - Ghose, Amur
AU  - Gupta, Apurv
AU  - Yu, Yaoliang
AU  - Poupart, Pascal
T2  - NeurIPS ML Safety Workshop
AB  - Constructing adversarial examples usually requires labels, which provide a loss gradient to construct the example. We show that for batch normalized architectures, intermediate latents that are produced after a batch normalization step suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, without any label. We motivate our loss through the geometry of batch normed representations and concentration on a known hypersphere. Our losses build on and expand intermediate latent based attacks that usually require labels. The success of our method implies that leakage of intermediate representations may suffice to create a security breach for deployed models, which persist even when the model is transferred to downstream usage. We further show that removal of batch norm weakens our attack significantly, suggesting that batch norm's contribution to adversarial vulnerability may be understood by analyzing such attacks.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=92DijVF2Ryv
Y2  - 2024/07/14/22:14:04
L1  - https://openreview.net/pdf?id=92DijVF2Ryv
ER  - 

TY  - GEN
TI  - Learning Representations Robust to Group Shifts and Adversarial Examples
AU  - Chiu, Ming-Chang
AU  - Ma, Xuezhe
AB  - Despite the high performance achieved by deep neural networks on various tasks, extensive studies have demonstrated that small tweaks in the input could fail the model predictions. This issue of deep neural networks has led to a number of methods to improve model robustness, including adversarial training and distributionally robust optimization. Though both of these two methods are geared towards learning robust models, they have essentially different motivations: adversarial training attempts to train deep neural networks against perturbations, while distributional robust optimization aims at improving model performance on the most difficult "uncertain distributions". In this work, we propose an algorithm that combines adversarial training and group distribution robust optimization to improve robust representation learning. Experiments on three image benchmark datasets illustrate that the proposed method achieves superior results on robust metrics without sacrificing much of the standard measures.
DA  - 2022/02/18/
PY  - 2022
DO  - 10.48550/arXiv.2202.09446
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2202.09446
Y2  - 2024/07/14/22:13:00
L1  - https://arxiv.org/pdf/2202.09446.pdf
L2  - https://arxiv.org/abs/2202.09446
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Improving the Robustness of Conditional Language Models by Detecting and Removing Input Noise
AU  - Krishna, Kundan
AU  - Zhao, Yao
AU  - Ren, Jie
AU  - Lakshminarayanan, Balaji
AU  - Luo, Jiaming
AU  - Saleh, Mohammad
AU  - Liu, Peter J.
T2  - NeurIPS ML Safety Workshop
AB  - The evaluation of conditional language modeling tasks such as abstractive summarization typically uses test data that is identically distributed as training. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance under distribution shift caused by such noise is relatively under-studied. We present a large empirical study quantifying the sometimes severe loss in performance (up to 12 ROUGE-1 points) from different types of input noise for a range of datasets and model sizes. We then propose a light-weight method for detecting and removing such noise in the input during model inference without requiring any extra training or auxiliary models, which effectively mitigates the loss in performance, recovering up to 11 ROUGE-1 points.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=DQ5eenIbSSW
Y2  - 2024/07/14/22:11:40
L1  - https://openreview.net/pdf?id=DQ5eenIbSSW
ER  - 

TY  - CONF
TI  - Epistemic Side Effects & Avoiding Them (Sometimes)
AU  - Klassen, Toryn Q.
AU  - Alamdari, Parand Alizadeh
AU  - McIlraith, Sheila A.
T2  - NeurIPS ML Safety Workshop
AB  - AI safety research has investigated the problem of negative side effects -- undesirable changes made by AI systems in pursuit of an underspecified objective. However, the focus has been on physical side effects, such as a robot breaking a vase while moving. In this paper we introduce the notion of epistemic side effects, unintended changes made to the knowledge or beliefs of agents, and describe a way to avoid negative epistemic side effects in reinforcement learning, in some cases.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=7oDZ-6kIW1K
Y2  - 2024/07/14/22:11:29
L1  - https://openreview.net/pdf?id=7oDZ-6kIW1K
ER  - 

TY  - GEN
TI  - Adversarial Robustness for Tabular Data through Cost and Utility Awareness
AU  - Kireev, Klim
AU  - Kulynych, Bogdan
AU  - Troncoso, Carmela
AB  - Many safety-critical applications of machine learning, such as fraud or abuse detection, use data in tabular domains. Adversarial examples can be particularly damaging for these applications. Yet, existing works on adversarial robustness primarily focus on machine-learning models in image and text domains. We argue that, due to the differences between tabular data and images or text, existing threat models are not suitable for tabular domains. These models do not capture that the costs of an attack could be more significant than imperceptibility, or that the adversary could assign different values to the utility obtained from deploying different adversarial examples. We demonstrate that, due to these differences, the attack and defense methods used for images and text cannot be directly applied to tabular settings. We address these issues by proposing new cost and utility-aware threat models that are tailored to the adversarial capabilities and constraints of attackers targeting tabular domains. We introduce a framework that enables us to design attack and defense mechanisms that result in models protected against cost and utility-aware adversaries, for example, adversaries constrained by a certain financial budget. We show that our approach is effective on three datasets corresponding to applications for which adversarial examples can have economic and social implications.
DA  - 2023/02/24/
PY  - 2023
DO  - 10.48550/arXiv.2208.13058
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2208.13058
Y2  - 2024/07/14/22:11:15
L1  - https://arxiv.org/pdf/2208.13058.pdf
L2  - https://arxiv.org/abs/2208.13058
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models
AU  - Fowl, Liam
AU  - Geiping, Jonas
AU  - Reich, Steven
AU  - Wen, Yuxin
AU  - Czaja, Wojtek
AU  - Goldblum, Micah
AU  - Goldstein, Tom
AB  - A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.
DA  - 2023/05/31/
PY  - 2023
DO  - 10.48550/arXiv.2201.12675
DP  - arXiv.org
PB  - arXiv
ST  - Decepticons
UR  - http://arxiv.org/abs/2201.12675
Y2  - 2024/07/14/22:10:58
L1  - https://arxiv.org/pdf/2201.12675.pdf
L2  - https://arxiv.org/abs/2201.12675
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - On The Fragility of Learned Reward Functions
AU  - McKinney, Lev
AU  - Duan, Yawen
AU  - Krueger, David
AU  - Gleave, Adam
AB  - Reward functions are notoriously difficult to specify, especially for tasks with complex goals. Reward learning approaches attempt to infer reward functions from human feedback and preferences. Prior works on reward learning have mainly focused on the performance of policies trained alongside the reward function. This practice, however, may fail to detect learned rewards that are not capable of training new policies from scratch and thus do not capture the intended behavior. Our work focuses on demonstrating and studying the causes of these relearning failures in the domain of preference-based reward learning. We demonstrate with experiments in tabular and continuous control environments that the severity of relearning failures can be sensitive to changes in reward model design and the trajectory dataset composition. Based on our findings, we emphasize the need for more retraining-based evaluations in the literature.
DA  - 2023/01/09/
PY  - 2023
DO  - 10.48550/arXiv.2301.03652
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2301.03652
Y2  - 2024/07/14/22:10:45
L1  - https://arxiv.org/pdf/2301.03652.pdf
L2  - https://arxiv.org/abs/2301.03652
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Embedding Reliability: On the Predictability of Downstream Performance
AU  - Ardeshir, Shervin
AU  - Azizan, Navid
T2  - NeurIPS ML Safety Workshop
AB  - In (self-)supervised (pre-)training, such as in contrastive learning, often a network is presented with correspondent (positive) and non-correspondent (negative) pairs of datapoints, and is trained to find an embedding vector for each datapoint, i.e., a representation, which can be further fine-tuned for various downstream tasks. To safely deploy these models in critical decision-making systems, it is crucial to equip them with a measure of their reliability. Here we study whether such measures can be quantified for a datapoint in a meaningful way. In other words, we explore if the downstream performance on a given datapoint is predictable, directly from a few characteristics of its pre-trained embedding. We study whether this goal can be achieved by directly estimating the distribution of the training data in the embedding space, and accounting for the local consistency of the representations. Our experiments show that these notions of reliability often strongly correlate with its downstream accuracy.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Embedding Reliability
UR  - https://openreview.net/forum?id=TedqYedIERd
Y2  - 2024/07/14/22:10:33
L1  - https://openreview.net/pdf?id=TedqYedIERd
ER  - 

TY  - CONF
TI  - Adversarial Attacks on Feature Visualization Methods
AU  - Marty, Jonathan
AU  - Belilovsky, Eugene
AU  - Eickenberg, Michael
T2  - NeurIPS ML Safety Workshop
AB  - The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Feature visualization approaches are one set of techniques used to interpret and analyze trained deep learning models. On the other hand interpretability methods themselves may be subject to be deceived. In particular, we consider the idea of an adversary manipulating a model for the purpose of deceiving the interpretation. Focusing on the popular feature visualizations associated with CNNs we introduce an optimization framework for modifying the outcome of feature visualization methods.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=J51K0rszIjr
Y2  - 2024/07/14/22:07:19
L1  - https://openreview.net/pdf?id=J51K0rszIjr
ER  - 

TY  - GEN
TI  - From Feature Visualization to Visual Circuits: Effect of Adversarial Model Manipulation
AU  - Nanfack, Geraldin
AU  - Eickenberg, Michael
AU  - Belilovsky, Eugene
AB  - Understanding the inner working functionality of large-scale deep neural networks is challenging yet crucial in several high-stakes applications. Mechanistic inter- pretability is an emergent field that tackles this challenge, often by identifying human-understandable subgraphs in deep neural networks known as circuits. In vision-pretrained models, these subgraphs are usually interpreted by visualizing their node features through a popular technique called feature visualization. Recent works have analyzed the stability of different feature visualization types under the adversarial model manipulation framework. This paper starts by addressing limitations in existing works by proposing a novel attack called ProxPulse that simultaneously manipulates the two types of feature visualizations. Surprisingly, when analyzing these attacks under the umbrella of visual circuits, we find that visual circuits show some robustness to ProxPulse. We, therefore, introduce a new attack based on ProxPulse that unveils the manipulability of visual circuits, shedding light on their lack of robustness. The effectiveness of these attacks is validated using pre-trained AlexNet and ResNet-50 models on ImageNet.
DA  - 2024/06/03/
PY  - 2024
DO  - 10.48550/arXiv.2406.01365
DP  - arXiv.org
PB  - arXiv
ST  - From Feature Visualization to Visual Circuits
UR  - http://arxiv.org/abs/2406.01365
Y2  - 2024/07/14/22:07:15
L1  - https://arxiv.org/pdf/2406.01365.pdf
L2  - https://arxiv.org/abs/2406.01365
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - c-MBA: Adversarial Attack for Cooperative MARL Using Learned Dynamics Model
AU  - Pham, Nhan H.
AU  - Nguyen, Lam M.
AU  - Chen, Jie
AU  - Lam, Hoang Thanh
AU  - Das, Subhro
AU  - Weng, Lily
T2  - NeurIPS ML Safety Workshop
AB  - In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named \textbf{c-MBA}. Our proposed attack can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - c-MBA
UR  - https://openreview.net/forum?id=AFfKSfcF6Sv
Y2  - 2024/07/14/22:07:03
L1  - https://openreview.net/pdf?id=AFfKSfcF6Sv
ER  - 

TY  - GEN
TI  - Two Heads are Better than One: Towards Better Adversarial Robustness by Combining Transduction and Rejection
AU  - Palumbo, Nils
AU  - Guo, Yang
AU  - Wu, Xi
AU  - Chen, Jiefeng
AU  - Liang, Yingyu
AU  - Jha, Somesh
AB  - Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Tram\`er showed that, in the rejection-only case (no transduction), a strong rejection-solution can be turned into a strong (but computationally inefficient) non-rejection solution. This detector-to-classifier reduction has been mostly applied to give evidence that certain claims of strong selective-model solutions are susceptible, leaving the benefits of rejection unclear. On the other hand, a recent work by Goldwasser et al. showed that rejection combined with transduction can give provable guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA, which has been shown to be much more effective than AutoAttack against transduction), Goldwasser et al.'s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Theoretically, we show that a novel application of Tram\`er's classifier-to-detector technique in the transductive setting can give significantly improved sample-complexity for robust generalization. While our theoretical construction is computationally inefficient, it guides us to identify an efficient transductive algorithm to learn a selective model. Extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our solutions provide significantly better robust accuracy.
DA  - 2023/05/27/
PY  - 2023
DO  - 10.48550/arXiv.2305.17528
DP  - arXiv.org
PB  - arXiv
ST  - Two Heads are Better than One
UR  - http://arxiv.org/abs/2305.17528
Y2  - 2024/07/14/22:06:53
L1  - https://arxiv.org/pdf/2305.17528.pdf
L2  - https://arxiv.org/abs/2305.17528
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small
AU  - Wang, Kevin
AU  - Variengien, Alexandre
AU  - Conmy, Arthur
AU  - Shlegeris, Buck
AU  - Steinhardt, Jacob
AB  - Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.
DA  - 2022/11/01/
PY  - 2022
DO  - 10.48550/arXiv.2211.00593
DP  - arXiv.org
PB  - arXiv
ST  - Interpretability in the Wild
UR  - http://arxiv.org/abs/2211.00593
Y2  - 2024/07/14/22:06:12
L1  - https://arxiv.org/pdf/2211.00593.pdf
L2  - https://arxiv.org/abs/2211.00593
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety
AU  - Albrecht, Joshua
AU  - Kitanidis, Ellie
AU  - Fetterman, Abraham J.
AB  - Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly "super-human" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to "explain their reasoning" often leads to alarming justifications of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.
DA  - 2022/12/12/
PY  - 2022
DO  - 10.48550/arXiv.2212.06295
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.06295
Y2  - 2024/07/14/22:05:54
L1  - https://arxiv.org/pdf/2212.06295.pdf
L2  - https://arxiv.org/abs/2212.06295
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Boundary Adversarial Examples Against Adversarial Overfitting
AU  - Hameed, Muhammad Zaid
AU  - Buesser, Beat
AB  - Standard adversarial training approaches suffer from robust overfitting where the robust accuracy decreases when models are adversarially trained for too long. The origin of this problem is still unclear and conflicting explanations have been reported, i.e., memorization effects induced by large loss data or because of small loss data and growing differences in loss distribution of training samples as the adversarial training progresses. Consequently, several mitigation approaches including early stopping, temporal ensembling and weight perturbations on small loss data have been proposed to mitigate the effect of robust overfitting. However, a side effect of these strategies is a larger reduction in clean accuracy compared to standard adversarial training. In this paper, we investigate if these mitigation approaches are complimentary to each other in improving adversarial training performance. We further propose the use of helper adversarial examples that can be obtained with minimal cost in the adversarial example generation, and show how they increase the clean accuracy in the existing approaches without compromising the robust accuracy.
DA  - 2022/11/25/
PY  - 2022
DO  - 10.48550/arXiv.2211.14088
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.14088
Y2  - 2024/07/14/22:05:33
L1  - https://arxiv.org/pdf/2211.14088.pdf
L2  - https://arxiv.org/abs/2211.14088
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Can Active Sampling Reduce Causal Confusion in Offline Reinforcement Learning?
AU  - Gupta, Gunshi
AU  - Rudner, Tim G. J.
AU  - McAllister, Rowan Thomas
AU  - Gaidon, Adrien
AU  - Gal, Yarin
AB  - Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data. Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations. This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent. In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world. In this paper, we study causal confusion in offline reinforcement learning. We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment. To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation. We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling.
DA  - 2023/12/28/
PY  - 2023
DO  - 10.48550/arXiv.2312.17168
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.17168
Y2  - 2024/07/14/22:05:19
L1  - https://arxiv.org/pdf/2312.17168.pdf
L2  - https://arxiv.org/abs/2312.17168
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - System III: Learning with Domain Knowledge for Safety Constraints
AU  - Barez, Fazl
AU  - Hasanbieg, Hosien
AU  - Abbate, Alesandro
AB  - Reinforcement learning agents naturally learn from extensive exploration. Exploration is costly and can be unsafe in $\textit{safety-critical}$ domains. This paper proposes a novel framework for incorporating domain knowledge to help guide safe exploration and boost sample efficiency. Previous approaches impose constraints, such as regularisation parameters in neural networks, that rely on large sample sets and often are not suitable for safety-critical domains where agents should almost always avoid unsafe actions. In our approach, called $\textit{System III}$, which is inspired by psychologists' notions of the brain's $\textit{System I}$ and $\textit{System II}$, we represent domain expert knowledge of safety in form of first-order logic. We evaluate the satisfaction of these constraints via p-norms in state vector space. In our formulation, constraints are analogous to hazards, objects, and regions of state that have to be avoided during exploration. We evaluated the effectiveness of the proposed method on OpenAI's Gym and Safety-Gym environments. In all tasks, including classic Control and Safety Games, we show that our approach results in safer exploration and sample efficiency.
DA  - 2023/04/23/
PY  - 2023
DO  - 10.48550/arXiv.2304.11593
DP  - arXiv.org
PB  - arXiv
ST  - System III
UR  - http://arxiv.org/abs/2304.11593
Y2  - 2024/07/14/22:05:06
L1  - https://arxiv.org/pdf/2304.11593.pdf
L2  - https://arxiv.org/abs/2304.11593
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - All’s Well That Ends Well: Avoiding Side Effects with Distance-Impact Penalties
AU  - Griffin, Charlie
AU  - Skalse, Joar Max Viktor
AU  - Hammond, Lewis
AU  - Abate, Alessandro
T2  - NeurIPS ML Safety Workshop
AB  - Misspecifying the reward function of a reinforcement learning agent may cause catastrophic side effects. In this work, we investigate \textit{distance-impact penalties}: a general-purpose auxiliary reward based on a state-distance measure that captures, and thus can be used to penalise, side effects. We prove that the size of the penalty depends only on an agent's final impact on the environment. Distance-impact penalties are scalable, general, and immediately compatible with model-free algorithms. We analyse the sensitivity of an agent's behaviour to the choice of penalty, expanding results about reward-shaping, proving sufficient and necessary conditions for policy-optimality to be invariant to misspecification, and providing error bounds for optimal policies. Finally, we empirically investigate distance-impact penalties in a range of grid-world environments, demonstrating their ability to prevent side effects whilst permitting task completion.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - All’s Well That Ends Well
UR  - https://openreview.net/forum?id=3tgegVVh2j6
Y2  - 2024/07/14/22:04:52
L1  - https://openreview.net/pdf?id=3tgegVVh2j6
ER  - 

TY  - GEN
TI  - Unified Probabilistic Neural Architecture and Weight Ensembling Improves Model Robustness
AU  - Premchandar, Sumegha
AU  - Madireddy, Sandeep
AU  - Jantre, Sanket
AU  - Balaprakash, Prasanna
AB  - Robust machine learning models with accurately calibrated uncertainties are crucial for safety-critical applications. Probabilistic machine learning and especially the Bayesian formalism provide a systematic framework to incorporate robustness through the distributional estimates and reason about uncertainty. Recent works have shown that approximate inference approaches that take the weight space uncertainty of neural networks to generate ensemble prediction are the state-of-the-art. However, architecture choices have mostly been ad hoc, which essentially ignores the epistemic uncertainty from the architecture space. To this end, we propose a Unified probabilistic architecture and weight ensembling Neural Architecture Search (UraeNAS) that leverages advances in probabilistic neural architecture search and approximate Bayesian inference to generate ensembles form the joint distribution of neural network architectures and weights. The proposed approach showed a significant improvement both with in-distribution (0.86% in accuracy, 42% in ECE) CIFAR-10 and out-of-distribution (2.43% in accuracy, 30% in ECE) CIFAR-10-C compared to the baseline deterministic approach.
DA  - 2022/10/08/
PY  - 2022
DO  - 10.48550/arXiv.2210.04083
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.04083
Y2  - 2024/07/14/22:04:44
L1  - https://arxiv.org/pdf/2210.04083.pdf
L2  - https://arxiv.org/abs/2210.04083
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Adversarial Robustness of Vision Transformers
AU  - Shao, Rulin
AU  - Shi, Zhouxing
AU  - Yi, Jinfeng
AU  - Chen, Pin-Yu
AU  - Hsieh, Cho-Jui
AB  - Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.
DA  - 2022/11/02/
PY  - 2022
DO  - 10.48550/arXiv.2103.15670
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2103.15670
Y2  - 2024/07/14/22:04:16
L1  - https://arxiv.org/pdf/2103.15670.pdf
L2  - https://arxiv.org/abs/2103.15670
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Instance-Aware Observer Network for Out-of-Distribution Object Segmentation
AU  - Besnier, Victor
AU  - Bursuc, Andrei
AU  - Picard, David
AU  - Briot, Alexandre
AB  - Recent works on predictive uncertainty estimation have shown promising results on Out-Of-Distribution (OOD) detection for semantic segmentation. However, these methods struggle to precisely locate the point of interest in the image, i.e, the anomaly. This limitation is due to the difficulty of finegrained prediction at the pixel level. To address this issue, we build upon the recent ObsNet approach by providing object instance knowledge to the observer. We extend ObsNet by harnessing an instance-wise mask prediction. We use an additional, class agnostic, object detector to filter and aggregate observer predictions. Finally, we predict an unique anomaly score for each instance in the image. We show that our proposed method accurately disentangles in-distribution objects from OOD objects on three datasets.
DA  - 2022/08/29/
PY  - 2022
DO  - 10.48550/arXiv.2207.08782
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.08782
Y2  - 2024/07/14/22:03:21
L1  - https://arxiv.org/pdf/2207.08782.pdf
L2  - https://arxiv.org/abs/2207.08782
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs
AU  - Korkmaz, Ezgi
AB  - The use of deep neural networks as function approximators has led to striking progress for reinforcement learning algorithms and applications. Yet the knowledge we have on decision boundary geometry and the loss landscape of neural policies is still quite limited. In this paper we propose a framework to investigate the decision boundary and loss landscape similarities across states and across MDPs. We conduct experiments in various games from Arcade Learning Environment, and discover that high sensitivity directions for neural policies are correlated across MDPs. We argue that these high sensitivity directions support the hypothesis that non-robust features are shared across training environments of reinforcement learning agents. We believe our results reveal fundamental properties of the environments used in deep reinforcement learning training, and represent a tangible step towards building robust and reliable deep reinforcement learning agents.
DA  - 2021/12/16/
PY  - 2021
DO  - 10.48550/arXiv.2112.09025
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2112.09025
Y2  - 2024/07/14/22:03:12
L1  - https://arxiv.org/pdf/2112.09025.pdf
L2  - https://arxiv.org/abs/2112.09025
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - A Multi-Level Framework for the AI Alignment Problem
AU  - Hou, Betty Li
AU  - Green, Brian Patrick
AB  - AI alignment considers how we can encode AI systems in a way that is compatible with human values. The normative side of this problem asks what moral values or principles, if any, we should encode in AI. To this end, we present a framework to consider the question at four levels: Individual, Organizational, National, and Global. We aim to illustrate how AI alignment is made up of value alignment problems at each of these levels, where values at each level affect the others and effects can flow in either direction. We outline key questions and considerations of each level and demonstrate an application of this framework to the topic of AI content moderation.
DA  - 2023/01/09/
PY  - 2023
DO  - 10.48550/arXiv.2301.03740
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2301.03740
Y2  - 2024/07/14/22:02:36
L1  - https://arxiv.org/pdf/2301.03740.pdf
L2  - https://arxiv.org/abs/2301.03740
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Steering Large Language Models using APE
AU  - Zhou, Yongchao
AU  - Muresanu, Andrei Ioan
AU  - Han, Ziwen
AU  - Paster, Keiran
AU  - Pitis, Silviu
AU  - Chan, Harris
AU  - Ba, Jimmy
T2  - NeurIPS ML Safety Workshop
AB  - By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model. Due to the lack of knowledge of how LLMs work, most effective prompts have been handcrafted by humans through a demanding trial and error process. To reduce the human effort involved in this alignment process, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. We treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate how well the selected instruction can steer the model to desired behavior, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. Moreover, we show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=JjvNzMOiBEp
Y2  - 2024/07/14/22:02:22
L1  - https://openreview.net/pdf?id=JjvNzMOiBEp
ER  - 

TY  - GEN
TI  - Can Differentiable Decision Trees Enable Interpretable Reward Learning from Human Feedback?
AU  - Kalra, Akansha
AU  - Brown, Daniel S.
AB  - Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for capturing human intent to alleviate the challenges of hand-crafting the reward values. Despite the increasing interest in RLHF, most works learn black box reward functions that while expressive are difficult to interpret and often require running the whole costly process of RL before we can even decipher if these frameworks are actually aligned with human preferences. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs). Our experiments across several domains, including CartPole, Visual Gridworld environments and Atari games, provide evidence that the tree structure of our learned reward function is useful in determining the extent to which the reward function is aligned with human preferences. We also provide experimental evidence that not only shows that reward DDTs can often achieve competitive RL performance when compared with larger capacity deep neural network reward functions but also demonstrates the diagnostic utility of our framework in checking alignment of learned reward functions. We also observe that the choice between soft and hard (argmax) output of reward DDT reveals a tension between wanting highly shaped rewards to ensure good RL performance, while also wanting simpler, more interpretable rewards. Videos and code, are available at: https://sites.google.com/view/ddt-rlhf
DA  - 2024/06/24/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2306.13004
Y2  - 2024/07/14/22:02:07
L1  - https://arxiv.org/pdf/2306.13004
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Spectral Robustness Analysis of Deep Imitation Learning
AU  - Korkmaz, Ezgi
T2  - NeurIPS ML Safety Workshop
AB  - Deep reinforcement learning algorithms enabled learning functioning policies in MDPs with complex state representations. Following these advancements deep reinforcement learning polices have been deployed in many diverse settings. However, a line of research argued that in certain settings building a reward function can be more complicated than learning it. Hence, several studies proposed different methods to learn a reward function by observing trajectories of a functioning policy (i.e. inverse reinforcement learning). Following this line of research several studies proposed to directly learn a functioning policy by solely observing trajectories of an expert (i.e. imitation learning). In this paper, we propose a novel method to analyze the spectral robustness of deep neural policies. We conduct several experiments in the Arcade Learning Environment, and demonstrate that simple vanilla trained deep reinforcement learning policies are more robust than deep imitation learning policies. We believe that our method provides a comprehensive analysis on the policy robustness and can help in understanding the fundamental properties of different training techniques.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=RF97w2nVtQJ
Y2  - 2024/07/14/22:01:46
L1  - https://openreview.net/pdf?id=RF97w2nVtQJ
ER  - 

TY  - GEN
TI  - An Efficient Framework for Monitoring Subgroup Performance of Machine Learning Systems
AU  - Ha, Huong
AB  - Monitoring machine learning systems post deployment is critical to ensure the reliability of the systems. Particularly importance is the problem of monitoring the performance of machine learning systems across all the data subgroups (subpopulations). In practice, this process could be prohibitively expensive as the number of data subgroups grows exponentially with the number of input features, and the process of labelling data to evaluate each subgroup's performance is costly. In this paper, we propose an efficient framework for monitoring subgroup performance of machine learning systems. Specifically, we aim to find the data subgroup with the worst performance using a limited number of labeled data. We mathematically formulate this problem as an optimization problem with an expensive black-box objective function, and then suggest to use Bayesian optimization to solve this problem. Our experimental results on various real-world datasets and machine learning systems show that our proposed framework can retrieve the worst-performing data subgroup effectively and efficiently.
DA  - 2022/12/16/
PY  - 2022
DO  - 10.48550/arXiv.2212.08312
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.08312
Y2  - 2024/07/14/22:01:37
L1  - https://arxiv.org/pdf/2212.08312.pdf
L2  - https://arxiv.org/abs/2212.08312
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - On Outlier Exposure with Generative Models
AU  - Kirchheim, Konstantin
AU  - Ortmeier, Frank
T2  - NeurIPS ML Safety Workshop
AB  - While Outlier Exposure reliably increases the performance of Out-of-Distribution detectors, it requires a set of available outliers during training. In this paper, we propose Generative Outlier Exposure (GOE), which alleviates the need for available outliers by using generative models to sample synthetic outliers from low-density regions of the data distribution. The approach requires no modification of the generator, works on image and text data, and can be used with pre-trained models. We demonstrate the effectiveness of generated outliers on several image and text datasets, including ImageNet.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=SU7OAfhc8OM
Y2  - 2024/07/14/22:01:23
L1  - https://openreview.net/pdf?id=SU7OAfhc8OM
ER  - 

TY  - JOUR
TI  - Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness
AU  - Korkmaz, Ezgi
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - Learning from raw high dimensional data via interaction with a given environment has been effectively achieved through the utilization of deep neural networks. Yet the observed degradation in policy performance caused by imperceptible worst-case policy dependent translations along high sensitivity directions (i.e. adversarial perturbations) raises concerns on the robustness of deep reinforcement learning policies. In our paper, we show that these high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box setting. Furthermore, we show that vanilla training techniques intriguingly result in learning more robust policies compared to the policies learnt via the state-of-the-art adversarial training techniques. We believe our work lays out intriguing properties of the deep reinforcement learning policy manifold and our results can help to build robust and generalizable deep reinforcement learning policies.
DA  - 2023/06/26/
PY  - 2023
DO  - 10.1609/aaai.v37i7.26009
DP  - DOI.org (Crossref)
VL  - 37
IS  - 7
SP  - 8369
EP  - 8377
J2  - AAAI
LA  - en
SN  - 2374-3468, 2159-5399
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/26009
Y2  - 2024/07/14/22:01:13
L1  - https://arxiv.org/pdf/2301.07487
ER  - 

TY  - CONF
TI  - What 'Out-of-distribution' Is and Is Not
AU  - Farquhar, Sebastian
AU  - Gal, Yarin
T2  - NeurIPS ML Safety Workshop
AB  - Researchers want to generalize robustly to ‘out-of-distribution’ (OOD) data. Unfortunately, this term is used ambiguously causing confusion and creating risk—people might believe they have made progress on OOD data and not realize this progress only holds in limited cases. We critique a standard definition of OOD—difference-in-distribution—and then disambiguate four meaningful types of OOD data: transformed-distributions, related-distributions, complement-distributions, and synthetic-distributions. We describe how existing OOD datasets, evaluations, and techniques fit into this framework. We provide a template for researchers to carefully present the scope of distribution shift considered in their work.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=XCS_zBHQA2i
Y2  - 2024/07/14/22:00:48
L1  - https://openreview.net/pdf?id=XCS_zBHQA2i
ER  - 

TY  - GEN
TI  - Learning Transferable Adversarial Robust Representations via Multi-view Consistency
AU  - Kim, Minseon
AU  - Ha, Hyeonjeong
AU  - Lee, Dong Bok
AU  - Hwang, Sung Ju
AB  - Despite the success on few-shot learning problems, most meta-learned models only focus on achieving good performance on clean examples and thus easily break down when given adversarially perturbed samples. While some recent works have shown that a combination of adversarial learning and meta-learning could enhance the robustness of a meta-learner against adversarial attacks, they fail to achieve generalizable adversarial robustness to unseen domains and tasks, which is the ultimate goal of meta-learning. To address this challenge, we propose a novel meta-adversarial multi-view representation learning framework with dual encoders. Specifically, we introduce the discrepancy across the two differently augmented samples of the same data instance by first updating the encoder parameters with them and further imposing a novel label-free adversarial attack to maximize their discrepancy. Then, we maximize the consistency across the views to learn transferable robust representations across domains and tasks. Through experimental validation on multiple benchmarks, we demonstrate the effectiveness of our framework on few-shot learning tasks from unseen domains, achieving over 10\% robust accuracy improvements against previous adversarial meta-learning baselines.
DA  - 2023/10/26/
PY  - 2023
DO  - 10.48550/arXiv.2210.10485
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.10485
Y2  - 2024/07/14/22:00:39
L1  - https://arxiv.org/pdf/2210.10485.pdf
L2  - https://arxiv.org/abs/2210.10485
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Alignment as a Dynamic Process
AU  - Font-Reaulx, Paul de
T2  - NeurIPS ML Safety Workshop
AB  - Most learning AIs today have exogenously given and fixed aims which they gradually learn to optimize for. It has been an assumption in alignment research that artificial general intelligences of the kind that could pose an X-risk would too. On this assumption, value alignment becomes the task of finding the right set of aims before we allow the agent to act. However, an agent can also have aims that fundamentally change during their lifetime. The task of aligning such agents is not one of specifying a set of aims, but of designing a meta-function that guides the agent’s developing aims to an equilibrium that produces behaviour aligned with our human values. If artificial general intelligences would possess such dynamic aims, then this has significant implications for the kind of alignment research we should conduct today. In this paper, I argue that there is a substantial probability that artificial general intelligences would have such dynamic aims, and in response I articulate an agenda for dynamic alignment research.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=-6jortffIq
Y2  - 2024/07/14/22:00:15
L1  - https://openreview.net/pdf?id=-6jortffIq
ER  - 

TY  - GEN
TI  - Out-of-Distribution Detection with Class Ratio Estimation
AU  - Zhang, Mingtian
AU  - Zhang, Andi
AU  - Xiao, Tim Z.
AU  - Sun, Yitong
AU  - McDonagh, Steven
AB  - Density-based Out-of-distribution (OOD) detection has recently been shown unreliable for the task of detecting OOD images. Various density ratio based approaches achieve good empirical performance, however methods typically lack a principled probabilistic modelling explanation. In this work, we propose to unify density ratio based methods under a novel framework that builds energy-based models and employs differing base distributions. Under our framework, the density ratio can be viewed as the unnormalized density of an implicit semantic distribution. Further, we propose to directly estimate the density ratio of a data sample through class ratio estimation. We report competitive results on OOD image problems in comparison with recent work that alternatively requires training of deep generative models for the task. Our approach enables a simple and yet effective path towards solving the OOD detection problem.
DA  - 2022/06/08/
PY  - 2022
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2206.03955
Y2  - 2024/07/14/21:59:56
L1  - https://arxiv.org/pdf/2206.03955
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Adversarial poisoning attacks on reinforcement learning-driven energy pricing
AU  - Gunn, Sam
AU  - Jang, Doseok
AU  - Paradise, Orr
AU  - Spangher, Lucas
AU  - Spanos, Costas J.
T3  - BuildSys '22
AB  - Complex controls are increasingly common in power systems. Reinforcement learning (RL) has emerged as a strong candidate for implementing various controllers. One common use of RL in this context is for prosumer pricing aggregations, where prosumers consist of buildings with both solar generation and energy storage. Specifically, supply and demand data serve as the observation space for many microgrid controllers acting based on a policy passed from a central RL agent. Each controller outputs an action space consisting of hourly "buy" and "sell" prices for energy throughout the day; in turn, each prosumer can choose whether to transact with the RL agent or the utility. The RL agent, who is learning online, is rewarded through its ability to generate a profit.We ask: what happens when some of the microgrid controllers are compromised by a malicious entity? We demonstrate a novel attack in RL and a simple defense against the attack. Our attack perturbs each trajectory to reverse the direction of the estimated gradient. We demonstrate that if data from a small fraction of microgrid controllers is adversarially perturbed, the learning of the RL agent can be significantly slowed. With larger perturbations, the RL aggregator can be manipulated to learn a catastrophic pricing policy that causes the RL agent to operate at a loss. Other environmental characteristics are worsened too: prosumers face higher energy costs, use their batteries less, and suffer from higher peak demand when the pricing aggregator is adversarially poisoned.We address this vulnerability with a "defense" module; i.e., a "robustification" of RL algorithms against this attack. Our defense identifies the trajectories with the largest influence on the gradient and removes them from the training data. It is computationally light and reasonable to include in any RL algorithm.
C1  - New York, NY, USA
C3  - Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation
DA  - 2022/12/08/
PY  - 2022
DO  - 10.1145/3563357.3564075
DP  - ACM Digital Library
SP  - 262
EP  - 265
PB  - Association for Computing Machinery
SN  - 978-1-4503-9890-9
UR  - https://doi.org/10.1145/3563357.3564075
Y2  - 2024/07/14/
ER  - 

TY  - CONF
TI  - What you see is what you get: principled deep learning via distributional generalization
AU  - Kulynych, Bogdan
AU  - Yang, Yao-Yuan
AU  - Yu, Yaodong
AU  - Błasiok, Jarosław
AU  - Nakkiran, Preetum
T3  - NIPS '22
AB  - Having similar behavior at training time and test time—what we call a "What You See Is What You Get" (WYSIWYG) property—is desirable in machine learning. Models trained with standard stochastic gradient descent (SGD), however, do not necessarily have this property, as their complex behaviors such as robustness or subgroup performance can differ drastically between training and test time. In contrast, we show that Differentially-Private (DP) training provably ensures the high-level WYSIWYG property, which we quantify using a notion of distributional generalization. Applying this connection, we introduce new conceptual tools for designing deep-learning methods by reducing generalization concerns to optimization ones: to mitigate unwanted behavior at test time, it is provably sufficient to mitigate this behavior on the training data. By applying this novel design principle, which bypasses "pathologies" of SGD, we construct simple algorithms that are competitive with SOTA in several distributional-robustness applications, significantly improve the privacy vs. disparate impact trade-off of DP-SGD, and mitigate robust overfitting in adversarial training. Finally, we also improve on theoretical bounds relating DP, stability, and distributional generalization.
C1  - Red Hook, NY, USA
C3  - Proceedings of the 36th International Conference on Neural Information Processing Systems
DA  - 2024/04/03/
PY  - 2024
DP  - ACM Digital Library
SP  - 2168
EP  - 2183
PB  - Curran Associates Inc.
SN  - 978-1-71387-108-8
ST  - What you see is what you get
Y2  - 2024/07/14/
ER  - 

TY  - CONF
TI  - Do Domain Generalization Methods Generalize Well?
AU  - Mehra, Akshay
AU  - Kailkhura, Bhavya
AU  - Chen, Pin-Yu
AU  - Hamm, Jihun
T2  - NeurIPS ML Safety Workshop
AB  - Domain Generalization (DG) methods use data from multiple related source domains to learn models whose performance does not degrade on unseen domains at test time. Many DG algorithms rely on reducing the divergence between the source distributions in a representation space to potentially align unseen domains close to the sources. These algorithms are motivated by the analytical works that explain generalization to unseen domains based on their distributional distance (e.g., Wasserstein distance) to the sources. However, we show that the accuracy of a DG model varies significantly on unseen domains equidistant from the sources in the learned representation space. This makes it hard to gauge the generalization performance of DG models only based on their performance on benchmark datasets. Thus, we study the worst-case loss of a DG model at a particular distance from the sources and propose an evaluation methodology based on distributionally robust optimization that efficiently computes the worst-case loss on all distributions within a Wasserstein ball around the sources. Our results show that models trained with popular DG methods incur a high worst-case loss even close to the sources which show their lack of generalization to unseen domains. Moreover, we observe a large gap between the worst-case and the empirical losses of distributions at the same distance, showing the performance of the DG models on benchmark datasets is not representative of their performance on unseen domains. Thus, our (target) data-independent and worst-case loss-based methodology highlights the poor generalization performance of current DG models and provides insights beyond empirical evaluation on benchmark datasets for improving these models.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=SRWIQ0Yl53m
Y2  - 2024/07/14/21:59:00
L1  - https://openreview.net/pdf?id=SRWIQ0Yl53m
ER  - 

TY  - GEN
TI  - Broken Neural Scaling Laws
AU  - Caballero, Ethan
AU  - Gupta, Kshitij
AU  - Rish, Irina
AU  - Krueger, David
AB  - We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures & for each of various tasks within a large & diverse set of upstream & downstream tasks, in zero-shot, prompted, & finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, "emergent phase transitions", arithmetic, supervised learning, unsupervised/self-supervised learning, & reinforcement learning (single agent & multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models & extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent & the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws
DA  - 2023/07/23/
PY  - 2023
DO  - 10.48550/arXiv.2210.14891
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.14891
Y2  - 2024/07/14/21:58:47
L1  - https://arxiv.org/pdf/2210.14891.pdf
L2  - https://arxiv.org/abs/2210.14891
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts
AU  - Jang, Joel
AU  - Ye, Seonghyeon
AU  - Seo, Minjoon
AB  - Previous work has shown that there exists a scaling law between the size of Language Models (LMs) and their zero-shot performance on different downstream NLP tasks. In this work, we show that this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts. By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions. We provide the code and the datasets to explore negated prompts at https://github.com/joeljang/negated-prompts-for-llms
DA  - 2022/09/26/
PY  - 2022
DO  - 10.48550/arXiv.2209.12711
DP  - arXiv.org
PB  - arXiv
ST  - Can Large Language Models Truly Understand Prompts?
UR  - http://arxiv.org/abs/2209.12711
Y2  - 2024/07/14/21:58:30
L1  - https://arxiv.org/pdf/2209.12711.pdf
L2  - https://arxiv.org/abs/2209.12711
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries
AU  - Wen, Yuxin
AU  - Bansal, Arpit
AU  - Kazemi, Hamid
AU  - Borgnia, Eitan
AU  - Goldblum, Micah
AU  - Geiping, Jonas
AU  - Goldstein, Tom
AB  - As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine.
DA  - 2023/06/01/
PY  - 2023
DO  - 10.48550/arXiv.2210.10750
DP  - arXiv.org
PB  - arXiv
ST  - Canary in a Coalmine
UR  - http://arxiv.org/abs/2210.10750
Y2  - 2024/07/14/21:58:17
L1  - https://arxiv.org/pdf/2210.10750.pdf
L2  - https://arxiv.org/abs/2210.10750
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Effective Targeted Attacks for Adversarial Self-Supervised Learning
AU  - Kim, Minseon
AU  - Ha, Hyeonjeong
AU  - Son, Sooel
AU  - Hwang, Sung Ju
AB  - Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given instance towards the selected target. Our method demonstrates significant enhancements in robustness when applied to non-contrastive SSL frameworks, and less but consistent robustness improvements with contrastive SSL frameworks, on the benchmark datasets.
DA  - 2023/10/26/
PY  - 2023
DO  - 10.48550/arXiv.2210.10482
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.10482
Y2  - 2024/07/14/21:58:05
L1  - https://arxiv.org/pdf/2210.10482.pdf
L2  - https://arxiv.org/abs/2210.10482
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A Unifying Framework for Online Optimization with Long-Term Constraints
AU  - Castiglioni, Matteo
AU  - Celli, Andrea
AU  - Marchesi, Alberto
AU  - Romano, Giulia
AU  - Gatti, Nicola
AB  - We study online learning problems in which a decision maker has to take a sequence of decisions subject to $m$ long-term constraints. The goal of the decision maker is to maximize their total reward, while at the same time achieving small cumulative constraints violation across the $T$ rounds. We present the first best-of-both-world type algorithm for this general class of problems, with no-regret guarantees both in the case in which rewards and constraints are selected according to an unknown stochastic model, and in the case in which they are selected at each round by an adversary. Our algorithm is the first to provide guarantees in the adversarial setting with respect to the optimal fixed strategy that satisfies the long-term constraints. In particular, it guarantees a $\rho/(1+\rho)$ fraction of the optimal reward and sublinear regret, where $\rho$ is a feasibility parameter related to the existence of strictly feasible solutions. Our framework employs traditional regret minimizers as black-box components. Therefore, by instantiating it with an appropriate choice of regret minimizers it can handle the full-feedback as well as the bandit-feedback setting. Moreover, it allows the decision maker to seamlessly handle scenarios with non-convex rewards and constraints. We show how our framework can be applied in the context of budget-management mechanisms for repeated auctions in order to guarantee long-term constraints that are not packing (e.g., ROI constraints).
DA  - 2022/09/15/
PY  - 2022
DO  - 10.48550/arXiv.2209.07454
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.07454
Y2  - 2024/07/14/21:57:54
L1  - https://arxiv.org/pdf/2209.07454.pdf
L2  - https://arxiv.org/abs/2209.07454
KW  - Computer Science - Machine Learning
KW  - Mathematics - Optimization and Control
ER  - 

TY  - CONF
TI  - A General Framework for Safe Decision Making: A Convex Duality Approach
AU  - Bernasconi, Martino
AU  - Cacciamani, Federico
AU  - Gatti, Nicola
AU  - Trovò, Francesco
T2  - NeurIPS ML Safety Workshop
AB  - We study the problem of online interaction in general decision making problems, where the objective is not only to find optimal strategies, but also to satisfy some safety guarantees, expressed in terms of costs accrued. We propose a theoretical framework to address such problems and present BAN-SOLO, a UCB-like algorithm that, in an online interaction with an unknown environment, attains sublinear regret of order O(T^{1/2}) and plays safely with high probability at each iteration. At its core, BAN-SOLO relies on tools from convex duality to manage environment exploration while satisfying the safety constraints imposed by the problem.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - A General Framework for Safe Decision Making
UR  - https://openreview.net/forum?id=qHc5B5iEaSx
Y2  - 2024/07/14/21:57:36
L1  - https://openreview.net/pdf?id=qHc5B5iEaSx
ER  - 

TY  - GEN
TI  - Mitigating Dataset Bias by Using Per-sample Gradient
AU  - Ahn, Sumyeong
AU  - Kim, Seongyoon
AU  - Yun, Se-young
AB  - The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes having a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels involving human or empirical correlation metrics (e.g., training loss). However, such metrics require human costs or have insufficient theoretical explanation. In this study, we propose a debiasing algorithm, called PGD (Per-sample Gradient-based Debiasing), that comprises three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various synthetic and real-world datasets, the proposed method showed state-of-the-art accuracy for a the classification task. Furthermore, we describe theoretical understandings about how PGD can mitigate dataset bias.
DA  - 2023/02/10/
PY  - 2023
DO  - 10.48550/arXiv.2205.15704
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.15704
Y2  - 2024/07/14/21:57:20
L1  - https://arxiv.org/pdf/2205.15704.pdf
L2  - https://arxiv.org/abs/2205.15704
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Identification of the Adversary from a Single Adversarial Example
AU  - Cheng, Minhao
AU  - Min, Rui
AU  - Sun, Haochen
AU  - Chen, Pin-Yu
T2  - International Conference on Machine Learning
AB  - Deep neural networks have been shown vulnerable to adversarial examples. Even though many defense methods have been proposed to enhance the robustness, it is still a long way toward providing an attack-free method to build a trustworthy machine learning system. In this paper, instead of enhancing the robustness, we take the investigator’s perspective and propose a new framework to trace the first compromised model copy in a forensic investigation manner. Specifically, we focus on the following setting: the machine learning service provider provides model copies for a set of customers. However, one of the customers conducted adversarial attacks to fool the system. Therefore, the investigator’s objective is to identify the first compromised copy by collecting and analyzing evidence from only available adversarial examples. To make the tracing viable, we design a random mask watermarking mechanism to differentiate adversarial examples from different copies. First, we propose a tracing approach in the data-limited case where the original example is also available. Then, we design a data-free approach to identify the adversary without accessing the original example. Finally, the effectiveness of our proposed framework is evaluated by extensive experiments with different model architectures, adversarial attacks, and datasets.
C3  - Proceedings of the 40th International Conference on Machine Learning
DA  - 2023/07/03/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 5472
EP  - 5484
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v202/cheng23c.html
Y2  - 2024/07/14/21:57:09
L1  - https://proceedings.mlr.press/v202/cheng23c/cheng23c.pdf
ER  - 

TY  - GEN
TI  - Visual Prompting for Adversarial Robustness
AU  - Chen, Aochuan
AU  - Lorenz, Peter
AU  - Yao, Yuguang
AU  - Chen, Pin-Yu
AU  - Liu, Sijia
AB  - In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at testing time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at testing time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1X standard accuracy gain and 2X robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42X inference time speedup.
DA  - 2023/04/30/
PY  - 2023
DO  - 10.48550/arXiv.2210.06284
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.06284
Y2  - 2024/07/14/21:56:56
L1  - https://arxiv.org/pdf/2210.06284.pdf
L2  - https://arxiv.org/abs/2210.06284
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Mechanistic Mode Connectivity
AU  - Lubana, Ekdeep Singh
AU  - Bigelow, Eric J.
AU  - Dick, Robert P.
AU  - Krueger, David
AU  - Tanaka, Hidenori
T2  - International Conference on Machine Learning
AB  - We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model’s mechanisms, e.g., fine-tuning can fail to eliminate a model’s reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model’s mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model’s reliance on spurious attributes.
C3  - Proceedings of the 40th International Conference on Machine Learning
DA  - 2023/07/03/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 22965
EP  - 23004
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v202/lubana23a.html
Y2  - 2024/07/14/21:56:44
L1  - https://proceedings.mlr.press/v202/lubana23a/lubana23a.pdf
ER  - 

TY  - CONF
TI  - Deceiving the CKA Similarity Measure in Deep Learning
AU  - Davari, MohammadReza
AU  - Horoi, Stefan
AU  - Natik, Amine
AU  - Lajoie, Guillaume
AU  - Wolf, Guy
AU  - Belilovsky, Eugene
T2  - NeurIPS ML Safety Workshop
AB  - Understanding the behaviour of trained deep neural networks is a critical step in allowing reliable deployment of these networks in critical applications. One direction for obtaining insights on neural networks is through comparison of their internal representations. Comparing neural representations in neural networks is thus a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network's different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of conclusions about similarity and dissimilarity of these various representations have been made using CKA. In this work we present an analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation of CKA sensitivity to outliers and to transformations that preserve the linear separability of the data, an important generalization attribute. Finally we propose an optimization-based approach for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=hITONWhDIIJ
Y2  - 2024/07/14/21:56:24
L1  - https://openreview.net/pdf?id=hITONWhDIIJ
ER  - 

TY  - GEN
TI  - Diagnosing and Rectifying Vision Models using Language
AU  - Zhang, Yuhui
AU  - HaoChen, Jeff Z.
AU  - Huang, Shih-Cheng
AU  - Wang, Kuan-Chieh
AU  - Zou, James
AU  - Yeung, Serena
AB  - Recent multi-modal contrastive learning models have demonstrated the ability to learn an embedding space suitable for building strong vision classifiers, by leveraging the rich information in large-scale image-caption datasets. Our work highlights a distinct advantage of this multi-modal embedding space: the ability to diagnose vision classifiers through natural language. The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality. On a range of image datasets with known error slices, we demonstrate that our method can effectively identify the error slices and influential attributes, and can further use language to rectify failure modes of the classifier.
DA  - 2023/02/08/
PY  - 2023
DO  - 10.48550/arXiv.2302.04269
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.04269
Y2  - 2024/07/14/21:56:13
L1  - https://arxiv.org/pdf/2302.04269.pdf
L2  - https://arxiv.org/abs/2302.04269
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
AU  - Di, Jimmy Z.
AU  - Douglas, Jack
AU  - Acharya, Jayadev
AU  - Kamath, Gautam
AU  - Sekhari, Ayush
AB  - We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.
DA  - 2022/12/20/
PY  - 2022
DO  - 10.48550/arXiv.2212.10717
DP  - arXiv.org
PB  - arXiv
ST  - Hidden Poison
UR  - http://arxiv.org/abs/2212.10717
Y2  - 2024/07/14/21:55:54
L1  - https://arxiv.org/pdf/2212.10717.pdf
L2  - https://arxiv.org/abs/2212.10717
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - JOUR
TI  - Smoothed-SGDmax: A Stability-Inspired Algorithm to Improve Adversarial Generalization
AU  - Xiao, Jiancong
AU  - Zhang, Jiawei
AU  - Luo, Zhi-Quan
AU  - Ozdaglar, Asuman E.
AB  - Unlike standard training, deep neural networks can suffer from serious overfitting problems in adversarial settings, which is studied extensively by empirical papers. Recent research (e.g., Xing et al. (2021); Xiao et al. (2022)) show that SGDmax-based adversarial training algorithms with $1/s(T)$ training loss incurs a stability-based generalization bound in $\Theta(c+s(T)/n)$. Here $T$ is the number of iterations, $n$ is the number of samples, $s(T)\rightarrow \infty$ as $T\rightarrow \infty$, and $c$ is a $n$-independent term. This reveals that adversarial training can have nonvanishing generalization errors even if the sample size $n$ goes to infinity. A natural question arises: can we eliminate the nonvanishing term $c$ by designing a more generalizable algorithm? We give an affirmative answer in this paper. First, by an adaptation of information-theoretical lower bound on the complexity of solving Lipschitz-convex problems using randomized algorithms, we show that a minimax lower bound for adversarial generalization gap is $\Omega(s(T)/n)$ given training loss $1/s(T)$. This implies that SGDmax does not achieve the lower bound. Next, by observing that the nonvanishing generalization error term for SGDmax comes from the non-smoothness of the adversarial loss function, we employ a smoothing technique to smooth the adversarial loss function. Based on the smoothed loss function, we design a smoothed SGDmax algorithm achieving generalization bound $\mathcal{O}(s(T)/n)$, which matches the minimax lower bound. Experimentally, we show that our algorithm improves adversarial generalization on common datasets.
DA  - 2022/09/29/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - Smoothed-SGDmax
UR  - https://openreview.net/forum?id=O0sS_cujvV0
Y2  - 2024/07/14/21:55:13
L1  - https://openreview.net/pdf?id=O0sS_cujvV0
ER  - 

TY  - GEN
TI  - Part-Based Models Improve Adversarial Robustness
AU  - Sitawarin, Chawin
AU  - Pongmala, Kornrapat
AU  - Chen, Yizheng
AU  - Carlini, Nicholas
AU  - Wagner, David
AB  - We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.
DA  - 2023/03/08/
PY  - 2023
DO  - 10.48550/arXiv.2209.09117
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.09117
Y2  - 2024/07/14/21:55:04
L1  - https://arxiv.org/pdf/2209.09117.pdf
L2  - https://arxiv.org/abs/2209.09117
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - A Deep Dive into Dataset Imbalance and Bias in Face Identification
AU  - Cherepanova, Valeriia
AU  - Reich, Steven
AU  - Dooley, Samuel
AU  - Souri, Hossein
AU  - Dickerson, John
AU  - Goldblum, Micah
AU  - Goldstein, Tom
T2  - AIES '23: AAAI/ACM Conference on AI, Ethics, and Society
AB  - As the deployment of automated face recognition (FR) systems proliferates, bias in these systems is not just an academic question, but a matter of public concern. Media portrayals often center imbalance as the main source of bias, i.e., that FR models perform worse on images of non-white people or women because these demographic groups are underrepresented in training data. Recent academic research paints a more nuanced picture of this relationship. However, previous studies of data imbalance in FR have focused exclusively on the face verification setting, while the face identification setting has been largely ignored, despite being deployed in sensitive applications such as law enforcement. This is an unfortunate omission, as ‘imbalance’ is a more complex matter in identification; imbalance may arise in not only the training data, but also the testing data, and furthermore may affect the proportion of identities belonging to each demographic group or the number of images belonging to each identity. In this work, we address this gap in the research by thoroughly exploring the effects of each kind of imbalance possible in face identification, and discuss other factors which may impact bias in this setting.
C1  - Montr\'{e}al QC Canada
C3  - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2023/08/08/
PY  - 2023
DO  - 10.1145/3600211.3604691
DP  - DOI.org (Crossref)
SP  - 229
EP  - 247
LA  - en
PB  - ACM
SN  - 9798400702310
UR  - https://dl.acm.org/doi/10.1145/3600211.3604691
Y2  - 2024/07/14/21:54:31
L1  - https://dl.acm.org/doi/pdf/10.1145/3600211.3604691
KW  - data imbalance
KW  - face recognition
KW  - fairness
KW  - neural networks
ER  - 

TY  - GEN
TI  - REAP: A Large-Scale Realistic Adversarial Patch Benchmark
AU  - Hingun, Nabeel
AU  - Sitawarin, Chawin
AU  - Li, Jerry
AU  - Wagner, David
AB  - Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto the sign. Using our benchmark, we perform the first large-scale assessments of adversarial patch attacks under realistic conditions. Our experiments suggest that adversarial patch attacks may present a smaller threat than previously believed and that the success rate of an attack on simpler digital simulations is not predictive of its actual effectiveness in practice. We release our benchmark publicly at https://github.com/wagner-group/reap-benchmark.
DA  - 2023/08/18/
PY  - 2023
DO  - 10.48550/arXiv.2212.05680
DP  - arXiv.org
PB  - arXiv
ST  - REAP
UR  - http://arxiv.org/abs/2212.05680
Y2  - 2024/07/14/21:53:31
L1  - https://arxiv.org/pdf/2212.05680.pdf
L2  - https://arxiv.org/abs/2212.05680
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - A Cooperative Reinforcement Learning Environment for Detecting and Penalizing Betrayal
AU  - Pittaras, Nikiforos
AB  - In this paper we present a Reinforcement Learning environment that leverages agent cooperation and communication, aimed at detection, learning and ultimately penalizing betrayal patterns that emerge in the behavior of self-interested agents. We provide a description of game rules, along with interesting cases of betrayal and trade-offs that arise. Preliminary experimental investigations illustrate a) betrayal emergence, b) deceptive agents outperforming honest baselines and b) betrayal detection based on classification of behavioral features, which surpasses probabilistic detection baselines. Finally, we propose approaches for penalizing betrayal, list directions for future work and suggest interesting extensions of the environment towards capturing and exploring increasingly complex patterns of social interactions.
DA  - 2022/10/23/
PY  - 2022
DO  - 10.48550/arXiv.2210.12841
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.12841
Y2  - 2024/07/14/21:53:12
L1  - https://arxiv.org/pdf/2210.12841.pdf
L2  - https://arxiv.org/abs/2210.12841
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Adversarial Attacks on Transformers-Based Malware Detectors
AU  - Jakhotiya, Yash
AU  - Patil, Heramb
AU  - Rawlani, Jugal
AU  - Mane, Dr Sunil B.
AB  - Signature-based malware detectors have proven to be insufficient as even a small change in malignant executable code can bypass these signature-based detectors. Many machine learning-based models have been proposed to efficiently detect a wide variety of malware. Many of these models are found to be susceptible to adversarial attacks - attacks that work by generating intentionally designed inputs that can force these models to misclassify. Our work aims to explore vulnerabilities in the current state of the art malware detectors to adversarial attacks. We train a Transformers-based malware detector, carry out adversarial attacks resulting in a misclassification rate of 23.9% and propose defenses that reduce this misclassification rate to half. An implementation of our work can be found at https://github.com/yashjakhotiya/Adversarial-Attacks-On-Transformers.
DA  - 2022/11/05/
PY  - 2022
DO  - 10.48550/arXiv.2210.00008
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.00008
Y2  - 2024/07/14/21:52:55
L1  - https://arxiv.org/pdf/2210.00008.pdf
L2  - https://arxiv.org/abs/2210.00008
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Poisoning Generative Replay in Continual Learning to Promote Forgetting
AU  - Kang, Siteng
AU  - Shi, Zhan
AU  - Zhang, Xinhua
T2  - International Conference on Machine Learning
AB  - Generative models have grown into the workhorse of many state-of-the-art machine learning methods. However, their vulnerability under poisoning attacks has been largely understudied. In this work, we investigate this issue in the context of continual learning, where generative replayers are utilized to tackle catastrophic forgetting. By developing a novel customization of dirty-label input-aware backdoors to the online setting, our attacker manages to stealthily promote forgetting while retaining high accuracy at the current task and sustaining strong defenders. Our approach taps into an intriguing property of generative models, namely that they cannot well capture input-dependent triggers. Experiments on four standard datasets corroborate the poisoner’s effectiveness.
C3  - Proceedings of the 40th International Conference on Machine Learning
DA  - 2023/07/03/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 15769
EP  - 15785
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v202/kang23c.html
Y2  - 2024/07/14/21:52:37
L1  - https://proceedings.mlr.press/v202/kang23c/kang23c.pdf
ER  - 

TY  - GEN
TI  - Towards Adversarial Purification using Denoising AutoEncoders
AU  - Kalaria, Dvij
AU  - Hazra, Aritra
AU  - Chakrabarti, Partha Pratim
AB  - With the rapid advancement and increased use of deep learning models in image identification, security becomes a major concern to their deployment in safety-critical systems. Since the accuracy and robustness of deep learning models are primarily attributed from the purity of the training samples, therefore the deep learning architectures are often susceptible to adversarial attacks. Adversarial attacks are often obtained by making subtle perturbations to normal images, which are mostly imperceptible to humans, but can seriously confuse the state-of-the-art machine learning models. We propose a framework, named APuDAE, leveraging Denoising AutoEncoders (DAEs) to purify these samples by using them in an adaptive way and thus improve the classification accuracy of the target classifier networks that have been attacked. We also show how using DAEs adaptively instead of using them directly, improves classification accuracy further and is more robust to the possibility of designing adaptive attacks to fool them. We demonstrate our results over MNIST, CIFAR-10, ImageNet dataset and show how our framework (APuDAE) provides comparable and in most cases better performance to the baseline methods in purifying adversaries. We also design adaptive attack specifically designed to attack our purifying model and demonstrate how our defense is robust to that.
DA  - 2022/08/29/
PY  - 2022
DO  - 10.48550/arXiv.2208.13838
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2208.13838
Y2  - 2024/07/14/21:52:25
L1  - https://arxiv.org/pdf/2208.13838.pdf
L2  - https://arxiv.org/abs/2208.13838
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Ignore Previous Prompt: Attack Techniques For Language Models
AU  - Perez, Fábio
AU  - Ribeiro, Ian
AB  - Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.
DA  - 2022/11/17/
PY  - 2022
DO  - 10.48550/arXiv.2211.09527
DP  - arXiv.org
PB  - arXiv
ST  - Ignore Previous Prompt
UR  - http://arxiv.org/abs/2211.09527
Y2  - 2024/07/14/21:51:53
L1  - https://arxiv.org/pdf/2211.09527.pdf
L2  - https://arxiv.org/abs/2211.09527
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Falsehoods that ML researchers believe about OOD detection
AU  - Zhang, Andi
AU  - Wischik, Damon
AB  - An intuitive way to detect out-of-distribution (OOD) data is via the density function of a fitted probabilistic generative model: points with low density may be classed as OOD. But this approach has been found to fail, in deep learning settings. In this paper, we list some falsehoods that machine learning researchers believe about density-based OOD detection. Many recent works have proposed likelihood-ratio-based methods to `fix' the problem. We propose a framework, the OOD proxy framework, to unify these methods, and we argue that likelihood ratio is a principled method for OOD detection and not a mere `fix'. Finally, we discuss the relationship between domain discrimination and semantics.
DA  - 2022/11/01/
PY  - 2022
DO  - 10.48550/arXiv.2210.12767
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.12767
Y2  - 2024/07/14/21:51:41
L1  - https://arxiv.org/pdf/2210.12767.pdf
L2  - https://arxiv.org/abs/2210.12767
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Robust One-Class Classification with Signed Distance Function using 1-Lipschitz Neural Networks
AU  - Bethune, Louis
AU  - Novello, Paul
AU  - Boissin, Thibaut
AU  - Coiffier, Guillaume
AU  - Serrurier, Mathieu
AU  - Vincenot, Quentin
AU  - Troya-Galvis, Andres
AB  - We propose a new method, dubbed One Class Signed Distance Function (OCSDF), to perform One Class Classification (OCC) by provably learning the Signed Distance Function (SDF) to the boundary of the support of any distribution. The distance to the support can be interpreted as a normality score, and its approximation using 1-Lipschitz neural networks provides robustness bounds against $l2$ adversarial attacks, an under-explored weakness of deep learning-based OCC algorithms. As a result, OCSDF comes with a new metric, certified AUROC, that can be computed at the same cost as any classical AUROC. We show that OCSDF is competitive against concurrent methods on tabular and image data while being way more robust to adversarial attacks, illustrating its theoretical properties. Finally, as exploratory research perspectives, we theoretically and empirically show how OCSDF connects OCC with image generation and implicit neural surface parametrization. Our code is available at https://github.com/Algue-Rythme/OneClassMetricLearning
DA  - 2024/04/01/
PY  - 2024
DO  - 10.48550/arXiv.2303.01978
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.01978
Y2  - 2024/07/14/21:50:40
L1  - https://arxiv.org/pdf/2303.01978v2.pdf
L2  - https://arxiv.org/abs/2303.01978
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - The Expertise Problem: Learning from Specialized Feedback
AU  - Daniels-Koch, Oliver
AU  - Freedman, Rachel
AB  - Reinforcement learning from human feedback (RLHF) is a powerful technique for training agents to perform difficult-to-specify tasks. However, human feedback can be noisy, particularly when human teachers lack relevant knowledge or experience. Levels of expertise vary across teachers, and a given teacher may have differing levels of expertise for different components of a task. RLHF algorithms that learn from multiple teachers therefore face an expertise problem: the reliability of a given piece of feedback depends both on the teacher that it comes from and how specialized that teacher is on relevant components of the task. Existing state-of-the-art RLHF algorithms assume that all evaluations come from the same distribution, obscuring this inter- and intra-human variance, and preventing them from accounting for or taking advantage of variations in expertise. We formalize this problem, implement it as an extension of an existing RLHF benchmark, evaluate the performance of a state-of-the-art RLHF algorithm, and explore techniques to improve query and teacher selection. Our key contribution is to demonstrate and characterize the expertise problem, and to provide an open-source implementation for testing future solutions.
DA  - 2022/11/11/
PY  - 2022
DO  - 10.48550/arXiv.2211.06519
DP  - arXiv.org
PB  - arXiv
ST  - The Expertise Problem
UR  - http://arxiv.org/abs/2211.06519
Y2  - 2024/07/14/21:18:29
L1  - https://arxiv.org/pdf/2211.06519.pdf
L2  - https://arxiv.org/abs/2211.06519
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration for Control Policy Synthesis
AU  - Mitta, Rohan
AU  - Hasanbeig, Hosein
AU  - Wang, Jun
AU  - Kroening, Daniel
AU  - Kantaros, Yiannis
AU  - Abate, Alessandro
AB  - This paper addresses the problem of maintaining safety during training in Reinforcement Learning (RL), such that the safety constraint violations are bounded at any point during learning. In a variety of RL applications the safety of the agent is particularly important, e.g. autonomous platforms or robots that work in proximity of humans. As enforcing safety during training might severely limit the agent's exploration, we propose here a new architecture that handles the trade-off between efficient progress and safety during exploration. As the exploration progresses, we update via Bayesian inference Dirichlet-Categorical models of the transition probabilities of the Markov decision process that describes the environment dynamics. This paper proposes a way to approximate moments of belief about the risk associated to the action selection policy. We construct those approximations, and prove the convergence results. We propose a novel method for leveraging the expectation approximations to derive an approximate bound on the confidence that the risk is below a certain level. This approach can be easily interleaved with RL and we present experimental results to showcase the performance of the overall architecture.
DA  - 2023/12/18/
PY  - 2023
DO  - 10.48550/arXiv.2312.11314
DP  - arXiv.org
PB  - arXiv
ST  - Safeguarded Progress in Reinforcement Learning
UR  - http://arxiv.org/abs/2312.11314
Y2  - 2024/07/14/21:18:16
L1  - https://arxiv.org/pdf/2312.11314v1.pdf
L2  - https://arxiv.org/abs/2312.11314
KW  - Computer Science - Machine Learning
KW  - Computer Science - Logic in Computer Science
KW  - Electrical Engineering and Systems Science - Systems and Control
ER  - 

TY  - CONF
TI  - Mitigating Lies in Vision-Language Models
AU  - Li, Junbo
AU  - Li, Xianhang
AU  - Xie, Cihang
T2  - NeurIPS ML Safety Workshop
AB  - In this work, we bring new insights into the honesty of vision-language models, particularly in visual question answering (VQA). After a throughout revisit of the existing ‘lie’ behavior in pure language models, our work makes an unprecedented extension of ’lies’ to vision-language models. The results indicate that the lie prefixes have a more obvious misleading effect on vision-language models than on language models. We also propose a novel visual prefix and prove that the consistent vision-language prefix is more threatening to vision-language models. To defend the models from the stated ’lies’, we put forward an unsupervised framework based on Gaussian mixture modeling and obtain improvement with 3% against the language prefix and 12% against the vision-language prefix.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=mAiTuIeWbxD
Y2  - 2024/07/14/21:17:18
L1  - https://openreview.net/pdf?id=mAiTuIeWbxD
ER  - 

TY  - GEN
TI  - Indiscriminate Data Poisoning Attacks on Neural Networks
AU  - Lu, Yiwei
AU  - Kamath, Gautam
AU  - Yu, Yaoliang
AB  - Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting "poisoned" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.
DA  - 2024/02/15/
PY  - 2024
DO  - 10.48550/arXiv.2204.09092
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2204.09092
Y2  - 2024/07/14/21:17:08
L1  - https://arxiv.org/pdf/2204.09092.pdf
L2  - https://arxiv.org/abs/2204.09092
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification
AU  - Linderman, Randolph
AU  - Zhang, Jingyang
AU  - Inkawhich, Nathan
AU  - Li, Hai
AU  - Chen, Yiran
AB  - Machine learning methods must be trusted to make appropriate decisions in real-world environments, even when faced with out-of-distribution (OOD) samples. Many current approaches simply aim to detect OOD examples and alert the user when an unrecognized input is given. However, when the OOD sample significantly overlaps with the training data, a binary anomaly detection is not interpretable or explainable, and provides little information to the user. We propose a new model for OOD detection that makes predictions at varying levels of granularity as the inputs become more ambiguous, the model predictions become coarser and more conservative. Consider an animal classifier that encounters an unknown bird species and a car. Both cases are OOD, but the user gains more information if the classifier recognizes that its uncertainty over the particular species is too large and predicts bird instead of detecting it as OOD. Furthermore, we diagnose the classifiers performance at each level of the hierarchy improving the explainability and interpretability of the models predictions. We demonstrate the effectiveness of hierarchical classifiers for both fine- and coarse-grained OOD tasks.
DA  - 2022/09/09/
PY  - 2022
DO  - 10.48550/arXiv.2209.04493
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.04493
Y2  - 2024/07/14/21:16:32
L1  - https://arxiv.org/pdf/2209.04493.pdf
L2  - https://arxiv.org/abs/2209.04493
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection
AU  - Lafon, Marc
AU  - Ramzi, Elias
AU  - Rambour, Clément
AU  - Thome, Nicolas
AB  - Out-of-distribution (OOD) detection is a critical requirement for the deployment of deep neural networks. This paper introduces the HEAT model, a new post-hoc OOD detection method estimating the density of in-distribution (ID) samples using hybrid energy-based models (EBM) in the feature space of a pre-trained backbone. HEAT complements prior density estimators of the ID density, e.g. parametric models like the Gaussian Mixture Model (GMM), to provide an accurate yet robust density estimation. A second contribution is to leverage the EBM framework to provide a unified density estimation and to compose several energy terms. Extensive experiments demonstrate the significance of the two contributions. HEAT sets new state-of-the-art OOD detection results on the CIFAR-10 / CIFAR-100 benchmark as well as on the large-scale Imagenet benchmark. The code is available at: https://github.com/MarcLafon/heatood.
DA  - 2023/06/01/
PY  - 2023
DO  - 10.48550/arXiv.2305.16966
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.16966
Y2  - 2024/07/14/21:16:18
L1  - https://arxiv.org/pdf/2305.16966.pdf
L2  - https://arxiv.org/abs/2305.16966
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker
AU  - Dai, Sihui
AU  - Ding, Wenxin
AU  - Bhagoji, Arjun Nitin
AU  - Cullina, Daniel
AU  - Zhao, Ben Y.
AU  - Zheng, Haitao
AU  - Mittal, Prateek
AB  - Finding classifiers robust to adversarial examples is critical for their safe deployment. Determining the robustness of the best possible classifier under a given threat model for a given data distribution and comparing it to that achieved by state-of-the-art training methods is thus an important diagnostic tool. In this paper, we find achievable information-theoretic lower bounds on loss in the presence of a test-time attacker for multi-class classifiers on any discrete dataset. We provide a general framework for finding the optimal 0-1 loss that revolves around the construction of a conflict hypergraph from the data and adversarial constraints. We further define other variants of the attacker-classifier game that determine the range of the optimal loss more efficiently than the full-fledged hypergraph construction. Our evaluation shows, for the first time, an analysis of the gap to optimal robustness for classifiers in the multi-class setting on benchmark datasets.
DA  - 2023/12/06/
PY  - 2023
DO  - 10.48550/arXiv.2302.10722
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.10722
Y2  - 2024/07/14/21:16:01
L1  - https://arxiv.org/pdf/2302.10722.pdf
L2  - https://arxiv.org/abs/2302.10722
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Quantifying Misalignment Between Agents
AU  - Kierans, Aidan
AU  - Ghosh, Avijit
AU  - Hazan, Hananel
AU  - Dori-Hacohen, Shiri
AB  - Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit. Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth. Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans). In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes. Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice. We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior. By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests.
DA  - 2024/06/06/
PY  - 2024
DO  - 10.48550/arXiv.2406.04231
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.04231
Y2  - 2024/07/14/21:15:43
L1  - https://arxiv.org/pdf/2406.04231.pdf
L2  - https://arxiv.org/abs/2406.04231
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Computer Science and Game Theory
KW  - I.2.11
KW  - K.4.m
ER  - 

TY  - GEN
TI  - System Safety Engineering for Social and Ethical ML Risks: A Case Study
AU  - Jatho III, Edgar W.
AU  - Mailloux, Logan O.
AU  - Rismani, Shalaleh
AU  - Williams, Eugene D.
AU  - Kroll, Joshua A.
AB  - Governments, industry, and academia have undertaken efforts to identify and mitigate harms in ML-driven systems, with a particular focus on social and ethical risks of ML components in complex sociotechnical systems. However, existing approaches are largely disjointed, ad-hoc and of unknown effectiveness. Systems safety engineering is a well established discipline with a track record of identifying and managing risks in many complex sociotechnical domains. We adopt the natural hypothesis that tools from this domain could serve to enhance risk analyses of ML in its context of use. To test this hypothesis, we apply a "best of breed" systems safety analysis, Systems Theoretic Process Analysis (STPA), to a specific high-consequence system with an important ML-driven component, namely the Prescription Drug Monitoring Programs (PDMPs) operated by many US States, several of which rely on an ML-derived risk score. We focus in particular on how this analysis can extend to identifying social and ethical risks and developing concrete design-level controls to mitigate them.
DA  - 2022/11/08/
PY  - 2022
DO  - 10.48550/arXiv.2211.04602
DP  - arXiv.org
PB  - arXiv
ST  - System Safety Engineering for Social and Ethical ML Risks
UR  - http://arxiv.org/abs/2211.04602
Y2  - 2024/07/14/21:15:31
L1  - https://arxiv.org/pdf/2211.04602.pdf
L2  - https://arxiv.org/abs/2211.04602
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Defining Deception in Structural Causal Games
AU  - Ward, Francis Rhys
AU  - Toni, Francesca
AU  - Belardinelli, Francesco
T3  - AAMAS '23
AB  - Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals. There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a functional definition of deception in structural causal games, grounded in the philosophical literature. We present several examples to establish that our formal definition captures philosophical desiderata for deception.
C1  - Richland, SC
C3  - Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems
DA  - 2023/05/30/
PY  - 2023
DP  - ACM Digital Library
SP  - 2902
EP  - 2904
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-9432-1
Y2  - 2024/07/14/
ER  - 

TY  - GEN
TI  - How Sure to Be Safe? Difficulty, Confidence and Negative Side Effects
AU  - Burden, John
AU  - Hernández-Orallo, José
AB  - A principal concern for AI systems is the occurrence of negative side effects, such as a robot cleaner breaking a vase. This is critical when these systems use machine learning models that were trained to maximise performance, without knowledge or feedback about the negative side effects. Within Vase World and SafeLife, two safety benchmarking domains, we analyse side effects during operation and demonstrate that their magnitude is influenced by task difficulty. Using two forms of confidence measure, we demonstrate that wrapping existing RL agents with safety policies that activate when the agent’s confidence falls below a specified threshold extends the Pareto frontier of both performance and safety.
CY  - NeurIPS 2022 ML Safety Workshop
DA  - 2022///
PY  - 2022
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=jJJQm476fNr
ER  - 

TY  - JOUR
TI  - Learning a robust foundation model against clean-label data poisoning attacks at downstream tasks
AU  - Zhou, Ting
AU  - Yan, Hanshu
AU  - Han, Bo
AU  - Liu, Lei
AU  - Zhang, Jingfeng
T2  - Neural Networks
AB  - In the transfer learning paradigm, models that are pre-trained on large datasets are used as the foundation models for various downstream tasks. However, this paradigm exposes downstream practitioners to data poisoning threats, as attackers can inject malicious samples into the re-training datasets to manipulate the behavior of models in downstream tasks. In this work, we propose a defense strategy that significantly reduces the success rate of various data poisoning attacks in downstream tasks. Our defense aims to pre-train a robust foundation model by reducing adversarial feature distance and increasing inter-class feature distance. Experiments demonstrate the excellent defense performance of the proposed strategy towards state-of-the-art clean-label poisoning attacks in the transfer learning scenario.
DA  - 2024/01/01/
PY  - 2024
DO  - 10.1016/j.neunet.2023.10.034
DP  - ScienceDirect
VL  - 169
SP  - 756
EP  - 763
J2  - Neural Networks
SN  - 0893-6080
UR  - https://www.sciencedirect.com/science/article/pii/S0893608023005890
Y2  - 2024/07/14/21:13:20
KW  - Transfer learning
KW  - Clean-label poisoning attacks
KW  - Robust foundation model
ER  - 

TY  - CONF
TI  - RobustAugMix: Joint Optimization of Natural and Adversarial Robustness
AU  - Martinez-Martinez, Josue
AU  - Brown, Olivia
T2  - NeurIPS ML Safety Workshop
AB  - Machine learning models often suffer performance degradation when faced with corrupted data. In this work, we explore a technique that combines a data augmentation strategy (AugMix) with adversarial training, in order to increase robustness to both natural and adversarial forms of data corruption.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
ST  - RobustAugMix
UR  - https://openreview.net/forum?id=8MfPfECiFET
Y2  - 2024/07/14/21:12:55
L1  - https://openreview.net/pdf?id=8MfPfECiFET
ER  - 

TY  - CONF
TI  - Bandits with costly reward observations
AU  - Tucker, Aaron D.
AU  - Biddulph, Caleb
AU  - Wang, Claire
AU  - Joachims, Thorsten
T2  - Uncertainty in Artificial Intelligence
AB  - Many machine learning applications rely on large datasets that are conveniently collected from existing sources or that are labeled automatically as a by-product of user actions. However, in settings such as content moderation, accurately and reliably labeled data comes at substantial cost. If a learning algorithm has to pay for reward information, for example by asking a human for feedback, how does this change the exploration/exploitation tradeoff? We study this question in the context of bandit learning. Specifically, we investigate Bandits with Costly Reward Observations, where a cost needs to be paid in order to observe the reward of the bandit’s action. We show that the observation cost implies an Ω(c1/3T2/3)Ω(c1/3T2/3)\Omega(c^{1/3}T^{2/3}) lower bound on the regret. Furthermore, we develop a general non-adaptive bandit algorithm which matches this lower bound, and we present several competitive adaptive learning algorithms for both k-armed and contextual bandits.
C3  - Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence
DA  - 2023/07/02/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 2147
EP  - 2156
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v216/tucker23a.html
Y2  - 2024/07/14/21:12:43
L1  - https://proceedings.mlr.press/v216/tucker23a/tucker23a.pdf
L1  - https://proceedings.mlr.press/v216/tucker23a/tucker23a-supp.pdf
ER  - 

TY  - CONF
TI  - Disclosing the Biases in Large Language Models via Reward Structured Questions
AU  - Korkmaz, Ezgi
T2  - NeurIPS ML Safety Workshop
AB  - The success of the large language models have been utterly demonstrated in the recent time. Using these models and fine tuning for the specific task at hand results in highly performing models. However, these models also learn biased representations from the data they have been trained on. In particular, several studies recently showed that language models can learn to be biased towards certain genders. Quite recently, several studies tried to eliminate this bias via proposing human feedback included in fine-tuning. In our study we show that by changing the question asked to the language model the log probabilities of the bias measured in the responses changes dramatically. Furthermore, in several cases the language model ends up providing a completely opposite response. The recent language models finetuned on the prior gender bias datasets do not resolve the actual problem, but rather alleviates the problem for the dataset on which the model is fine-tuned. We believe our results might lay the foundation for further alignment and safety problems in large language models.
DA  - 2022/11/18/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=495hkz94cIC
Y2  - 2024/07/14/21:12:26
L1  - https://openreview.net/pdf?id=495hkz94cIC
ER  - 

TY  - CONF
TI  - Improving Zero-shot Generalization and Robustness of Multi-Modal Models
AU  - Ge, Yunhao
AU  - Ren, Jie
AU  - Gallagher, Andrew
AU  - Wang, Yuxiao
AU  - Yang, Ming-Hsuan
AU  - Adam, Hartwig
AU  - Itti, Laurent
AU  - Lakshminarayanan, Balaji
AU  - Zhao, Jiaping
T2  - 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
AB  - Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNetbased datasets. For CLIP, our method improves the top1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT. The proposed method1 is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP.
C1  - Vancouver, BC, Canada
C3  - 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/CVPR52729.2023.01067
DP  - DOI.org (Crossref)
SP  - 11093
EP  - 11101
LA  - en
PB  - IEEE
SN  - 9798350301298
UR  - https://ieeexplore.ieee.org/document/10203777/
Y2  - 2024/07/14/21:12:15
L1  - https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf
ER  - 

TY  - GEN
TI  - Simple and Scalable Strategies to Continually Pre-train Large Language Models
AU  - Ibrahim, Adam
AU  - Thérien, Benjamin
AU  - Gupta, Kshitij
AU  - Richter, Mats L.
AU  - Anthony, Quentin
AU  - Lesort, Timothée
AU  - Belilovsky, Eugene
AU  - Rish, Irina
AB  - Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.
DA  - 2024/03/26/
PY  - 2024
DO  - 10.48550/arXiv.2403.08763
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.08763
Y2  - 2024/07/19/11:22:36
L1  - https://arxiv.org/pdf/2403.08763.pdf
L2  - https://arxiv.org/abs/2403.08763
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression
AU  - Kaushal, Ayush
AU  - Vaidhya, Tejas
AU  - Rish, Irina
AB  - Low Rank Decomposition of matrix - splitting a large matrix into a product of two smaller matrix offers a means for compression that reduces the parameters of a model without sparsification, and hence delivering more speedup on modern hardware. Moreover, unlike quantization, the compressed linear layers remain fully differentiable and all the parameters trainable, while being able to leverage the existing highly efficient kernels over floating point matrices. We study the potential to compress Large Language Models (LLMs) for monolingual Code generation via Low Rank Decomposition (LoRD) and observe that ranks for the linear layers in these models can be reduced by upto 39.58% with less than 1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single A100. The compressed models speeds up inference by up to 22.35% with just a single line of change in code over huggingface's implementation with pytorch backend. Low Rank Decomposition (LoRD) models remain compatible with state of the art near-lossless quantization method such as SpQR, which allows leveraging further compression gains of quantization. Lastly, QLoRA over Low Rank Decomposition (LoRD) model further reduces memory requirements by as much as 21.2% over vanilla QLoRA while offering similar gains from parameter efficient fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new paradigm for LLM compression.
DA  - 2023/09/25/
PY  - 2023
DO  - 10.48550/arXiv.2309.14021
DP  - arXiv.org
PB  - arXiv
ST  - LORD
UR  - http://arxiv.org/abs/2309.14021
Y2  - 2024/07/19/11:22:38
L1  - https://arxiv.org/pdf/2309.14021.pdf
L2  - https://arxiv.org/abs/2309.14021
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation
AU  - Vaithilingam, Priyan
AU  - Arawjo, Ian
AU  - Glassman, Elena L.
T3  - DIS '24
AB  - We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.
C1  - New York, NY, USA
C3  - Proceedings of the 2024 ACM Designing Interactive Systems Conference
DA  - 2024/07/01/
PY  - 2024
DO  - 10.1145/3643834.3661525
DP  - ACM Digital Library
SP  - 289
EP  - 300
PB  - Association for Computing Machinery
SN  - 9798400705830
ST  - Imagining a Future of Designing with AI
UR  - https://doi.org/10.1145/3643834.3661525
Y2  - 2024/07/19/
ER  - 

TY  - GEN
TI  - Global Rewards in Multi-Agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems
AU  - Hoppe, Heiko
AU  - Enders, Tobias
AU  - Cappart, Quentin
AU  - Schiffer, Maximilian
AB  - We study vehicle dispatching in autonomous mobility on demand (AMoD) systems, where a central operator assigns vehicles to customer requests or rejects these with the aim of maximizing its total profit. Recent approaches use multi-agent deep reinforcement learning (MADRL) to realize scalable yet performant algorithms, but train agents based on local rewards, which distorts the reward signal with respect to the system-wide profit, leading to lower performance. We therefore propose a novel global-rewards-based MADRL algorithm for vehicle dispatching in AMoD systems, which resolves so far existing goal conflicts between the trained agents and the operator by assigning rewards to agents leveraging a counterfactual baseline. Our algorithm shows statistically significant improvements across various settings on real-world data compared to state-of-the-art MADRL algorithms with local rewards. We further provide a structural analysis which shows that the utilization of global rewards can improve implicit vehicle balancing and demand forecasting abilities. Our code is available at https://github.com/tumBAIS/GR-MADRL-AMoD.
DA  - 2024/05/19/
PY  - 2024
DO  - 10.48550/arXiv.2312.08884
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.08884
Y2  - 2024/07/19/11:25:50
L1  - https://arxiv.org/pdf/2312.08884.pdf
L2  - https://arxiv.org/abs/2312.08884
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Electrical Engineering and Systems Science - Systems and Control
ER  - 

TY  - GEN
TI  - DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations
AU  - Fathi, Nima
AU  - Kumar, Amar
AU  - Nichyporuk, Brennan
AU  - Havaei, Mohammad
AU  - Arbel, Tal
AB  - Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability. Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.
DA  - 2024/05/15/
PY  - 2024
DO  - 10.48550/arXiv.2405.09288
DP  - arXiv.org
PB  - arXiv
ST  - DeCoDEx
UR  - http://arxiv.org/abs/2405.09288
Y2  - 2024/07/19/11:25:52
L1  - https://arxiv.org/pdf/2405.09288.pdf
L2  - https://arxiv.org/abs/2405.09288
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Towards Modular LLMs by Building and Reusing a Library of LoRAs
AU  - Ostapenko, Oleksiy
AU  - Su, Zhan
AU  - Ponti, Edoardo Maria
AU  - Charlin, Laurent
AU  - Roux, Nicolas Le
AU  - Pereira, Matheus
AU  - Caccia, Lucas
AU  - Sordoni, Alessandro
AB  - The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.
DA  - 2024/05/17/
PY  - 2024
DO  - 10.48550/arXiv.2405.11157
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2405.11157
Y2  - 2024/07/19/11:26:33
L1  - https://arxiv.org/pdf/2405.11157.pdf
L2  - https://arxiv.org/abs/2405.11157
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - What Mechanisms Does Knowledge Distillation Distill?
AU  - Wu, Cindy
AU  - Lubana, Ekdeep Singh
AU  - Mlodozeniec, Bruno Kacper
AU  - Kirk, Robert
AU  - Krueger, David
T2  - Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models
AB  - Knowledge distillation is a commonly-used compression method in ML due to the popularity of increasingly large-scale models, but it is unclear if all the information a teacher model contains is distilled into the smaller student model. We aim to formalize the concept of ‘knowledge’ to investigate how knowledge is transferred during distillation, focusing on shared invariant outputs to counterfactual changes of dataset latent variables (we call these latents mechanisms). We define a student model to be a good stand-in model for a teacher if it shares the teacher’s learned mechanisms, and find that Jacobian matching and contrastive representation learning are viable methods by which to train such models. While these methods do not result in perfect transfer of mechanisms, we show they often improve student fidelity or mitigate simplicity bias (as measured by the teacher-to-student KL divergence and accuracy on various out-of-distribution test datasets), especially on datasets with spurious statistical correlations.
C3  - Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models
DA  - 2024/05/14/
PY  - 2024
DP  - proceedings.mlr.press
SP  - 60
EP  - 75
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v243/wu24a.html
Y2  - 2024/07/19/11:26:33
L1  - https://proceedings.mlr.press/v243/wu24a/wu24a.pdf
ER  - 

TY  - CONF
TI  - ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing
AU  - Arawjo, Ian
AU  - Swoopes, Chelse
AU  - Vaithilingam, Priyan
AU  - Wattenberg, Martin
AU  - Glassman, Elena L.
T3  - CHI '24
AB  - Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.
C1  - New York, NY, USA
C3  - Proceedings of the CHI Conference on Human Factors in Computing Systems
DA  - 2024/05/11/
PY  - 2024
DO  - 10.1145/3613904.3642016
DP  - ACM Digital Library
SP  - 1
EP  - 18
PB  - Association for Computing Machinery
SN  - 9798400703300
ST  - ChainForge
UR  - https://doi.org/10.1145/3613904.3642016
Y2  - 2024/07/19/
L1  - https://dl.acm.org/doi/pdf/10.1145/3613904.3642016
ER  - 

TY  - GEN
TI  - All-in-one simulation-based inference
AU  - Gloeckler, Manuel
AU  - Deistler, Michael
AU  - Weilbach, Christian
AU  - Wood, Frank
AU  - Macke, Jakob H.
AB  - Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.
DA  - 2024/07/15/
PY  - 2024
DO  - 10.48550/arXiv.2404.09636
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.09636
Y2  - 2024/07/19/11:26:38
L1  - https://arxiv.org/pdf/2404.09636.pdf
L2  - https://arxiv.org/abs/2404.09636
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - Managing extreme AI risks amid rapid progress
AU  - Bengio, Yoshua
AU  - Hinton, Geoffrey
AU  - Yao, Andrew
AU  - Song, Dawn
AU  - Abbeel, Pieter
AU  - Darrell, Trevor
AU  - Harari, Yuval Noah
AU  - Zhang, Ya-Qin
AU  - Xue, Lan
AU  - Shalev-Shwartz, Shai
AU  - Hadfield, Gillian
AU  - Clune, Jeff
AU  - Maharaj, Tegan
AU  - Hutter, Frank
AU  - Baydin, Atılım Güneş
AU  - McIlraith, Sheila
AU  - Gao, Qiqi
AU  - Acharya, Ashwin
AU  - Krueger, David
AU  - Dragan, Anca
AU  - Torr, Philip
AU  - Russell, Stuart
AU  - Kahneman, Daniel
AU  - Brauner, Jan
AU  - Mindermann, Sören
T2  - Science
DA  - 2024/05/24/
PY  - 2024
DO  - 10.1126/science.adn0117
DP  - science.org (Atypon)
VL  - 384
IS  - 6698
SP  - 842
EP  - 845
UR  - https://www.science.org/doi/10.1126/science.adn0117
Y2  - 2024/07/19/11:26:39
L1  - https://www.science.org/doi/pdf/10.1126/science.adn0117
ER  - 

TY  - GEN
TI  - A Persuasive Approach to Combating Misinformation
AU  - Hossain, Safwan
AU  - Mladenovic, Andjela
AU  - Chen, Yiling
AU  - Gidel, Gauthier
AB  - Bayesian Persuasion is proposed as a tool for social media platforms to combat the spread of misinformation. Since platforms can use machine learning to predict the popularity and misinformation features of to-be-shared posts, and users are largely motivated to share popular content, platforms can strategically signal this informational advantage to change user beliefs and persuade them not to share misinformation. We characterize the optimal signaling scheme with imperfect predictions as a linear program and give sufficient and necessary conditions on the classifier to ensure optimal platform utility is non-decreasing and continuous. Next, this interaction is considered under a performative model, wherein platform intervention affects the user's future behaviour. The convergence and stability of optimal signaling under this performative process are fully characterized. Lastly, we experimentally validate that our approach significantly reduces misinformation in both the single round and performative setting and discuss the broader scope of using information design to combat misinformation.
DA  - 2024/02/13/
PY  - 2024
DO  - 10.48550/arXiv.2310.12065
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.12065
Y2  - 2024/07/19/11:30:18
L1  - https://arxiv.org/pdf/2310.12065.pdf
L2  - https://arxiv.org/abs/2310.12065
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - GEN
TI  - Robust Data-driven Prescriptiveness Optimization
AU  - Poursoltani, Mehran
AU  - Delage, Erick
AU  - Georghiou, Angelos
AB  - The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual shortest path problem, we evaluate the robustness of the resulting policies against alternative methods when the out-of-sample dataset is subject to varying amounts of distribution shift.
DA  - 2024/06/03/
PY  - 2024
DO  - 10.48550/arXiv.2306.05937
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.05937
Y2  - 2024/07/19/11:30:20
L1  - https://arxiv.org/pdf/2306.05937.pdf
L2  - https://arxiv.org/abs/2306.05937
KW  - Computer Science - Machine Learning
KW  - Statistics - Methodology
KW  - Mathematics - Optimization and Control
ER  - 

TY  - GEN
TI  - Code as Reward: Empowering Reinforcement Learning with VLMs
AU  - Venuto, David
AU  - Islam, Sami Nur
AU  - Klissarov, Martin
AU  - Precup, Doina
AU  - Yang, Sherry
AU  - Anand, Ankit
AB  - Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.
DA  - 2024/02/07/
PY  - 2024
DO  - 10.48550/arXiv.2402.04764
DP  - arXiv.org
PB  - arXiv
ST  - Code as Reward
UR  - http://arxiv.org/abs/2402.04764
Y2  - 2024/07/19/11:30:22
L1  - https://arxiv.org/pdf/2402.04764.pdf
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - A Distributional Analogue to the Successor Representation
AU  - Wiltzer, Harley
AU  - Farebrother, Jesse
AU  - Gretton, Arthur
AU  - Tang, Yunhao
AU  - Barreto, André
AU  - Dabney, Will
AU  - Bellemare, Marc G.
AU  - Rowland, Mark
AB  - This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.
DA  - 2024/05/24/
PY  - 2024
DO  - 10.48550/arXiv.2402.08530
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.08530
Y2  - 2024/07/19/11:30:24
L1  - https://arxiv.org/pdf/2402.08530.pdf
L2  - https://arxiv.org/abs/2402.08530
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs
AU  - Johnson, Daniel D.
AU  - Tarlow, Daniel
AU  - Duvenaud, David
AU  - Maddison, Chris J.
AB  - Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.
DA  - 2024/05/27/
PY  - 2024
DO  - 10.48550/arXiv.2402.08733
DP  - arXiv.org
PB  - arXiv
ST  - Experts Don't Cheat
UR  - http://arxiv.org/abs/2402.08733
Y2  - 2024/07/19/11:30:26
L1  - https://arxiv.org/pdf/2402.08733.pdf
L2  - https://arxiv.org/abs/2402.08733
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Faithfulness Measurable Masked Language Models
AU  - Madsen, Andreas
AU  - Reddy, Siva
AU  - Chandar, Sarath
AB  - A common approach to explaining NLP models is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues, and existing solutions that address this are computationally expensive and employ proxy models. Furthermore, other metrics are very limited in scope. This work proposes an inherently faithfulness measurable model that addresses these challenges. This is achieved using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to 16 different datasets and validate it using statistical in-distribution tests. The faithfulness is then measured with 9 different importance measures. Because masking is in-distribution, importance measures that themselves use masking become consistently more faithful. Additionally, because the model makes faithfulness cheap to measure, we can optimize explanations towards maximal faithfulness; thus, our model becomes indirectly inherently explainable.
DA  - 2024/05/09/
PY  - 2024
DO  - 10.48550/arXiv.2310.07819
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.07819
Y2  - 2024/07/19/11:30:29
L1  - https://arxiv.org/pdf/2310.07819.pdf
L2  - https://arxiv.org/abs/2310.07819
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - SelfIE: Self-Interpretation of Large Language Model Embeddings
AU  - Chen, Haozhe
AU  - Vondrick, Carl
AU  - Mao, Chengzhi
AB  - How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.
DA  - 2024/03/25/
PY  - 2024
DO  - 10.48550/arXiv.2403.10949
DP  - arXiv.org
PB  - arXiv
ST  - SelfIE
UR  - http://arxiv.org/abs/2403.10949
Y2  - 2024/07/19/11:33:40
L1  - https://arxiv.org/pdf/2403.10949.pdf
L2  - https://arxiv.org/abs/2403.10949
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Stealing Part of a Production Language Model
AU  - Carlini, Nicholas
AU  - Paleka, Daniel
AU  - Dvijotham, Krishnamurthy Dj
AU  - Steinke, Thomas
AU  - Hayase, Jonathan
AU  - Cooper, A. Feder
AU  - Lee, Katherine
AU  - Jagielski, Matthew
AU  - Nasr, Milad
AU  - Conmy, Arthur
AU  - Yona, Itay
AU  - Wallace, Eric
AU  - Rolnick, David
AU  - Tramèr, Florian
AB  - We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.
DA  - 2024/07/09/
PY  - 2024
DO  - 10.48550/arXiv.2403.06634
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.06634
Y2  - 2024/07/19/11:33:42
L1  - https://arxiv.org/pdf/2403.06634.pdf
L2  - https://arxiv.org/abs/2403.06634
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Stochastic positional embeddings improve masked image modeling
AU  - Bar, Amir
AU  - Bordes, Florian
AU  - Shocher, Assaf
AU  - Assran, Mido
AU  - Vincent, Pascal
AU  - Ballas, Nicolas
AU  - Darrell, Trevor
AU  - Globerson, Amir
AU  - LeCun, Yann
T2  - Forty-first International Conference on Machine Learning
AB  - Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty to MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a gaussian distribution. We show that using StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, using StoP improves downstream MIM performance on a variety of downstream tasks. For example, linear probing on ImageNet using ViT-B is improved by $+1.7\%$, and by $2.5\%$ for ViT-H using 1% of the data.
DA  - 2024/06/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=hr8OXXMb7a
Y2  - 2024/07/19/11:33:42
L1  - https://openreview.net/pdf?id=hr8OXXMb7a
ER  - 

TY  - GEN
TI  - Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
AU  - Farebrother, Jesse
AU  - Orbay, Jordi
AU  - Vuong, Quan
AU  - Taïga, Adrien Ali
AU  - Chebotar, Yevgen
AU  - Xiao, Ted
AU  - Irpan, Alex
AU  - Levine, Sergey
AU  - Castro, Pablo Samuel
AU  - Faust, Aleksandra
AU  - Kumar, Aviral
AU  - Agarwal, Rishabh
AB  - Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.
DA  - 2024/03/06/
PY  - 2024
DO  - 10.48550/arXiv.2403.03950
DP  - arXiv.org
PB  - arXiv
ST  - Stop Regressing
UR  - http://arxiv.org/abs/2403.03950
Y2  - 2024/07/19/11:33:47
L1  - https://arxiv.org/pdf/2403.03950.pdf
L2  - https://arxiv.org/abs/2403.03950
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Unsupervised Concept Discovery Mitigates Spurious Correlations
AU  - Arefin, Md Rifat
AU  - Zhang, Yan
AU  - Baratin, Aristide
AU  - Locatello, Francesco
AU  - Rish, Irina
AU  - Liu, Dianbo
AU  - Kawaguchi, Kenji
AB  - Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT.
DA  - 2024/07/16/
PY  - 2024
DO  - 10.48550/arXiv.2402.13368
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.13368
Y2  - 2024/07/19/11:33:50
L1  - https://arxiv.org/pdf/2402.13368.pdf
L2  - https://arxiv.org/abs/2402.13368
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Universal Adversarial Triggers Are Not Universal
AU  - Meade, Nicholas
AU  - Patel, Arkil
AU  - Reddy, Siva
AB  - Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models. These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not universal. We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models.
DA  - 2024/04/24/
PY  - 2024
DO  - 10.48550/arXiv.2404.16020
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.16020
Y2  - 2024/07/19/11:35:15
L1  - https://arxiv.org/pdf/2404.16020.pdf
L2  - https://arxiv.org/abs/2404.16020
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision
AU  - Vani, Ankit
AU  - Nguyen, Bac
AU  - Lavoie, Samuel
AU  - Krishna, Ranjay
AU  - Courville, Aaron
AB  - Selective attention helps us focus on task-relevant aspects in the constant flood of our sensory input. This constraint in our perception allows us to robustly generalize under distractions and to new compositions of perceivable concepts. Transformers employ a similar notion of attention in their architecture, but representation learning models with transformer backbones like CLIP and DINO often fail to demonstrate robustness and compositionality. We highlight a missing architectural prior: unlike human perception, transformer encodings do not separately attend over individual concepts. In response, we propose SPARO, a read-out mechanism that partitions encodings into separately-attended slots, each produced by a single attention head. Using SPARO with CLIP imparts an inductive bias that the vision and text modalities are different views of a shared compositional world with the same corresponding concepts. Using SPARO, we demonstrate improvements on downstream recognition, robustness, retrieval, and compositionality benchmarks with CLIP (up to +14% for ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe for ImageNet with DINO (+3% each). We also showcase a powerful ability to intervene and select individual SPARO concepts to further improve downstream task performance (up from +4% to +9% for SugarCrepe) and use this ability to study the robustness of SPARO's representation structure. Finally, we provide insights through ablation experiments and visualization of learned concepts.
DA  - 2024/04/24/
PY  - 2024
DO  - 10.48550/arXiv.2404.15721
DP  - arXiv.org
PB  - arXiv
ST  - SPARO
UR  - http://arxiv.org/abs/2404.15721
Y2  - 2024/07/19/11:35:16
L1  - https://arxiv.org/pdf/2404.15721.pdf
L2  - https://arxiv.org/abs/2404.15721
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Introducing v0.5 of the AI Safety Benchmark from MLCommons
AU  - Vidgen, Bertie
AU  - Agrawal, Adarsh
AU  - Ahmed, Ahmed M.
AU  - Akinwande, Victor
AU  - Al-Nuaimi, Namir
AU  - Alfaraj, Najla
AU  - Alhajjar, Elie
AU  - Aroyo, Lora
AU  - Bavalatti, Trupti
AU  - Bartolo, Max
AU  - Blili-Hamelin, Borhane
AU  - Bollacker, Kurt
AU  - Bomassani, Rishi
AU  - Boston, Marisa Ferrara
AU  - Campos, Siméon
AU  - Chakra, Kal
AU  - Chen, Canyu
AU  - Coleman, Cody
AU  - Coudert, Zacharie Delpierre
AU  - Derczynski, Leon
AU  - Dutta, Debojyoti
AU  - Eisenberg, Ian
AU  - Ezick, James
AU  - Frase, Heather
AU  - Fuller, Brian
AU  - Gandikota, Ram
AU  - Gangavarapu, Agasthya
AU  - Gangavarapu, Ananya
AU  - Gealy, James
AU  - Ghosh, Rajat
AU  - Goel, James
AU  - Gohar, Usman
AU  - Goswami, Sujata
AU  - Hale, Scott A.
AU  - Hutiri, Wiebke
AU  - Imperial, Joseph Marvin
AU  - Jandial, Surgan
AU  - Judd, Nick
AU  - Juefei-Xu, Felix
AU  - Khomh, Foutse
AU  - Kailkhura, Bhavya
AU  - Kirk, Hannah Rose
AU  - Klyman, Kevin
AU  - Knotz, Chris
AU  - Kuchnik, Michael
AU  - Kumar, Shachi H.
AU  - Kumar, Srijan
AU  - Lengerich, Chris
AU  - Li, Bo
AU  - Liao, Zeyi
AU  - Long, Eileen Peters
AU  - Lu, Victor
AU  - Luger, Sarah
AU  - Mai, Yifan
AU  - Mammen, Priyanka Mary
AU  - Manyeki, Kelvin
AU  - McGregor, Sean
AU  - Mehta, Virendra
AU  - Mohammed, Shafee
AU  - Moss, Emanuel
AU  - Nachman, Lama
AU  - Naganna, Dinesh Jinenhally
AU  - Nikanjam, Amin
AU  - Nushi, Besmira
AU  - Oala, Luis
AU  - Orr, Iftach
AU  - Parrish, Alicia
AU  - Patlak, Cigdem
AU  - Pietri, William
AU  - Poursabzi-Sangdeh, Forough
AU  - Presani, Eleonora
AU  - Puletti, Fabrizio
AU  - Röttger, Paul
AU  - Sahay, Saurav
AU  - Santos, Tim
AU  - Scherrer, Nino
AU  - Sebag, Alice Schoenauer
AU  - Schramowski, Patrick
AU  - Shahbazi, Abolfazl
AU  - Sharma, Vin
AU  - Shen, Xudong
AU  - Sistla, Vamsi
AU  - Tang, Leonard
AU  - Testuggine, Davide
AU  - Thangarasa, Vithursan
AU  - Watkins, Elizabeth Anne
AU  - Weiss, Rebecca
AU  - Welty, Chris
AU  - Wilbers, Tyler
AU  - Williams, Adina
AU  - Wu, Carole-Jean
AU  - Yadav, Poonam
AU  - Yang, Xianjun
AU  - Zeng, Yi
AU  - Zhang, Wenhui
AU  - Zhdanov, Fedor
AU  - Zhu, Jiacheng
AU  - Liang, Percy
AU  - Mattson, Peter
AU  - Vanschoren, Joaquin
AB  - This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.
DA  - 2024/05/13/
PY  - 2024
DO  - 10.48550/arXiv.2404.12241
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.12241
Y2  - 2024/07/19/11:35:18
L1  - https://arxiv.org/pdf/2404.12241.pdf
L2  - https://arxiv.org/abs/2404.12241
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences
AU  - Shankar, Shreya
AU  - Zamfirescu-Pereira, J. D.
AU  - Hartmann, Björn
AU  - Parameswaran, Aditya G.
AU  - Arawjo, Ian
AB  - Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.
DA  - 2024/04/18/
PY  - 2024
DO  - 10.48550/arXiv.2404.12272
DP  - arXiv.org
PB  - arXiv
ST  - Who Validates the Validators?
UR  - http://arxiv.org/abs/2404.12272
Y2  - 2024/07/19/11:35:21
L1  - https://arxiv.org/pdf/2404.12272.pdf
L2  - https://arxiv.org/abs/2404.12272
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - JOUR
TI  - Government Interventions to Avert Future Catastrophic AI Risks
AU  - Bengio, Yoshua
T2  - Harvard Data Science Review
AB  - This essay is a revised transcription of Yoshua Bengio’s July 2023 testimony in front of the U.S. Senate Subcommittee on Privacy, Technology, and the Law meeting on the topic of oversight of AI. It argues for caution and government interventions in regulation and research investments to mitigate the potentially catastrophic outcomes from future advances in AI as the technology approaches human-level cognitive abilities. It summarizes the trends in advancing capabilities and the uncertain timeline to these future advances, as well as the different types of catastrophic scenarios that could follow, including both intentional and unintentional cases, misuse by bad actors, and intentional as well as unintentional loss of control of powerful AIs. It makes public policy recommendations that include national regulation, international agreements, and public research investments in AI safety as well as classified research investments to design aligned AI systems that can safely protect us from bad actors and uncontrolled dangerous AI systems. It highlights the need for strong democratic governance processes to control the safety and ethical use of future powerful AI systems, whether they are in private hands or under government authority.
DA  - 2024/06/04/
PY  - 2024
DO  - 10.1162/99608f92.d949f941
DP  - hdsr.mitpress.mit.edu
IS  - Special Issue 5
LA  - en
SN  - 2644-2353, 2688-8513
UR  - https://hdsr.mitpress.mit.edu/pub/w974bwb0/release/2
Y2  - 2024/07/19/11:35:29
L1  - https://hdsr.mitpress.mit.edu/pub/w974bwb0/download/pdf
ER  - 

TY  - GEN
TI  - What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models
AU  - Yu, Jeongrok
AU  - Kim, Seong Ug
AU  - Choi, Jacob
AU  - Choi, Jinho D.
AB  - Bias is a disproportionate prejudice in favor of one side against another. Due to the success of transformer-based Masked Language Models (MLMs) and their impact on many NLP tasks, a systematic evaluation of bias in these models is needed more than ever. While many studies have evaluated gender bias in English MLMs, only a few works have been conducted for the task in other languages. This paper proposes a multilingual approach to estimate gender bias in MLMs from 5 languages: Chinese, English, German, Portuguese, and Spanish. Unlike previous work, our approach does not depend on parallel corpora coupled with English to detect gender bias in other languages using multilingual lexicons. Moreover, a novel model-based method is presented to generate sentence pairs for a more robust analysis of gender bias, compared to the traditional lexicon-based method. For each language, both the lexicon-based and model-based methods are applied to create two datasets respectively, which are used to evaluate gender bias in an MLM specifically trained for that language using one existing and 3 new scoring metrics. Our results show that the previous approach is data-sensitive and not stable as it does not remove contextual dependencies irrelevant to gender. In fact, the results often flip when different scoring metrics are used on the same dataset, suggesting that gender bias should be studied on a large dataset using multiple evaluation metrics for best practice.
DA  - 2024/04/09/
PY  - 2024
DO  - 10.48550/arXiv.2404.06621
DP  - arXiv.org
PB  - arXiv
ST  - What is Your Favorite Gender, MLM?
UR  - http://arxiv.org/abs/2404.06621
Y2  - 2024/07/19/11:36:39
L1  - https://arxiv.org/pdf/2404.06621.pdf
L2  - https://arxiv.org/abs/2404.06621
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Evaluating Interventional Reasoning Capabilities of Large Language Models
AU  - Kasetty, Tejas
AU  - Mahajan, Divyat
AU  - Dziugaite, Gintare Karolina
AU  - Drouin, Alexandre
AU  - Sridhar, Dhanya
AB  - Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts.
DA  - 2024/04/08/
PY  - 2024
DO  - 10.48550/arXiv.2404.05545
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.05545
Y2  - 2024/07/19/11:36:42
L1  - https://arxiv.org/pdf/2404.05545.pdf
L2  - https://arxiv.org/abs/2404.05545
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Statistics - Methodology
ER  - 

TY  - JOUR
TI  - Regulating advanced artificial agents
AU  - Cohen, Michael K.
AU  - Kolt, Noam
AU  - Bengio, Yoshua
AU  - Hadfield, Gillian K.
AU  - Russell, Stuart
T2  - Science
DA  - 2024/04/05/
PY  - 2024
DO  - 10.1126/science.adl0625
DP  - science.org (Atypon)
VL  - 384
IS  - 6691
SP  - 36
EP  - 38
UR  - https://www.science.org/doi/10.1126/science.adl0625
Y2  - 2024/07/19/11:36:43
L1  - https://www.science.org/doi/pdf/10.1126/science.adl0625
ER  - 

TY  - JOUR
TI  - Scope Ambiguities in Large Language Models
AU  - Kamath, Gaurav
AU  - Schuster, Sebastian
AU  - Vajjala, Sowmya
AU  - Reddy, Siva
T2  - Transactions of the Association for Computational Linguistics
AB  - Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models -- GPT-2, GPT-3/3.5, Llama 2 and GPT-4 -- treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).
DA  - 2024/06/04/
PY  - 2024
DO  - 10.1162/tacl_a_00670
DP  - arXiv.org
VL  - 12
SP  - 738
EP  - 754
SN  - 2307-387X
UR  - http://arxiv.org/abs/2404.04332
Y2  - 2024/07/19/11:36:48
L1  - https://arxiv.org/pdf/2404.04332.pdf
L2  - https://arxiv.org/abs/2404.04332
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Towards Causal Deep Learning for Vulnerability Detection
AU  - Rahman, Md Mahbubur
AU  - Ceka, Ira
AU  - Mao, Chengzhi
AU  - Chakraborty, Saikat
AU  - Ray, Baishakhi
AU  - Le, Wei
T3  - ICSE '24
AB  - Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.
C1  - New York, NY, USA
C3  - Proceedings of the IEEE/ACM 46th International Conference on Software Engineering
DA  - 2024/04/12/
PY  - 2024
DO  - 10.1145/3597503.3639170
DP  - ACM Digital Library
SP  - 1
EP  - 11
PB  - Association for Computing Machinery
SN  - 9798400702174
UR  - https://doi.org/10.1145/3597503.3639170
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2310.07958
ER  - 

TY  - GEN
TI  - LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
AU  - BehnamGhader, Parishad
AU  - Adlakha, Vaibhav
AU  - Mosbach, Marius
AU  - Bahdanau, Dzmitry
AU  - Chapados, Nicolas
AU  - Reddy, Siva
AB  - Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.
DA  - 2024/04/08/
PY  - 2024
DO  - 10.48550/arXiv.2404.05961
DP  - arXiv.org
PB  - arXiv
ST  - LLM2Vec
UR  - http://arxiv.org/abs/2404.05961
Y2  - 2024/07/19/11:36:57
L1  - https://arxiv.org/pdf/2404.05961.pdf
L2  - https://arxiv.org/abs/2404.05961
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Machine Learning Robustness: A Primer
AU  - Braiek, Houssem Ben
AU  - Khomh, Foutse
AB  - This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms. It covers non-adversarial data shifts and nuances of Deep Learning (DL) software testing methodologies. The discussion progresses to explore amelioration strategies for bolstering robustness, starting with data-centric approaches like debiasing and augmentation. Further examination includes a variety of model-centric methods such as transfer learning, adversarial training, and randomized smoothing. Lastly, post-training methods are discussed, including ensemble techniques, pruning, and model repairs, emerging as cost-effective strategies to make models more resilient against the unpredictable. This chapter underscores the ongoing challenges and limitations in estimating and achieving ML robustness by existing approaches. It offers insights and directions for future research on this crucial concept, as a prerequisite for trustworthy AI systems.
DA  - 2024/05/03/
PY  - 2024
DO  - 10.48550/arXiv.2404.00897
DP  - arXiv.org
PB  - arXiv
ST  - Machine Learning Robustness
UR  - http://arxiv.org/abs/2404.00897
Y2  - 2024/07/19/11:38:01
L1  - https://arxiv.org/pdf/2404.00897.pdf
L2  - https://arxiv.org/abs/2404.00897
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Improving Text-to-Image Consistency via Automatic Prompt Optimization
AU  - Mañas, Oscar
AU  - Astolfi, Pietro
AU  - Hall, Melissa
AU  - Ross, Candace
AU  - Urbanek, Jack
AU  - Williams, Adina
AU  - Agrawal, Aishwarya
AU  - Romero-Soriano, Adriana
AU  - Drozdzal, Michal
AB  - Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.
DA  - 2024/03/26/
PY  - 2024
DO  - 10.48550/arXiv.2403.17804
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.17804
Y2  - 2024/07/19/11:38:03
L1  - https://arxiv.org/pdf/2403.17804.pdf
L2  - https://arxiv.org/abs/2403.17804
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Application-Driven Innovation in Machine Learning
AU  - Rolnick, David
AU  - Aspuru-Guzik, Alan
AU  - Beery, Sara
AU  - Dilkina, Bistra
AU  - Donti, Priya L.
AU  - Ghassemi, Marzyeh
AU  - Kerner, Hannah
AU  - Monteleoni, Claire
AU  - Rolf, Esther
AU  - Tambe, Milind
AU  - White, Adam
AB  - As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.
DA  - 2024/03/26/
PY  - 2024
DO  - 10.48550/arXiv.2403.17381
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.17381
Y2  - 2024/07/19/11:38:06
L1  - https://arxiv.org/pdf/2403.17381.pdf
L2  - https://arxiv.org/abs/2403.17381
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation
AU  - Kartik, Kartik
AU  - Soni, Sanjana
AU  - Kunchukuttan, Anoop
AU  - Chakraborty, Tanmoy
AU  - Akhtar, Md Shad
AB  - The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.
DA  - 2024/04/29/
PY  - 2024
DO  - 10.48550/arXiv.2403.16771
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.16771
Y2  - 2024/07/19/11:38:08
L1  - https://arxiv.org/pdf/2403.16771.pdf
L2  - https://arxiv.org/abs/2403.16771
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Generalizing across Temporal Domains with Koopman Operators
AU  - Zeng, Qiuhao
AU  - Wang, Wei
AU  - Zhou, Fan
AU  - Xu, Gezheng
AU  - Pu, Ruizhi
AU  - Shui, Changjian
AU  - Gagné, Christian
AU  - Yang, Shichun
AU  - Ling, Charles X.
AU  - Wang, Boyu
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Neural Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.
DA  - 2024/03/24/
PY  - 2024
DO  - 10.1609/aaai.v38i15.29604
DP  - ojs.aaai.org
VL  - 38
IS  - 15
SP  - 16651
EP  - 16659
LA  - en
SN  - 2374-3468
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/29604
Y2  - 2024/07/19/11:38:25
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/29604/31020
KW  - ML: Time-Series/Data Streams
ER  - 

TY  - JOUR
TI  - Adversarial Attacks on the Interpretation of Neuron Activation Maximization
AU  - Nanfack, Geraldin
AU  - Fulleringer, Alexander
AU  - Marty, Jonathan
AU  - Eickenberg, Michael
AU  - Belilovsky, Eugene
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.
DA  - 2024/03/24/
PY  - 2024
DO  - 10.1609/aaai.v38i5.28228
DP  - ojs.aaai.org
VL  - 38
IS  - 5
SP  - 4315
EP  - 4324
LA  - en
SN  - 2374-3468
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/28228
Y2  - 2024/07/19/11:38:25
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/28228/28451
KW  - Interpretable
KW  - Explainable ML
KW  - ML: Transparent
ER  - 

TY  - GEN
TI  - From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
AU  - Chehbouni, Khaoula
AU  - Roshan, Megha
AU  - Ma, Emmanuel
AU  - Wei, Futian Andrew
AU  - Taik, Afaf
AU  - Cheung, Jackie CK
AU  - Farnadi, Golnoosh
AB  - Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.
DA  - 2024/07/05/
PY  - 2024
DO  - 10.48550/arXiv.2403.13213
DP  - arXiv.org
PB  - arXiv
ST  - From Representational Harms to Quality-of-Service Harms
UR  - http://arxiv.org/abs/2403.13213
Y2  - 2024/07/19/11:40:16
L1  - https://arxiv.org/pdf/2403.13213.pdf
L2  - https://arxiv.org/abs/2403.13213
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Safety Cases: How to Justify the Safety of Advanced AI Systems
AU  - Clymer, Joshua
AU  - Gabrieli, Nick
AU  - Krueger, David
AU  - Larsen, Thomas
AB  - As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.
DA  - 2024/03/18/
PY  - 2024
DO  - 10.48550/arXiv.2403.10462
DP  - arXiv.org
PB  - arXiv
ST  - Safety Cases
UR  - http://arxiv.org/abs/2403.10462
Y2  - 2024/07/19/11:40:17
L1  - https://arxiv.org/pdf/2403.10462.pdf
L2  - https://arxiv.org/abs/2403.10462
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - On the Identifiability of Quantized Factors
AU  - Barin-Pacela, Vitória
AU  - Ahuja, Kartik
AU  - Lacoste-Julien, Simon
AU  - Vincent, Pascal
T2  - Causal Learning and Reasoning
AB  - Disentanglement  aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors is proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations. In this work, however, we demonstrate that it is possible to recover quantized latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have independent discontinuities in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed quantized factor identifiability, and provide a comprehensive proof of the recovery of the quantized factors.
C3  - Proceedings of the Third Conference on Causal Learning and Reasoning
DA  - 2024/03/15/
PY  - 2024
DP  - proceedings.mlr.press
SP  - 384
EP  - 422
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v236/barin-pacela24a.html
Y2  - 2024/07/19/11:40:19
L1  - https://proceedings.mlr.press/v236/barin-pacela24a/barin-pacela24a.pdf
ER  - 

TY  - GEN
TI  - Bugs in Large Language Models Generated Code: An Empirical Study
AU  - Tambon, Florian
AU  - Dakhel, Arghavan Moradi
AU  - Nikanjam, Amin
AU  - Khomh, Foutse
AU  - Desmarais, Michel C.
AU  - Antoniol, Giuliano
AB  - Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.
DA  - 2024/03/18/
PY  - 2024
DO  - 10.48550/arXiv.2403.08937
DP  - arXiv.org
PB  - arXiv
ST  - Bugs in Large Language Models Generated Code
UR  - http://arxiv.org/abs/2403.08937
Y2  - 2024/07/19/11:40:23
L1  - https://arxiv.org/pdf/2403.08937.pdf
L2  - https://arxiv.org/abs/2403.08937
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons
AU  - Dufort-Labbé, Simon
AU  - D'Oro, Pierluca
AU  - Nikishin, Evgenii
AU  - Pascanu, Razvan
AU  - Bacon, Pierre-Luc
AU  - Baratin, Aristide
AB  - When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization.
DA  - 2024/03/12/
PY  - 2024
DO  - 10.48550/arXiv.2403.07688
DP  - arXiv.org
PB  - arXiv
ST  - Maxwell's Demon at Work
UR  - http://arxiv.org/abs/2403.07688
Y2  - 2024/07/19/11:40:27
L1  - https://arxiv.org/pdf/2403.07688.pdf
L2  - https://arxiv.org/abs/2403.07688
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Rethinking Machine Learning Benchmarks in the Context of Professional Codes of Conduct
AU  - Henderson, Peter
AU  - Hu, Jieru
AU  - Diab, Mona
AU  - Pineau, Joelle
T3  - CSLAW '24
AB  - Benchmarking efforts for machine learning have often mimicked (or even explicitly used) professional licensing exams to assess capabilities in a given area, focusing primarily on accuracy as the metric of choice. However, this approach neglects a variety of essential skills required in professional settings. We propose that professional codes of conduct and rules can guide machine learning researchers to address potential gaps in benchmark construction. These guidelines frequently account for situations professionals may encounter and must handle with care. A model may excel on an exam but still fall short in critical scenarios, deemed unacceptable under professional codes or rules. To motivate this idea, we conduct a case study and comparative examination of machine translation in legal settings. We point out several areas where standard deployments and benchmarks do not assess key requirements under professional rules. We suggest further refinements that would bring the two closer together, including requiring a measurement of uncertainty so that models opt out of uncertain translations. We then share broader insights on constructing and deploying foundation models, particularly in critical domains like law and legal translation.
C1  - New York, NY, USA
C3  - Proceedings of the Symposium on Computer Science and Law
DA  - 2024/03/12/
PY  - 2024
DO  - 10.1145/3614407.3643708
DP  - ACM Digital Library
SP  - 109
EP  - 120
PB  - Association for Computing Machinery
SN  - 9798400703331
UR  - https://doi.org/10.1145/3614407.3643708
Y2  - 2024/07/19/
L1  - https://dl.acm.org/doi/pdf/10.1145/3614407.3643708
ER  - 

TY  - GEN
TI  - Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping
AU  - Lehnert, Lucas
AU  - Sukhbaatar, Sainbayar
AU  - Su, DiJia
AU  - Zheng, Qinqing
AU  - Mcvay, Paul
AU  - Rabbat, Michael
AU  - Tian, Yuandong
AB  - While Transformers have enabled tremendous progress in various application settings, such architectures still trail behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than the $A^*$ implementation that was used for training initially. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\times$ smaller model size and a 10$\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics.
DA  - 2024/04/26/
PY  - 2024
DO  - 10.48550/arXiv.2402.14083
DP  - arXiv.org
PB  - arXiv
ST  - Beyond A*
UR  - http://arxiv.org/abs/2402.14083
Y2  - 2024/07/19/11:40:36
L1  - https://arxiv.org/pdf/2402.14083.pdf
L2  - https://arxiv.org/abs/2402.14083
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Investigating Robot Influence on Human Behaviour By Leveraging Entrainment Effects
AU  - Zhu, Lixiao
AU  - Moon, AJung
T3  - HRI '24
AB  - Humans naturally tend to synchronize their movements with others, a phenomenon known as the entrainment effect, whether voluntarily or involuntarily. This phenomenon extends to interactions between humans and robots, which could have either positive or negative consequences for the human partner. We propose a human-subject study aimed at investigating the use of robots to influence human behaviour through entrainment in diverse Human-Robot Interaction (HRI) scenarios. The current work involves two human-subject experiments investigating the impact of robots on short-term human behaviour, encompassing human-human and human-robot interactions. The goal is to comprehend how variations in robot actions, such as movement frequency during repetitive tasks, influence human perceptions and behaviours in collaborative lab-based settings. Another objective is to investigate the factors that make participants aware of the entrainment effect during HRI. The preliminary results of the HHI experiment provide evidence that individuals tend to synchronize their movements with another person.
C1  - New York, NY, USA
C3  - Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2024/03/11/
PY  - 2024
DO  - 10.1145/3610978.3638367
DP  - ACM Digital Library
SP  - 169
EP  - 171
PB  - Association for Computing Machinery
SN  - 9798400703232
UR  - https://doi.org/10.1145/3610978.3638367
Y2  - 2024/07/19/
ER  - 

TY  - CONF
TI  - Global AI Cultures
AU  - Qadri, Rida
AU  - Diaz, Fernando
AU  - Subramonian, Arjun
AU  - Dev, Sunipa
AU  - Born, Georgina Emma
AU  - Gray, Mary L.
AU  - Quaye, Jessica
AU  - Bergmann, Rachel
T2  - ICLR 2024 Workshops
AB  - Building globally-inclusive generative artificial intelligence (genAI) that encodes, respects, and valorizes cultural sensibilities as well as performs well for users across cultural contexts, is an important goal as we deploy generative AI products globally. If we are to build such inclusive AI for people, we must both have a better understanding of how we can make our AI pipeline more globally inclusive and how AI technologies can impact or shape cultures it is deployed in. If unexamined, this relationship between AI and culture will universalize western-centered AI and have unforeseen impacts on global cultural production, values and consumptions. However, this is not a relationship AI scholarship can understand on its own. We need an engagement with existing theories on the interplay between technology and culture, and how both can shape each other. We urgently thus need a cross-disciplinary and cross-community framework for understanding this multifaceted relationship between AI and Culture. This workshop aims to begin a conversation between core AI researchers and experts from the social sciences and humanities with a focus on the impact of generative AI on cultures and the cultural exclusions embedded in our on generative AI pipelines. Through this focus, the workshop will encourage field building on deepening our understanding for how we can build and deploy globally inclusive genAI and how we can responsibly encode cultural knowledge into our technologies.
DA  - 2024/03/08/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=gw8aq110vD
Y2  - 2024/07/19/11:43:10
L1  - https://openreview.net/pdf?id=gw8aq110vD
ER  - 

TY  - CONF
TI  - Generative Models for Decision Making
AU  - Mazoure, Bogdan
AU  - Lee, Lisa
AU  - Raileanu, Roberta
AU  - Du, Yilun
AU  - Talbott, Walter
AU  - Metcalf, Katherine
AU  - Hjelm, R. Devon
AU  - Toshev, Alexander T.
T2  - ICLR 2024 Workshops
AB  - Generative Artificial Intelligence (AI) has made significant advancements in recent years, particularly with the development of large language and diffusion models. These generative models have demonstrated impressive capabilities in various tasks, such as text generation and image and audio synthesis. Concurrently, Reinforcement Learning (RL) has made significant strides in solving complex sequential decision-making problems with the help of external knowledge sources . However, there remains untapped potential in combining generative models with RL algorithms to tackle real-world challenges, particularly to improve sample efficiency of tabula rasa training by introducing priors from related domains such as visual question-answering, image captioning and image generation. This workshop aims to bring together researchers and practitioners from the fields of generative AI and reinforcement learning to explore the latest advances, methodologies, and applications. By fostering collaborations between these two domains, we intend to unlock new opportunities for addressing complex problems that lie at the intersection of both fields.
DA  - 2024/03/08/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=lzXisiFRgD
Y2  - 2024/07/19/11:43:13
L1  - https://openreview.net/pdf?id=lzXisiFRgD
ER  - 

TY  - CONF
TI  - Self-evaluation and self-prompting to improve the reliability of LLMs
AU  - Piché, Alexandre
AU  - Milios, Aristides
AU  - Bahdanau, Dzmitry
AU  - Pal, Christopher
T2  - ICLR 2024 Workshop on Secure and Trustworthy Large Language Models
AB  - In order to safely deploy Large Language Models (LLMs), they must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a simple objective that can encourage the model to produce generation that the model is confident in. To optimize this objective, we introduce ReSearch, an iterative search algorithm based on self-evaluation and self-prompting. Our method results in fewer hallucinations overall, both for known and unknown topics, as the model learns to selectively restrain itself. In addition, our method elegantly incorporates the ability to decline, when the model assesses that it cannot provide a response without a high proportion of hallucination.
DA  - 2024/04/14/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=IwB45cjqC8
Y2  - 2024/07/19/11:43:44
L1  - https://openreview.net/pdf?id=IwB45cjqC8
ER  - 

TY  - GEN
TI  - Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok
AU  - Notsawo, Pascal Jr Tikeng
AU  - Zhou, Hattie
AU  - Pezeshki, Mohammad
AU  - Rish, Irina
AU  - Dumas, Guillaume
AB  - This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quantify the amplitude of low-frequency components to detect the presence of such oscillations. We also present additional experiments aimed at explaining the cause of these oscillations and characterizing the loss landscape.
DA  - 2023/09/28/
PY  - 2023
DO  - 10.48550/arXiv.2306.13253
DP  - arXiv.org
PB  - arXiv
ST  - Predicting Grokking Long Before it Happens
UR  - http://arxiv.org/abs/2306.13253
Y2  - 2024/07/19/11:43:47
L1  - https://arxiv.org/pdf/2306.13253.pdf
L2  - https://arxiv.org/abs/2306.13253
KW  - Computer Science - Machine Learning
KW  - I.2.6
ER  - 

TY  - GEN
TI  - A Generative Model of Symmetry Transformations
AU  - Allingham, James Urquhart
AU  - Mlodozeniec, Bruno Kacper
AU  - Padhy, Shreyas
AU  - Antorán, Javier
AU  - Krueger, David
AU  - Turner, Richard E.
AU  - Nalisnick, Eric
AU  - Hernández-Lobato, José Miguel
AB  - Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we take inspiration from group theoretic ideas to construct a generative model that explicitly aims to capture the data's approximate symmetries. This results in a model that, given a prespecified broad set of possible symmetries, learns to what extent, if at all, those symmetries are actually present. Our model can be seen as a generative process for data augmentation. We provide a simple algorithm for learning our generative model and empirically demonstrate its ability to capture symmetries under affine and color transformations, in an interpretable way. Combining our symmetry model with standard generative models results in higher marginal test-log-likelihoods and improved data efficiency.
DA  - 2024/06/20/
PY  - 2024
DO  - 10.48550/arXiv.2403.01946
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.01946
Y2  - 2024/07/19/11:43:51
L1  - https://arxiv.org/pdf/2403.01946.pdf
L2  - https://arxiv.org/abs/2403.01946
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Disentangling the Causes of Plasticity Loss in Neural Networks
AU  - Lyle, Clare
AU  - Zheng, Zeyu
AU  - Khetarpal, Khimya
AU  - van Hasselt, Hado
AU  - Pascanu, Razvan
AU  - Martens, James
AU  - Dabney, Will
AB  - Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \textit{stationary} data distribution. In settings where this assumption is violated, e.g.\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.
DA  - 2024/02/28/
PY  - 2024
DO  - 10.48550/arXiv.2402.18762
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.18762
Y2  - 2024/07/19/11:43:51
L1  - https://arxiv.org/pdf/2402.18762.pdf
L2  - https://arxiv.org/abs/2402.18762
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Challenges and Opportunities in Generative AI
AU  - Manduchi, Laura
AU  - Pandey, Kushagra
AU  - Bamler, Robert
AU  - Cotterell, Ryan
AU  - Däubener, Sina
AU  - Fellenz, Sophie
AU  - Fischer, Asja
AU  - Gärtner, Thomas
AU  - Kirchler, Matthias
AU  - Kloft, Marius
AU  - Li, Yingzhen
AU  - Lippert, Christoph
AU  - de Melo, Gerard
AU  - Nalisnick, Eric
AU  - Ommer, Björn
AU  - Ranganath, Rajesh
AU  - Rudolph, Maja
AU  - Ullrich, Karen
AU  - Broeck, Guy Van den
AU  - Vogt, Julia E.
AU  - Wang, Yixin
AU  - Wenzel, Florian
AU  - Wood, Frank
AU  - Mandt, Stephan
AU  - Fortuin, Vincent
AB  - The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI solutions.
DA  - 2024/02/28/
PY  - 2024
DO  - 10.48550/arXiv.2403.00025
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.00025
Y2  - 2024/07/19/11:43:53
L1  - https://arxiv.org/pdf/2403.00025.pdf
L2  - https://arxiv.org/abs/2403.00025
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Societal Impact of Open Foundation Models
AU  - Kapoor, Sayash
AU  - Bommasani, Rishi
AU  - Klyman, Kevin
AU  - Longpre, Shayne
AU  - Ramaswami, Ashwin
AU  - Cihon, Peter
AU  - Hopkins, Aspen
AU  - Bankston, Kevin
AU  - Biderman, Stella
AU  - Bogen, Miranda
AU  - Chowdhury, Rumman
AU  - Engler, Alex
AU  - Henderson, Peter
AU  - Jernite, Yacine
AU  - Lazar, Seth
AU  - Maffulli, Stefano
AU  - Nelson, Alondra
AU  - Pineau, Joelle
AU  - Skowron, Aviya
AU  - Song, Dawn
AU  - Storchan, Victor
AU  - Zhang, Daniel
AU  - Ho, Daniel E.
AU  - Liang, Percy
AU  - Narayanan, Arvind
AB  - Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.
DA  - 2024/02/27/
PY  - 2024
DO  - 10.48550/arXiv.2403.07918
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.07918
Y2  - 2024/07/19/11:43:55
L1  - https://arxiv.org/pdf/2403.07918.pdf
L2  - https://arxiv.org/abs/2403.07918
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Computing Power and the Governance of Artificial Intelligence
AU  - Sastry, Girish
AU  - Heim, Lennart
AU  - Belfield, Haydn
AU  - Anderljung, Markus
AU  - Brundage, Miles
AU  - Hazell, Julian
AU  - O'Keefe, Cullen
AU  - Hadfield, Gillian K.
AU  - Ngo, Richard
AU  - Pilz, Konstantin
AU  - Gor, George
AU  - Bluemke, Emma
AU  - Shoker, Sarah
AU  - Egan, Janet
AU  - Trager, Robert F.
AU  - Avin, Shahar
AU  - Weller, Adrian
AU  - Bengio, Yoshua
AU  - Coyle, Diane
AB  - Computing power, or "compute," is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.
DA  - 2024/02/13/
PY  - 2024
DO  - 10.48550/arXiv.2402.08797
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.08797
Y2  - 2024/07/19/12:01:47
L1  - https://arxiv.org/pdf/2402.08797.pdf
L2  - https://arxiv.org/abs/2402.08797
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
AU  - Majdinasab, Vahid
AU  - Nikanjam, Amin
AU  - Khomh, Foutse
AB  - Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.
DA  - 2024/02/14/
PY  - 2024
DO  - 10.48550/arXiv.2402.09299
DP  - arXiv.org
PB  - arXiv
ST  - Trained Without My Consent
UR  - http://arxiv.org/abs/2402.09299
Y2  - 2024/07/19/12:01:51
L1  - https://arxiv.org/pdf/2402.09299.pdf
L2  - https://arxiv.org/abs/2402.09299
KW  - Computer Science - Machine Learning
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Mixtures of Experts Unlock Parameter Scaling for Deep RL
AU  - Obando-Ceron, Johan
AU  - Sokar, Ghada
AU  - Willi, Timon
AU  - Lyle, Clare
AU  - Farebrother, Jesse
AU  - Foerster, Jakob
AU  - Dziugaite, Gintare Karolina
AU  - Precup, Doina
AU  - Castro, Pablo Samuel
AB  - The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
DA  - 2024/06/26/
PY  - 2024
DO  - 10.48550/arXiv.2402.08609
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.08609
Y2  - 2024/07/19/12:01:55
L1  - https://arxiv.org/pdf/2402.08609.pdf
L2  - https://arxiv.org/abs/2402.08609
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Antagonistic AI
AU  - Cai, Alice
AU  - Arawjo, Ian
AU  - Glassman, Elena L.
AB  - The vast majority of discourse around AI development assumes that subservient, "moral" models aligned with "human values" are universally beneficial -- in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values. Far from being "bad" or "immoral," we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethical challenges of this space and identify three dimensions for the responsible design of antagonistic AI -- consent, context, and framing.
DA  - 2024/02/11/
PY  - 2024
DO  - 10.48550/arXiv.2402.07350
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.07350
Y2  - 2024/07/19/12:02:01
L1  - https://arxiv.org/pdf/2402.07350.pdf
L2  - https://arxiv.org/abs/2402.07350
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
KW  - J.0
KW  - I.2.0
KW  - K.4.0
ER  - 

TY  - GEN
TI  - In-Context Learning Can Re-learn Forbidden Tasks
AU  - Xhonneux, Sophie
AU  - Dobre, David
AU  - Tang, Jian
AU  - Gidel, Gauthier
AU  - Sridhar, Dhanya
AB  - Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains LLM-generated text with violence, suicide, and misinformation.
DA  - 2024/02/08/
PY  - 2024
DO  - 10.48550/arXiv.2402.05723
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.05723
Y2  - 2024/07/19/12:02:06
L1  - https://arxiv.org/pdf/2402.05723.pdf
L2  - https://arxiv.org/abs/2402.05723
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation
AU  - Vianna, Pedro
AU  - Chaudhary, Muawiz
AU  - Mehrbod, Paria
AU  - Tang, An
AU  - Cloutier, Guy
AU  - Wolf, Guy
AU  - Eickenberg, Michael
AU  - Belilovsky, Eugene
AB  - Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution. For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets. To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference. Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks. It is implemented by recalculating batch normalization statistics on test batches. Prior work has focused on analysis with test data that has the same label distribution as the training data. However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. This presents a risk in applying test time adaptation methods in deployment. We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts. Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes. We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts. We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.
DA  - 2024/05/29/
PY  - 2024
DO  - 10.48550/arXiv.2402.04958
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.04958
Y2  - 2024/07/19/12:02:11
L1  - https://arxiv.org/pdf/2402.04958.pdf
L2  - https://arxiv.org/abs/2402.04958
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Toward Human-AI Alignment in Large-Scale Multi-Player Games
AU  - Sharma, Sugandha
AU  - Davidson, Guy
AU  - Khetarpal, Khimya
AU  - Kanervisto, Anssi
AU  - Arora, Udit
AU  - Hofmann, Katja
AU  - Momennejad, Ida
AB  - Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.
DA  - 2024/06/18/
PY  - 2024
DO  - 10.48550/arXiv.2402.03575
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.03575
Y2  - 2024/07/19/12:02:18
L1  - https://arxiv.org/pdf/2402.03575.pdf
L2  - https://arxiv.org/abs/2402.03575
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Blockwise Self-Supervised Learning at Scale
AU  - Siddiqui, Shoaib Ahmed
AU  - Krueger, David
AU  - LeCun, Yann
AU  - Deny, Stéphane
AB  - Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.
DA  - 2023/02/03/
PY  - 2023
DO  - 10.48550/arXiv.2302.01647
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.01647
Y2  - 2024/07/19/12:03:46
L1  - https://arxiv.org/pdf/2302.01647.pdf
L2  - https://arxiv.org/abs/2302.01647
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - Black-Box Access is Insufficient for Rigorous AI Audits
AU  - Casper, Stephen
AU  - Ezell, Carson
AU  - Siegmann, Charlotte
AU  - Kolt, Noam
AU  - Curtis, Taylor Lynn
AU  - Bucknall, Benjamin
AU  - Haupt, Andreas
AU  - Wei, Kevin
AU  - Scheurer, Jérémy
AU  - Hobbhahn, Marius
AU  - Sharkey, Lee
AU  - Krishna, Satyapriya
AU  - Von Hagen, Marvin
AU  - Alberti, Silas
AU  - Chan, Alan
AU  - Sun, Qinyi
AU  - Gerovitch, Michael
AU  - Bau, David
AU  - Tegmark, Max
AU  - Krueger, David
AU  - Hadfield-Menell, Dylan
AB  - External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.
C3  - The 2024 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2024/06/03/
PY  - 2024
DO  - 10.1145/3630106.3659037
DP  - arXiv.org
SP  - 2254
EP  - 2272
UR  - http://arxiv.org/abs/2401.14446
Y2  - 2024/07/19/12:03:49
L1  - https://arxiv.org/pdf/2401.14446.pdf
L2  - https://arxiv.org/abs/2401.14446
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Visibility into AI Agents
AU  - Chan, Alan
AU  - Ezell, Carson
AU  - Kaufmann, Max
AU  - Wei, Kevin
AU  - Hammond, Lewis
AU  - Bradley, Herbie
AU  - Bluemke, Emma
AU  - Rajkumar, Nitarshan
AU  - Krueger, David
AU  - Kolt, Noam
AU  - Heim, Lennart
AU  - Anderljung, Markus
AB  - Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.
DA  - 2024/05/17/
PY  - 2024
DO  - 10.48550/arXiv.2401.13138
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.13138
Y2  - 2024/07/19/12:04:08
L1  - https://arxiv.org/pdf/2401.13138.pdf
L2  - https://arxiv.org/abs/2401.13138
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Amortizing intractable inference in large language models
AU  - Hu, Edward J.
AU  - Jain, Moksh
AU  - Elmoznino, Eric
AU  - Kaddar, Younesse
AU  - Lajoie, Guillaume
AU  - Bengio, Yoshua
AU  - Malkin, Nikolay
AB  - Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.
DA  - 2024/03/13/
PY  - 2024
DO  - 10.48550/arXiv.2310.04363
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.04363
Y2  - 2024/07/19/12:04:11
L1  - https://arxiv.org/pdf/2310.04363.pdf
L2  - https://arxiv.org/abs/2310.04363
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Balancing Act: Constraining Disparate Impact in Sparse Models
AU  - Hashemizadeh, Meraj
AU  - Ramirez, Juan
AU  - Sukumaran, Rohan
AU  - Farnadi, Golnoosh
AU  - Lacoste-Julien, Simon
AU  - Gallego-Posada, Jose
AB  - Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.
DA  - 2024/03/07/
PY  - 2024
DO  - 10.48550/arXiv.2310.20673
DP  - arXiv.org
PB  - arXiv
ST  - Balancing Act
UR  - http://arxiv.org/abs/2310.20673
Y2  - 2024/07/19/12:04:14
L1  - https://arxiv.org/pdf/2310.20673.pdf
L2  - https://arxiv.org/abs/2310.20673
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Bridging State and History Representations: Understanding Self-Predictive RL
AU  - Ni, Tianwei
AU  - Eysenbach, Benjamin
AU  - Seyedsalehi, Erfan
AU  - Ma, Michel
AU  - Gehring, Clement
AU  - Mahajan, Aditya
AU  - Bacon, Pierre-Luc
AB  - Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with distractors, and POMDPs with sparse rewards. These findings culminate in a set of preliminary guidelines for RL practitioners.
DA  - 2024/04/21/
PY  - 2024
DO  - 10.48550/arXiv.2401.08898
DP  - arXiv.org
PB  - arXiv
ST  - Bridging State and History Representations
UR  - http://arxiv.org/abs/2401.08898
Y2  - 2024/07/19/12:04:21
L1  - https://arxiv.org/pdf/2401.08898.pdf
L2  - https://arxiv.org/abs/2401.08898
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View
AU  - Ghugare, Raj
AU  - Geist, Matthieu
AU  - Berseth, Glen
AU  - Eysenbach, Benjamin
AB  - Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models. Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization. Nonetheless, the connection between stitching and combinatorial generalisation also suggests a simple remedy for improving generalisation in SL: data augmentation. We propose a temporal data augmentation and demonstrate that adding it to SL-based methods enables them to successfully complete tasks not seen together during training. On a high level, this connection illustrates the importance of combinatorial generalization for data efficiency in time-series data beyond tasks beyond RL, like audio, video, or text.
DA  - 2024/03/11/
PY  - 2024
DO  - 10.48550/arXiv.2401.11237
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.11237
Y2  - 2024/07/19/12:04:30
L1  - https://arxiv.org/pdf/2401.11237.pdf
L2  - https://arxiv.org/abs/2401.11237
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning
AU  - Zhao, Harry
AU  - Alver, Safa
AU  - Seijen, Harm van
AU  - Laroche, Romain
AU  - Precup, Doina
AU  - Bengio, Yoshua
T2  - The Twelfth International Conference on Learning Representations
AB  - Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning framework utilizing spatio-temporal abstractions to generalize better in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and thus enables sparse decision-making and focused computation on the relevant parts of the environment. The decomposition relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper’s significant advantage in zero-shot generalization, compared to some existing state-of-the-art hierarchical planning methods.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=eo9dHwtTFt
Y2  - 2024/07/19/12:04:44
L1  - https://openreview.net/pdf?id=eo9dHwtTFt
ER  - 

TY  - CONF
TI  - Decoupling regularization from the action space
AU  - Mohammadpour, Sobhan
AU  - Frejinger, Emma
AU  - Bacon, Pierre-Luc
T2  - The Twelfth International Conference on Learning Representations
AB  - Regularized reinforcement learning (RL), particularly the entropy-regularized kind, has gained traction in optimal control and inverse RL. While standard unregularized RL methods remain unaffected by changes in the number of actions, we show that it can severely impact their regularized counterparts. This paper demonstrates the importance of decoupling the regularizer from the action space: that is, to maintain a consistent level of regularization regardless of how many actions are involved to avoid over-regularization. Whereas the problem can be avoided by introducing a task-specific temperature parameter, it is often undesirable and cannot solve the problem when action spaces are state-dependent. In the state-dependent action context, different states with varying action spaces are regularized inconsistently. We introduce two solutions: a static temperature selection approach and a dynamic counterpart, universally applicable where this problem arises. Implementing these changes improves performance on the DeepMind control suite in static and dynamic temperature regimes and a biological design task.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=UaMgmoKEBj
Y2  - 2024/07/19/12:04:49
L1  - https://openreview.net/pdf?id=UaMgmoKEBj
ER  - 

TY  - GEN
TI  - Hallucination Detection and Hallucination Mitigation: An Investigation
AU  - Luo, Junliang
AU  - Li, Tianyu
AU  - Wu, Di
AU  - Jenkin, Michael
AU  - Liu, Steve
AU  - Dudek, Gregory
AB  - Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.
DA  - 2024/01/16/
PY  - 2024
DO  - 10.48550/arXiv.2401.08358
DP  - arXiv.org
PB  - arXiv
ST  - Hallucination Detection and Hallucination Mitigation
UR  - http://arxiv.org/abs/2401.08358
Y2  - 2024/07/19/12:07:58
L1  - https://arxiv.org/pdf/2401.08358.pdf
L2  - https://arxiv.org/abs/2401.08358
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Improving Intrinsic Exploration by Creating Stationary Objectives
AU  - Castanyer, Roger Creus
AU  - Romoff, Joshua
AU  - Berseth, Glen
AB  - Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Several exploration objectives like count-based bonuses, pseudo-counts, and state-entropy maximization are non-stationary and hence are difficult to optimize for the agent. While this issue is generally known, it is usually omitted and solutions remain under-explored. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. We show that SOFE improves the performance of several exploration objectives, including count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover, SOFE outperforms prior methods that attempt to stabilize the optimization of intrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.
DA  - 2024/04/22/
PY  - 2024
DO  - 10.48550/arXiv.2310.18144
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.18144
Y2  - 2024/07/19/12:08:03
L1  - https://arxiv.org/pdf/2310.18144.pdf
L2  - https://arxiv.org/abs/2310.18144
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Intelligent Switching for Reset-Free RL
AU  - Patil, Darshan
AU  - Rajendran, Janarthanan
AU  - Berseth, Glen
AU  - Chandar, Sarath
T2  - The Twelfth International Conference on Learning Representations
AB  - In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The resetting assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (forward) with learned resets by constructing a second (backward) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent’s confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Nq45xeghcL
Y2  - 2024/07/19/12:08:06
L1  - https://openreview.net/pdf?id=Nq45xeghcL
ER  - 

TY  - GEN
TI  - Large Language Models as Generalizable Policies for Embodied Tasks
AU  - Szot, Andrew
AU  - Schwarzer, Max
AU  - Agrawal, Harsh
AU  - Mazoure, Bogdan
AU  - Talbott, Walter
AU  - Metcalf, Katherine
AU  - Mackraz, Natalie
AU  - Hjelm, Devon
AU  - Toshev, Alexander
AB  - We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.
DA  - 2024/04/16/
PY  - 2024
DO  - 10.48550/arXiv.2310.17722
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.17722
Y2  - 2024/07/19/12:08:10
L1  - https://arxiv.org/pdf/2310.17722.pdf
L2  - https://arxiv.org/abs/2310.17722
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks
AU  - Jain, Samyak
AU  - Kirk, Robert
AU  - Lubana, Ekdeep Singh
AU  - Dick, Robert P.
AU  - Tanaka, Hidenori
AU  - Rocktäschel, Tim
AU  - Grefenstette, Edward
AU  - Krueger, David
T2  - The Twelfth International Conference on Learning Representations
AB  - Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=A0HKeKl4Nl
Y2  - 2024/07/19/12:08:12
L1  - https://openreview.net/pdf?id=A0HKeKl4Nl
ER  - 

TY  - CONF
TI  - Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning
AU  - Wabartha, Maxime
AU  - Pineau, Joelle
T2  - XAI in Action: Past, Present, and Future Applications
AB  - Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. We argue for the use of policies that are piecewise-linear. We carefully study to what extent they can retain the interpretable properties of linear policies while performing competitively with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the agent's decision process without needing an additional explanation model. We evaluate HC policies in control and navigation experiments, visualize the improved interpretability of the agent and highlight its trade-off with performance.
DA  - 2023/10/27/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Piecewise Linear Parametrization of Policies
UR  - https://openreview.net/forum?id=Zbt9z0a95l
Y2  - 2024/07/19/12:08:14
L1  - https://openreview.net/pdf?id=Zbt9z0a95l
ER  - 

TY  - CONF
TI  - Reward Model Ensembles Help Mitigate Overoptimization
AU  - Coste, Thomas
AU  - Anwar, Usman
AU  - Kirk, Robert
AU  - Krueger, David
T2  - NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following
AB  - Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to overoptimization. Gao et al. studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.
DA  - 2023/11/26/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=NiQYQEPUsA
Y2  - 2024/07/19/12:08:18
L1  - https://openreview.net/pdf?id=NiQYQEPUsA
ER  - 

TY  - GEN
TI  - Are self-explanations from Large Language Models faithful?
AU  - Madsen, Andreas
AU  - Chandar, Sarath
AU  - Reddy, Siva
AB  - Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.
DA  - 2024/05/16/
PY  - 2024
DO  - 10.48550/arXiv.2401.07927
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.07927
Y2  - 2024/07/19/12:08:24
L1  - https://arxiv.org/pdf/2401.07927.pdf
L2  - https://arxiv.org/abs/2401.07927
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - JaxPruner: A concise library for sparsity research
AU  - Lee, Joo Hyung
AU  - Park, Wonpyo
AU  - Mitchell, Nicole
AU  - Pilault, Jonathan
AU  - Obando-Ceron, Johan
AU  - Kim, Han-Byul
AU  - Lee, Namhoon
AU  - Frantar, Elias
AU  - Long, Yun
AU  - Yazdanbakhsh, Amir
AU  - Agrawal, Shivani
AU  - Subramanian, Suvinay
AU  - Wang, Xin
AU  - Kao, Sheng-Chun
AU  - Zhang, Xingyao
AU  - Gale, Trevor
AU  - Bik, Aart
AU  - Han, Woohyun
AU  - Ferev, Milen
AU  - Han, Zhonglin
AU  - Kim, Hong-Seok
AU  - Dauphin, Yann
AU  - Dziugaite, Gintare Karolina
AU  - Castro, Pablo Samuel
AU  - Evci, Utku
AB  - This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
DA  - 2023/12/18/
PY  - 2023
DO  - 10.48550/arXiv.2304.14082
DP  - arXiv.org
PB  - arXiv
ST  - JaxPruner
UR  - http://arxiv.org/abs/2304.14082
Y2  - 2024/07/19/12:08:28
L1  - https://arxiv.org/pdf/2304.14082.pdf
L2  - https://arxiv.org/abs/2304.14082
KW  - Computer Science - Machine Learning
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Are LLMs Robust for Spoken Dialogues?
AU  - Mousavi, Seyed Mahed
AU  - Roccabruna, Gabriel
AU  - Alghisi, Simone
AU  - Rizzoli, Massimo
AU  - Ravanelli, Mirco
AU  - Riccardi, Giuseppe
AB  - Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation. Nevertheless, most of the publicly available datasets and benchmarks on task-oriented dialogues focus on written conversations. Consequently, the robustness of the developed models to spoken interactions is unknown. In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the lack of proper spoken dialogue datasets, we have automatically transcribed a development set of spoken dialogues with a state-of-the-art ASR engine. We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues. We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models in two subtasks of response generation and dialogue state tracking, respectively. The results show that LLMs are not robust to spoken noise by default, however, fine-tuning/training such models on a proper dataset of spoken TODs can result in a more robust performance.
DA  - 2024/01/04/
PY  - 2024
DO  - 10.48550/arXiv.2401.02297
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.02297
Y2  - 2024/07/19/12:08:33
L1  - https://arxiv.org/pdf/2401.02297.pdf
L2  - https://arxiv.org/abs/2401.02297
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - AITA: AI trustworthiness assessment
AU  - Braunschweig, Bertrand
AU  - Buijsman, Stefan
AU  - Chamroukhi, Faïcel
AU  - Heintz, Fredrik
AU  - Khomh, Foutse
AU  - Mattioli, Juliette
AU  - Poretschkin, Maximilian
T2  - AI and Ethics
DA  - 2024/02/01/
PY  - 2024
DO  - 10.1007/s43681-023-00397-z
DP  - Springer Link
VL  - 4
IS  - 1
SP  - 1
EP  - 3
J2  - AI Ethics
LA  - en
SN  - 2730-5961
ST  - AITA
UR  - https://doi.org/10.1007/s43681-023-00397-z
Y2  - 2024/07/19/12:08:36
L1  - https://link.springer.com/content/pdf/10.1007%2Fs43681-023-00397-z.pdf
ER  - 

TY  - GEN
TI  - Dataset Difficulty and the Role of Inductive Bias
AU  - Kwok, Devin
AU  - Anand, Nikhil
AU  - Frankle, Jonathan
AU  - Dziugaite, Gintare Karolina
AU  - Rolnick, David
AB  - Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call "example difficulty scores", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive examples. These findings guide practitioners in maximizing the consistency of their scores (e.g. by choosing appropriate scoring methods, number of runs, and subsets of examples), and establishes comprehensive baselines for evaluating scores in the future.
DA  - 2024/01/03/
PY  - 2024
DO  - 10.48550/arXiv.2401.01867
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2401.01867
Y2  - 2024/07/19/12:08:42
L1  - https://arxiv.org/pdf/2401.01867.pdf
L2  - https://arxiv.org/abs/2401.01867
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces
AU  - Ehyaei, Ahmad-Reza
AU  - Mohammadi, Kiarash
AU  - Karimi, Amir-Hossein
AU  - Samadi, Samira
AU  - Farnadi, Golnoosh
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - As responsible AI gains importance in machine learning algorithms, properties like fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models (SCMs) in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use SCMs and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation (CAP) and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.
DA  - 2024/03/24/
PY  - 2024
DO  - 10.1609/aaai.v38i10.29070
DP  - ojs.aaai.org
VL  - 38
IS  - 10
SP  - 11847
EP  - 11855
LA  - en
SN  - 2374-3468
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/29070
Y2  - 2024/07/19/12:08:44
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/29070/30027
KW  - ML: Adversarial Learning & Robustness
ER  - 

TY  - GEN
TI  - Fairness-Aware Structured Pruning in Transformers
AU  - Zayed, Abdelrahman
AU  - Mordido, Goncalo
AU  - Shabanian, Samira
AU  - Baldini, Ioana
AU  - Chandar, Sarath
AB  - The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance.
DA  - 2023/12/23/
PY  - 2023
DO  - 10.48550/arXiv.2312.15398
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.15398
Y2  - 2024/07/19/12:13:10
L1  - https://arxiv.org/pdf/2312.15398.pdf
L2  - https://arxiv.org/abs/2312.15398
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Harnessing Pre-trained Generalist Agents for Software Engineering Tasks
AU  - Mindom, Paulina Stevia Nouwou
AU  - Nikanjam, Amin
AU  - Khomh, Foutse
AB  - Nowadays, we are witnessing an increasing adoption of Artificial Intelligence (AI) to develop techniques aimed at improving the reliability, effectiveness, and overall quality of software systems. Deep reinforcement learning (DRL) has recently been successfully used for automation in complex tasks such as game testing and solving the job-shop scheduling problem. However, these specialized DRL agents, trained from scratch on specific tasks, suffer from a lack of generalizability to other tasks and they need substantial time to be developed and re-trained effectively. Recently, DRL researchers have begun to develop generalist agents, able to learn a policy from various environments and capable of achieving performances similar to or better than specialist agents in new tasks. In the Natural Language Processing or Computer Vision domain, these generalist agents are showing promising adaptation capabilities to never-before-seen tasks after a light fine-tuning phase and achieving high performance. This paper investigates the potential of generalist agents for solving SE tasks. Specifically, we conduct an empirical study aimed at assessing the performance of two generalist agents on two important SE tasks: the detection of bugs in games (for two games) and the minimization of makespan in a scheduling task, to solve the job-shop scheduling problem (for two instances). Our results show that the generalist agents outperform the specialist agents with very little effort for fine-tuning, achieving a 20% reduction of the makespan over specialized agent performance on task-based scheduling. In the context of game testing, some generalist agent configurations detect 85% more bugs than the specialist agents. Building on our analysis, we provide recommendations for researchers and practitioners looking to select generalist agents for SE tasks, to ensure that they perform effectively.
DA  - 2023/12/24/
PY  - 2023
DO  - 10.48550/arXiv.2312.15536
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.15536
Y2  - 2024/07/19/12:13:13
L1  - https://arxiv.org/pdf/2312.15536.pdf
L2  - https://arxiv.org/abs/2312.15536
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Towards Machines that Trust: AI Agents Learn to Trust in the Trust Game
AU  - Nobandegani, Ardavan S.
AU  - Rish, Irina
AU  - Shultz, Thomas R.
AB  - Widely considered a cornerstone of human morality, trust shapes many aspects of human social interactions. In this work, we present a theoretical analysis of the $\textit{trust game}$, the canonical task for studying trust in behavioral and brain sciences, along with simulation results supporting our analysis. Specifically, leveraging reinforcement learning (RL) to train our AI agents, we systematically investigate learning trust under various parameterizations of this task. Our theoretical analysis, corroborated by the simulations results presented, provides a mathematical basis for the emergence of trust in the trust game.
DA  - 2023/12/20/
PY  - 2023
DO  - 10.48550/arXiv.2312.12868
DP  - arXiv.org
PB  - arXiv
ST  - Towards Machines that Trust
UR  - http://arxiv.org/abs/2312.12868
Y2  - 2024/07/19/12:13:14
L1  - https://arxiv.org/pdf/2312.12868.pdf
L2  - https://arxiv.org/abs/2312.12868
KW  - Computer Science - Artificial Intelligence
KW  - Quantitative Biology - Neurons and Cognition
ER  - 

TY  - GEN
TI  - Addressing Sample Inefficiency in Multi-View Representation Learning
AU  - Agrawal, Kumar Krishna
AU  - Ghosh, Arna
AU  - Oberman, Adam
AU  - Richards, Blake
AB  - Non-contrastive self-supervised learning (NC-SSL) methods like BarlowTwins and VICReg have shown great promise for label-free representation learning in computer vision. Despite the apparent simplicity of these techniques, researchers must rely on several empirical heuristics to achieve competitive performance, most notably using high-dimensional projector heads and two augmentations of the same image. In this work, we provide theoretical insights on the implicit bias of the BarlowTwins and VICReg loss that can explain these heuristics and guide the development of more principled recommendations. Our first insight is that the orthogonality of the features is more critical than projector dimensionality for learning good representations. Based on this, we empirically demonstrate that low-dimensional projector heads are sufficient with appropriate regularization, contrary to the existing heuristic. Our second theoretical insight suggests that using multiple data augmentations better represents the desiderata of the SSL objective. Based on this, we demonstrate that leveraging more augmentations per sample improves representation quality and trainability. In particular, it improves optimization convergence, leading to better features emerging earlier in the training. Remarkably, we demonstrate that we can reduce the pretraining dataset size by up to 4x while maintaining accuracy and improving convergence simply by using more data augmentations. Combining these insights, we present practical pretraining recommendations that improve wall-clock time by 2x and improve performance on CIFAR-10/STL-10 datasets using a ResNet-50 backbone. Thus, this work provides a theoretical insight into NC-SSL and produces practical recommendations for enhancing its sample and compute efficiency.
DA  - 2023/12/17/
PY  - 2023
DO  - 10.48550/arXiv.2312.10725
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.10725
Y2  - 2024/07/19/12:13:16
L1  - https://arxiv.org/pdf/2312.10725.pdf
L2  - https://arxiv.org/abs/2312.10725
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization
AU  - Li, Jimmy
AU  - Kozlov, Igor
AU  - Wu, Di
AU  - Liu, Xue
AU  - Dudek, Gregory
AB  - The use of learning-based methods for optimizing cellular radio access networks (RAN) has received increasing attention in recent years. This coincides with a rapid increase in the number of cell sites worldwide, driven largely by dramatic growth in cellular network traffic. Training and maintaining learned models that work well across a large number of cell sites has thus become a pertinent problem. This paper proposes a scalable framework for constructing a reinforcement learning policy bank that can perform RAN optimization across a large number of cell sites with varying traffic patterns. Central to our framework is a novel application of anomaly detection techniques to assess the compatibility between sites (tasks) and the policy bank. This allows our framework to intelligently identify when a policy can be reused for a task, and when a new policy needs to be trained and added to the policy bank. Our results show that our approach to compatibility assessment leads to an efficient use of computational resources, by allowing us to construct a performant policy bank without exhaustively training on all tasks, which makes it applicable under real-world constraints.
DA  - 2023/12/05/
PY  - 2023
DO  - 10.48550/arXiv.2312.03277
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.03277
Y2  - 2024/07/19/12:13:20
L1  - https://arxiv.org/pdf/2312.03277.pdf
L2  - https://arxiv.org/abs/2312.03277
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Bug characterization in machine learning-based systems
AU  - Morovati, Mohammad Mehdi
AU  - Nikanjam, Amin
AU  - Tambon, Florian
AU  - Khomh, Foutse
AU  - Jiang, Zhen Ming (Jack)
T2  - Empirical Software Engineering
AB  - The rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Since corrective maintenance, i.e. identifying and resolving systems bugs, is a key task in the software development process to deliver reliable software components, it is necessary to investigate the usage of ML components, from the software maintenance perspective. Understanding the bugs’ characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, and symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs is more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying significant attention to the reliability of the ML components is crucial in ML-based systems. These results deepen the understanding of ML bugs and we hope that our findings help shed light on opportunities for designing effective tools for testing and debugging ML-based systems.
DA  - 2023/12/05/
PY  - 2023
DO  - 10.1007/s10664-023-10400-0
DP  - Springer Link
VL  - 29
IS  - 1
SP  - 14
J2  - Empir Software Eng
LA  - en
SN  - 1573-7616
UR  - https://doi.org/10.1007/s10664-023-10400-0
Y2  - 2024/07/19/12:13:36
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10664-023-10400-0.pdf
KW  - Deep learning
KW  - Software testing
KW  - Empirical study
KW  - ML bug
KW  - ML-based systems
KW  - Software bug
KW  - Software maintenance
ER  - 

TY  - GEN
TI  - Nash Learning from Human Feedback
AU  - Munos, Rémi
AU  - Valko, Michal
AU  - Calandriello, Daniele
AU  - Azar, Mohammad Gheshlaghi
AU  - Rowland, Mark
AU  - Guo, Zhaohan Daniel
AU  - Tang, Yunhao
AU  - Geist, Matthieu
AU  - Mesnard, Thomas
AU  - Michi, Andrea
AU  - Selvi, Marco
AU  - Girgin, Sertan
AU  - Momchev, Nikola
AU  - Bachem, Olivier
AU  - Mankowitz, Daniel J.
AU  - Precup, Doina
AU  - Piot, Bilal
AB  - Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.
DA  - 2024/06/11/
PY  - 2024
DO  - 10.48550/arXiv.2312.00886
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.00886
Y2  - 2024/07/19/12:13:40
L1  - https://arxiv.org/pdf/2312.00886.pdf
L2  - https://arxiv.org/abs/2312.00886
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Computer Science - Computer Science and Game Theory
ER  - 

TY  - CONF
TI  - RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data
AU  - Darrin, Maxime
AU  - Piantanida, Pablo
AU  - Colombo, Pierre
T2  - EMNLP 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF provides OOD detection methods more aligned with task-specific performance metrics than traditional OOD detectors.
C1  - Singapore
C3  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.emnlp-main.357
DP  - ACLWeb
SP  - 5831
EP  - 5857
PB  - Association for Computational Linguistics
ST  - RainProof
UR  - https://aclanthology.org/2023.emnlp-main.357
Y2  - 2024/07/19/12:13:40
L1  - https://aclanthology.org/2023.emnlp-main.357.pdf
ER  - 

TY  - JOUR
TI  - Shape-Based Measures Improve Scene Categorization
AU  - Rezanejad, Morteza
AU  - Wilder, John
AU  - Walther, Dirk B.
AU  - Jepson, Allan D.
AU  - Dickinson, Sven
AU  - Siddiqi, Kaleem
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
AB  - Converging evidence indicates that deep neural network models that are trained on large datasets are biased toward color and texture information. Humans, on the other hand, can easily recognize objects and scenes from images as well as from bounding contours. Mid-level vision is characterized by the recombination and organization of simple primary features into more complex ones by a set of so-called Gestalt grouping rules. While described qualitatively in the human literature, a computational implementation of these perceptual grouping rules is so far missing. In this article, we contribute a novel set of algorithms for the detection of contour-based cues in complex scenes. We use the medial axis transform (MAT) to locally score contours according to these grouping rules. We demonstrate the benefit of these cues for scene categorization in two ways: (i) Both human observers and CNN models categorize scenes most accurately when perceptual grouping information is emphasized. (ii) Weighting the contours with these measures boosts performance of a CNN model significantly compared to the use of unweighted contours. Our work suggests that, even though these measures are computed directly from contours in the image, current CNN models do not appear to extract or utilize these grouping cues.
DA  - 2024/04//
PY  - 2024
DO  - 10.1109/TPAMI.2023.3333352
DP  - IEEE Xplore
VL  - 46
IS  - 4
SP  - 2041
EP  - 2053
SN  - 1939-3539
UR  - https://ieeexplore.ieee.org/document/10337773
Y2  - 2024/07/19/12:13:49
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10337773&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMzM3Nzcz
KW  - Task analysis
KW  - Feature extraction
KW  - Visualization
KW  - Convolutional neural networks
KW  - Transforms
KW  - Organizations
KW  - Contour geometry
KW  - Gestalt grouping cues
KW  - medial axis transform (MAT)
KW  - Observers
KW  - scene categorization
KW  - scene perception
KW  - shape based measures
ER  - 

TY  - GEN
TI  - Spectral Temporal Contrastive Learning
AU  - Morin, Sacha
AU  - Nath, Somjit
AU  - Kahou, Samira Ebrahimi
AU  - Wolf, Guy
AB  - Learning useful data representations without requiring labels is a cornerstone of modern deep learning. Self-supervised learning methods, particularly contrastive learning (CL), have proven successful by leveraging data augmentations to define positive pairs. This success has prompted a number of theoretical studies to better understand CL and investigate theoretical bounds for downstream linear probing tasks. This work is concerned with the temporal contrastive learning (TCL) setting where the sequential structure of the data is used instead to define positive pairs, which is more commonly used in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a population loss based on a state graph derived from a time-homogeneous reversible Markov chain with uniform stationary distribution. The STCL loss enables to connect the linear probing performance to the spectral properties of the graph, and can be estimated by considering previously observed data sequences as an ensemble of MCMC chains.
DA  - 2023/12/07/
PY  - 2023
DO  - 10.48550/arXiv.2312.00966
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.00966
Y2  - 2024/07/19/12:13:54
L1  - https://arxiv.org/pdf/2312.00966.pdf
L2  - https://arxiv.org/abs/2312.00966
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Toward Stronger Textual Attack Detectors
AU  - Colombo, Pierre
AU  - Picot, Marine
AU  - Noiry, Nathan
AU  - Staerman, Guillaume
AU  - Piantanida, Pablo
T2  - Findings 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding deep NLP systems integrity. However, the crucial problem of defending against malicious attacks has only drawn few attention in the NLP community. The latter is nonetheless instrumental to develop robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, an extended benchmark composed of nine popular attack methods, three datasets and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter free and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factor of detection rate variations.
C1  - Singapore
C3  - Findings of the Association for Computational Linguistics: EMNLP 2023
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.findings-emnlp.35
DP  - ACLWeb
SP  - 484
EP  - 505
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.findings-emnlp.35
Y2  - 2024/07/19/12:13:55
L1  - https://aclanthology.org/2023.findings-emnlp.35.pdf
ER  - 

TY  - JOUR
TI  - Silent bugs in deep learning frameworks: an empirical study of Keras and TensorFlow
AU  - Tambon, Florian
AU  - Nikanjam, Amin
AU  - An, Le
AU  - Khomh, Foutse
AU  - Antoniol, Giuliano
T2  - Empirical Software Engineering
AB  - Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their integration into various applications even among non-DL experts. However, like any other programs, they are prone to bugs. This paper deals with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the “black-box” and stochastic nature of the DL systems (i.e., the end user can not understand how the model makes decisions). This paper presents the first empirical study of the silent bugs in Tensorflow, specifically its high-level API Keras, and their impact on users’ programs. We extracted closed issues related to Keras API from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users’ programs. We categorized the bugs based on the effects on the users’ programs and the components where the issues occurred, using information from the issue reports. We then derived a threat level for each of the issues, based on the impact they had on the users’ programs. To assess the relevance of identified categories and the impact scale, we conducted an online survey with 103 DL developers. The participants generally agreed with the significant impact of silent bugs in DL frameworks and how they impact users and acknowledged our findings (i.e., categories of silent bugs and the proposed impact scale).
DA  - 2023/11/29/
PY  - 2023
DO  - 10.1007/s10664-023-10389-6
DP  - Springer Link
VL  - 29
IS  - 1
SP  - 10
J2  - Empir Software Eng
LA  - en
SN  - 1573-7616
ST  - Silent bugs in deep learning frameworks
UR  - https://doi.org/10.1007/s10664-023-10389-6
Y2  - 2024/07/19/12:13:57
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10664-023-10389-6.pdf
KW  - Deep learning
KW  - Keras
KW  - TensorFlow
KW  - Empirical study
KW  - Bug analysis
ER  - 

TY  - GEN
TI  - Scalar Invariant Networks with Zero Bias
AU  - Geng, Chuqin
AU  - Xu, Xiaojie
AU  - Ye, Haolin
AU  - Si, Xujie
AB  - Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are thought to enhance the representational power of neural networks, enabling them to solve a variety of tasks in computer vision. However, we argue that biases can be disregarded for some image-related tasks such as image classification, by considering the intrinsic distribution of images in the input space and desired model properties from first principles. Our findings suggest that zero-bias neural networks can perform comparably to biased networks for practical image classification tasks. We demonstrate that zero-bias neural networks possess a valuable property called scalar (multiplication) invariance. This means that the prediction of the network remains unchanged when the contrast of the input image is altered. We extend scalar invariance to more general cases, enabling formal verification of certain convex regions of the input space. Additionally, we prove that zero-bias neural networks are fair in predicting the zero image. Unlike state-of-the-art models that may exhibit bias toward certain labels, zero-bias networks have uniform belief in all labels. We believe dropping bias terms can be considered as a geometric prior in designing neural network architecture for image classification, which shares the spirit of adapting convolutions as the transnational invariance prior. The robustness and fairness advantages of zero-bias neural networks may also indicate a promising path towards trustworthy and ethical AI.
DA  - 2023/05/29/
PY  - 2023
DO  - 10.48550/arXiv.2211.08486
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.08486
Y2  - 2024/07/19/12:14:11
L1  - https://arxiv.org/pdf/2211.08486.pdf
L2  - https://arxiv.org/abs/2211.08486
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - The search for the lost attractor
AU  - Pasquato, Mario
AU  - Haddad, Syphax
AU  - Di Cintio, Pierfrancesco
AU  - Adam, Alexandre
AU  - Lemos, Pablo
AU  - Dia, Noé
AU  - Petrache, Mircea
AU  - Di Carlo, Ugo Niccolò
AU  - Trani, Alessandro Alberto
AU  - Perreault-Levasseur, Laurence
AU  - Hezaveh, Yashar
AB  - N-body systems characterized by inverse square attractive forces may display a self similar collapse known as the gravo-thermal catastrophe. In star clusters, collapse is halted by binary stars, and a large fraction of Milky Way clusters may have already reached this phase. It has been speculated -- with guidance from simulations -- that macroscopic variables such as central density and velocity dispersion are governed post-collapse by an effective, low-dimensional system of ODEs. It is still hard to distinguish chaotic, low dimensional motion, from high dimensional stochastic noise. Here we apply three machine learning tools to state-of-the-art dynamical simulations to constrain the post collapse dynamics: topological data analysis (TDA) on a lag embedding of the relevant time series, Sparse Identification of Nonlinear Dynamics (SINDY), and Tests of Accuracy with Random Points (TARP).
DA  - 2023/11/27/
PY  - 2023
DO  - 10.48550/arXiv.2311.16306
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.16306
Y2  - 2024/07/19/12:14:13
L1  - https://arxiv.org/pdf/2311.16306.pdf
L2  - https://arxiv.org/abs/2311.16306
KW  - Astrophysics - Astrophysics of Galaxies
KW  - Nonlinear Sciences - Chaotic Dynamics
ER  - 

TY  - GEN
TI  - Mitigating Biases with Diverse Ensembles and Diffusion Models
AU  - Scimeca, Luca
AU  - Rubinstein, Alexander
AU  - Teney, Damien
AU  - Oh, Seong Joon
AU  - Nicolicioiu, Armand Mihai
AU  - Bengio, Yoshua
AB  - Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification performance on par with prior work that relies on auxiliary data collection.
DA  - 2024/03/06/
PY  - 2024
DO  - 10.48550/arXiv.2311.16176
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.16176
Y2  - 2024/07/19/12:14:14
L1  - https://arxiv.org/pdf/2311.16176.pdf
L2  - https://arxiv.org/abs/2311.16176
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play
AU  - Bairamian, Daniel
AU  - Marcotte, Philippe
AU  - Romoff, Joshua
AU  - Robert, Gabriel
AU  - Nowrouzezahrai, Derek
AB  - Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL). One core component of these methods relies on creating a pool of learning agents -- consisting of the Main Agent, past versions of this agent, and Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main Agents. A key drawback of these approaches is the large computational cost and physical time that is required to train the system, making them impractical to deploy in highly iterative real-life settings such as video game productions. In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency. We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game. The Minimax Exploiter consistently outperforms strong baselines, demonstrating improved stability and data efficiency, leading to a robust CSP-MARL method that is both flexible and easy to deploy.
DA  - 2023/11/28/
PY  - 2023
DO  - 10.48550/arXiv.2311.17190
DP  - arXiv.org
PB  - arXiv
ST  - Minimax Exploiter
UR  - http://arxiv.org/abs/2311.17190
Y2  - 2024/07/19/12:14:16
L1  - https://arxiv.org/pdf/2311.17190.pdf
L2  - https://arxiv.org/abs/2311.17190
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Optimal Zero-Shot Detector for Multi-Armed Attacks
AU  - Granese, Federica
AU  - Romanelli, Marco
AU  - Piantanida, Pablo
AB  - This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available "off the shelf". To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario for empirical evaluation, where the attacker possesses a pre-trained classifier and launches well-known adversarial attacks against it. Our experiments highlight the effectiveness of our proposed solution, even in scenarios that deviate from the optimal setup.
DA  - 2024/02/27/
PY  - 2024
DO  - 10.48550/arXiv.2402.15808
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.15808
Y2  - 2024/07/19/12:14:19
L1  - https://arxiv.org/pdf/2402.15808.pdf
L2  - https://arxiv.org/abs/2402.15808
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
AU  - Casper, Stephen
AU  - Davies, Xander
AU  - Shi, Claudia
AU  - Gilbert, Thomas Krendl
AU  - Scheurer, Jérémy
AU  - Rando, Javier
AU  - Freedman, Rachel
AU  - Korbak, Tomasz
AU  - Lindner, David
AU  - Freire, Pedro
AU  - Wang, Tony
AU  - Marks, Samuel
AU  - Segerie, Charbel-Raphaël
AU  - Carroll, Micah
AU  - Peng, Andi
AU  - Christoffersen, Phillip
AU  - Damani, Mehul
AU  - Slocum, Stewart
AU  - Anwar, Usman
AU  - Siththaranjan, Anand
AU  - Nadeau, Max
AU  - Michaud, Eric J.
AU  - Pfau, Jacob
AU  - Krasheninnikov, Dmitrii
AU  - Chen, Xin
AU  - Langosco, Lauro
AU  - Hase, Peter
AU  - Bıyık, Erdem
AU  - Dragan, Anca
AU  - Krueger, David
AU  - Sadigh, Dorsa
AU  - Hadfield-Menell, Dylan
AB  - Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
DA  - 2023/09/11/
PY  - 2023
DO  - 10.48550/arXiv.2307.15217
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.15217
Y2  - 2024/07/19/12:14:20
L1  - https://arxiv.org/pdf/2307.15217.pdf
L2  - https://arxiv.org/abs/2307.15217
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Responsible AI Research Needs Impact Statements Too
AU  - Olteanu, Alexandra
AU  - Ekstrand, Michael
AU  - Castillo, Carlos
AU  - Suh, Jina
AB  - All types of research, development, and policy work can have unintended, adverse consequences - work in responsible artificial intelligence (RAI), ethical AI, or ethics in AI is no exception.
DA  - 2023/11/20/
PY  - 2023
DO  - 10.48550/arXiv.2311.11776
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.11776
Y2  - 2024/07/19/12:54:15
L1  - https://arxiv.org/pdf/2311.11776.pdf
L2  - https://arxiv.org/abs/2311.11776
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges
AU  - Caccia, Massimo
AU  - Mueller, Jonas
AU  - Kim, Taesup
AU  - Charlin, Laurent
AU  - Fakoor, Rasool
T2  - Conference on Lifelong Learning Agents
AB  - Continual learning (CL) enables the development of models and agents that learn from a sequence of tasks while addressing the limitations of standard deep learning approaches, such as catastrophic forgetting. In this work, we investigate the factors that contribute to the performance differences between task-agnostic CL and multi-task (MTL) agents. We pose two hypotheses: (1) task-agnostic methods might provide advantages in settings with limited data, computation, or high dimensionality, and (2) faster adaptation may be particularly beneficial in continual learning settings, helping to mitigate the effects of catastrophic forgetting. To investigate these hypotheses, we introduce a replay-based recurrent reinforcement learning (3RL) methodology for task-agnostic CL agents. We assess 3RL on a synthetic task and the Meta-World benchmark, which includes 50 unique manipulation tasks. Our results demonstrate that 3RL outperforms baseline methods and can even surpass its multi-task equivalent in challenging settings with high dimensionality. We also show that the recurrent task-agnostic agent consistently outperforms or matches the performance of its transformer-based counterpart.  These findings provide insights into the advantages of task-agnostic CL over task-aware MTL approaches and highlight the potential of task-agnostic methods in resource-constrained, high-dimensional, and multi-task environments.
C3  - Proceedings of The 2nd Conference on Lifelong Learning Agents
DA  - 2023/11/20/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 89
EP  - 119
LA  - en
PB  - PMLR
ST  - Task-Agnostic Continual Reinforcement Learning
UR  - https://proceedings.mlr.press/v232/caccia23a.html
Y2  - 2024/07/19/12:54:19
L1  - https://proceedings.mlr.press/v232/caccia23a/caccia23a.pdf
ER  - 

TY  - GEN
TI  - Generalizable Imitation Learning Through Pre-Trained Representations
AU  - Chang, Wei-Di
AU  - Hogan, Francois
AU  - Meger, David
AU  - Dudek, Gregory
AB  - In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.
DA  - 2023/11/15/
PY  - 2023
DO  - 10.48550/arXiv.2311.09350
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2311.09350
Y2  - 2024/07/19/12:54:24
L1  - https://arxiv.org/pdf/2311.09350.pdf
L2  - https://arxiv.org/abs/2311.09350
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
AU  - Sudhakar, Arjun Vaithilingam
AU  - Parthasarathi, Prasanna
AU  - Rajendran, Janarthanan
AU  - Chandar, Sarath
AB  - Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM, a popular approach, leverages linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.
DA  - 2023/11/13/
PY  - 2023
DO  - 10.48550/arXiv.2311.07687
DP  - arXiv.org
PB  - arXiv
ST  - Language Model-In-The-Loop
UR  - http://arxiv.org/abs/2311.07687
Y2  - 2024/07/19/12:54:26
L1  - https://arxiv.org/pdf/2311.07687.pdf
L2  - https://arxiv.org/abs/2311.07687
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies
AU  - Sujit, Shivakanth
AU  - Braga, Pedro H. M.
AU  - Bornschein, Jorg
AU  - Kahou, Samira Ebrahimi
AB  - Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. A crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. This can be infeasible in situations where such interactions are expensive; such as in robotics. Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. While online RL algorithms are typically evaluated as a function of the number of environment interactions, there exists no single established protocol for evaluating offline RL methods.In this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency. Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. Our approach is generally applicable and easy to implement. We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets.
DA  - 2023/11/21/
PY  - 2023
DO  - 10.48550/arXiv.2212.08131
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.08131
Y2  - 2024/07/19/12:54:29
L1  - https://arxiv.org/pdf/2212.08131.pdf
L2  - https://arxiv.org/abs/2212.08131
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Capture the Flag: Uncovering Data Insights with Large Language Models
AU  - Laradji, Issam
AU  - Taslakian, Perouz
AU  - Rajeswar, Sai
AU  - Zantedeschi, Valentina
AU  - Lacoste, Alexandre
AU  - Chapados, Nicolas
AU  - Vazquez, David
AU  - Pal, Christopher
AU  - Drouin, Alexandre
AB  - The extraction of a small number of relevant insights from vast amounts of data is a crucial component of data-driven decision-making. However, accomplishing this task requires considerable technical skills, domain expertise, and human labor. This study explores the potential of using Large Language Models (LLMs) to automate the discovery of insights in data, leveraging recent advances in reasoning and code generation techniques. We propose a new evaluation methodology based on a "capture the flag" principle, measuring the ability of such models to recognize meaningful and pertinent information (flags) in a dataset. We further propose two proof-of-concept agents, with different inner workings, and compare their ability to capture such flags in a real-world sales dataset. While the work reported here is preliminary, our results are sufficiently interesting to mandate future exploration by the community.
DA  - 2023/12/21/
PY  - 2023
DO  - 10.48550/arXiv.2312.13876
DP  - arXiv.org
PB  - arXiv
ST  - Capture the Flag
UR  - http://arxiv.org/abs/2312.13876
Y2  - 2024/07/19/12:54:32
L1  - https://arxiv.org/pdf/2312.13876.pdf
L2  - https://arxiv.org/abs/2312.13876
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - The Unsolved Challenges of LLMs as Generalist Web Agents: A Case Study
AU  - Assouel, Rim
AU  - Marty, Tom
AU  - Caccia, Massimo
AU  - Laradji, Issam H.
AU  - Drouin, Alexandre
AU  - Rajeswar, Sai
AU  - Palacios, Hector
AU  - Cappart, Quentin
AU  - Vazquez, David
AU  - Chapados, Nicolas
AU  - Gasse, Maxime
AU  - Lacoste, Alexandre
T2  - NeurIPS 2023 Foundation Models for Decision Making Workshop
AB  - In this work, we investigate the challenges associated with developing goal-driven AI agents capable of performing novel tasks in a web environment using zero-shot learning. Our primary focus is on harnessing the capabilities of large language models (LLMs) as generalist web agents interacting with HTML-based user interfaces (UIs). We evaluate the MiniWoB benchmark and show that it is a suitable yet challenging platform for assessing an agent's ability to comprehend and solve tasks without prior human demonstrations. Our main contribution encompasses a set of extensive experiments where we compare and contrast various agent design considerations, such as action space, observation space, and the choice of LLM, with the aim of shedding light on the bottlenecks and limitations of LLM-based zero-shot learning in this domain, in order to foster research endeavours in this area. In our empirical analysis, we find that: (1) the effectiveness of the different action spaces are notably dependent on the specific LLM used; (2) open-source LLMs hold their own as competitive generalist web agents when compared to their proprietary counterparts; and (3) using an accessibility-based representation for web pages, despite resulting in some performance loss, emerges as a cost-effective strategy, particularly as web page sizes increase.
DA  - 2023/11/08/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - The Unsolved Challenges of LLMs as Generalist Web Agents
UR  - https://openreview.net/forum?id=jt3il4fC5B
Y2  - 2024/07/19/12:54:34
L1  - https://openreview.net/pdf?id=jt3il4fC5B
ER  - 

TY  - CONF
TI  - Player-Guided AI outperforms standard AI in Sequence Alignment Puzzles
AU  - Mutalova, Renata
AU  - Sarrazin-Gendron, Roman
AU  - Ghasemloo Gheidari, Parham
AU  - Cai, Eddie
AU  - Richard, Gabriel
AU  - Caisse, Sebastien
AU  - Knight, Rob
AU  - Blanchette, Mathieu
AU  - Szantner, Attila
AU  - Waldispühl, Jérôme
T3  - CI '23
AB  - Although Artificial Intelligence (AI) has gained widespread popularity across different fields, it is essential to recognize that AI systems, while impressive, do not consistently exhibit robust generalization, particularly for difficult problems such as the Multiple Sequence Alignment (MSA). In this study, we focus on bridging this performance gap by integrating human solutions into AI training. To illustrate these principles, we leverage data from Borderlands Science, a popular citizen science game in which small instances of the MSA problem are represented as puzzles. Our goal is to leverage the collective intelligence of human players to enhance the capabilities of AI agents. To achieve this, we have developed a Player-guided AI system that enables the AI model to learn from both standard training processes and the solutions provided by players. Our findings demonstrate that incorporating human-annotated information into the AI model improves its performance on puzzle tasks. Furthermore, the Player-guided AI model shows a decrease in noise compared to a pure AI model. This advancement allows for leveraging the model to align new sequences with improved accuracy and effectiveness. Moreover, this research brings attention to the potential of integrating AI and human expertise to address other challenges where the performance of AI models may be unsatisfactory.
C1  - New York, NY, USA
C3  - Proceedings of The ACM Collective Intelligence Conference
DA  - 2023/11/05/
PY  - 2023
DO  - 10.1145/3582269.3615597
DP  - ACM Digital Library
SP  - 53
EP  - 62
PB  - Association for Computing Machinery
SN  - 9798400701139
UR  - https://doi.org/10.1145/3582269.3615597
Y2  - 2024/07/19/
L1  - https://dl.acm.org/doi/pdf/10.1145/3582269.3615597
ER  - 

TY  - CONF
TI  - Goal Misgeneralization as Implicit Goal Conditioning
AU  - Dorn, Diego
AU  - Alex, Neel
AU  - Krueger, David
T2  - NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning
AB  - While many examples of goal misspecification have been dissected in the reinforcement learning literature, few works have focused on the relatively new goal misgeneralization. As goal misgeneralization often stems from underspecification, we explore a simple environment with some goals specifiable through explicit conditioning, and others not. We find that agents generally pursue a mixture of possible goals, and the choice of goal to pursue is often inexplicable. Nonetheless, we attempt an explanation of implicit goal conditioning -- wherein subtle environment features determine which goal is pursued -- and aim to understand which features induce pursuit of one goal over another.
DA  - 2023/11/27/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=QT4tXTqTTr
Y2  - 2024/07/19/13:01:48
L1  - https://openreview.net/pdf?id=QT4tXTqTTr
ER  - 

TY  - GEN
TI  - Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness
AU  - Ehyaei, Ahmad-Reza
AU  - Farnadi, Golnoosh
AU  - Samadi, Samira
AB  - Despite the essential need for comprehensive considerations in responsible AI, factors like robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data or structural causal models and were unable to reflect counterfactual proximity. To address this, our paper introduces a causal fair metric formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the application of our novel metric in classifiers. Empirical evaluation of real-world and synthetic datasets illustrates the effectiveness of our proposed metric in achieving an accurate classifier with fairness, resilience to adversarial perturbations, and a nuanced understanding of causal relationships.
DA  - 2024/02/06/
PY  - 2024
DO  - 10.48550/arXiv.2310.19391
DP  - arXiv.org
PB  - arXiv
ST  - Causal Fair Metric
UR  - http://arxiv.org/abs/2310.19391
Y2  - 2024/07/19/13:01:52
L1  - https://arxiv.org/pdf/2310.19391.pdf
L2  - https://arxiv.org/abs/2310.19391
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Characterizing Manipulation from AI Systems
AU  - Carroll, Micah
AU  - Chan, Alan
AU  - Ashton, Henry
AU  - Krueger, David
T3  - EAAMO '23
AB  - Manipulation is a concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our digital interactions, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring this kind of manipulation from AI systems. Firstly, we build upon prior literature on manipulation and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, covertness, and harm. We review proposals on how to operationalize each concept and we outline challenges in including each concept in a definition of manipulation. Second, we discuss the connections between manipulation and related concepts, such as deception and coercion. We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains. While some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization
DA  - 2023/10/30/
PY  - 2023
DO  - 10.1145/3617694.3623226
DP  - ACM Digital Library
SP  - 1
EP  - 13
PB  - Association for Computing Machinery
SN  - 9798400703812
UR  - https://doi.org/10.1145/3617694.3623226
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2303.09387
ER  - 

TY  - CONF
TI  - FETA: Fairness Enforced Verifying, Training, and Predicting Algorithms for Neural Networks
AU  - Mohammadi, Kiarash
AU  - Sivaraman, Aishwarya
AU  - Farnadi, Golnoosh
T3  - EAAMO '23
AB  - Algorithmic decision-making driven by neural networks has become very prominent in applications that directly affect people’s quality of life. This paper focuses on the problem of ensuring individual fairness in neural network models during verification, training, and prediction. A popular approach for enforcing fairness is to translate a fairness notion into constraints over the parameters of the model. However, such a translation does not always guarantee fair predictions of the trained neural network model. To address this challenge, we develop a counterexample-guided post-processing technique to provably enforce fairness constraints at prediction time. Contrary to prior work that enforces fairness only on points around test or train data, we are able to enforce and guarantee fairness on all points in the domain. Additionally, we propose a counterexample-guided loss as an in-processing technique to use fairness as an inductive bias by iteratively incorporating fairness counterexamples in the learning process. We have implemented these techniques in a tool called FETA. Empirical evaluation on real-world datasets indicates that FETA is not only able to guarantee fairness on-the-fly at prediction time but also is able to train accurate models exhibiting a much higher degree of individual fairness.
C1  - New York, NY, USA
C3  - Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization
DA  - 2023/10/30/
PY  - 2023
DO  - 10.1145/3617694.3623243
DP  - ACM Digital Library
SP  - 1
EP  - 11
PB  - Association for Computing Machinery
SN  - 9798400703812
ST  - FETA
UR  - https://doi.org/10.1145/3617694.3623243
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2206.00553
ER  - 

TY  - JOUR
TI  - Generative AI models should include detection mechanisms as a condition for public release
AU  - Knott, Alistair
AU  - Pedreschi, Dino
AU  - Chatila, Raja
AU  - Chakraborti, Tapabrata
AU  - Leavy, Susan
AU  - Baeza-Yates, Ricardo
AU  - Eyers, David
AU  - Trotman, Andrew
AU  - Teal, Paul D.
AU  - Biecek, Przemyslaw
AU  - Russell, Stuart
AU  - Bengio, Yoshua
T2  - Ethics and Information Technology
AB  - The new wave of ‘foundation models’—general-purpose generative AI models, for production of text (e.g., ChatGPT) or images (e.g., MidJourney)—represent a dramatic advance in the state of the art for AI. But their use also introduces a range of new risks, which has prompted an ongoing conversation about possible regulatory mechanisms. Here we propose a specific principle that should be incorporated into legislation: that any organization developing a foundation model intended for public use must demonstrate a reliable detection mechanism for the content it generates, as a condition of its public release. The detection mechanism should be made publicly available in a tool that allows users to query, for an arbitrary item of content, whether the item was generated (wholly or partly) by the model. In this paper, we argue that this requirement is technically feasible and would play an important role in reducing certain risks from new AI models in many domains. We also outline a number of options for the tool’s design, and summarize a number of points where further input from policymakers and researchers would be required.
DA  - 2023/10/28/
PY  - 2023
DO  - 10.1007/s10676-023-09728-4
DP  - Springer Link
VL  - 25
IS  - 4
SP  - 55
J2  - Ethics Inf Technol
LA  - en
SN  - 1572-8439
UR  - https://doi.org/10.1007/s10676-023-09728-4
Y2  - 2024/07/19/13:02:03
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10676-023-09728-4.pdf
KW  - AI ethics
KW  - Foundation models
KW  - AI regulation
KW  - Generative AI
KW  - AI social impacts
ER  - 

TY  - GEN
TI  - Attention Schema in Neural Agents
AU  - Liu, Dianbo
AU  - Bolotta, Samuele
AU  - Zhu, He
AU  - Bengio, Yoshua
AU  - Dumas, Guillaume
AB  - Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these exploratory experiments suggest that equipping artificial agents with a model of attention can enhance their social intelligence.
DA  - 2023/07/13/
PY  - 2023
DO  - 10.48550/arXiv.2305.17375
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.17375
Y2  - 2024/07/19/13:02:07
L1  - https://arxiv.org/pdf/2305.17375.pdf
L2  - https://arxiv.org/abs/2305.17375
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations
AU  - Eastwood, Cian
AU  - Kügelgen, Julius von
AU  - Ericsson, Linus
AU  - Bouchacourt, Diane
AU  - Vincent, Pascal
AU  - Ibrahim, Mark
AU  - Schölkopf, Bernhard
T2  - Causal Representation Learning Workshop at NeurIPS 2023
AB  - Self-supervised representation learning often uses data augmentations to induce some invariance to "style" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits our approach on synthetic datasets and then present promising but limited results on ImageNet.
DA  - 2023/10/27/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=JoISqbH8vl
Y2  - 2024/07/19/13:04:03
L1  - https://openreview.net/pdf?id=JoISqbH8vl
ER  - 

TY  - GEN
TI  - A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification
AU  - Colombo, Pierre
AU  - Noiry, Nathan
AU  - Staerman, Guillaume
AU  - Piantanida, Pablo
AB  - One of the pursued objectives of deep learning is to provide tools that learn abstract representations of reality from the observation of multiple contextual situations. More precisely, one wishes to extract disentangled representations which are (i) low dimensional and (ii) whose components are independent and correspond to concepts capturing the essence of the objects under consideration (Locatello et al., 2019b). One step towards this ambitious project consists in learning disentangled representations with respect to a predefined (sensitive) attribute, e.g., the gender or age of the writer. Perhaps one of the main application for such disentangled representations is fair classification. Existing methods extract the last layer of a neural network trained with a loss that is composed of a cross-entropy objective and a disentanglement regularizer. In this work, we adopt an information-theoretic view of this problem which motivates a novel family of regularizers that minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. The resulting set of losses, called CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks. We demonstrate that our methods offer a better disentanglement/accuracy trade-off than previous techniques, and generalize better than training with cross-entropy loss solely provided that the disentanglement task is not too constraining.
DA  - 2023/10/21/
PY  - 2023
DO  - 10.48550/arXiv.2310.13990
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.13990
Y2  - 2024/07/19/13:04:08
L1  - https://arxiv.org/pdf/2310.13990.pdf
L2  - https://arxiv.org/abs/2310.13990
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning
AU  - Hugessen, Adriana
AU  - Castanyer, Roger Creus
AU  - Berseth, Glen
T2  - Intrinsically-Motivated and Open-Ended Learning Workshop @NeurIPS2023
AB  - Both surprise-minimizing and surprise-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method can perform well across all entropy regimes. In an effort to find a single surprise-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective depending on the entropy conditions in its environment by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes.
DA  - 2023/11/30/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=E0LWTN1xPX
Y2  - 2024/07/19/13:04:09
L1  - https://openreview.net/pdf?id=E0LWTN1xPX
ER  - 

TY  - GEN
TI  - Detection and Evaluation of bias-inducing Features in Machine learning
AU  - Openja, Moses
AU  - Laberge, Gabriel
AU  - Khomh, Foutse
AB  - The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.
DA  - 2023/10/19/
PY  - 2023
DO  - 10.48550/arXiv.2310.12805
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.12805
Y2  - 2024/07/19/13:04:14
L1  - https://arxiv.org/pdf/2310.12805.pdf
L2  - https://arxiv.org/abs/2310.12805
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Explainable Attention for Few-shot Learning and Beyond
AU  - Nikpour, Bahareh
AU  - Armanfard, Narges
AB  - Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.
DA  - 2023/10/11/
PY  - 2023
DO  - 10.48550/arXiv.2310.07800
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.07800
Y2  - 2024/07/19/13:04:16
L1  - https://arxiv.org/pdf/2310.07800.pdf
L2  - https://arxiv.org/abs/2310.07800
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Debiasing Counterfactuals in the Presence of Spurious Correlations
AU  - Kumar, Amar
AU  - Fathi, Nima
AU  - Mehta, Raghav
AU  - Nichyporuk, Brennan
AU  - Falet, Jean-Pierre R.
AU  - Tsaftaris, Sotirios
AU  - Arbel, Tal
A2  - Wesarg, Stefan
A2  - Puyol Antón, Esther
A2  - Baxter, John S. H.
A2  - Erdt, Marius
A2  - Drechsler, Klaus
A2  - Oyarzun Laura, Cristina
A2  - Freiman, Moti
A2  - Chen, Yufei
A2  - Rekik, Islem
A2  - Eagleson, Roy
A2  - Feragen, Aasa
A2  - King, Andrew P.
A2  - Cheplygina, Veronika
A2  - Ganz-Benjaminsen, Melani
A2  - Ferrante, Enzo
A2  - Glocker, Ben
A2  - Moyer, Daniel
A2  - Petersen, Eikel
AB  - Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
C1  - Cham
C3  - Clinical Image-Based Procedures,  Fairness of AI in Medical Imaging, and Ethical and Philosophical Issues in Medical Imaging
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-45249-9_27
DP  - Springer Link
SP  - 276
EP  - 286
LA  - en
PB  - Springer Nature Switzerland
SN  - 978-3-031-45249-9
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-031-45249-9_27.pdf
KW  - Debiasing
KW  - Counterfactuals
KW  - Biomarker
KW  - Explainablity
ER  - 

TY  - GEN
TI  - Guiding Language Model Math Reasoning with Planning Tokens
AU  - Wang, Xinyi
AU  - Caccia, Lucas
AU  - Ostapenko, Oleksiy
AU  - Yuan, Xingdi
AU  - Wang, William Yang
AU  - Sordoni, Alessandro
AB  - Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard fine-tuning baselines.
DA  - 2024/02/05/
PY  - 2024
DO  - 10.48550/arXiv.2310.05707
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.05707
Y2  - 2024/07/19/13:04:20
L1  - https://arxiv.org/pdf/2310.05707.pdf
L2  - https://arxiv.org/abs/2310.05707
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Sparse Universal Transformer
AU  - Tan, Shawn
AU  - Shen, Yikang
AU  - Chen, Zhenfang
AU  - Courville, Aaron
AU  - Gan, Chuang
AB  - The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in computation during inference with very little performance decrease on formal language tasks.
DA  - 2023/10/10/
PY  - 2023
DO  - 10.48550/arXiv.2310.07096
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.07096
Y2  - 2024/07/19/13:06:53
L1  - https://arxiv.org/pdf/2310.07096.pdf
L2  - https://arxiv.org/abs/2310.07096
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning
AU  - Jin, Tian
AU  - Clement, Nolan
AU  - Dong, Xin
AU  - Nagarajan, Vaishnavh
AU  - Carbin, Michael
AU  - Ragan-Kelley, Jonathan
AU  - Dziugaite, Gintare Karolina
AB  - How does scaling the number of parameters in large language models (LLMs) affect their core capabilities? We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference. By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling. Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training. Yet, a 60--70\% reduction largely preserves the various ways the model can process in-context information, ranging from retrieving answers from a long context to learning parameterized functions from in-context exemplars. The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning.
DA  - 2023/10/06/
PY  - 2023
DO  - 10.48550/arXiv.2310.04680
DP  - arXiv.org
PB  - arXiv
ST  - The Cost of Down-Scaling Language Models
UR  - http://arxiv.org/abs/2310.04680
Y2  - 2024/07/19/13:06:55
L1  - https://arxiv.org/pdf/2310.04680.pdf
L2  - https://arxiv.org/abs/2310.04680
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Using In-Context Learning to Improve Dialogue Safety
AU  - Meade, Nicholas
AU  - Gella, Spandana
AU  - Hazarika, Devamanyu
AU  - Gupta, Prakhar
AU  - Jin, Di
AU  - Reddy, Siva
AU  - Liu, Yang
AU  - Hakkani-Tür, Dilek
AB  - While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.
DA  - 2023/10/22/
PY  - 2023
DO  - 10.48550/arXiv.2302.00871
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.00871
Y2  - 2024/07/19/13:06:58
L1  - https://arxiv.org/pdf/2302.00871.pdf
L2  - https://arxiv.org/abs/2302.00871
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Data Cleaning and Machine Learning: A Systematic Literature Review
AU  - Côté, Pierre-Olivier
AU  - Nikanjam, Amin
AU  - Ahmed, Nafisa
AU  - Humeniuk, Dmytro
AU  - Khomh, Foutse
AB  - Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. Results: We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. Conclusion: We believe that our review of the literature will help the community develop better approaches to clean data.
DA  - 2024/05/30/
PY  - 2024
DO  - 10.48550/arXiv.2310.01765
DP  - arXiv.org
PB  - arXiv
ST  - Data Cleaning and Machine Learning
UR  - http://arxiv.org/abs/2310.01765
Y2  - 2024/07/19/13:07:02
L1  - https://arxiv.org/pdf/2310.01765.pdf
L2  - https://arxiv.org/abs/2310.01765
KW  - Computer Science - Machine Learning
KW  - Computer Science - Databases
ER  - 

TY  - JOUR
TI  - AI and Catastrophic Risk
AU  - Bengio, Yoshua
T2  - Journal of Democracy
AB  - Since OpenAI's release of the very large language models Chat-GPT and GPT-4, the potential dangers of AI have garnered widespread public attention. In this essay, the author reviews the threats to democracy posed by the possibility of "rogue AIs," dangerous and powerful AIs that would execute harmful goals, irrespective of whether the outcomes are intended by humans. To mitigate against the risk that rogue AIs present to democracy and geopolitical stability, the author argues that research into safe and defensive AIs should be conducted by a multilateral, international network of research laboratories.
DA  - 2023///
PY  - 2023
DP  - Project MUSE
VL  - 34
IS  - 4
SP  - 111
EP  - 121
SN  - 1086-3214
UR  - https://muse.jhu.edu/pub/1/article/907692
Y2  - 2024/07/19/13:07:07
L1  - https://muse.jhu.edu/pub/1/article/907692/pdf
ER  - 

TY  - CONF
TI  - An Empirical Study on Bugs Inside PyTorch: A Replication Study
AU  - Yin Ho, Sharon Chee
AU  - Majdinasab, Vahid
AU  - Islam, Mohayeminul
AU  - Costa, Diego Elias
AU  - Shihab, Emad
AU  - Khomh, Foutse
AU  - Nadi, Sarah
AU  - Raza, Muhammad
T2  - 2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)
AB  - Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch’s development, and assess their locality within the project, and extract patterns of bug fixes. Our results highlight that PyTorch bugs are more like traditional software projects bugs, than related to deep learning characteristics. Finally, we also compare our results with the study on TensorFlow, highlighting similarities and differences across the bug identification and fixing process.
C3  - 2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/ICSME58846.2023.00031
DP  - IEEE Xplore
SP  - 220
EP  - 231
ST  - An Empirical Study on Bugs Inside PyTorch
UR  - https://ieeexplore.ieee.org/document/10336350
Y2  - 2024/07/19/13:07:09
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10336350&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMzM2MzUw
L2  - https://ieeexplore.ieee.org/document/10336350
KW  - Deep learning
KW  - Intelligent systems
KW  - Deep Learning
KW  - Libraries
KW  - Software algorithms
KW  - Computer bugs
KW  - Software maintenance
KW  - Bug Analysis
KW  - Empirical Study
KW  - PyTorch
KW  - Software Library Defect
KW  - Software systems
ER  - 

TY  - JOUR
TI  - Towards More General Loss and Setting in Unsupervised Domain Adaptation
AU  - Shui, Changjian
AU  - Pu, Ruizhi
AU  - Xu, Gezheng
AU  - Wen, Jun
AU  - Zhou, Fan
AU  - Gagné, Christian
AU  - Ling, Charles X.
AU  - Wang, Boyu
T2  - IEEE Transactions on Knowledge and Data Engineering
AB  - In this article, we present an analysis of unsupervised domain adaptation with a series of theoretical and algorithmic results. We derive a novel Rényi-αα divergence-based generalization bound, which is tailored to domain adaptation algorithms with arbitrary loss functions in a stochastic setting. Moreover, our theoretical results provide new insights into the assumptions for successful domain adaptation: the closeness between the conditional distributions of the domains and the Lipschitzness on the source domain. With these assumptions, we reveal the following: if their conditional generation distributions are close, the Lipschitzness property of the target domain can be transferred from the Lipschitzness on the source domain, without knowing the exact target distribution. Motivated by our analysis and assumptions, we further derive practical principles for deep domain adaptation: 1) Rényi-2 adversarial training for marginal distributions matching and 2) Lipschitz regularization for the classifier. Our experimental results on both synthetic and real-world datasets support our theoretical findings and the practical efficiency of the proposed principles.
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/TKDE.2023.3266785
DP  - IEEE Xplore
VL  - 35
IS  - 10
SP  - 10140
EP  - 10150
SN  - 1558-2191
UR  - https://ieeexplore.ieee.org/document/10102307
Y2  - 2024/07/19/13:07:14
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10102307&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMTAyMzA3
KW  - Task analysis
KW  - Urban areas
KW  - Training
KW  - Computer science
KW  - Supervised learning
KW  - Domain adaptation
KW  - representation learning
KW  - Labeling
KW  - rényi divergence
KW  - Upper bound
ER  - 

TY  - CONF
TI  - Zero-Shot Fault Detection for Manipulators Through Bayesian Inverse Reinforcement Learning
AU  - Zhao, Hanqing
AU  - Liu, Xue
AU  - Dudek, Gregory
T2  - 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
AB  - We consider the detection of faults in robotic manipulators, with particular emphasis on faults that have not been observed or identified in advance, which naturally includes those that occur very infrequently. Recent studies indicate that the reward function obtained through Inverse Reinforcement Learning (IRL) can help detect anomalies caused by faults in a control system (i.e. fault detection). Current IRL methods for fault detection, however, either use a linear reward representation or require extensive sampling from the environment to estimate the policy, rendering them inappropriate for safety-critical situations where sampling of failure observations via fault injection can be expensive and dangerous. To address this issue, this paper proposes a zero-shot and exogenous fault detector based on an approximate variational reward imitation learning (AVRIL) structure. The fault detector recovers a reward signal as a function of externally observable information to describe the normal operation, which can then be used to detect anomalies caused by faults. Our method incorporates expert knowledge through a customizable reward prior distribution, allowing the fault detector to learn the reward solely from normal operation samples, without the need for a simulator or costly interactions with the environment. We evaluate our approach for exogenous partial fault detection in multi-stage robotic manipulator tasks, comparing it with several baseline methods. The results demonstrate that our method more effectively identifies unseen faults even when they occur within just three controller time steps.
C3  - 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/IROS55552.2023.10342143
DP  - IEEE Xplore
SP  - 3582
EP  - 3589
UR  - https://ieeexplore.ieee.org/document/10342143
Y2  - 2024/07/19/13:07:17
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10342143&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMzQyMTQz
L2  - https://ieeexplore.ieee.org/document/10342143
KW  - Reinforcement learning
KW  - Support vector machines
KW  - Manipulators
KW  - Fault detection
KW  - Fault diagnosis
KW  - Rendering (computer graphics)
KW  - Robot vision systems
ER  - 

TY  - GEN
TI  - Tree Cross Attention
AU  - Feng, Leo
AU  - Tung, Frederick
AU  - Hajimirsadeghi, Hossein
AU  - Bengio, Yoshua
AU  - Ahmed, Mohamed Osama
AB  - Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens $L < N$ on which cross attention is then applied, resulting in only $\mathcal{O}(L)$ complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic $\mathcal{O}(\log(N))$ number of tokens for performing inference. TCA organizes the data in a tree structure and performs a tree search at inference time to retrieve the relevant tokens for prediction. Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference. We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient. Furthermore, we compare ReTreever against Perceiver IO, showing significant gains while using the same number of tokens for inference.
DA  - 2024/03/01/
PY  - 2024
DO  - 10.48550/arXiv.2309.17388
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2309.17388
Y2  - 2024/07/19/13:10:11
L1  - https://arxiv.org/pdf/2309.17388.pdf
L2  - https://arxiv.org/abs/2309.17388
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Language Model Alignment with Elastic Reset
AU  - Noukhovitch, Michael
AU  - Lavoie, Samuel
AU  - Strub, Florian
AU  - Courville, Aaron
AB  - Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available at github.com/mnoukhov/elastic-reset.
DA  - 2023/12/06/
PY  - 2023
DO  - 10.48550/arXiv.2312.07551
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2312.07551
Y2  - 2024/07/19/13:10:27
L1  - https://arxiv.org/pdf/2312.07551.pdf
L2  - https://arxiv.org/abs/2312.07551
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Joint Prompt Optimization of Stacked LLMs using Variational Inference
AU  - Sordoni, Alessandro
AU  - Yuan, Xingdi
AU  - Côté, Marc-Alexandre
AU  - Pereira, Matheus
AU  - Trischler, Adam
AU  - Xiao, Ziang
AU  - Hosseini, Arian
AU  - Niedtner, Friederike
AU  - Roux, Nicolas Le
AB  - Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.
DA  - 2023/12/04/
PY  - 2023
DO  - 10.48550/arXiv.2306.12509
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.12509
Y2  - 2024/07/19/13:10:37
L1  - https://arxiv.org/pdf/2306.12509.pdf
L2  - https://arxiv.org/abs/2306.12509
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Multi-Head Adapter Routing for Cross-Task Generalization
AU  - Caccia, Lucas
AU  - Ponti, Edoardo
AU  - Su, Zhan
AU  - Pereira, Matheus
AU  - Roux, Nicolas Le
AU  - Sordoni, Alessandro
AB  - Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing) which combines subsets of adapter parameters and outperforms $\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\texttt{MHR}$-$z$) we achieve competitive performance with extreme parameter efficiency. Second, we find that $\texttt{Poly}$/$\texttt{MHR}$ performance is a result of better multi-task optimization, rather than modular inductive biases that facilitate adapter recombination and local adaptation, as previously hypothesized. In fact, we find that $\texttt{MHR}$ exhibits high gradient alignment between training tasks. We find that routing is most beneficial during multi-task pre-training rather than during few-shot adaptation and propose $\texttt{MHR}$-$\mu$, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks. This establishes $\texttt{MHR}$-$\mu$ as an effective method for single-adapter fine-tuning. We also show that $\texttt{MHR}$-$\mu$ can be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set: this yields gains up to 3% on absolute accuracy w.r.t. the baselines.
DA  - 2023/11/13/
PY  - 2023
DO  - 10.48550/arXiv.2211.03831
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2211.03831
Y2  - 2024/07/19/13:11:00
L1  - https://arxiv.org/pdf/2211.03831.pdf
L2  - https://arxiv.org/abs/2211.03831
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Thinker: Learning to Plan and Act
AU  - Chung, Stephen
AU  - Anokhin, Ivan
AU  - Krueger, David
AB  - We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments.
DA  - 2023/10/26/
PY  - 2023
DO  - 10.48550/arXiv.2307.14993
DP  - arXiv.org
PB  - arXiv
ST  - Thinker
UR  - http://arxiv.org/abs/2307.14993
Y2  - 2024/07/19/13:11:10
L1  - https://arxiv.org/pdf/2307.14993.pdf
L2  - https://arxiv.org/abs/2307.14993
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - I.2.6
KW  - I.2.8
KW  - I.5.1
ER  - 

TY  - GEN
TI  - Tidying Up the Conversational Recommender Systems' Biases
AU  - Moradi, Armin
AU  - Farnadi, Golnoosh
AB  - The growing popularity of language models has sparked interest in conversational recommender systems (CRS) within both industry and research circles. However, concerns regarding biases in these systems have emerged. While individual components of CRS have been subject to bias studies, a literature gap remains in understanding specific biases unique to CRS and how these biases may be amplified or reduced when integrated into complex CRS models. In this paper, we provide a concise review of biases in CRS by surveying recent literature. We examine the presence of biases throughout the system's pipeline and consider the challenges that arise from combining multiple models. Our study investigates biases in classic recommender systems and their relevance to CRS. Moreover, we address specific biases in CRS, considering variations with and without natural language understanding capabilities, along with biases related to dialogue systems and language models. Through our findings, we highlight the necessity of adopting a holistic perspective when dealing with biases in complex CRS models.
DA  - 2023/09/05/
PY  - 2023
DO  - 10.48550/arXiv.2309.02550
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2309.02550
Y2  - 2024/07/19/13:11:10
L1  - https://arxiv.org/pdf/2309.02550.pdf
L2  - https://arxiv.org/abs/2309.02550
KW  - Computer Science - Information Retrieval
ER  - 

TY  - CONF
TI  - What does it mean to be a responsible AI practitioner: An ontology of roles and skills
AU  - Rismani, Shalaleh
AU  - Moon, AJung
T3  - AIES '23
AB  - With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2023/08/29/
PY  - 2023
DO  - 10.1145/3600211.3604702
DP  - ACM Digital Library
SP  - 584
EP  - 595
PB  - Association for Computing Machinery
SN  - 9798400702310
ST  - What does it mean to be a responsible AI practitioner
UR  - https://doi.org/10.1145/3600211.3604702
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2205.03946
ER  - 

TY  - GEN
TI  - Efficient Epistemic Uncertainty Estimation in Regression Ensemble Models Using Pairwise-Distance Estimators
AU  - Berry, Lucas
AU  - Meger, David
AB  - This work introduces an efficient novel approach for epistemic uncertainty estimation for ensemble models for regression tasks using pairwise-distance estimators (PaiDEs). Utilizing the pairwise-distance between model components, these estimators establish bounds on entropy. We leverage this capability to enhance the performance of Bayesian Active Learning by Disagreement (BALD). Notably, unlike sample-based Monte Carlo estimators, PaiDEs exhibit a remarkable capability to estimate epistemic uncertainty at speeds up to 100 times faster while covering a significantly larger number of inputs at once and demonstrating superior performance in higher dimensions. To validate our approach, we conducted a varied series of regression experiments on commonly used benchmarks: 1D sinusoidal data, $\textit{Pendulum}$, $\textit{Hopper}$, $\textit{Ant}$ and $\textit{Humanoid}$. For each experimental setting, an active learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation. We compare our approach to existing active learning methods and find that our approach outperforms on high-dimensional regression tasks.
DA  - 2024/02/14/
PY  - 2024
DO  - 10.48550/arXiv.2308.13498
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2308.13498
Y2  - 2024/07/19/13:12:21
L1  - https://arxiv.org/pdf/2308.13498.pdf
L2  - https://arxiv.org/abs/2308.13498
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Open, Closed, or Small Language Models for Text Classification?
AU  - Yu, Hao
AU  - Yang, Zachary
AU  - Pelrine, Kellin
AU  - Godbout, Jean Francois
AU  - Rabbany, Reihaneh
AB  - Recent advancements in large language models have demonstrated remarkable capabilities across various NLP tasks. But many questions remain, including whether open-source models match closed ones, why these models excel or struggle with certain tasks, and what types of practical procedures can improve performance. We address these questions in the context of classification by evaluating three classes of models using eight datasets across three distinct tasks: named entity recognition, political party prediction, and misinformation detection. While larger LLMs often lead to improved performance, open-source models can rival their closed-source counterparts by fine-tuning. Moreover, supervised smaller models, like RoBERTa, can achieve similar or even greater performance in many datasets compared to generative LLMs. On the other hand, closed models maintain an advantage in hard tasks that demand the most generalizability. This study underscores the importance of model selection based on task requirements
DA  - 2023/08/19/
PY  - 2023
DO  - 10.48550/arXiv.2308.10092
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2308.10092
Y2  - 2024/07/19/13:12:25
L1  - https://arxiv.org/pdf/2308.10092.pdf
L2  - https://arxiv.org/abs/2308.10092
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi
AU  - Nekoei, Hadi
AU  - Zhao, Xutong
AU  - Rajendran, Janarthanan
AU  - Liu, Miao
AU  - Chandar, Sarath
AB  - Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate zero-shot (without additional interaction experience) with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent's ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents.
DA  - 2023/08/20/
PY  - 2023
DO  - 10.48550/arXiv.2308.10284
DP  - arXiv.org
PB  - arXiv
ST  - Towards Few-shot Coordination
UR  - http://arxiv.org/abs/2308.10284
Y2  - 2024/07/19/13:12:28
L1  - https://arxiv.org/pdf/2308.10284.pdf
L2  - https://arxiv.org/abs/2308.10284
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Consciousness in Artificial Intelligence: Insights from the Science of Consciousness
AU  - Butlin, Patrick
AU  - Long, Robert
AU  - Elmoznino, Eric
AU  - Bengio, Yoshua
AU  - Birch, Jonathan
AU  - Constant, Axel
AU  - Deane, George
AU  - Fleming, Stephen M.
AU  - Frith, Chris
AU  - Ji, Xu
AU  - Kanai, Ryota
AU  - Klein, Colin
AU  - Lindsay, Grace
AU  - Michel, Matthias
AU  - Mudrik, Liad
AU  - Peters, Megan A. K.
AU  - Schwitzgebel, Eric
AU  - Simon, Jonathan
AU  - VanRullen, Rufin
AB  - Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.
DA  - 2023/08/22/
PY  - 2023
DO  - 10.48550/arXiv.2308.08708
DP  - arXiv.org
PB  - arXiv
ST  - Consciousness in Artificial Intelligence
UR  - http://arxiv.org/abs/2308.08708
Y2  - 2024/07/19/13:16:34
L1  - https://arxiv.org/pdf/2308.08708.pdf
L2  - https://arxiv.org/abs/2308.08708
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Quantitative Biology - Neurons and Cognition
ER  - 

TY  - JOUR
TI  - Using Confounded Data in Latent Model-Based Reinforcement Learning
AU  - Gasse, Maxime
AU  - Grasset, Damien
AU  - Gaudron, Guillaume
AU  - Oudeyer, Pierre-Yves
T2  - Transactions on Machine Learning Research
AB  - In the presence of confounding, naively using off-the-shelf offline reinforcement learning (RL) algorithms leads to sub-optimal behaviour. In this work, we propose a safe method to exploit confounded offline data in model-based RL, which improves the sample-efficiency of an interactive agent that also collects online, unconfounded data. First, we import ideas from the well-established framework of $do$-calculus to express model-based RL as a causal inference problem, thus bridging the gap between the fields of RL and causality. Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable. We prove that our method is correct and efficient, in the sense that it attains better generalization guarantees thanks to the confounded offline data (in the asymptotic case), regardless of the confounding effect (the offline expert's behaviour). We showcase our method on a series of synthetic experiments, which demonstrate that a) using confounded offline data naively degrades the sample-efficiency of an RL agent; b) using confounded offline data correctly improves sample-efficiency.
DA  - 2023/03/23/
PY  - 2023
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=nFWRuJXPkU
Y2  - 2024/07/19/13:16:41
L1  - https://openreview.net/pdf?id=nFWRuJXPkU
ER  - 

TY  - GEN
TI  - On the Convergence of Bounded Agents
AU  - Abel, David
AU  - Barreto, André
AU  - van Hasselt, Hado
AU  - Van Roy, Benjamin
AU  - Precup, Doina
AU  - Singh, Satinder
AB  - When has an agent converged? Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing. However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear. In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents. The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease. The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship. We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.
DA  - 2023/07/20/
PY  - 2023
DO  - 10.48550/arXiv.2307.11044
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.11044
Y2  - 2024/07/19/13:16:48
L1  - https://arxiv.org/pdf/2307.11044.pdf
L2  - https://arxiv.org/abs/2307.11044
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - International Institutions for Advanced AI
AU  - Ho, Lewis
AU  - Barnhart, Joslyn
AU  - Trager, Robert
AU  - Bengio, Yoshua
AU  - Brundage, Miles
AU  - Carnegie, Allison
AU  - Chowdhury, Rumman
AU  - Dafoe, Allan
AU  - Hadfield, Gillian
AU  - Levi, Margaret
AU  - Snidal, Duncan
AB  - International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.
DA  - 2023/07/11/
PY  - 2023
DO  - 10.48550/arXiv.2307.04699
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.04699
Y2  - 2024/07/19/13:16:50
L1  - https://arxiv.org/pdf/2307.04699.pdf
L2  - https://arxiv.org/abs/2307.04699
KW  - Computer Science - Computers and Society
KW  - K.4.1
ER  - 

TY  - JOUR
TI  - Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness
AU  - Wang, Yixin
AU  - Sridhar, Dhanya
AU  - Blei, David
T2  - Transactions on Machine Learning Research
AB  - Machine learning (ML) methods have the potential to automate high-stakes decisions, such as bail admissions or credit lending, by analyzing and learning from historical data. But these algorithmic decisions may be unfair: in learning from historical data, they may replicate discriminatory practices from the past. In this paper, we propose two algorithms that adjust fitted ML predictors to produce decisions that are fair. Our methods provide post-hoc adjustments to the predictors, without requiring that they be retrained. We consider a causal model of the ML decisions, define fairness through counterfactual decisions within the model, and then form algorithmic decisions that capture the historical data as well as possible but are provably fair. In particular, we consider two definitions of fairness. The first is ``equal counterfactual opportunity,'' where the counterfactual distribution of the decision is the same regardless of the protected attribute; the second is counterfactual fairness. We evaluate the algorithms, and the trade-off between accuracy and fairness, on datasets about admissions, income, credit, and recidivism.
DA  - 2022/09/20/
PY  - 2022
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=P6NcRPb13w
Y2  - 2024/07/19/13:16:56
L1  - https://openreview.net/pdf?id=P6NcRPb13w
ER  - 

TY  - GEN
TI  - Scaling Laws Do Not Scale
AU  - Diaz, Fernando
AU  - Madaio, Michael
AB  - Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.
DA  - 2023/07/05/
PY  - 2023
DO  - 10.48550/arXiv.2307.03201
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.03201
Y2  - 2024/07/19/13:17:00
L1  - https://arxiv.org/pdf/2307.03201.pdf
L2  - https://arxiv.org/abs/2307.03201
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computers and Society
KW  - Condensed Matter - Disordered Systems and Neural Networks
ER  - 

TY  - GEN
TI  - Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts
AU  - Marcotte, Étienne
AU  - Zantedeschi, Valentina
AU  - Drouin, Alexandre
AU  - Chapados, Nicolas
AB  - Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as commonly performed in the literature.
DA  - 2023/06/06/
PY  - 2023
DO  - 10.48550/arXiv.2304.09836
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2304.09836
Y2  - 2024/07/19/13:17:06
L1  - https://arxiv.org/pdf/2304.09836.pdf
L2  - https://arxiv.org/abs/2304.09836
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - FairPrism: Evaluating Fairness-Related Harms in Text Generation
AU  - Fleisig, Eve
AU  - Amstutz, Aubrie
AU  - Atalla, Chad
AU  - Blodgett, Su Lin
AU  - Daumé III, Hal
AU  - Olteanu, Alexandra
AU  - Sheng, Emily
AU  - Vann, Dan
AU  - Wallach, Hanna
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer specification of dataset coverage, and accounting for annotator disagreement and harms that are context-dependent. FairPrism's annotations include the extent of stereotyping and demeaning harms, the demographic groups targeted, and appropriateness for different applications. The annotations also include specific harms that occur in interactive contexts and harms that raise normative concerns when the “speaker” is an AI system. Due to its precision and granularity, FairPrism can be used to diagnose (1) the types of fairness-related harms that AI text generation systems cause, and (2) the potential limitations of mitigation methods, both of which we illustrate through case studies. Finally, the process we followed to develop FairPrism offers a recipe for building improved datasets for measuring and mitigating harms caused by AI systems.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.343
DP  - ACLWeb
SP  - 6231
EP  - 6251
PB  - Association for Computational Linguistics
ST  - FairPrism
UR  - https://aclanthology.org/2023.acl-long.343
Y2  - 2024/07/19/13:17:09
L1  - https://aclanthology.org/2023.acl-long.343.pdf
ER  - 

TY  - JOUR
TI  - Hypernetworks for Zero-Shot Transfer in Reinforcement Learning
AU  - Rezaei-Shoshtari, Sahand
AU  - Morissette, Charlotte
AU  - Hogan, Francois R.
AU  - Dudek, Gregory
AU  - Meger, David
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
AB  - In this paper, hypernetworks are trained to generate behaviors across a range of unseen task conditions, via a novel TD-based training objective and data from a set of near-optimal RL solutions for training tasks. This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on  zero-shot performance at test time, enabled by knowledge of the task parameters (also known as context). Our technical approach is based upon viewing each RL algorithm as a mapping from the MDP specifics to the near-optimal value function and policy and seek to approximate it with a hypernetwork that can generate near-optimal value functions and policies, given the parameters of the MDP. We show that, under certain conditions, this mapping can be considered as a supervised learning problem. We empirically evaluate the effectiveness of our method for zero-shot transfer to new reward and transition dynamics on a series of continuous control tasks from DeepMind Control Suite. Our method demonstrates significant improvements over baselines from multitask and meta RL approaches.
DA  - 2023/06/26/
PY  - 2023
DO  - 10.1609/aaai.v37i8.26146
DP  - ojs.aaai.org
VL  - 37
IS  - 8
SP  - 9579
EP  - 9587
LA  - en
SN  - 2374-3468
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/26146
Y2  - 2024/07/19/13:17:13
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/26146/25918
KW  - Domain Adaptation
KW  - ML: Transfer
KW  - Multi-Task Learning
ER  - 

TY  - JOUR
TI  - Cognitive Models as Simulators: Using Cognitive Models to Tap into Implicit Human Feedback
AU  - Nobandegani, Ardavan S.
AU  - Shultz, Thomas
AU  - Rish, Irina
AB  - In this work, we substantiate the idea of $\textit{cognitive models as simulators}$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making the training process safer, cheaper, and faster. We leverage this idea in the context of learning a fair behavior toward a counterpart exhibiting various emotional states — as implicit human feedback. As a case study, we adopt the Ultimatum game (UG), a canonical task in behavioral and brain sciences for studying fairness. We show that our reinforcement learning (RL) agents learn to exhibit differential, rationally-justified behaviors under various emotional states of their UG counterpart. We discuss the implications of our work for AI and cognitive science research, and its potential for interactive learning with implicit human feedback.
DA  - 2023/06/20/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - Cognitive Models as Simulators
UR  - https://openreview.net/forum?id=tuxCm2h5JL#all
Y2  - 2024/07/19/13:21:10
L1  - https://openreview.net/pdf?id=tuxCm2h5JL
ER  - 

TY  - GEN
TI  - Accelerating Generalized Random Forests with Fixed-Point Trees
AU  - Fleischer, David
AU  - Stephens, David A.
AU  - Yang, Archer
AB  - Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators obtained from forests whose trees are grown through the fixed-point splitting rule, and provide numerical simulations demonstrating that the estimators obtained from such forests are comparable to those obtained from the more costly gradient-based rule.
DA  - 2023/06/20/
PY  - 2023
DO  - 10.48550/arXiv.2306.11908
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.11908
Y2  - 2024/07/19/13:21:14
L1  - https://arxiv.org/pdf/2306.11908.pdf
L2  - https://arxiv.org/abs/2306.11908
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Statistics - Methodology
ER  - 

TY  - GEN
TI  - Constant Memory Attention Block
AU  - Feng, Leo
AU  - Tung, Frederick
AU  - Hajimirsadeghi, Hossein
AU  - Bengio, Yoshua
AU  - Ahmed, Mohamed Osama
AB  - Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation. Highlighting CMABs efficacy, we introduce methods for Neural Processes and Temporal Point Processes. Empirically, we show our proposed methods achieve results competitive with state-of-the-art while being significantly more memory efficient.
DA  - 2023/06/21/
PY  - 2023
DO  - 10.48550/arXiv.2306.12599
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.12599
Y2  - 2024/07/19/13:21:16
L1  - https://arxiv.org/pdf/2306.12599.pdf
L2  - https://arxiv.org/abs/2306.12599
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Towards Out-of-Distribution Adversarial Robustness
AU  - Ibrahim, Adam
AU  - Guille-Escuret, Charles
AU  - Mitliagkas, Ioannis
AU  - Rish, Irina
AU  - Krueger, David
AU  - Bashivan, Pouya
AB  - Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.
DA  - 2023/06/26/
PY  - 2023
DO  - 10.48550/arXiv.2210.03150
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.03150
Y2  - 2024/07/19/13:21:18
L1  - https://arxiv.org/pdf/2210.03150.pdf
L2  - https://arxiv.org/abs/2210.03150
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Benchmarking Neural Network Training Algorithms
AU  - Dahl, George E.
AU  - Schneider, Frank
AU  - Nado, Zachary
AU  - Agarwal, Naman
AU  - Sastry, Chandramouli Shama
AU  - Hennig, Philipp
AU  - Medapati, Sourabh
AU  - Eschenhagen, Runa
AU  - Kasimbeg, Priya
AU  - Suo, Daniel
AU  - Bae, Juhan
AU  - Gilmer, Justin
AU  - Peirson, Abel L.
AU  - Khan, Bilal
AU  - Anil, Rohan
AU  - Rabbat, Mike
AU  - Krishnan, Shankar
AU  - Snider, Daniel
AU  - Amid, Ehsan
AU  - Chen, Kongtao
AU  - Maddison, Chris J.
AU  - Vasudev, Rakshith
AU  - Badura, Michal
AU  - Garg, Ankush
AU  - Mattson, Peter
AB  - Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass.
DA  - 2023/06/12/
PY  - 2023
DO  - 10.48550/arXiv.2306.07179
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.07179
Y2  - 2024/07/19/13:21:19
L1  - https://arxiv.org/pdf/2306.07179.pdf
L2  - https://arxiv.org/abs/2306.07179
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Harms from Increasingly Agentic Algorithmic Systems
AU  - Chan, Alan
AU  - Salganik, Rebecca
AU  - Markelius, Alva
AU  - Pang, Chris
AU  - Rajkumar, Nitarshan
AU  - Krasheninnikov, Dmitrii
AU  - Langosco, Lauro
AU  - He, Zhonghao
AU  - Duan, Yawen
AU  - Carroll, Micah
AU  - Lin, Michelle
AU  - Mayhew, Alex
AU  - Collins, Katherine
AU  - Molamohammadi, Maryam
AU  - Burden, John
AU  - Zhao, Wanru
AU  - Rismani, Shalaleh
AU  - Voudouris, Konstantinos
AU  - Bhatt, Umang
AU  - Weller, Adrian
AU  - Krueger, David
AU  - Maharaj, Tegan
T3  - FAccT '23
AB  - Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2023/06/12/
PY  - 2023
DO  - 10.1145/3593013.3594033
DP  - ACM Digital Library
SP  - 651
EP  - 666
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594033
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2302.10329
ER  - 

TY  - GEN
TI  - AHA!: Facilitating AI Impact Assessment by Generating Examples of Harms
AU  - Buçinca, Zana
AU  - Pham, Chau Minh
AU  - Jakesch, Maurice
AU  - Ribeiro, Marco Tulio
AU  - Olteanu, Alexandra
AU  - Amershi, Saleema
AB  - While demands for change and accountability for harmful AI consequences mount, foreseeing the downstream effects of deploying AI systems remains a challenging task. We developed AHA! (Anticipating Harms of AI), a generative framework to assist AI practitioners and decision-makers in anticipating potential harms and unintended consequences of AI systems prior to development or deployment. Given an AI deployment scenario, AHA! generates descriptions of possible harms for different stakeholders. To do so, AHA! systematically considers the interplay between common problematic AI behaviors as well as their potential impacts on different stakeholders, and narrates these conditions through vignettes. These vignettes are then filled in with descriptions of possible harms by prompting crowd workers and large language models. By examining 4113 harms surfaced by AHA! for five different AI deployment scenarios, we found that AHA! generates meaningful examples of harms, with different problematic AI behaviors resulting in different types of harms. Prompting both crowds and a large language model with the vignettes resulted in more diverse examples of harms than those generated by either the crowd or the model alone. To gauge AHA!'s potential practical utility, we also conducted semi-structured interviews with responsible AI professionals (N=9). Participants found AHA!'s systematic approach to surfacing harms important for ethical reflection and discovered meaningful stakeholders and harms they believed they would not have thought of otherwise. Participants, however, differed in their opinions about whether AHA! should be used upfront or as a secondary-check and noted that AHA! may shift harm anticipation from an ideation problem to a potentially demanding review problem. Drawing on our results, we discuss design implications of building tools to help practitioners envision possible harms.
DA  - 2023/06/05/
PY  - 2023
DO  - 10.48550/arXiv.2306.03280
DP  - arXiv.org
PB  - arXiv
ST  - AHA!
UR  - http://arxiv.org/abs/2306.03280
Y2  - 2024/07/19/13:21:25
L1  - https://arxiv.org/pdf/2306.03280.pdf
L2  - https://arxiv.org/abs/2306.03280
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Responsible Design Patterns for Machine Learning Pipelines
AU  - Harbi, Saud Hakem Al
AU  - Tidjon, Lionel Nganyewou
AU  - Khomh, Foutse
AB  - Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsible AI systems in production.
DA  - 2023/06/07/
PY  - 2023
DO  - 10.48550/arXiv.2306.01788
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.01788
Y2  - 2024/07/19/13:21:27
L1  - https://arxiv.org/pdf/2306.01788.pdf
L2  - https://arxiv.org/abs/2306.01788
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Model evaluation for extreme risks
AU  - Shevlane, Toby
AU  - Farquhar, Sebastian
AU  - Garfinkel, Ben
AU  - Phuong, Mary
AU  - Whittlestone, Jess
AU  - Leung, Jade
AU  - Kokotajlo, Daniel
AU  - Marchal, Nahema
AU  - Anderljung, Markus
AU  - Kolt, Noam
AU  - Ho, Lewis
AU  - Siddarth, Divya
AU  - Avin, Shahar
AU  - Hawkins, Will
AU  - Kim, Been
AU  - Gabriel, Iason
AU  - Bolina, Vijay
AU  - Clark, Jack
AU  - Bengio, Yoshua
AU  - Christiano, Paul
AU  - Dafoe, Allan
AB  - Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.
DA  - 2023/09/22/
PY  - 2023
DO  - 10.48550/arXiv.2305.15324
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.15324
Y2  - 2024/07/19/13:21:30
L1  - https://arxiv.org/pdf/2305.15324.pdf
L2  - https://arxiv.org/abs/2305.15324
KW  - Computer Science - Artificial Intelligence
KW  - K.4.1
ER  - 

TY  - GEN
TI  - Think Before You Act: Decision Transformers with Working Memory
AU  - Kang, Jikun
AU  - Laroche, Romain
AU  - Yuan, Xingdi
AU  - Trischler, Adam
AU  - Liu, Xue
AU  - Fu, Jie
AB  - Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.
DA  - 2024/05/28/
PY  - 2024
DO  - 10.48550/arXiv.2305.16338
DP  - arXiv.org
PB  - arXiv
ST  - Think Before You Act
UR  - http://arxiv.org/abs/2305.16338
Y2  - 2024/07/19/13:21:32
L1  - https://arxiv.org/pdf/2305.16338.pdf
L2  - https://arxiv.org/abs/2305.16338
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Raising the Bar for Certified Adversarial Robustness with Diffusion Models
AU  - Altstidl, Thomas
AU  - Dobre, David
AU  - Eskofier, Björn
AU  - Gidel, Gauthier
AU  - Schwinn, Leo
AB  - Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. One of our main insights is that the generalization gap, i.e., the difference between the training and test accuracy of the original model, is a good predictor of the magnitude of the robustness improvement when using additional generated data. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_\infty$ ($\epsilon = 8/255$) threat models, outperforming the previous best results by $+3.95\%$ and $+1.39\%$, respectively. Furthermore, we report similar improvements for CIFAR-100.
DA  - 2023/05/17/
PY  - 2023
DO  - 10.48550/arXiv.2305.10388
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.10388
Y2  - 2024/07/19/13:26:18
L1  - https://arxiv.org/pdf/2305.10388.pdf
L2  - https://arxiv.org/abs/2305.10388
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Towards ethical multimodal systems
AU  - Roger, Alexis
AU  - Aïmeur, Esma
AU  - Rish, Irina
AB  - Generative AI systems (ChatGPT, DALL-E, etc) are expanding into multiple areas of our lives, from art Rombach et al. [2021] to mental health Rob Morris and Kareem Kouddous [2022]; their rapidly growing societal impact opens new opportunities, but also raises ethical concerns. The emerging field of AI alignment aims to make AI systems reflect human values. This paper focuses on evaluating the ethics of multimodal AI systems involving both text and images - a relatively under-explored area, as most alignment work is currently focused on language models. We first create a multimodal ethical database from human feedback on ethicality. Then, using this database, we develop algorithms, including a RoBERTa-large classifier and a multilayer perceptron, to automatically assess the ethicality of system responses.
DA  - 2024/05/20/
PY  - 2024
DO  - 10.48550/arXiv.2304.13765
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2304.13765
Y2  - 2024/07/19/13:26:20
L1  - https://arxiv.org/pdf/2304.13765.pdf
L2  - https://arxiv.org/abs/2304.13765
KW  - Computer Science - Artificial Intelligence
KW  - I.2.7
ER  - 

TY  - GEN
TI  - Posthoc Interpretation via Quantization
AU  - Paissan, Francesco
AU  - Subakan, Cem
AU  - Ravanelli, Mirco
AB  - In this paper, we introduce a new approach, called Posthoc Interpretation via Quantization (PIQ), for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. Our model formulation also enables learning concepts by incorporating the supervision of pretrained annotation models such as state-of-the-art image segmentation models. We evaluated our method through quantitative and qualitative studies involving black-and-white images, color images, and audio. As a result of these studies we found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
DA  - 2023/05/27/
PY  - 2023
DO  - 10.48550/arXiv.2303.12659
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2303.12659
Y2  - 2024/07/19/13:26:30
L1  - https://arxiv.org/pdf/2303.12659.pdf
L2  - https://arxiv.org/abs/2303.12659
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Sound
KW  - Electrical Engineering and Systems Science - Audio and Speech Processing
ER  - 

TY  - GEN
TI  - Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods
AU  - Ferry, Julien
AU  - Laberge, Gabriel
AU  - Aïvodji, Ulrich
AB  - A hybrid model involves the cooperation of an interpretable model and a complex black box. At inference, any input of the hybrid model is assigned to either its interpretable or complex component based on a gating mechanism. The advantages of such models over classical ones are two-fold: 1) They grant users precise control over the level of transparency of the system and 2) They can potentially perform better than a standalone black box since redirecting some of the inputs to an interpretable model implicitly acts as regularization. Still, despite their high potential, hybrid models remain under-studied in the interpretability/explainability literature. In this paper, we remedy this fact by presenting a thorough investigation of such models from three perspectives: Theory, Taxonomy, and Methods. First, we explore the theory behind the generalization of hybrid models from the Probably-Approximately-Correct (PAC) perspective. A consequence of our PAC guarantee is the existence of a sweet spot for the optimal transparency of the system. When such a sweet spot is attained, a hybrid model can potentially perform better than a standalone black box. Secondly, we provide a general taxonomy for the different ways of training hybrid models: the Post-Black-Box and Pre-Black-Box paradigms. These approaches differ in the order in which the interpretable and complex components are trained. We show where the state-of-the-art hybrid models Hybrid-Rule-Set and Companion-Rule-List fall in this taxonomy. Thirdly, we implement the two paradigms in a single method: HybridCORELS, which extends the CORELS algorithm to hybrid modeling. By leveraging CORELS, HybridCORELS provides a certificate of optimality of its interpretable component and precise control over transparency. We finally show empirically that HybridCORELS is competitive with existing hybrid models, and performs just as well as a standalone black box (or even better) while being partly transparent.
DA  - 2023/03/08/
PY  - 2023
DO  - 10.48550/arXiv.2303.04437
DP  - arXiv.org
PB  - arXiv
ST  - Learning Hybrid Interpretable Models
UR  - http://arxiv.org/abs/2303.04437
Y2  - 2024/07/19/13:26:33
L1  - https://arxiv.org/pdf/2303.04437.pdf
L2  - https://arxiv.org/abs/2303.04437
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Out-of-context Meta-learning in Large Language Models
AU  - Krasheninnikov, Dmitrii
AU  - Krasheninnikov, Egor
AU  - Krueger, David
T2  - ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models
AB  - Brown et al. (2020) famously introduced the phenomenon of in-context meta-learning in large language models (LLMs). Our work establishes the existence of a phenomenon we call out-of-context meta-learning via carefully designed synthetic experiments with large language models. We argue that out-of-context meta-learning is an important and surprising capability of LLMs, which may lead them to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and apply it in appropriate contexts. We also raise the question of how this phenomenon emerges, and discuss two possible explanations: one relying on the way LLMs store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based methods may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks.
DA  - 2023/02/08/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=X3JFgY4gvf
Y2  - 2024/07/19/13:26:34
L1  - https://openreview.net/pdf?id=X3JFgY4gvf
ER  - 

TY  - GEN
TI  - Disentanglement of Correlated Factors via Hausdorff Factorized Support
AU  - Roth, Karsten
AU  - Ibrahim, Mark
AU  - Akata, Zeynep
AU  - Vincent, Pascal
AU  - Bouchacourt, Diane
AB  - A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized \emph{support}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over $+60\%$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.
DA  - 2023/02/25/
PY  - 2023
DO  - 10.48550/arXiv.2210.07347
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.07347
Y2  - 2024/07/19/13:26:41
L1  - https://arxiv.org/pdf/2210.07347.pdf
L2  - https://arxiv.org/abs/2210.07347
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Systematic Rectification of Language Models via Dead-end Analysis
AU  - Cao, Meng
AU  - Fatemi, Mehdi
AU  - Cheung, Jackie Chi Kit
AU  - Shabanian, Samira
AB  - With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. This is crucial as many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.
DA  - 2023/02/27/
PY  - 2023
DO  - 10.48550/arXiv.2302.14003
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.14003
Y2  - 2024/07/19/13:32:29
L1  - https://arxiv.org/pdf/2302.14003.pdf
L2  - https://arxiv.org/abs/2302.14003
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning
AU  - Nguyen, John
AU  - Wang, Jianyu
AU  - Malik, Kshitiz
AU  - Sanjabi, Maziar
AU  - Rabbat, Michael
AB  - An oft-cited challenge of federated learning is the presence of heterogeneity. \emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \emph{System heterogeneity} refers to the fact that client devices have different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. We empirically study the impact of starting from a pre-trained model in federated learning using four standard federated learning benchmark datasets. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\%) than is possible when starting from random initialization. Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity. We recommend that future work proposing and evaluating federated optimization methods evaluate the performance when starting from random and pre-trained initializations. We also believe this study raises several questions for further work on understanding the role of heterogeneity in federated optimization.
DA  - 2022/10/14/
PY  - 2022
DO  - 10.48550/arXiv.2210.08090
DP  - arXiv.org
PB  - arXiv
ST  - Where to Begin?
UR  - http://arxiv.org/abs/2210.08090
Y2  - 2024/07/19/13:32:31
L1  - https://arxiv.org/pdf/2210.08090.pdf
L2  - https://arxiv.org/abs/2210.08090
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Exploring validation metrics for ofﬂine model-based optimisation
AU  - Beckham, Christopher
AU  - Piché, Alexandre
AU  - Vázquez, David
AU  - Pal, C.
AB  - In ofﬂine model-based optimisation (MBO) we are interested in using machine learning to de-sign candidates that maximise some measure of desirability through an expensive but real-world scoring process. Ofﬂine MBO tries to approximate this expensive scoring function and use that to evaluate generated designs, however evaluation is non-exact because one approximation is being evaluated with another. Instead, we ask ourselves: if we did have the real world scoring function at hand, what cheap-to-compute validation metrics would correlate best with this? Since the real-world scoring function is available for simulated MBO datasets, insights obtained from this can be transferred over to real-world ofﬂine MBO tasks where the real-world scoring function is expensive to compute. To address this, we propose a conceptual evaluation framework that is amenable to measuring extrapolation, and apply this to conditional denoising diffusion models. Empirically, we ﬁnd that two validation metrics – agreement and Frechet distance – correlate quite well with the ground truth. When there is high variability in conditional generation, feedback is required in the form of an approximated version of the real-world scoring function. Furthermore, we ﬁnd that generating high-scoring samples may require heavily weighting the generative model in favour of sample quality, potentially at the cost of sample diversity.
DA  - 2023///
PY  - 2023
DP  - Semantic Scholar
UR  - https://www.semanticscholar.org/paper/Exploring-validation-metrics-for-of%EF%AC%82ine-model-based-Beckham-Pich%C3%A9/8f995cbe15e1c1fc2dbc9024b1b56440859761de
Y2  - 2024/07/19/13:32:33
L2  - https://www.semanticscholar.org/paper/Exploring-validation-metrics-for-of%EF%AC%82ine-model-based-Beckham-Pich%C3%A9/8f995cbe15e1c1fc2dbc9024b1b56440859761de
ER  - 

TY  - CONF
TI  - Human-Centered Responsible Artificial Intelligence: Current & Future Trends
AU  - Tahaei, Mohammad
AU  - Constantinides, Marios
AU  - Quercia, Daniele
AU  - Kennedy, Sean
AU  - Muller, Michael
AU  - Stumpf, Simone
AU  - Liao, Q. Vera
AU  - Baeza-Yates, Ricardo
AU  - Aroyo, Lora
AU  - Holbrook, Jess
AU  - Luger, Ewa
AU  - Madaio, Michael
AU  - Blumenfeld, Ilana Golbin
AU  - De-Arteaga, Maria
AU  - Vitak, Jessica
AU  - Olteanu, Alexandra
T3  - CHI EA '23
AB  - In recent years, the CHI community has seen significant growth in research on Human-Centered Responsible Artificial Intelligence. While different research communities may use different terminology to discuss similar topics, all of this work is ultimately aimed at developing AI that benefits humanity while being grounded in human rights and ethics, and reducing the potential harms of AI. In this special interest group, we aim to bring together researchers from academia and industry interested in these topics to map current and future research trends to advance this important area of research by fostering collaboration and sharing ideas.
C1  - New York, NY, USA
C3  - Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems
DA  - 2023/04/19/
PY  - 2023
DO  - 10.1145/3544549.3583178
DP  - ACM Digital Library
SP  - 1
EP  - 4
PB  - Association for Computing Machinery
SN  - 978-1-4503-9422-2
ST  - Human-Centered Responsible Artificial Intelligence
UR  - https://doi.org/10.1145/3544549.3583178
Y2  - 2024/07/19/
L1  - https://arxiv.org/pdf/2302.08157
ER  - 

TY  - CONF
TI  - Survey of Scientific Rigor Studied in Machine Learning
AU  - Sculley, D.
AU  - Holt, Gary
AU  - Golovin, D.
AU  - Davydov, Eugene
AU  - Phillips, Todd
AU  - Ebner, D.
AU  - Young, Michael
AU  - Crespo, Jean-François
AU  - Dennison, Dan
AU  - Fox, Emily
AU  - Larochelle, H.
AB  - The concern that Artificial Intelligence (AI) and Machine Learning (ML) are entering a “reproducibility crisis” has spurred significant research in the past few years. Yet with each paper, it is often unclear what someone means by “reproducibility” and where it fits in the larger scope of what we will call the “scientific rigor” literature. Ultimately, the lack of clear rigor standards can affect the manner in which businesses seeking to adopt AI/ML implement such capabilities. In this survey, we will use 66 papers published since 2017 to construct a proposed set of 8 high-level categories of scientific rigor, what they are, and the history of work conducted in each. Our proposal is that these eight rigor types are not mutually exclusive and present a model for how they influence each other. To encourage more to study these questions, we map these rigors to the adoption process in real-world business use cases. In doing so, we can quantify gaps in the literature that suggest an under focus on the issues necessary for scientific rigor research to transition to practice
DA  - 2023///
PY  - 2023
DP  - Semantic Scholar
UR  - https://www.semanticscholar.org/paper/Survey-of-Scientific-Rigor-Studied-in-Machine-Sculley-Holt/2e6953cfda81ebbf85a44bc8be72c639ec592b3a
Y2  - 2024/07/19/13:32:39
L2  - https://www.semanticscholar.org/paper/Survey-of-Scientific-Rigor-Studied-in-Machine-Sculley-Holt/2e6953cfda81ebbf85a44bc8be72c639ec592b3a
ER  - 

TY  - CONF
TI  - Towards Reliable Neural Specifications
AU  - Geng, Chuqin
AU  - Le, Nham
AU  - Xu, Xiaojie
AU  - Wang, Zhaoyue
AU  - Gurfinkel, Arie
AU  - Si, Xujie
T2  - International Conference on Machine Learning
AB  - Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs. We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be used as a more reliable and extensible specification for neural network verification.
C3  - Proceedings of the 40th International Conference on Machine Learning
DA  - 2023/07/03/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 11196
EP  - 11212
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v202/geng23a.html
Y2  - 2024/07/19/13:32:42
L1  - https://proceedings.mlr.press/v202/geng23a/geng23a.pdf
ER  - 

TY  - JOUR
TI  - Towards Continual Reinforcement Learning: A Review and Perspectives
AU  - Khetarpal, Khimya
AU  - Riemer, Matthew
AU  - Rish, Irina
AU  - Precup, Doina
T2  - Journal of Artificial Intelligence Research
AB  - In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.
DA  - 2022/12/22/
PY  - 2022
DO  - 10.1613/jair.1.13673
DP  - jair.org
VL  - 75
SP  - 1401
EP  - 1476
LA  - en
SN  - 1076-9757
ST  - Towards Continual Reinforcement Learning
UR  - https://jair.org/index.php/jair/article/view/13673
Y2  - 2024/07/19/13:32:44
L1  - https://jair.org/index.php/jair/article/download/13673/26878
KW  - markov decision processes
KW  - reinforcement learning
ER  - 

TY  - JOUR
TI  - Post-hoc Interpretability for Neural NLP: A Survey
AU  - Madsen, Andreas
AU  - Reddy, Siva
AU  - Chandar, Sarath
T2  - ACM Comput. Surv.
AB  - Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.
DA  - 2022/12/23/
PY  - 2022
DO  - 10.1145/3546577
DP  - ACM Digital Library
VL  - 55
IS  - 8
SP  - 155:1
EP  - 155:42
SN  - 0360-0300
ST  - Post-hoc Interpretability for Neural NLP
UR  - https://doi.org/10.1145/3546577
Y2  - 2024/07/19/13:32:47
L1  - https://arxiv.org/pdf/2108.04840
ER  - 

TY  - CONF
TI  - Learning from uncertain concepts via test time interventions
AU  - Sheth, Ivaxi
AU  - Rahman, Aamer Abdul
AU  - Sevyeri, Laya Rafiee
AU  - Havaei, Mohammad
AU  - Kahou, Samira Ebrahimi
T2  - Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022
AB  - With neural networks applied to safety-critical applications, it has become increasingly important to understand the defining features of decision-making. Therefore, the need to uncover the black boxes to rational representational space of these neural networks is apparent. Concept bottleneck model (CBM) encourages interpretability by predicting human-understandable concepts. They predict concepts from input images and then labels from concepts. Test time intervention, a salient feature of CBM, allows for human-model interactions. However, these interactions are prone to information leakage and can often be ineffective inappropriate communication with humans. We propose a novel uncertainty based strategy, \emph{SIUL: Single Interventional Uncertainty Learning} to select the interventions. Additionally, we empirically test the robustness of CBM and the effect of SIUL interventions under adversarial attack and distributional shift. Using SIUL, we observe that the interventions suggested lead to meaningful corrections along with mitigation of concept leakage. Extensive experiments on three vision datasets along with a histopathology dataset validate the effectiveness of our interventional learning.
DA  - 2022/11/21/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=WVe3vok8Cc3
Y2  - 2024/07/19/13:32:49
L1  - https://openreview.net/pdf?id=WVe3vok8Cc3
ER  - 

TY  - GEN
TI  - Aligning MAGMA by Few-Shot Learning and Finetuning
AU  - Layoun, Jean-Charles
AU  - Roger, Alexis
AU  - Rish, Irina
AB  - The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.
DA  - 2022/10/18/
PY  - 2022
DO  - 10.48550/arXiv.2210.14161
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.14161
Y2  - 2024/07/19/13:32:54
L1  - https://arxiv.org/pdf/2210.14161.pdf
L2  - https://arxiv.org/abs/2210.14161
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - Inductive biases for deep learning of higher-level cognition
AU  - Goyal, Anirudh
AU  - Bengio, Yoshua
T2  - Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences
AB  - A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.
DA  - 2022/10/12/
PY  - 2022
DO  - 10.1098/rspa.2021.0068
DP  - royalsocietypublishing.org (Atypon)
VL  - 478
IS  - 2266
SP  - 20210068
UR  - https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068
Y2  - 2024/07/19/13:42:05
L1  - https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2021.0068
KW  - deep learning
KW  - reasoning
KW  - causality
KW  - system 2
KW  - systematic and out-of-distribution generalization
ER  - 

TY  - JOUR
TI  - Application of Artificial Intelligence in Shared Decision Making: Scoping Review
AU  - Rahimi, Samira Abbasgholizadeh
AU  - Cwintal, Michelle
AU  - Huang, Yuhui
AU  - Ghadiri, Pooria
AU  - Grad, Roland
AU  - Poenaru, Dan
AU  - Gore, Genevieve
AU  - Zomahoun, Hervé Tchala Vignon
AU  - Légaré, France
AU  - Pluye, Pierre
T2  - JMIR Medical Informatics
AB  - Background: Artificial intelligence (AI) has shown promising results in various fields of medicine. It has the potential to facilitate shared decision making (SDM). However, there is no comprehensive mapping of how AI may be used for SDM.
Objective: We aimed to identify and evaluate published studies that have tested or implemented AI to facilitate SDM.
Methods: We performed a scoping review informed by the methodological framework proposed by Levac et al, modifications to the original Arksey and O'Malley framework of a scoping review, and the Joanna Briggs Institute scoping review framework. We reported our results based on the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) reporting guideline. At the identification stage, an information specialist performed a comprehensive search of 6 electronic databases from their inception to May 2021. The inclusion criteria were: all populations; all AI interventions that were used to facilitate SDM, and if the AI intervention was not used for the decision-making point in SDM, it was excluded; any outcome related to patients, health care providers, or health care systems; studies in any health care setting, only studies published in the English language, and all study types. Overall, 2 reviewers independently performed the study selection process and extracted data. Any disagreements were resolved by a third reviewer. A descriptive analysis was performed.
Results: The search process yielded 1445 records. After removing duplicates, 894 documents were screened, and 6 peer-reviewed publications met our inclusion criteria. Overall, 2 of them were conducted in North America, 2 in Europe, 1 in Australia, and 1 in Asia. Most articles were published after 2017. Overall, 3 articles focused on primary care, and 3 articles focused on secondary care. All studies used machine learning methods. Moreover, 3 articles included health care providers in the validation stage of the AI intervention, and 1 article included both health care providers and patients in clinical validation, but none of the articles included health care providers or patients in the design and development of the AI intervention. All used AI to support SDM by providing clinical recommendations or predictions.
Conclusions: Evidence of the use of AI in SDM is in its infancy. We found AI supporting SDM in similar ways across the included articles. We observed a lack of emphasis on patients’ values and preferences, as well as poor reporting of AI interventions, resulting in a lack of clarity about different aspects. Little effort was made to address the topics of explainability of AI interventions and to include end-users in the design and development of the interventions. Further efforts are required to strengthen and standardize the use of AI in different steps of SDM and to evaluate its impact on various decisions, populations, and settings.
DA  - 2022/08/09/
PY  - 2022
DO  - 10.2196/36199
DP  - medinform.jmir.org
VL  - 10
IS  - 8
SP  - e36199
LA  - EN
ST  - Application of Artificial Intelligence in Shared Decision Making
UR  - https://medinform.jmir.org/2022/8/e36199
Y2  - 2024/07/19/13:42:10
L1  - https://medinform.jmir.org/2022/8/e36199/PDF
L2  - https://medinform.jmir.org/2022/8/e36199
ER  - 

TY  - CONF
TI  - On the Effectiveness of Interpretable Feedforward Neural Network
AU  - Li, Miles Q.
AU  - Fung, Benjamin C. M.
AU  - Abusitta, Adel
T2  - 2022 International Joint Conference on Neural Networks (IJCNN)
AB  - Deep learning models have achieved state-of-the-art performance in many classification tasks. However, most of them cannot provide an explanation for their classification results. Machine learning models that are interpretable are usually linear or piecewise linear and yield inferior performance. Non-linear models achieve much better classification performance, but it is usually hard to explain their classification results. As a counter-example, an interpretable feedforward neural network (IFFNN) is proposed to achieve both high classification performance and interpretability for malware detection. If the IFFNN can perform well in a more flexible and general form for other classification tasks while providing meaningful explanations, it may be of great interest to the applied machine learning community. In this paper, we propose a way to generalize the interpretable feedforward neural network to multi-class classification scenarios and any type of feedforward neural networks, and evaluate its classification performance and interpretability on interpretable datasets. We conclude by finding that the generalized IFFNNs achieve comparable classification performance to their normal feedforward neural network counterparts and provide meaningful explanations. Thus, this kind of neural network architecture has great practical use.
C3  - 2022 International Joint Conference on Neural Networks (IJCNN)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/IJCNN55064.2022.9892343
DP  - IEEE Xplore
SP  - 1
EP  - 8
UR  - https://ieeexplore.ieee.org/document/9892343
Y2  - 2024/07/19/13:42:14
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9892343&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzk4OTIzNDM=
L2  - https://ieeexplore.ieee.org/document/9892343
KW  - Deep learning
KW  - Task analysis
KW  - Feedforward neural networks
KW  - Malware
ER  - 

TY  - CONF
TI  - Identification of out-of-distribution cases of CNN using class-based surprise adequacy
AU  - Marhaba, Mira
AU  - Merlo, Ettore
AU  - Khomh, Foutse
AU  - Antoniol, Giuliano
T3  - CAIN '22
AB  - Machine learning is vulnerable to possible incorrect classification of cases that are out of the distribution observed during training and calibration.There has been much recent research revolving around the detection of adversarial images, when it comes to neural networks [3, 6, 10, 12--14]. Good results have been obtained by somehow learning adversarial features and behaviors and using this knowledge to distinguish between correctly and incorrectly predicted classifications due to adversarial attacks.To identify OOD cases, we propose to use Surprise Adequacy Deep Learning Likelihood (SADL) [6] instantiated to each output class, to measure In-Distribution or Out-Of-Distribution computational likelihood of classifications performed by a network.Out-of-distribution cases were not drawn from the same distribution of the training sets and they were created using affine transformations of legitimate inputs and adversarial attacks.Presented experimental results show that OOD analysis allows up to 70% to 90% OOD detection. The identification of OOD computations may be beneficial in sensitive and critical domains such as aerospace, medicine, cyber-security, and many others.
C1  - New York, NY, USA
C3  - Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI
DA  - 2022/10/17/
PY  - 2022
DO  - 10.1145/3522664.3528617
DP  - ACM Digital Library
SP  - 39
EP  - 40
PB  - Association for Computing Machinery
SN  - 978-1-4503-9275-4
UR  - https://doi.org/10.1145/3522664.3528617
Y2  - 2024/07/19/
ER  - 

TY  - GEN
TI  - Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation
AU  - Arora, Kushal
AU  - Asri, Layla El
AU  - Bahuleyan, Hareesh
AU  - Cheung, Jackie Chi Kit
AB  - Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_exposure_bias
DA  - 2023/01/09/
PY  - 2023
DO  - 10.48550/arXiv.2204.01171
DP  - arXiv.org
PB  - arXiv
ST  - Why Exposure Bias Matters
UR  - http://arxiv.org/abs/2204.01171
Y2  - 2024/07/19/13:42:18
L1  - https://arxiv.org/pdf/2204.01171.pdf
L2  - https://arxiv.org/abs/2204.01171
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - JANOS: An Integrated Predictive and Prescriptive Modeling Framework
AU  - Bergman, David
AU  - Huang, Teng
AU  - Brooks, Philip
AU  - Lodi, Andrea
AU  - Raghunathan, Arvind U.
T2  - INFORMS Journal on Computing
AB  - Business research practice is witnessing a surge in the integration of predictive modeling and prescriptive analysis. We describe a modeling framework JANOS that seamlessly integrates the two streams of analytics, allowing researchers and practitioners to embed machine learning models in an end-to-end optimization framework. JANOS allows for specifying a prescriptive model using standard optimization modeling elements such as constraints and variables. The key novelty lies in providing modeling constructs that enable the specification of commonly used predictive models within an optimization model, have the features of the predictive model as variables in the optimization model, and incorporate the output of the predictive models as part of the objective. The framework considers two sets of decision variables: regular and predicted. The relationship between the regular and the predicted variables is specified by the user as pretrained predictive models. JANOS currently supports linear regression, logistic regression, and neural network with rectified linear activation functions. In this paper, we demonstrate the flexibility of the framework through an example on scholarship allocation in a student enrollment problem and provide a numeric performance evaluation.

Summary of Contribution. This paper describes a new software tool, JANOS, that integrates predictive modeling and discrete optimization to assist decision making. Specifically, the proposed solver takes as input user-specified pretrained predictive models and formulates optimization models directly over those predictive models by embedding them within an optimization model through linear transformations.
DA  - 2022/03//
PY  - 2022
DO  - 10.1287/ijoc.2020.1023
DP  - pubsonline.informs.org (Atypon)
VL  - 34
IS  - 2
SP  - 807
EP  - 816
SN  - 1091-9856
ST  - JANOS
UR  - https://pubsonline.informs.org/doi/10.1287/ijoc.2020.1023
Y2  - 2024/07/19/13:42:52
L1  - https://arxiv.org/pdf/1911.09461
KW  - predictive modeling
KW  - discrete optimization
KW  - prescriptive analysis
KW  - solver
ER  - 

TY  - JOUR
TI  - Faults in deep reinforcement learning programs: a taxonomy and a detection approach
AU  - Nikanjam, Amin
AU  - Morovati, Mohammad Mehdi
AU  - Khomh, Foutse
AU  - Ben Braiek, Houssem
T2  - Automated Software Engineering
AB  - A growing demand is witnessed in both industry and academia for employing Deep Learning (DL) in various domains to solve real-world problems. Deep reinforcement learning (DRL) is the application of DL in the domain of Reinforcement Learning. Like any software system, DRL applications can fail because of faults in their programs. In this paper, we present the first attempt to categorize faults occurring in DRL programs. We manually analyzed 761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues) developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl, Tensorforce) and identified faults reported by developers/users. We labeled and taxonomized the identified faults through several rounds of discussions. The resulting taxonomy is validated using an online survey with 19 developers/researchers. To allow for the automatic detection of faults in DRL programs, we have defined a meta-model of DRL programs and developed DRLinter, a model-based fault detection approach that leverages static analysis and graph transformations. The execution flow of DRLinter consists in parsing a DRL program to generate a model conforming to our meta-model and applying detection rules on the model to identify faults occurrences. The effectiveness of DRLinter is evaluated using 21 synthetic and real faulty DRL programs. For synthetic samples, we injected faults observed in the analyzed artifacts from Stack Overflow and GitHub. The results show that DRLinter can successfully detect faults in both synthesized and real-world examples with a recall of 75% and a precision of 100%.
DA  - 2021/12/20/
PY  - 2021
DO  - 10.1007/s10515-021-00313-x
DP  - Springer Link
VL  - 29
IS  - 1
SP  - 8
J2  - Autom Softw Eng
LA  - en
SN  - 1573-7535
ST  - Faults in deep reinforcement learning programs
UR  - https://doi.org/10.1007/s10515-021-00313-x
Y2  - 2024/07/19/13:48:09
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10515-021-00313-x.pdf
KW  - Deep reinforcement learning
KW  - Fault detection
KW  - Software testing
KW  - Graph transformations
ER  - 

TY  - CONF
TI  - Decision Referrals in Human-Automation Teams
AU  - Kaza, Kesav
AU  - Ny, Jerome Le
AU  - Mahajan, Aditya
T2  - 2021 60th IEEE Conference on Decision and Control (CDC)
AB  - We consider a model for optimal decision referrals in human-automation teams performing binary classification tasks. The automation observes a batch of independent tasks, analyzes them, and has the option to refer a subset of them to a human operator. The human operator performs fresh analysis of the tasks referred to him. Our key modeling assumption is that the human performance degrades with workload (i.e., the number of tasks referred to human). We model the problem as a stochastic optimization problem. We first consider the special case when the workload of the human is pre-specified. We show that in this setting it is optimal to myopically refer tasks which lead to the largest reduction in the conditional expected cost until the desired workload target is met. We next consider the general setting where there is no constraint on the workload. We leverage the solution of the previous step and provide a search algorithm to efficiently find the optimal set of tasks to refer. Finally, we present a numerical study to compare the performance of our algorithm with some baseline allocation policies.
C3  - 2021 60th IEEE Conference on Decision and Control (CDC)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/CDC45484.2021.9683407
DP  - IEEE Xplore
SP  - 2842
EP  - 2847
UR  - https://ieeexplore.ieee.org/document/9683407
Y2  - 2024/07/19/13:48:13
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9683407&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzk2ODM0MDc=
KW  - Decision making
KW  - Numerical models
KW  - Decision support systems
KW  - Costs
KW  - Classification algorithms
KW  - Stochastic processes
KW  - Numerical simulation
ER  - 

TY  - GEN
TI  - Cognitive Models as Simulators: The Case of Moral Decision-Making
AU  - Nobandegani, Ardavan S.
AU  - Shultz, Thomas R.
AU  - Rish, Irina
AB  - To achieve desirable performance, current AI systems often require huge amounts of training data. This is especially problematic in domains where collecting data is both expensive and time-consuming, e.g., where AI systems require having numerous interactions with humans, collecting feedback from them. In this work, we substantiate the idea of $\textit{cognitive models as simulators}$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making their training process both less costly and faster. Here, we leverage this idea in the context of moral decision-making, by having reinforcement learning (RL) agents learn about fairness through interacting with a cognitive model of the Ultimatum Game (UG), a canonical task in behavioral and brain sciences for studying fairness. Interestingly, these RL agents learn to rationally adapt their behavior depending on the emotional state of their simulated UG responder. Our work suggests that using cognitive models as simulators of humans is an effective approach for training AI systems, presenting an important way for computational cognitive science to make contributions to AI.
DA  - 2022/10/08/
PY  - 2022
DO  - 10.48550/arXiv.2210.04121
DP  - arXiv.org
PB  - arXiv
ST  - Cognitive Models as Simulators
UR  - http://arxiv.org/abs/2210.04121
Y2  - 2024/07/19/13:48:17
L1  - https://arxiv.org/pdf/2210.04121.pdf
L2  - https://arxiv.org/abs/2210.04121
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
KW  - Quantitative Biology - Neurons and Cognition
ER  - 

TY  - CONF
TI  - Robust Policy Learning over Multiple Uncertainty Sets
AU  - Xie, Annie
AU  - Sodhani, Shagun
AU  - Finn, Chelsea
AU  - Pineau, Joelle
AU  - Zhang, Amy
T2  - International Conference on Machine Learning
AB  - Reinforcement learning (RL) agents need to be robust to variations in safety-critical environments. While system identification methods provide a way to infer the variation from online experience, they can fail in settings where fast identification is not possible. Another dominant approach is robust RL which produces a policy that can handle worst-case scenarios, but these methods are generally designed to achieve robustness to a single uncertainty set that must be specified at train time. Towards a more general solution, we formulate the multi-set robustness problem to learn a policy robust to different perturbation sets. We then design an algorithm that enjoys the benefits of both system identification and robust RL: it reduces uncertainty where possible given a few interactions, but can still act robustly with respect to the remaining uncertainty. On a diverse set of control tasks, our approach demonstrates improved worst-case performance on new environments compared to prior methods based on system identification and on robust RL alone.
C3  - Proceedings of the 39th International Conference on Machine Learning
DA  - 2022/06/28/
PY  - 2022
DP  - proceedings.mlr.press
SP  - 24414
EP  - 24429
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v162/xie22c.html
Y2  - 2024/07/19/13:48:18
L1  - https://proceedings.mlr.press/v162/xie22c/xie22c.pdf
ER  - 

TY  - JOUR
TI  - Washing The Unwashable : On The (Im)possibility of Fairwashing Detection
AU  - Shahin Shamsabadi, Ali
AU  - Yaghini, Mohammad
AU  - Dullerud, Natalie
AU  - Wyllie, Sierra
AU  - Aïvodji, Ulrich
AU  - Alaagib, Aisha
AU  - Gambs, Sébastien
AU  - Papernot, Nicolas
T2  - Advances in Neural Information Processing Systems
DA  - 2022/12/06/
PY  - 2022
DP  - proceedings.neurips.cc
VL  - 35
SP  - 14170
EP  - 14182
LA  - en
ST  - Washing The Unwashable
UR  - https://proceedings.neurips.cc/paper_files/paper/2022/hash/5b84864ff8474fd742c66f219b2eaac1-Abstract-Conference.html
Y2  - 2024/07/19/13:48:22
L1  - https://proceedings.neurips.cc/paper_files/paper/2022/file/5b84864ff8474fd742c66f219b2eaac1-Paper-Conference.pdf
ER  - 

TY  - JOUR
TI  - Sabotage Evaluations for Frontier Models
AU  - Benton, Joe
AU  - Wagner, Misha
AU  - Christiansen, Eric
AU  - Anil, Cem
AU  - Perez, Ethan
AU  - Srivastav, Jai
AU  - Durmus, Esin
AU  - Ganguli, Deep
AU  - Kravec, Shauna
AU  - Shlegeris, Buck
AU  - Kaplan, Jared
AU  - Karnofsky, Holden
AU  - Hubinger, Evan
AU  - Grosse, Roger
AU  - Bowman, Samuel R
AU  - Duvenaud, David
AB  - Sufficiently capable models could subvert human oversight and decisionmaking in important contexts. For example, in the context of AI development, models could covertly sabotage efforts to evaluate their own dangerous capabilities, to monitor their behavior, or to make decisions about their deployment. We refer to this family of abilities as sabotage capabilities. We develop a set of related threat models and evaluations. These evaluations are designed to provide evidence that a given model, operating under a given set of mitigations, could not successfully sabotage a frontier model developer or other large organization’s activities in any of these ways. We demonstrate these evaluations on Anthropic’s Claude 3 Opus and Claude 3.5 Sonnet models. Our results suggest that for these models, minimal mitigations are currently sufficient to address sabotage risks, but that more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve. We also survey related evaluations we tried and abandoned. Finally, we discuss the advantages of mitigation-aware capability evaluations, and of simulating large-scale deployments using smallscale statistics.
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://assets.anthropic.com/m/377027d5b36ac1eb/original/Sabotage-Evaluations-for-Frontier-Models.pdf
ER  - 

TY  - GEN
TI  - Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?
AU  - Ren, Richard
AU  - Basart, Steven
AU  - Khoja, Adam
AU  - Gatti, Alice
AU  - Phan, Long
AU  - Yin, Xuwang
AU  - Mazeika, Mantas
AU  - Pan, Alexander
AU  - Mukobi, Gabriel
AU  - Kim, Ryan H.
AU  - Fitz, Stephen
AU  - Hendrycks, Dan
AB  - As artificial intelligence systems grow more powerful, there has been increasing interest in “AI safety” research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling “safetywashing”—where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.
DA  - 2024/07/31/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Safetywashing
UR  - http://arxiv.org/abs/2407.21792
Y2  - 2024/11/14/12:19:33
L1  - https://arxiv.org/pdf/2407.21792
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Tamper-Resistant Safeguards for Open-Weight LLMs
AU  - Tamirisa, Rishub
AU  - Bharathi, Bhrugu
AU  - Phan, Long
AU  - Zhou, Andy
AU  - Gatti, Alice
AU  - Suresh, Tarun
AU  - Lin, Maxwell
AU  - Wang, Justin
AU  - Wang, Rowan
AU  - Arel, Ron
AU  - Zou, Andy
AU  - Song, Dawn
AU  - Li, Bo
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AB  - Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after thousands of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that tamper-resistance is a tractable problem, opening up a promising new avenue to improve the safety and security of open-weight LLMs.
DA  - 2024/09/14/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2408.00761
Y2  - 2024/11/14/12:19:48
L1  - https://arxiv.org/pdf/2408.00761
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Chain of Thought Imitation with Procedure Cloning
AU  - Yang, Mengjiao
AU  - Schuurmans, Dale
AU  - Abbeel, Pieter
AU  - Nachum, Ofir
AB  - Imitation learning aims to extract high-performance policies from logged demonstrations of expert behavior. It is common to frame imitation learning as a supervised learning problem in which one fits a function approximator to the input-output mapping exhibited by the logged demonstrations (input observations to output actions). While the framing of imitation learning as a supervised input-output learning problem allows for applicability in a wide variety of settings, it is also an overly simplistic view of the problem in situations where the expert demonstrations provide much richer insight into expert behavior. For example, applications such as path navigation, robot manipulation, and strategy games acquire expert demonstrations via planning, search, or some other multi-step algorithm, revealing not just the output action to be imitated but also the procedure for how to determine this action. While these intermediate computations may use tools not available to the agent during inference (e.g., environment simulators), they are nevertheless informative as a way to explain an expert's mapping of state to actions. To properly leverage expert procedure information without relying on the privileged tools the expert may have used to perform the procedure, we propose procedure cloning, which applies supervised sequence prediction to imitate the series of expert computations. This way, procedure cloning learns not only what to do (i.e., the output action), but how and why to do it (i.e., the procedure). Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, we show that imitating the intermediate computations of an expert's behavior enables procedure cloning to learn policies exhibiting significant generalization to unseen environment configurations, including those configurations for which running the expert's procedure directly is infeasible.
DA  - 2022/05/22/
PY  - 2022
DO  - 10.48550/arXiv.2205.10816
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2205.10816
Y2  - 2024/11/14/12:23:55
L1  - http://arxiv.org/pdf/2205.10816v1
L2  - https://arxiv.org/abs/2205.10816
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery
AU  - Laskin, Michael
AU  - Liu, Hao
AU  - Peng, Xue Bin
AU  - Yarats, Denis
AU  - Rajeswaran, Aravind
AU  - Abbeel, Pieter
AB  - We introduce Contrastive Intrinsic Control (CIC), an algorithm for unsupervised skill discovery that maximizes the mutual information between state-transitions and latent skill vectors. CIC utilizes contrastive learning between state-transitions and skills to learn behavior embeddings and maximizes the entropy of these embeddings as an intrinsic reward to encourage behavioral diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. CIC substantially improves over prior methods in terms of adaptation efficiency, outperforming prior unsupervised skill discovery methods by 1.79x and the next leading overall exploration algorithm by 1.18x.
DA  - 2022/03/30/
PY  - 2022
DO  - 10.48550/arXiv.2202.00161
DP  - arXiv.org
PB  - arXiv
ST  - CIC
UR  - http://arxiv.org/abs/2202.00161
Y2  - 2024/11/14/12:24:08
L1  - http://arxiv.org/pdf/2202.00161v3
L2  - https://arxiv.org/abs/2202.00161
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning
AU  - Mandi, Zhao
AU  - Abbeel, Pieter
AU  - James, Stephen
AB  - Intelligent agents should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly and efficiently. Meta-learning approaches have emerged as a popular solution to achieve this. However, meta-reinforcement learning (meta-RL) algorithms have thus far been restricted to simple environments with narrow task distributions. Moreover, the paradigm of pretraining followed by fine-tuning to adapt to new tasks has emerged as a simple yet effective solution in supervised and self-supervised learning. This calls into question the benefits of meta-learning approaches also in reinforcement learning, which typically come at the cost of high complexity. We hence investigate meta-RL approaches in a variety of vision-based benchmarks, including Procgen, RLBench, and Atari, where evaluations are made on completely novel tasks. Our findings show that when meta-learning approaches are evaluated on different tasks (rather than different variations of the same task), multi-task pretraining with fine-tuning on new tasks performs equally as well, or better, than meta-pretraining with meta test-time adaptation. This is encouraging for future research, as multi-task pretraining tends to be simpler and computationally cheaper than meta-RL. From these findings, we advocate for evaluating future meta-RL methods on more challenging tasks and including multi-task pretraining with fine-tuning as a simple, yet strong baseline.
DA  - 2023/02/16/
PY  - 2023
DO  - 10.48550/arXiv.2206.03271
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.03271
Y2  - 2024/11/14/12:24:31
L1  - http://arxiv.org/pdf/2206.03271v2
L2  - https://arxiv.org/abs/2206.03271
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity
AU  - Liang, Kaiqu
AU  - Zhang, Zixu
AU  - Fisac, Jaime Fernández
AB  - Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty–aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries. Code is available at https://github.com/kevinliang888/IntroPlan.
DA  - 2024/06/04/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Introspective Planning
UR  - http://arxiv.org/abs/2402.06529
Y2  - 2024/11/14/12:36:13
L1  - https://arxiv.org/pdf/2402.06529
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Active Uncertainty Reduction for Safe and Efficient Interaction Planning: A Shielding-Aware Dual Control Approach
AU  - Hu, Haimin
AU  - Isele, David
AU  - Bae, Sangjae
AU  - Fisac, Jaime F.
AB  - The ability to accurately predict others' behavior is central to the safety and efficiency of interactive robotics. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as other agents' goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for a broad class of predictive models with both continuous and categorical uncertainty. To ensure the safe operation of the interacting agents, we use a runtime safety filter (also referred to as a "shielding" scheme), which overrides the robot's dual control policy with a safety fallback strategy when a safety-critical event is imminent. We then augment the dual control framework with an improved variant of the recently proposed shielding-aware robust planning scheme, which proactively balances the nominal planning performance with the risk of high-cost emergency maneuvers triggered by low-probability agent behaviors. We demonstrate the efficacy of our approach with both simulated driving studies and hardware experiments using 1/10 scale autonomous vehicles.
DA  - 2023/11/01/
PY  - 2023
DO  - 10.48550/arXiv.2302.00171
DP  - arXiv.org
PB  - arXiv
ST  - Active Uncertainty Reduction for Safe and Efficient Interaction Planning
UR  - http://arxiv.org/abs/2302.00171
Y2  - 2024/11/14/12:36:18
L1  - http://arxiv.org/pdf/2302.00171v2
L2  - https://arxiv.org/abs/2302.00171
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Systems and Control
KW  - Electrical Engineering and Systems Science - Systems and Control
KW  - Mathematics - Optimization and Control
ER  - 

TY  - GEN
TI  - Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy
AU  - Hu, Haimin
AU  - Zhang, Zixu
AU  - Nakamura, Kensuke
AU  - Bajcsy, Andrea
AU  - Fisac, Jaime F.
AB  - An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot's evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot's learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework's ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.
DA  - 2023/11/01/
PY  - 2023
DO  - 10.48550/arXiv.2309.01267
DP  - arXiv.org
PB  - arXiv
ST  - Deception Game
UR  - http://arxiv.org/abs/2309.01267
Y2  - 2024/11/14/12:36:26
L1  - http://arxiv.org/pdf/2309.01267v2
L2  - https://arxiv.org/abs/2309.01267
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
KW  - Computer Science - Systems and Control
KW  - Electrical Engineering and Systems Science - Systems and Control
ER  - 

TY  - CONF
TI  - Interpretable Trajectory Prediction for Autonomous Vehicles via Counterfactual Responsibility
AU  - Hsu, Kai-Chieh
AU  - Leung, Karen
AU  - Chen, Yuxiao
AU  - Fisac, Jaime F.
AU  - Pavone, Marco
T2  - 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
AB  - The ability to anticipate surrounding agents' behaviors is critical to enable safe and seamless autonomous vehicles (AVs). While phenomenological methods have successfully predicted future trajectories from scene context, these predictions lack interpretability. On the other hand, ontological approaches assume an underlying structure able to describe the interaction dynamics or agents' internal decision processes. Still, they often suffer from poor scalability or cannot reflect diverse human behaviors. This work proposes an interpretability framework for a phenomenological method through responsibility evaluations. We formulate responsibility as a measure of how much an agent takes into account the welfare of other agents through counterfactual reasoning. Additionally, this framework abstracts the computed responsibility sequences into different responsibility levels and grounds these latent levels into reward functions. The proposed responsibility-based interpretability framework is modular and easily integrated into a wide range of prediction models. To demonstrate the utility of the proposed framework in providing added interpretability, we adapt an existing AV prediction model and perform a simulation study on a real-world nuScenes traffic dataset. Experimental results show that we can perform offline ex-post traffic analysis by incorporating the responsibility signal and rendering interpretable but accurate online trajectory predictions.
C3  - 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/IROS55552.2023.10341712
DP  - IEEE Xplore
SP  - 5918
EP  - 5925
UR  - https://ieeexplore.ieee.org/document/10341712
Y2  - 2024/11/14/12:36:51
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10341712&ref=
KW  - Adaptation models
KW  - Cognition
KW  - Computational modeling
KW  - Predictive models
KW  - Rendering (computer graphics)
KW  - Scalability
KW  - Trajectory
ER  - 

TY  - CONF
TI  - ISAACS: Iterative Soft Adversarial Actor-Critic for Safety
AU  - Hsu, Kai-Chieh
AU  - Nguyen, Duy Phuong
AU  - Fisac, Jaime Fernàndez
T2  - Learning for Dynamics and Control Conference
AB  - The deployment of robots in uncontrolled environments requires them to operate 
robustly under previously unseen scenarios, like irregular terrain and wind conditions. 
Unfortunately, while rigorous safety frameworks from robust optimal control theory 
scale poorly to high-dimensional nonlinear dynamics, control policies computed by 
more tractable “deep” methods lack guarantees and tend to exhibit little robustness 
to uncertain operating conditions. This work introduces a novel approach enabling scalable 
synthesis of robust safety-preserving controllers for robotic systems with general 
nonlinear dynamics subject to bounded modeling error, by combining game-theoretic safety 
analysis with adversarial reinforcement learning in simulation. Following a soft actor-
critic scheme, a safety-seeking fallback policy is co-trained with an adversarial 
“disturbance” agent that aims to invoke the worst-case realization of model error and 
training-to-deployment discrepancy allowed by the designer’s uncertainty. While the 
learned control policy does not intrinsically guarantee safety, it is used to construct a
real-time safety filter with robust safety guarantees based on forward reachability 
rollouts. This safety filter can be used in conjunction with a safety-agnostic control
policy, precluding any task-driven actions that could result in loss of safety. We 
evaluate our learning-based safety approach in a 5D race car simulator, compare the 
learned safety policy to the numerically obtained optimal solution, and empiricall 
validate the robust safety guarantee of our proposed safety filter against worst-case 
model discrepancy.
C3  - Proceedings of The 5th Annual Learning for Dynamics and Control Conference
DA  - 2023/06/06/
PY  - 2023
DP  - proceedings.mlr.press
SP  - 90
EP  - 103
LA  - en
PB  - PMLR
ST  - ISAACS
UR  - https://proceedings.mlr.press/v211/hsu23a.html
Y2  - 2024/11/14/12:37:34
L1  - https://proceedings.mlr.press/v211/hsu23a/hsu23a.pdf
ER  - 

TY  - JOUR
TI  - Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees
AU  - Hsu, Kai-Chieh
AU  - Ren, Allen Z.
AU  - Nguyen, Duy P.
AU  - Majumdar, Anirudha
AU  - Fisac, Jaime F.
T2  - Artificial Intelligence
AB  - Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.
DA  - 2023/01/01/
PY  - 2023
DO  - 10.1016/j.artint.2022.103811
DP  - ScienceDirect
VL  - 314
SP  - 103811
J2  - Artificial Intelligence
SN  - 0004-3702
ST  - Sim-to-Lab-to-Real
UR  - https://www.sciencedirect.com/science/article/pii/S0004370222001515
Y2  - 2024/11/14/12:37:38
L1  - https://www.sciencedirect.com/science/article/pii/S0004370222001515/pdfft?download=true
KW  - Generalization
KW  - Reinforcement learning
KW  - Safety analysis
KW  - Sim-to-Real transfer
ER  - 

TY  - CHAP
TI  - Active Uncertainty Reduction for Human-Robot Interaction: An Implicit Dual Control Approach
AU  - Hu, Haimin
AU  - Fisac, Jaime F.
T2  - Algorithmic Foundations of Robotics XV
A2  - LaValle, Steven M.
A2  - O’Kane, Jason M.
A2  - Otte, Michael
A2  - Sadigh, Dorsa
A2  - Tokekar, Pratap
AB  - The ability to accurately predict human behavior is central to the safety and efficiency of robot autonomy in interactive settings. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as people’s goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning, mainly due to the fundamental coupling between robot trajectory optimization and human intent inference. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for a broad class of predictive human models with both continuous and categorical uncertainty. The efficacy of our approach is demonstrated with simulated driving examples.
CY  - Cham
DA  - 2023///
PY  - 2023
DP  - DOI.org (Crossref)
VL  - 25
SP  - 385
EP  - 401
LA  - en
PB  - Springer International Publishing
SN  - 978-3-031-21089-1 978-3-031-21090-7
ST  - Active Uncertainty Reduction for Human-Robot Interaction
UR  - https://link.springer.com/10.1007/978-3-031-21090-7_23
Y2  - 2024/11/14/12:37:53
L1  - https://wafr2022.github.io/proceedings/WAFR_2022_Final_63.pdf
ER  - 

TY  - JOUR
TI  - SHARP: Shielding-Aware Robust Planning for Safe and Efficient Human-Robot Interaction
AU  - Hu, Haimin
AU  - Nakamura, Kensuke
AU  - Fisac, Jaime F.
T2  - IEEE Robotics and Automation Letters
AB  - Jointly achieving safety and efficiency in human-robot interaction settings is a challenging problem, as the robot’s planning objectives may be at odds with the human’s own intent and expectations. Recent approaches ensure safe robot operation in uncertain environments through a supervisory control scheme, sometimes called “shielding,” which overrides the robot’s nominal plan with a safety fallback strategy when a safety-critical event is imminent. These reactive “last-resort” strategies (typically in the form of aggressive emergency maneuvers) focus on preserving safety without efficiency considerations; when the nominal planner is unaware of possible safety overrides, shielding can be activated more frequently than necessary, leading to degraded performance. In this letter, we propose a new shielding-based planning approach that allows the robot to plan efficiently by explicitly accounting for possible future shielding events. Leveraging recent work on Bayesian human motion prediction, the resulting robot policy proactively balances nominal performance with the risk of high-cost emergency maneuvers triggered by low-probability human behaviors. We formalize Shielding-Aware Robust Planning (SHARP) as a stochastic optimal control problem and propose a computationally efficient framework for finding tractable approximate solutions at runtime. Our method outperforms the shielding-agnostic motion planning baseline (equipped with the same human intent inference scheme) on simulated driving examples with human trajectories taken from the recently released Waymo Open Motion Dataset.
DA  - 2022/04//
PY  - 2022
DO  - 10.1109/LRA.2022.3155229
DP  - IEEE Xplore
VL  - 7
IS  - 2
SP  - 5591
EP  - 5598
SN  - 2377-3766
ST  - SHARP
UR  - https://ieeexplore.ieee.org/abstract/document/9723544
Y2  - 2024/11/14/12:38:01
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9723544&ref=
KW  - Human-aware motion planning
KW  - Human-robot interaction
KW  - Planning
KW  - planning under uncertainty
KW  - Robot kinematics
KW  - Robots
KW  - Safety
KW  - safety in HRI
KW  - Stochastic processes
KW  - Trajectory
ER  - 

TY  - CONF
TI  - Safety and Liveness Guarantees through Reach-Avoid Reinforcement Learning
AU  - Hsu, Kai-Chieh
AU  - Rubies-Royo, Vicenç
AU  - Tomlin, Claire
AU  - Fisac, Jaime F.
T2  - Robotics: Science and Systems XVII
DA  - 2021/07/12/
PY  - 2021
DP  - www.roboticsproceedings.org
VL  - 17
SN  - 978-0-9923747-7-8
UR  - https://www.roboticsproceedings.org/rss17/p077.html
Y2  - 2024/11/14/12:38:11
L1  - http://www.roboticsproceedings.org/rss17/p077.pdf
ER  - 

TY  - ELEC
TI  - Collaborative game specification: arriving at common models in bargaining
AU  - Clifton, Jesse
AB  - Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient. 

 In another post, I described the "prior selection problem", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on solution concepts or surrogate goals / safe Pareto improvements seem to require agents to have a common, explicit game-theoretic model.

In this post, I introduce collaborative game specification (CGS), a family of techniques designed to address the problem of agents lacking a shared model. In CGS, agents agree on a common model of their bargaining situation and use this to come to an agreement. Here is the basic idea:

    Two agents are playing an unknown game. They each have private models of this game. (These may be explicit models, as in model-based reinforcement learning, or models implicit in a black-box policy which can be extracted.) By default, they will use these models to make a decision. The problem is that their models may differ, possibly leading to bad outcomes and precluding the use of bargaining protocols which require a shared, explicit model.
     Rather than using these default strategies, agents agree on a common model, talk, and use this model to reach an agreement.

Of course, when agreeing on a common model, agents must handle incentives for their counterparts to deceive each other. In the toy illustration below, we’ll see how handling incentives to misrepresent one’s model can be handled in a pure cheap-talk setting. 

How might we use CGS to reduce the risks of conflict involving powerful AI systems? One use is to provide demonstrations of good bargaining behavior. Some approaches to AI development may involve training AI systems to imitate the behavior of some demonstrator (e.g., imitative amplification), and so we may need to be able to provide many demonstrations of good bargaining behavior to ensure that the resulting system is robustly able and motivated to bargain successfully. Another is to facilitate bargaining between humans with powerful AI tools, e.g. in a comprehensive AI services scenario. 

Aside from actually implementing CGS in AI systems, studying protocols of this kind can give us a better understanding of the limits on agents’ ability to overcome differences in their private models. Under the simple version of CGS discussed here, because agents have to incentivize truth-telling by refusing to engage in CGS sometimes, agents will fail to agree on a common model with positive probability in equilibrium.  

I will first give a toy example of CGS (Section 1), and then discuss how it might be implemented in practice (Section 2). I close by discussing some potential problems and open questions for CGS (Section 3). In the Appendix, I discuss a game-theoretic formalism in which CGS can be given a more rigorous basis.
DA  - 2021/03/06/2021
PY  - 2021
UR  - https://longtermrisk.org/collaborative-game-specification/, HTML
ER  - 

TY  - ELEC
TI  - The optimal timing of spending on AGI safety work; why we should probably be spending more now
AU  - Cook, Tristan
AU  - Corlouer, Guillaume
AB  - When should funders wanting to increase the probability of AGI going well spend their money? We have created a tool to calculate the optimum spending schedule and tentatively conclude funders collectively should be spending at least 5% of their capital each year on AI risk interventions and in some cases up to 35%.

This is likely higher than the current AI risk community spending rate which is at most 3%48. In most cases, we find that the optimal spending schedule is between 5% and 15% better than the ‘default’ strategy of just spending the interest one accrues and from 15% to 50% better than a naive projection of the community’s spending rate49.

We strongly encourage users to put their own inputs into the tool to draw their own conclusions.

The key finding of a higher spending rate is supported by two distinct models we have created, one that splits spending of capital into research and influence, and a second model  (the ‘alternate model’) that supposes we can spend our stock of things that grow on direct work. We focus on the former with the latter described in the appendix since its output is more obviously action-guiding50.

The table below shows our best guess for the optimal spending schedule using the former model when varying the difficulty of achieving a good AGI outcome and AGI timelines. We keep other inputs, such as diminishing returns to spending and interest rate constant51.

Some of the critical limitations of our model include: poorly modelling exogenous research, which is particularly important for those with longer timelines, and many parts of the model - such as diminishing returns - remaining constant over time.

Further, we find that robust spending strategies - those that work in a wide variety of worlds - also support a higher spending rate. We show the results of a Monte Carlo simulation in the appendix60.
DA  - 2022/11/29/2022
PY  - 2022
UR  - https://longtermrisk.org/the-optimal-timing-of-spending-on-agi-safety-work-why-we-should-probably-be-spending-more-now/, HTML https://forum.effectivealtruism.org/posts/Ne8ZS6iJJp7EpzztP/the-optimal-timing-of-spending-on-agi-safety-work-why-we, EA Forum
ER  - 

TY  - JOUR
TI  - AI, Governance Displacement, and the (De)Fragmentation of International Law
AU  - Maas, Matthijs M.
T2  - SSRN Electronic Journal
AB  - The emergence, proliferation, and use of new general-purpose technologies can often produce significant political, redistributive, normative and legal effects on the world. Artificial intelligence (AI) has been identified as one such transformative technology. Many of its impacts may require global governance responses. However, what are the direct and indirect effects of AI technologies on the viability, form, or functioning of the international legal order itself? What, if any, are the prospects, peril or promise of AI-driven legal automation at the international level? This paper draws on an ‘AI Governance Disruption’ framework to understanding AI’s impacts on the global governance architecture. Focusing particularly on the potential for legal automation at the international law level, it explores three potential pathways of such ‘legal displacement’: (1) the automation of rule creation and arbitration; (2) the automation of monitoring & enforcement; or (3) the ‘replacement’ of international law with new architectural modes of (international) behaviour control. It then focuses on the effects of these trends on the architecture of international law. It distinguishes 10 different roles that AI applications could play, with distinct effects on the international legal order. That is, AI systems can serve as (1) legal ‘canary in the coal mine’, highlighting the need for greater cross-regime harmonization. However, it can also serve as (2) tough knot or (3) generator of regime fault lines. Under even modest scenarios of legal automation, AI systems may serve variably as a (4) shield, (5) patch, (6) cure, or (7) accelerator of international legal fragmentation. Finally, AI tools may serve as (8) differential enabler; (9) driver of value shifts, or (10) asymmetric weapon, potentially contributing to trends of contestation or erosion in the international legal order. The paper concludes with a brief review of the ways in which international lawyers or regime scholars might approach the risks and opportunities of increasing automation in international law, in order to leverage these trends and tools towards improved efficacy, resilience, and legitimacy of global governance.
DA  - 2021///
PY  - 2021
DO  - 10.2139/ssrn.3806624
DP  - DOI.org (Crossref)
J2  - SSRN Journal
LA  - en
SN  - 1556-5068
UR  - https://www.ssrn.com/abstract=3806624
Y2  - 2024/11/14/13:02:45
L1  - https://www.cser.ac.uk/media/uploads/files/21-03-20_-_ISA2021_-_paper_-_Maas_-_AI_Governance_Displacement_and_the_DeFragmentation_of_International_Law_-_for_CSER.pdf
ER  - 

TY  - GEN
TI  - Aligning AI Regulation to Sociotechnical Change
AU  - Maas, Matthijs M.
AB  - How do we regulate a changing technology, with changing uses, in a changing world? This chapter argues that while existing (inter)national AI governance approaches are important, they are often siloed. Technology-centric approaches focus on individual AI applications; law-centric approaches emphasize AI’s effects on pre-existing legal fields or doctrines. This chapter argues that to foster a more systematic, functional and effective AI regulatory ecosystem, policy actors should instead complement these approaches with a regulatory perspective that emphasizes how, when, and why AI applications enable patterns of ‘sociotechnical change’. Drawing on theories from the emerging field of ‘TechLaw’, it explores how this perspective can provide informed, more nuanced, and actionable perspectives on AI regulation. A focus on sociotechnical change can help analyze when and why AI applications actually do create a meaningful rationale for new regulation — and how they are consequently best approached as targets for regulatory intervention, considering not just the technology, but also six distinct ‘problem logics’ that appear around AI issues across domains. The chapter concludes by briefly reviewing concrete institutional and regulatory actions that can draw on this approach in order to improve the regulatory triage, tailoring, timing & responsiveness, and design of AI policy.
CY  - Rochester, NY
DA  - 2021/06/16/
PY  - 2021
DO  - 10.2139/ssrn.3871635
DP  - papers.ssrn.com
LA  - en
PB  - Social Science Research Network
UR  - https://papers.ssrn.com/abstract=3871635
Y2  - 2024/11/14/13:04:06
L1  - https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3888099_code2918001.pdf?abstractid=3871635&mirid=1
KW  - AI
KW  - Artificial Intelligence
KW  - Problem Logics
KW  - Regulation
KW  - Regulatory Rationale
KW  - Regulatory Target
KW  - Sociotechnical Change
KW  - Techlaw
ER  - 

TY  - GEN
TI  - Why and How Governments Should Monitor AI Development
AU  - Whittlestone, Jess
AU  - Clark, Jack
AB  - In this paper we outline a proposal for improving the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this would give governments greater information about the AI ecosystem, equipping them to more effectively direct AI development and deployment in the most societally and economically beneficial directions. It would also create infrastructure that could rapidly identify potential threats or harms that could occur as a consequence of changes in the AI ecosystem, such as the emergence of strategically transformative capabilities, or the deployment of harmful systems. We begin by outlining the problem which motivates this proposal: in brief, traditional governance approaches struggle to keep pace with the speed of progress in AI. We then present our proposal for addressing this problem: governments must invest in measurement and monitoring infrastructure. We discuss this proposal in detail, outlining what specific things governments could focus on measuring and monitoring, and the kinds of benefits this would generate for policymaking. Finally, we outline some potential pilot projects and some considerations for implementing this in practice.
DA  - 2021/08/31/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2108.12427
Y2  - 2024/11/14/13:04:23
L1  - https://arxiv.org/pdf/2108.12427
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy
AU  - Stix, Charlotte
AU  - Maas, Matthijs M.
T2  - AI and Ethics
AB  - Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.
DA  - 2021/08/01/
PY  - 2021
DO  - 10.1007/s43681-020-00037-w
DP  - Springer Link
VL  - 1
IS  - 3
SP  - 261
EP  - 271
J2  - AI Ethics
LA  - en
SN  - 2730-5961
ST  - Bridging the gap
UR  - https://doi.org/10.1007/s43681-020-00037-w
Y2  - 2024/11/14/13:05:19
L1  - https://link.springer.com/content/pdf/10.1007%2Fs43681-020-00037-w.pdf
KW  - AI
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - Artificial intelligence ethics
KW  - Artificial intelligence policy
KW  - Cooperation models
KW  - Incompletely theorized agreement
KW  - Long term
KW  - Overlapping consensus
KW  - Short term
ER  - 

TY  - JOUR
TI  - Fragmentation and the Future: Investigating Architectures for International AI Governance
AU  - Cihon, Peter
AU  - Maas, Matthijs M.
AU  - Kemp, Luke
T2  - Global Policy
AB  - The international governance of artificial intelligence (AI) is at a crossroads: should it remain fragmented or be centralised? We draw on the history of environment, trade, and security regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak for centralisation. The risk of creating a slow and brittle institution, and the difficulty of pairing deep rules with adequate participation, speak against it. Other considerations depend on the specific design. A centralised body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial, and fragmented institutions could self-organise. In sum, these trade-offs should inform development of the AI governance architecture, which is only now emerging. We apply the trade-offs to the case of the potential development of high-level machine intelligence. We conclude with two recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, fragmentation will likely persist for now. The developing landscape should be monitored to see if it is self-organising or simply inadequate.
DA  - 2020///
PY  - 2020
DO  - 10.1111/1758-5899.12890
DP  - Wiley Online Library
VL  - 11
IS  - 5
SP  - 545
EP  - 556
LA  - en
SN  - 1758-5899
ST  - Fragmentation and the Future
UR  - https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12890
Y2  - 2024/11/14/13:05:28
L1  - https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1758-5899.12890
ER  - 

TY  - CHAP
TI  - It Takes a Village: The Shared Responsibility of “Raising” an Autonomous Weapon
AU  - Jayanthi, Amritha
AU  - Avin, Shahar
T2  - An Anthology of Global Risk
A2  - Beard, Sj
A2  - Hobson, Tom
AB  - Expectations around future capabilities of lethal autonomous weapons systems (LAWS) have raised concerns for military risks, ethics, and accountability. The U.K.’s position, as presented among various international voices at the UN’s Convention on Certain Conventional Weapons (CCW) meetings, has attempted to address these concerns through a focused look at the weapons review process, humanmachine teaming or “meaningful human control” (see e.g. JCN1/18), and the ability of autonomous systems to adhere to the Rules of Engagement. Further, the U.K. has stated that the existing governance structures—both domestic and international—around weapons systems are sufficient in dealing with any concerns around the development, deployment, and accountability for emerging LAWS; there is no need for novel agreements on the control of these weapons systems. In an effort to better understand and test the U.K. position on LAWS, the Centre for the Study of Existential Risk has run a research project in which we interviewed experts in multiple relevant organisations, structured around a mock parliamentary inquiry of a hypothetical LAWS-related civilian death. The responses to this scenario have highlighted different, sometimes complementary and sometimes contradicting, conceptions of future systems, challenges, and accountability measures. They have provided rich "on the ground” perspectives, while also highlighting key gaps that should be addressed by every military that is considering acquisition and deployment of autonomous and semi-autonomous weapon systems.
CY  - Cambridge, UK
DA  - 2024/09/03/
PY  - 2024
DP  - DOI.org (Crossref)
ET  - 1
SP  - 603
EP  - 612
LA  - en
PB  - Open Book Publishers
SN  - 978-1-80511-114-6 978-1-80511-115-3 978-1-80511-116-0 978-1-80511-120-7 978-1-80511-117-7
ST  - 21. It Takes a Village
UR  - https://www.openbookpublishers.com/books/10.11647/obp.0360/chapters/10.11647/obp.0360.21
Y2  - 2024/11/14/13:05:55
L1  - https://www.cser.ac.uk/media/uploads/files/It_Takes_a_Village__The_Shared_Responsibility_of_Raising_an_Autonomous_Weapon.pdf
ER  - 

TY  - JOUR
TI  - AI & Antitrust: Reconciling Tensions Between Competition Law and Cooperative AI Development
AU  - Hua, Shin-Shin
AU  - Belfield, Haydn
AB  - Cooperation between companies developing artificial intelligence (AI) can help them create AI systems that are safe, secure, and with broadly shared benefits. Researchers have proposed a range of cooperation strategies, ranging from redistributing “windfall” profits to assistance to address the harmful dynamics of a competitive race for technological superiority.

A critical tension arises, however, between cooperation and the goal of competition law, which is to protect the very process of competition between rival companies. Whilst these potential conflicts are significant, they are currently underexplored in the literature. This paper examines the relationship between proposed forms of AI cooperation and competition law, focusing on the competition law of the European Union (EU).

EU competition law governs the behavior of the world’s largest AI companies, though many are based abroad, especially in the US. Its jurisdiction can extend to any foreign company that is active in the EU. Scrutiny of US “Big Tech” is also an area of strategic focus for the European Commission (EC).

This paper seeks to reconcile the cooperative AI development and competition law. It examines fourteen forms of AI cooperation, both those that are applicable today and longer-term strategies that will apply when AI development is more advanced. Where we identify potential tensions with EU competition law, we suggest mitigation steps. Our aim is to ensure the long-term sustainability of these important safeguards to the responsible and beneficial development of AI.
DA  - 2021/11//
PY  - 2021
DP  - Zotero
LA  - en
L1  - https://yjolt.org/sites/default/files/23_yale_j.l._tech._415_ai_antitrust_nov_0.pdf
ER  - 

TY  - JOUR
TI  - The transformative potential of artificial intelligence
AU  - Gruetzemacher, Ross
AU  - Whittlestone, Jess
T2  - Futures
AB  - The terms ‘human-level artificial intelligence’ and ‘artificial general intelligence’ are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term ‘transformative AI’ is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be ‘transformative’ is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.futures.2021.102884
DP  - ScienceDirect
VL  - 135
SP  - 102884
J2  - Futures
SN  - 0016-3287
UR  - https://www.sciencedirect.com/science/article/pii/S0016328721001932
Y2  - 2024/11/14/13:10:31
L1  - https://www.sciencedirect.com/science/article/pii/S0016328721001932/pdfft?download=true
L2  - https://www.sciencedirect.com/science/article/pii/S0016328721001932?via=ihub
KW  - Artificial general intelligence
KW  - Artificial Intelligence
KW  - Human-level AI
KW  - Transformative AI
ER  - 

TY  - JOUR
TI  - The Societal Implications of Deep Reinforcement Learning
AU  - Whittlestone, Jess
AU  - Arulkumaran, Kai
AU  - Crosby, Matthew
AB  - Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications.
DA  - 2021/03/08/
PY  - 2021
DO  - 10.1613/jair.1.12360
DP  - www.repository.cam.ac.uk
LA  - eng
SN  - 1076-9757
UR  - https://www.repository.cam.ac.uk/handle/1810/318700
Y2  - 2024/11/14/13:12:25
L1  - https://www.repository.cam.ac.uk/bitstreams/dda323cb-4f6f-4473-906a-b460b0c08132/download
ER  - 

TY  - JOUR
TI  - Compute and Antitrust
AU  - Belfield, Haydn
AU  - Hua, Shin-Shin
T2  - Verfassungsblog
AB  - Compute or computing power refers to a software and hardware stack, such as in a data centre or computer, engineered for AI-specific applications. We argue that the antitrust and regulatory literature to date has failed to pay sufficient attention to compute, despite compute being a key input to AI progress and services, the potentially substantial market power of companies in the supply chain, and the advantages of compute as a ‘unit’ of regulation in terms of detection and remedies.
DA  - 2022/08/19/
PY  - 2022
DO  - 10.17176/20220819-181907-0
DP  - verfassungsblog.de
LA  - eng
SN  - 2366-7044
UR  - https://verfassungsblog.de/compute-and-antitrust/
Y2  - 2024/11/14/13:13:11
L1  - https://intr2dok.vifa-recht.de/servlets/MCRFileNodeServlet/mir_derivate_00013335/Compute_and_Antitrust.pdf
ER  - 

TY  - JOUR
TI  - Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI
AU  - Zoe Cremer, Carla
AU  - Whittlestone, Jess
T2  - International Journal of Interactive Multimedia and Artificial Intelligence
AB  - We propose a method for identifying early warning signs of transformative progress in artificial intelligence (AI), and discuss how these can support the anticipatory and democratic governance of AI. We call these early warning signs ‘canaries’, based on the use of canaries to provide early warnings of unsafe air pollution in coal mines. Our method combines expert elicitation and collaborative causal graphs to identify key milestones and identify the relationships between them. We present two illustrations of how this method could be used: to identify early warnings of harmful impacts of language models; and of progress towards high-level machine intelligence. Identifying early warning signs of transformative applications can support more efficient monitoring and timely regulation of progress in AI: as AI advances, its impacts on society may be too great to be governed retrospectively. It is essential that those impacted by AI have a say in how it is governed. Early warnings can give the public time and focus to influence emerging technologies using democratic, participatory technology assessments. We discuss the challenges in identifying early warning signals and propose directions for future work.
DA  - 2021///
PY  - 2021
DO  - 10.9781/ijimai.2021.02.011
DP  - DOI.org (Crossref)
VL  - 6
IS  - 5
SP  - 100
J2  - IJIMAI
LA  - en
SN  - 1989-1660
ST  - Artificial Canaries
UR  - https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf
Y2  - 2024/11/14/13:13:25
L1  - https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf
ER  - 

TY  - CONF
TI  - The Scientometrics of AI Benchmarks: Unveiling the Underlying Mechanics of AI Research
AU  - Barredo, Pablo
AU  - Hernández-Orallo, José
AU  - Martínez-Plumed, Fernando
T2  - ECAI 2020
AB  - The widespread use of experimental benchmarks in AI research has created new competition and collaboration dynamics that are still poorly understood. In this paper we provide an innovative methodology to explore this dynamics and analyse the way different entrants in these competitions, from academia to tech giants, behave and react depending on their own or others’ achievements. We perform an analysis of over twenty popular benchmarks in AI, linking their underlying research papers. We identify links between researchers and institutions (i.e., communities) beyond the standard co-authorship relations, and we explore a series of hypotheses about their behaviour as well as some aggregated results in terms of activity, breakthroughs and efﬁciency. As a result, we detect and characterise the dynamics of research communities at different levels of abstraction, including organisation, afﬁliation, trajectories, results and activity.
C3  - 1st International Workshop on Evaluating Progress in Artificial Intelligence
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - http://dmip.webs.upv.es/EPAI2020/papers/EPAI_2020_paper_12.pdf
ER  - 

TY  - JOUR
TI  - Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities
AU  - Tzachor, Asaf
AU  - Devare, Medha
AU  - King, Brian
AU  - Avin, Shahar
AU  - Ó hÉigeartaigh, Seán
T2  - Nature Machine Intelligence
AB  - Global agriculture is poised to benefit from the rapid advance and diffusion of artificial intelligence (AI) technologies. AI in agriculture could improve crop management and agricultural productivity through plant phenotyping, rapid diagnosis of plant disease, efficient application of agrochemicals and assistance for growers with location-relevant agronomic advice. However, the ramifications of machine learning (ML) models, expert systems and autonomous machines for farms, farmers and food security are poorly understood and under-appreciated. Here, we consider systemic risk factors of AI in agriculture. Namely, we review risks relating to interoperability, reliability and relevance of agricultural data, unintended socio-ecological consequences resulting from ML models optimized for yields, and safety and security concerns associated with deployment of ML platforms at scale. As a response, we suggest risk-mitigation measures, including inviting rural anthropologists and applied ecologists into the technology design process, applying frameworks for responsible and human-centred innovation, setting data cooperatives for improved data transparency and ownership rights, and initial deployment of agricultural AI in digital sandboxes.
DA  - 2022/02//
PY  - 2022
DO  - 10.1038/s42256-022-00440-4
DP  - www.nature.com
VL  - 4
IS  - 2
SP  - 104
EP  - 109
J2  - Nat Mach Intell
LA  - en
SN  - 2522-5839
UR  - https://www.nature.com/articles/s42256-022-00440-4
Y2  - 2024/11/14/13:14:00
L1  - https://www.nature.com/articles/s42256-022-00440-4.pdf
KW  - Agriculture
KW  - Ethics
KW  - Science
KW  - technology and society
ER  - 

TY  - GEN
TI  - Military Artificial Intelligence as Contributor to Global Catastrophic Risk
AU  - Maas, Matthijs M.
AU  - Matteucci, Kayla
AU  - Cooke, Di
AB  - Recent years have seen growing attention for the use of AI technologies in warfare, which has been rapidly advancing. This chapter explores in what ways such military AI technologies might contribute to Global Catastrophic Risks (GCR). After reviewing the GCR field’s limited previous engagement with military AI, and giving an overview of recent advances in military AI, this chapter focuses on two risk scenarios that have been proposed. First, we discuss arguments around the use of swarms of Lethal Autonomous Weapons Systems, and suggest that while these systems are concerning, they appear not yet likely to be a GCR in the near-term, on the basis of current and anticipated production limits and costs which make these systems still uncompetitive with extant systems for mass destruction. Second, we delve into the intersection of military AI and nuclear weapons, which we argue has a significantly higher GCR potential. We review historical debates over when, where, and why nuclear weapons could lead to GCR, along with recent geopolitical developments that could raise these risks further. We then outline six ways in which the use of AI systems in-, around-, or against- nuclear weapons and their command infrastructures could increase the likelihood of nuclear escalation and global catastrophe. The chapter concludes with suggestions for a research agenda that can gain a more comprehensive and multidisciplinary understanding of the potential risks from military AI, both today and in the future.
CY  - Rochester, NY
DA  - 2022/05/22/
PY  - 2022
DO  - 10.2139/ssrn.4115010
DP  - papers.ssrn.com
LA  - en
PB  - Social Science Research Network
UR  - https://papers.ssrn.com/abstract=4115010
Y2  - 2024/11/14/13:14:49
L1  - https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID4136610_code2918001.pdf?abstractid=4115010&mirid=1
KW  - Artificial intelligence
KW  - autonomous weapons
KW  - global catastrophic risk
KW  - Military AI
KW  - nuclear war
KW  - nuclear weapons
ER  - 

TY  - JOUR
TI  - General intelligence disentangled via a generality metric for natural and artificial intelligence
AU  - Hernández-Orallo, José
AU  - Loe, Bao Sheng
AU  - Cheke, Lucy
AU  - Martínez-Plumed, Fernando
AU  - Ó hÉigeartaigh, Seán
T2  - Scientific Reports
AB  - Success in all sorts of situations is the most classical interpretation of general intelligence. Under limited resources, however, the capability of an agent must necessarily be limited too, and generality needs to be understood as comprehensive performance up to a level of difficulty. The degree of generality then refers to the way an agent’s capability is distributed as a function of task difficulty. This dissects the notion of general intelligence into two non-populational measures, generality and capability, which we apply to individuals and groups of humans, other animals and AI systems, on several cognitive and perceptual tests. Our results indicate that generality and capability can decouple at the individual level: very specialised agents can show high capability and vice versa. The metrics also decouple at the population level, and we rarely see diminishing returns in generality for those groups of high capability. We relate the individual measure of generality to traditional notions of general intelligence and cognitive efficiency in humans, collectives, non-human animals and machines. The choice of the difficulty function now plays a prominent role in this new conception of generality, which brings a quantitative tool for shedding light on long-standing questions about the evolution of general intelligence and the evaluation of progress in Artificial General Intelligence.
DA  - 2021/11/24/
PY  - 2021
DO  - 10.1038/s41598-021-01997-7
DP  - www.nature.com
VL  - 11
IS  - 1
SP  - 22822
J2  - Sci Rep
LA  - en
SN  - 2045-2322
UR  - https://www.nature.com/articles/s41598-021-01997-7
Y2  - 2024/11/14/13:15:22
L1  - https://www.nature.com/articles/s41598-021-01997-7.pdf
KW  - Behavioural methods
KW  - Computer science
KW  - Psychology
ER  - 

TY  - GEN
TI  - International Governance of Civilian AI: A Jurisdictional Certification Approach
AU  - Trager, Robert
AU  - Harack, Ben
AU  - Reuel, Anka
AU  - Carnegie, Allison
AU  - Heim, Lennart
AU  - Ho, Lewis
AU  - Kreps, Sarah
AU  - Lall, Ranjit
AU  - Larter, Owen
AU  - hÉigeartaigh, Seán Ó
AU  - Staffell, Simon
AU  - Villalobos, José Jaime
AB  - This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified jurisdictions. Indeed, both the import and export standards could be required for certification. As international actors reach consensus on risks of and minimum standards for advanced AI, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.
DA  - 2023/09/11/
PY  - 2023
DO  - 10.48550/arXiv.2308.15514
DP  - arXiv.org
PB  - arXiv
ST  - International Governance of Civilian AI
UR  - http://arxiv.org/abs/2308.15514
Y2  - 2024/11/14/13:17:26
L1  - http://arxiv.org/pdf/2308.15514v2
L2  - https://arxiv.org/abs/2308.15514
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Activism by the AI Community: Analysing Recent Achievements and Future Prospects
AU  - Belfield, Haydn
T2  - AIES '20: AAAI/ACM Conference on AI, Ethics, and Society
AB  - The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI ‘talent’. Both are crucial to the future of AI activism and worthy of sustained attention.
C1  - New York NY USA
C3  - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
DA  - 2020/02/07/
PY  - 2020
DO  - 10.1145/3375627.3375814
DP  - DOI.org (Crossref)
SP  - 15
EP  - 21
LA  - en
PB  - ACM
SN  - 978-1-4503-7110-0
ST  - Activism by the AI Community
UR  - https://dl.acm.org/doi/10.1145/3375627.3375814
Y2  - 2024/11/14/13:20:04
L1  - https://www.cser.ac.uk/media/uploads/files/Haydn_Belfield_Activism_by_the_AI_Community_Analysing_Recent_Achievements_and_Future_Prospects.pdf
ER  - 

TY  - GEN
TI  - Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society
AU  - Prunkl, Carina
AU  - Whittlestone, Jess
AB  - One way of carving up the broad "AI ethics and society" research space that has emerged in recent years is to distinguish between "near-term" and "long-term" research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.
DA  - 2020/01/21/
PY  - 2020
DO  - 10.48550/arXiv.2001.04335
DP  - arXiv.org
PB  - arXiv
ST  - Beyond Near- and Long-Term
UR  - http://arxiv.org/abs/2001.04335
Y2  - 2024/11/14/13:20:13
L1  - http://arxiv.org/pdf/2001.04335v2
L2  - https://arxiv.org/abs/2001.04335
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - The tension between openness and prudence in responsible AI research
AU  - Whittlestone, Jess
AU  - Ovadya, Aviv
AB  - This paper explores the tension between openness and prudence in AI research, evident in two core principles of the Montréal Declaration for Responsible AI. While the AI community has strong norms around open sharing of research, concerns about the potential harms arising from misuse of research are growing, prompting some to consider whether the ﬁeld of AI needs to reconsider publication norms. We discuss how different beliefs and values can lead to differing perspectives on how the AI community should manage this tension, and explore implications for what responsible publication norms in AI research might look like in practice.
C3  - AI for Social Good workshop at NeurIPS
DA  - 2019///
PY  - 2019
DP  - Zotero
LA  - en
L1  - https://aiforsocialgood.github.io/neurips2019/accepted/track2/pdfs/18_aisg_neurips2019.pdf
ER  - 

TY  - GEN
TI  - Exploring AI Futures Through Role Play
AU  - Avin, Shahar
AU  - Gruetzemacher, Ross
AU  - Fox, James
AB  - We present an innovative methodology for studying and teaching the impacts of AI through a role play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter relations between short --, mid and long term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.
DA  - 2019/12/19/
PY  - 2019
DO  - 10.48550/arXiv.1912.08964
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1912.08964
Y2  - 2024/11/14/13:22:19
L1  - http://arxiv.org/pdf/1912.08964v1
L2  - https://arxiv.org/abs/1912.08964
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
ER  - 

TY  - JOUR
TI  - AI can help humans find common ground in democratic deliberation
AU  - Tessler, Michael Henry
AU  - Bakker, Michiel A.
AU  - Jarrett, Daniel
AU  - Sheahan, Hannah
AU  - Chadwick, Martin J.
AU  - Koster, Raphael
AU  - Evans, Georgina
AU  - Campbell-Gillingham, Lucy
AU  - Collins, Tantum
AU  - Parkes, David C.
AU  - Botvinick, Matthew
AU  - Summerfield, Christopher
T2  - Science
AB  - Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human ...
DA  - 2024/10/18/
PY  - 2024
DO  - 10.1126/science.adq2852
DP  - www.science.org
LA  - EN
UR  - https://www.science.org/doi/10.1126/science.adq2852
AN  - world
Y2  - 2024/11/14/13:23:14
L1  - https://www.science.org/doi/pdf/10.1126/science.adq2852?download=true
L2  - https://www.science.org/stoken/author-tokens/ST-2196/full
ER  - 

TY  - JOUR
TI  - Prompting Considered Harmful
AU  - Morris, Meredith Ringel
T2  - Communications of the ACM
AB  - As systems graduate from labs to the open world, moving beyond prompting is central to ensuring that AI is useful, usable, and safe for end users as well as experts such as AI developers and researchers.
DA  - 2024/10/17/
PY  - 2024
DO  - 10.1145/3673861
DP  - DOI.org (Crossref)
SP  - 3673861
J2  - Commun. ACM
LA  - en
SN  - 0001-0782, 1557-7317
UR  - https://dl.acm.org/doi/10.1145/3673861
Y2  - 2024/11/14/13:23:59
L1  - https://dl.acm.org/doi/pdf/10.1145/3673861
ER  - 

TY  - JOUR
TI  - Learned feature representations are biased by complexity, learning order, position, and more
AU  - Lampinen, Andrew Kyle
AU  - Chan, Stephanie C. Y.
AU  - Hermann, Katherine
T2  - Transactions on Machine Learning Research
AB  - Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. We then illustrate the downstream effects of these biases on various commonly-used methods for analyzing or intervening on representations. These results highlight a key challenge for interpretability---or for comparing the representations of models and brains---disentangling extraneous biases from the computationally important aspects of a system's internal representations.
DA  - 2024/06/13/
PY  - 2024
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=aY2nsgE97a
Y2  - 2024/11/14/13:25:02
L1  - https://openreview.net/pdf?id=aY2nsgE97a
ER  - 

TY  - GEN
TI  - Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries
AU  - Vodrahalli, Kiran
AU  - Ontanon, Santiago
AU  - Tripuraneni, Nilesh
AU  - Xu, Kelvin
AU  - Jain, Sanil
AU  - Shivanna, Rakesh
AU  - Hui, Jeffrey
AU  - Dikkala, Nishanth
AU  - Kazemi, Mehran
AU  - Fatemi, Bahare
AU  - Anil, Rohan
AU  - Dyer, Ethan
AU  - Shakeri, Siamak
AU  - Vij, Roopali
AU  - Mehta, Harsh
AU  - Ramasesh, Vinay
AU  - Le, Quoc
AU  - Chi, Ed
AU  - Lu, Yifeng
AU  - Firat, Orhan
AU  - Lazaridou, Angeliki
AU  - Lespiau, Jean-Baptiste
AU  - Attaluri, Nithya
AU  - Olszewska, Kate
AB  - We introduce Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to ``chisel away'' the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, we query the model for details of the structure. Using LSQ, we produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. We perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information.
DA  - 2024/09/20/
PY  - 2024
DO  - 10.48550/arXiv.2409.12640
DP  - arXiv.org
PB  - arXiv
ST  - Michelangelo
UR  - http://arxiv.org/abs/2409.12640
Y2  - 2024/11/14/13:25:18
L1  - http://arxiv.org/pdf/2409.12640v2
L2  - https://arxiv.org/abs/2409.12640
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Swim till You Sink: Computing the Limit of a Game
AU  - Hakim, Rashida
AU  - Milionis, Jason
AU  - Papadimitriou, Christos
AU  - Piliouras, Georgios
AB  - During 2023, two interesting results were proven about the limit behavior of game dynamics: First, it was shown that there is a game for which no dynamics converges to the Nash equilibria. Second, it was shown that the sink equilibria of a game adequately capture the limit behavior of natural game dynamics. These two results have created a need and opportunity to articulate a principled computational theory of the meaning of the game that is based on game dynamics. Given any game in normal form, and any prior distribution of play, we study the problem of computing the asymptotic behavior of a class of natural dynamics called the noisy replicator dynamics as a limit distribution over the sink equilibria of the game. When the prior distribution has pure strategy support, we prove this distribution can be computed efficiently, in near-linear time to the size of the best-response graph. When the distribution can be sampled -- for example, if it is the uniform distribution over all mixed strategy profiles -- we show through experiments that the limit distribution of reasonably large games can be estimated quite accurately through sampling and simulation.
DA  - 2024/08/20/
PY  - 2024
DO  - 10.48550/arXiv.2408.11146
DP  - arXiv.org
PB  - arXiv
ST  - Swim till You Sink
UR  - http://arxiv.org/abs/2408.11146
Y2  - 2024/11/14/13:25:42
L1  - http://arxiv.org/pdf/2408.11146v1
L2  - https://arxiv.org/abs/2408.11146
KW  - Computer Science - Computer Science and Game Theory
KW  - Computer Science - Machine Learning
KW  - Economics - Theoretical Economics
ER  - 

TY  - GEN
TI  - Transformer Circuit Faithfulness Metrics are not Robust
AU  - Miller, Joseph
AU  - Chughtai, Bilal
AU  - Saunders, William
AB  - Mechanistic interpretability work attempts to reverse engineer the learned algorithms present inside neural networks. One focus of this work has been to discover ‘circuits’ – subgraphs of the full model that explain behaviour on specific tasks. But how do we measure the performance of such circuits? Prior work has attempted to measure circuit ‘faithfulness’ – the degree to which the circuit replicates the performance of the full model. In this work, we survey many considerations for designing experiments that measure circuit faithfulness by ablating portions of the model’s computation. Concerningly, we find existing methods are highly sensitive to seemingly insignificant changes in the ablation methodology. We conclude that existing circuit faithfulness scores reflect both the methodological choices of researchers as well as the actual components of the circuit - the task a circuit is required to perform depends on the ablation used to test it. The ultimate goal of mechanistic interpretability work is to understand neural networks, so we emphasize the need for more clarity in the precise claims being made about circuits. We open source a library at this https URL that includes highly efficient implementations of a wide range of ablation methodologies and circuit discovery algorithms.
DA  - 2024/07/11/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2407.08734
Y2  - 2024/11/14/13:26:42
L1  - https://arxiv.org/pdf/2407.08734
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Investigating the Indirect Object Identification circuit in Mamba
AU  - Ensign, Danielle
AU  - Garriga-Alonso, Adrià
AB  - How well will current interpretability techniques generalize to future models? A relevant case study is Mamba, a recent recurrent architecture with scaling comparable to Transformers. We adapt pre-Mamba techniques to Mamba and partially reverse-engineer the circuit responsible for the Indirect Object Identification (IOI) task. Our techniques provide evidence that 1) Layer 39 is a key bottleneck, 2) Convolutions in layer 39 shift names one position forward, and 3) The name entities are stored linearly in Layer 39’s SSM. Finally, we adapt an automatic circuit discovery tool, positional Edge Attribution Patching, to identify a Mamba IOI circuit. Our contributions provide initial evidence that circuit-based mechanistic interpretability tools work well for the Mamba architecture.
DA  - 2024/07/22/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2407.14008
Y2  - 2024/11/14/13:27:02
L1  - https://arxiv.org/pdf/2407.14008
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification
AU  - Kwa, Thomas
AU  - Thomas, Drake
AU  - Garriga-Alonso, Adrià
AB  - When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model–a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization.
DA  - 2024/11/08/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Catastrophic Goodhart
UR  - http://arxiv.org/abs/2407.14503
Y2  - 2024/11/14/13:27:15
L1  - https://arxiv.org/pdf/2407.14503
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques
AU  - Gupta, Rohan
AU  - Arcuschin, Iván
AU  - Kwa, Thomas
AU  - Garriga-Alonso, Adrià
AB  - Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents INTERPBENCH, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model’s output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr’s original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.
DA  - 2024/10/30/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - InterpBench
UR  - http://arxiv.org/abs/2407.14494
Y2  - 2024/11/14/13:27:28
L1  - https://arxiv.org/pdf/2407.14494
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Planning in a recurrent neural network that plays Sokoban
AU  - Taufeeque, Mohammad
AU  - Quirke, Philip
AU  - Li, Maximilian
AU  - Cundy, Chris
AU  - Tucker, Aaron David
AU  - Gleave, Adam
AU  - Garriga-Alonso, Adrià
AB  - How a neural network (NN) generalizes to novel situations depends on whether it has learned to select actions heuristically or via a planning process. "An investigation of model-free planning" (Guez et al. 2019) found that a recurrent NN (RNN) trained to play Sokoban appears to plan, with extra computation steps improving the RNN's success rate. We replicate and expand on their behavioral analysis, finding the RNN learns to give itself extra computation steps in complex situations by "pacing" in cycles. Moreover, we train linear probes that predict the future actions taken by the network and find that intervening on the hidden state using these probes controls the agent's subsequent actions. Leveraging these insights, we perform model surgery, enabling the convolutional NN to generalize beyond its 10x10 architectural limit to arbitrarily sized inputs. The resulting model solves challenging, highly off-distribution levels. We open-source our model and code, and believe the neural network's small size (1.29M parameters) makes it an excellent model organism to deepen our understanding of learned planning.
DA  - 2024/10/24/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2407.15421
Y2  - 2024/11/14/13:27:46
L1  - https://arxiv.org/pdf/2407.15421
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Effects of Scale on Language Model Robustness
AU  - Howe, Nikolaus
AU  - McKenzie, Ian
AU  - Hollinsworth, Oskar
AU  - Zajac, Michał
AU  - Tseng, Tom
AU  - Tucker, Aaron
AU  - Bacon, Pierre-Luc
AU  - Gleave, Adam
AB  - Language models exhibit scaling laws, whereby increasing model and dataset size yields predictable decreases in negative log likelihood, unlocking a dazzling array of capabilities. This phenomenon spurs many companies to train ever larger models in pursuit of ever improved performance. Yet, these models are vulnerable to adversarial inputs such as ``jailbreaks'' and prompt injections that induce models to perform undesired behaviors, posing a growing risk as models become more capable. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale? We study this question empirically in the classification setting, finding that without explicit defense training, larger models tend to be modestly more robust on most tasks, though the effect is not reliable. Even with the advantage conferred by scale, undefended models remain easy to attack in absolute terms, and we thus turn our attention to explicitly training models for adversarial robustness, which we show to be a much more compute-efficient defense than scaling model size alone. In this setting, we also observe that adversarially trained larger models generalize faster and better to modified attacks not seen during training when compared with smaller models. Finally, we analyze the offense/defense balance of increasing compute, finding parity in some settings and an advantage for offense in others, suggesting that adversarial training alone is not sufficient to solve robustness, even at greater model scales.
DA  - 2024/10/24/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2407.18213
Y2  - 2024/11/14/13:27:54
L1  - https://arxiv.org/pdf/2407.18213
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws
AU  - Bowen, Dillon
AU  - Murphy, Brendan
AU  - Cai, Will
AU  - Khachaturov, David
AU  - Gleave, Adam
AU  - Pelrine, Kellin
AB  - LLMs produce harmful and undesirable behavior when trained on poisoned datasets that contain a small fraction of corrupted or harmful data. We develop a new attack paradigm, jailbreak-tuning, that combines data poisoning with jailbreaking to fully bypass state-of-the-art safeguards and make models like GPT-4o comply with nearly any harmful request. Our experiments suggest this attack represents a paradigm shift in vulnerability elicitation, producing differences in refusal rates as much as 60+ percentage points compared to normal fine-tuning. Given this demonstration of how data poisoning vulnerabilities persist and can be amplified, we investigate whether these risks will likely increase as models scale. We evaluate three threat models—malicious fine-tuning, imperfect data curation, and intentional data contamination—across 23 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.
DA  - 2024/10/29/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Data Poisoning in LLMs
UR  - http://arxiv.org/abs/2408.02946
Y2  - 2024/11/14/13:28:03
L1  - https://arxiv.org/pdf/2408.02946
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - "We Need Structured Output": Towards User-centered Constraints on Large Language Model Output
AU  - Liu, Michael Xieyang
AU  - Liu, Frederick
AU  - Fiannaca, Alexander J.
AU  - Koo, Terry
AU  - Dixon, Lucas
AU  - Terry, Michael
AU  - Cai, Carrie J.
T3  - CHI EA '24
AB  - Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.
C1  - New York, NY, USA
C3  - Extended Abstracts of the CHI Conference on Human Factors in Computing Systems
DA  - 2024/05/11/
PY  - 2024
DO  - 10.1145/3613905.3650756
DP  - ACM Digital Library
SP  - 1
EP  - 9
PB  - Association for Computing Machinery
SN  - 9798400703317
ST  - "We Need Structured Output"
UR  - https://dl.acm.org/doi/10.1145/3613905.3650756
Y2  - 2024/11/14/
L1  - https://dl.acm.org/doi/pdf/10.1145/3613905.3650756
ER  - 

TY  - GEN
TI  - Bridging the Preference Gap between Retrievers and LLMs
AU  - Ke, Zixuan
AU  - Kong, Weize
AU  - Li, Cheng
AU  - Zhang, Mingyang
AU  - Mei, Qiaozhu
AU  - Bendersky, Michael
AB  - Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-“friendly” information and assembling a LLM-“friendly” context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.
DA  - 2024/02/20/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2401.06954
Y2  - 2024/11/14/13:33:12
L1  - https://arxiv.org/pdf/2401.06954
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models
AU  - Zhuang, Shengyao
AU  - Zhuang, Honglei
AU  - Koopman, Bevan
AU  - Zuccon, Guido
T2  - SIGIR 2024: The 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
AB  - We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at https://github.com/ielab/llm-rankers.
C1  - Washington DC USA
C3  - Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
DA  - 2024/07/10/
PY  - 2024
DO  - 10.1145/3626772.3657813
DP  - DOI.org (Crossref)
SP  - 38
EP  - 47
LA  - en
PB  - ACM
SN  - 9798400704314
UR  - https://dl.acm.org/doi/10.1145/3626772.3657813
Y2  - 2024/11/14/13:33:31
L1  - https://dl.acm.org/doi/pdf/10.1145/3626772.3657813
ER  - 

TY  - CONF
TI  - Instance-conditional timescales of decay for non-stationary learning
AU  - Jain, Nishant
AU  - Shenoy, Pradeep
C3  - Proceedings of the AAAI Conference on Artificial Intelligence
DA  - 2024///
PY  - 2024
DP  - Google Scholar
VL  - 38
SP  - 12773
EP  - 12781
UR  - https://ojs.aaai.org/index.php/AAAI/article/view/29173
Y2  - 2024/11/14/13:34:43
L1  - https://ojs.aaai.org/index.php/AAAI/article/download/29173/30219
ER  - 

TY  - GEN
TI  - Instance-Conditional Timescales of Decay for Non-Stationary Learning
AU  - Jain, Nishant
AU  - Shenoy, Pradeep
AB  - Slow concept drift is a ubiquitous, yet under-studied problem in practical machine learning systems. In such settings, although recent data is more indicative of future data, naively prioritizing recent instances runs the risk of losing valuable information from the past. We propose an optimization-driven approach towards balancing instance importance over large training windows. First, we model instance relevance using a mixture of multiple timescales of decay, allowing us to capture rich temporal trends. Second, we learn an auxiliary scorer model that recovers the appropriate mixture of timescales as a function of the instance itself. Finally, we propose a nested optimization objective for learning the scorer, by which it maximizes forward transfer for the learned model. Experiments on a large real-world dataset of 39M photos over a 9 year period show upto 15% relative gains in accuracy compared to other robust learning baselines. We replicate our gains on two collections of real-world datasets for non-stationary learning, and extend our work to continual learning settings where, too, we beat SOTA methods by large margins.
DA  - 2023/12/20/
PY  - 2023
DO  - 10.48550/arXiv.2212.05908
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.05908
Y2  - 2024/11/14/13:34:54
L1  - http://arxiv.org/pdf/2212.05908v2
L2  - https://arxiv.org/abs/2212.05908
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Multimodal Modeling For Spoken Language Identification
AU  - Bharadwaj, Shikhar
AU  - Ma, Min
AU  - Vashishth, Shikhar
AU  - Bapna, Ankur
AU  - Ganapathy, Sriram
AU  - Axelrod, Vera
AU  - Dalmia, Siddharth
AU  - Han, Wei
AU  - Zhang, Yu
AU  - Esch, Daan van
AU  - Ritchie, Sandy
AU  - Talukdar, Partha
AU  - Riesa, Jason
AB  - Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
DA  - 2023/09/19/
PY  - 2023
DO  - 10.48550/arXiv.2309.10567
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2309.10567
Y2  - 2024/11/14/13:35:03
L1  - http://arxiv.org/pdf/2309.10567v1
L2  - https://arxiv.org/abs/2309.10567
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
KW  - Computer Science - Sound
KW  - Electrical Engineering and Systems Science - Audio and Speech Processing
ER  - 

TY  - GEN
TI  - When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making
AU  - Donahue, Kate
AU  - Gollapudi, Sreenivas
AU  - Kollias, Kostas
AB  - Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.
DA  - 2024/02/26/
PY  - 2024
DO  - 10.48550/arXiv.2308.11721
DP  - arXiv.org
PB  - arXiv
ST  - When Are Two Lists Better than One?
UR  - http://arxiv.org/abs/2308.11721
Y2  - 2024/11/14/13:35:38
L1  - http://arxiv.org/pdf/2308.11721v3
L2  - https://arxiv.org/abs/2308.11721
KW  - Computer Science - Computers and Society
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Fairness under Covariate Shift: Improving Fairness-Accuracy tradeoff with few Unlabeled Test Samples
AU  - Havaldar, Shreyas
AU  - Chauhan, Jatin
AU  - Shanmugam, Karthikeyan
AU  - Nandy, Jay
AU  - Raghuveer, Aravindan
AB  - Covariate shift in the test data is a common practical phenomena that can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups under covariate shift is of paramount importance due to societal implications like criminal justice. We operate in the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards improving fairness under this highly challenging yet realistic scenario, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines.
DA  - 2024/01/08/
PY  - 2024
DO  - 10.48550/arXiv.2310.07535
DP  - arXiv.org
PB  - arXiv
ST  - Fairness under Covariate Shift
UR  - http://arxiv.org/abs/2310.07535
Y2  - 2024/11/14/13:35:51
L1  - http://arxiv.org/pdf/2310.07535v3
L2  - https://arxiv.org/abs/2310.07535
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Conformal Language Modeling
AU  - Quach, Victor
AU  - Fisch, Adam
AU  - Schuster, Tal
AU  - Yala, Adam
AU  - Sohn, Jae Ho
AU  - Jaakkola, Tommi S.
AU  - Barzilay, Regina
AB  - We propose a novel approach to conformal prediction for generative language models (LMs). Standard conformal prediction produces prediction sets -- in place of single predictions -- that have rigorous, statistical performance guarantees. LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components -- such as phrases or sentences -- that are each independently correct (e.g., that are not "hallucinations"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.
DA  - 2024/06/01/
PY  - 2024
DO  - 10.48550/arXiv.2306.10193
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2306.10193
Y2  - 2024/11/14/13:36:05
L1  - http://arxiv.org/pdf/2306.10193v2
L2  - https://arxiv.org/abs/2306.10193
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Selective classification using a robust meta-learning approach
AU  - Jain, Nishant
AU  - Shanmugam, Karthikeyan
AU  - Shenoy, Pradeep
AB  - Predictive uncertainty-a model's self awareness regarding its accuracy on an input-is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditioned reweighting approach that captures predictive uncertainty using an auxiliary network and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing the dropout variance, an approximation of Bayesian Predictive uncertainty. We show in controlled experiments that we effectively capture the diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings-selective classification, label noise, domain adaptation, calibration-and across datasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto 3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX.
DA  - 2024/01/02/
PY  - 2024
DO  - 10.48550/arXiv.2212.05987
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2212.05987
Y2  - 2024/11/14/13:37:04
L1  - http://arxiv.org/pdf/2212.05987v2
L2  - https://arxiv.org/abs/2212.05987
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation
AU  - Havaldar, Shreyas
AU  - Sharma, Navodita
AU  - Sareen, Shubhi
AU  - Shanmugam, Karthikeyan
AU  - Raghuveer, Aravindan
AB  - Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (up to 15%) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples.
DA  - 2024/03/20/
PY  - 2024
DO  - 10.48550/arXiv.2310.08056
DP  - arXiv.org
PB  - arXiv
ST  - Learning from Label Proportions
UR  - http://arxiv.org/abs/2310.08056
Y2  - 2024/11/14/13:37:14
L1  - http://arxiv.org/pdf/2310.08056v4
L2  - https://arxiv.org/abs/2310.08056
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - An intentional approach to managing bias in general purpose embedding models
AU  - Weng, Wei-Hung
AU  - Sellergen, Andrew
AU  - Kiraly, Atilla P.
AU  - D’Amour, Alexander
AU  - Park, Jungyeon
AU  - Pilgrim, Rory
AU  - Pfohl, Stephen
AU  - Lau, Charles
AU  - Natarajan, Vivek
AU  - Azizi, Shekoofeh
AU  - Karthikesalingam, Alan
AU  - Cole-Lewis, Heather
AU  - Matias, Yossi
AU  - Corrado, Greg S.
AU  - Webster, Dale R.
AU  - Shetty, Shravya
AU  - Prabhakara, Shruthi
AU  - Eswaran, Krish
AU  - Celi, Leo A. G.
AU  - Liu, Yun
T2  - The Lancet Digital Health
AB  - Advances in machine learning for health care have brought concerns about bias from the research community; specifically, the introduction, perpetuation, or exacerbation of care disparities. Reinforcing these concerns is the finding that medical images often reveal signals about sensitive attributes in ways that are hard to pinpoint by both algorithms and people. This finding raises a question about how to best design general purpose pretrained embeddings (GPPEs, defined as embeddings meant to support a broad array of use cases) for building downstream models that are free from particular types of bias. The downstream model should be carefully evaluated for bias, and audited and improved as appropriate. However, in our view, well intentioned attempts to prevent the upstream components—GPPEs—from learning sensitive attributes can have unintended consequences on the downstream models. Despite producing a veneer of technical neutrality, the resultant end-to-end system might still be biased or poorly performing. We present reasons, by building on previously published data, to support the reasoning that GPPEs should ideally contain as much information as the original data contain, and highlight the perils of trying to remove sensitive attributes from a GPPE. We also emphasise that downstream prediction models trained for specific tasks and settings, whether developed using GPPEs or not, should be carefully designed and evaluated to avoid bias that makes models vulnerable to issues such as distributional shift. These evaluations should be done by a diverse team, including social scientists, on a diverse cohort representing the full breadth of the patient population for which the final model is intended.
DA  - 2024/02/01/
PY  - 2024
DO  - 10.1016/S2589-7500(23)00227-3
DP  - www.thelancet.com
VL  - 6
IS  - 2
SP  - e126
EP  - e130
J2  - The Lancet Digital Health
LA  - English
SN  - 2589-7500
UR  - https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00227-3/fulltext
Y2  - 2024/11/14/13:37:35
L1  - http://www.thelancet.com/article/S2589750023002273/pdf
L2  - http://www.ncbi.nlm.nih.gov/pubmed/38278614
ER  - 

TY  - CONF
TI  - Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding
AU  - Pace, Alizée
AU  - Yèche, Hugo
AU  - Schölkopf, Bernhard
AU  - Ratsch, Gunnar
AU  - Tennenholtz, Guy
T2  - The Twelfth International Conference on Learning Representations
AB  - A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=lUYY2qsRTI&noteId=NBlfr4LHx0
Y2  - 2024/11/14/13:38:03
L1  - https://openreview.net/pdf?id=lUYY2qsRTI
ER  - 

TY  - CONF
TI  - Demystifying Embedding Spaces using Large Language Models
AU  - Tennenholtz, Guy
AU  - Chow, Yinlam
AU  - Hsu, ChihWei
AU  - Jeong, Jihwan
AU  - Shani, Lior
AU  - Tulepbergenov, Azamat
AU  - Ramachandran, Deepak
AU  - Mladenov, Martin
AU  - Boutilier, Craig
T2  - The Twelfth International Conference on Learning Representations
AB  - Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=qoYogklIPz
Y2  - 2024/11/14/13:38:15
L1  - https://openreview.net/pdf?id=qoYogklIPz
ER  - 

TY  - JOUR
TI  - General Identifiability and Achievability for Causal Representation Learning
AU  - Varıcı, Burak
AU  - Acartürk, Emre
AU  - Shanmugam, Karthikeyan
AU  - Tajer, Ali
AB  - This paper focuses on causal representation learning (CRL) under a general nonparametric latent causal model and a general transformation model that maps the latent data to the observational data. It establishes identifiability and achievability results using two hard uncoupled interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, additionally, recovers the identifiability result for two hard coupled interventions, that is when metadata about the pair of environments that have the same node intervened is known. This paper also shows that when observational data is available, additional faithfulness assumptions that are adopted by the existing literature are unnecessary.
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://proceedings.mlr.press/v238/varici24a/varici24a.pdf
ER  - 

TY  - GEN
TI  - Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method
AU  - Baluta, Teodora
AU  - Lamblin, Pascal
AU  - Tarlow, Daniel
AU  - Pedregosa, Fabian
AU  - Dziugaite, Gintare Karolina
AB  - Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.
DA  - 2024/11/07/
PY  - 2024
DO  - 10.48550/arXiv.2411.04388
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2411.04388
Y2  - 2024/11/14/13:41:10
L1  - http://arxiv.org/pdf/2411.04388v1
L2  - https://arxiv.org/abs/2411.04388
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Structured pruning of neural networks for constraints learning
AU  - Cacciola, Matteo
AU  - Frangioni, Antonio
AU  - Lodi, Andrea
T2  - Operations Research Letters
AB  - In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity in applications such as cancer treatment, algorithmic configuration, and chemical process optimization. This integration often uses Mixed Integer Programming (MIP) formulations to represent the chosen ML model, that is often an Artificial Neural Networks (ANNs) due to their widespread use. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations impractical to solve. In this paper we showcase the effectiveness of a ANN pruning, when applied to models prior to their integration into MIPs. We discuss why pruning is more suitable in this context than other ML compression techniques, and we highlight the potential of appropriate pruning strategies via experiments on MIPs used to construct adversarial examples to ANNs. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.
DA  - 2024/11/01/
PY  - 2024
DO  - 10.1016/j.orl.2024.107194
DP  - ScienceDirect
VL  - 57
SP  - 107194
J2  - Operations Research Letters
SN  - 0167-6377
UR  - https://www.sciencedirect.com/science/article/pii/S0167637724001305
Y2  - 2024/11/14/13:41:22
L1  - https://www.sciencedirect.com/science/article/pii/S0167637724001305/pdfft?download=true
KW  - Artificial neural networks
KW  - Mixed integer programming
KW  - Model compression
KW  - Pruning
ER  - 

TY  - CONF
TI  - Overcoming State and Action Space Disparities in Multi-Domain, Multi-Task Reinforcement Learning
AU  - McLean, Reginald
AU  - Yuan, Kai
AU  - Woungang, Isaac
AU  - Farsad, Nariman
AU  - Castro, Pablo Samuel
T2  - [CoRL 2024] Morphology-Aware Policy and Design Learning Workshop (MAPoDeL)
AB  - Current multi-task reinforcement learning (MTRL) methods have the ability to perform a large number of tasks with a single policy. However when attempting to interact with a new domain, the MTRL agent would need to be re-trained due to differences in domain dynamics and structure. Because of these limitations, we are forced to train multiple policies even though tasks may have shared dynamics, leading to needing more samples and is thus sample inefficient. In this work, we explore the ability of MTRL agents to learn in various domains with various dynamics by simultaneously learning in multiple domains, without the need to fine-tune extra policies. In doing so we find that a MTRL agent trained in multiple domains induces an increase in sample efficiency of up to 70\% while maintaining the overall success rate of the MTRL agent.
DA  - 2024/11/04/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=T7bA2zjobB
Y2  - 2024/11/14/13:42:29
L1  - https://openreview.net/pdf?id=T7bA2zjobB
ER  - 

TY  - GEN
TI  - Stick-breaking Attention
AU  - Tan, Shawn
AU  - Shen, Yikang
AU  - Yang, Songlin
AU  - Courville, Aaron
AU  - Panda, Rameswar
AB  - The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We propose an alternative attention mechanism based on the stick-breaking process: For each token before the current, we determine a break point $\beta_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token. We repeat the process until the stick is fully allocated, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et. al., 2017). We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.
DA  - 2024/10/23/
PY  - 2024
DO  - 10.48550/arXiv.2410.17980
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.17980
Y2  - 2024/11/14/13:42:32
L1  - http://arxiv.org/pdf/2410.17980v1
L2  - https://arxiv.org/abs/2410.17980
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination
AU  - Huang, Jerry
AU  - Parthasarathi, Prasanna
AU  - Rezagholizadeh, Mehdi
AU  - Chen, Boxing
AU  - Chandar, Sarath
AB  - The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to \textit{hallucinate} false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.
DA  - 2024/10/29/
PY  - 2024
DO  - 10.48550/arXiv.2410.17477
DP  - arXiv.org
PB  - arXiv
ST  - Do Robot Snakes Dream like Electric Sheep?
UR  - http://arxiv.org/abs/2410.17477
Y2  - 2024/11/14/13:43:40
L1  - http://arxiv.org/pdf/2410.17477v2
L2  - https://arxiv.org/abs/2410.17477
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization
AU  - Guo, Phillip
AU  - Syed, Aaquib
AU  - Sheshadri, Abhay
AU  - Ewart, Aidan
AU  - Dziugaite, Gintare Karolina
AB  - Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.
DA  - 2024/10/16/
PY  - 2024
DO  - 10.48550/arXiv.2410.12949
DP  - arXiv.org
PB  - arXiv
ST  - Mechanistic Unlearning
UR  - http://arxiv.org/abs/2410.12949
Y2  - 2024/11/14/13:43:42
L1  - http://arxiv.org/pdf/2410.12949v1
L2  - https://arxiv.org/abs/2410.12949
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces
AU  - Su, DiJia
AU  - Sukhbaatar, Sainbayar
AU  - Rabbat, Michael
AU  - Tian, Yuandong
AU  - Zheng, Qinqing
AB  - In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.
DA  - 2024/10/13/
PY  - 2024
DO  - 10.48550/arXiv.2410.09918
DP  - arXiv.org
PB  - arXiv
ST  - Dualformer
UR  - http://arxiv.org/abs/2410.09918
Y2  - 2024/11/14/13:43:46
L1  - http://arxiv.org/pdf/2410.09918v1
L2  - https://arxiv.org/abs/2410.09918
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Logic in Computer Science
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Dynamic Abstractions: Building the Next Generation of Cognitive Tools and Interfaces
AU  - Suh, Sangho
AU  - Dang, Hai
AU  - Yen, Ryan
AU  - Pollock, Josh M.
AU  - Arawjo, Ian
AU  - Kazi, Rubaiat Habib
AU  - Subramonyam, Hariharan
AU  - Li, Jingyi
AU  - Saquib, Nazmus
AU  - Satyanarayan, Arvind
T2  - UIST '24: The 37th Annual ACM Symposium on User Interface Software and Technology
AB  - This workshop provides a forum to discuss, brainstorm, and prototype the next generation of interfaces that leverage the dynamic experiences enabled by recent advances in AI and the generative capabilities of foundation models. These models simplify complex tasks by generating outputs in various representations (e.g., text, images, videos) through diverse input modalities like natural language, voice, and sketch. They interpret user intent to generate and transform representations, potentially changing how we interact with information and express ideas. Inspired by this potential, technologists, theorists, and researchers are exploring new forms of interaction by building demos and communities dedicated to concretizing and advancing the vision of working with dynamic abstractions. This UIST workshop provides a timely space to discuss AI’s impact on how we might design and use cognitive tools (e.g., languages, notations, diagrams). We will explore the challenges, critiques, and opportunities of this space by thinking through and prototyping use cases across various domains.
C1  - Pittsburgh PA USA
C3  - The 37th Annual ACM Symposium on User Interface Software and Technology
DA  - 2024/10/13/
PY  - 2024
DO  - 10.1145/3672539.3686706
DP  - DOI.org (Crossref)
SP  - 1
EP  - 3
LA  - en
PB  - ACM
SN  - 9798400707186
ST  - Dynamic Abstractions
UR  - https://dl.acm.org/doi/10.1145/3672539.3686706
Y2  - 2024/11/14/13:45:08
L1  - https://dl.acm.org/doi/pdf/10.1145/3672539.3686706
ER  - 

TY  - GEN
TI  - Identifying and Addressing Delusions for Target-Directed Decision-Making
AU  - Zhao, Mingde
AU  - Sylvain, Tristan
AU  - Precup, Doina
AU  - Bengio, Yoshua
AB  - Target-directed agents utilize self-generated targets, to guide their behaviors for better generalization. These agents are prone to blindly chasing problematic targets, resulting in worse generalization and safety catastrophes. We show that these behaviors can be results of delusions, stemming from improper designs around training: the agent may naturally come to hold false beliefs about certain targets. We identify different types of delusions via intuitive examples in controlled environments, and investigate their causes and mitigations. With the insights, we demonstrate how we can make agents address delusions preemptively and autonomously. We validate empirically the effectiveness of the proposed strategies in correcting delusional behaviors and improving out-of-distribution generalization.
DA  - 2024/10/16/
PY  - 2024
DO  - 10.48550/arXiv.2410.07096
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.07096
Y2  - 2024/11/14/13:46:33
L1  - http://arxiv.org/pdf/2410.07096v4
L2  - https://arxiv.org/abs/2410.07096
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training
AU  - Mohammadzadeh, Shahrad
AU  - Guerra, Juan David
AU  - Bonizzato, Marco
AU  - Rabbany, Reihaneh
AU  - Farnadi, Golnoosh
AB  - As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.
DA  - 2024/10/20/
PY  - 2024
DO  - 10.48550/arXiv.2410.15460
DP  - arXiv.org
PB  - arXiv
ST  - Hallucination Detox
UR  - http://arxiv.org/abs/2410.15460
Y2  - 2024/11/14/13:46:35
L1  - http://arxiv.org/pdf/2410.15460v1
L2  - https://arxiv.org/abs/2410.15460
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Mathematics - Spectral Theory
ER  - 

TY  - GEN
TI  - Epistemic Integrity in Large Language Models
AU  - Ghafouri, Bijean
AU  - Mohammadzadeh, Shahrad
AU  - Zhou, James
AU  - Nair, Pratheeksha
AU  - Tian, Jacob-Junqi
AU  - Goel, Mayank
AU  - Rabbany, Reihaneh
AU  - Godbout, Jean-François
AU  - Pelrine, Kellin
AB  - Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.
DA  - 2024/11/10/
PY  - 2024
DO  - 10.48550/arXiv.2411.06528
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2411.06528
Y2  - 2024/11/14/13:46:37
L1  - http://arxiv.org/pdf/2411.06528v1
L2  - https://arxiv.org/abs/2411.06528
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - CONF
TI  - Quantifying Likeness: A Simple Machine Learning Approach to Identifying Copyright Infringement in (AI-Generated) Artwork
T2  - The Thirteenth International Conference on Learning Representations
AB  - This study proposes an approach aligned with the legal process to quantify copyright infringement, via stylistic similarity, in AI-generated artwork. In contrast to typical work in this field, and more in line with a realistic legal setting, our approach quantifies the similarity of a set of potentially-infringing “defendant” artworks to a set of copyrighted “plaintiff" artworks. We frame this as an image classification task, using a fine-tuned ResNet trained on small, customized datasets relevant to each use case. Softmax-normalized probabilities from the model serve as similarity scores for potentially infringing “defendant” artworks, and saliency maps and features visualizations complement the score by highlighting key features and allowing for interpretability. This straightforward image classification approach can be accomplished in a quite simple, low-resource setting, making it accessible for real-world applications. We present a case study using Mickey Mouse as the plaintiff, performing thorough hyperparameter tuning and robustness analysis. Our experiments include optimizing batch size, weight decay, and learning rate, as well as exploring the impact of additional distractor classes. We employ data augmentation, cross-validation, and a linear decay learning rate scheduler to improve model performance, along with conducting scaling experiments with different types of distractor classes. The aims of this work are to illustrate the potential of the approach, and identify settings which generalize well, such that it is as "plug and play" as possible for users to apply with their own plaintiff sets of artworks.
DA  - 2024/10/04/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - Quantifying Likeness
UR  - https://openreview.net/forum?id=9zKm3TytBG
Y2  - 2024/11/14/13:46:58
L1  - https://openreview.net/pdf?id=9zKm3TytBG
ER  - 

TY  - GEN
TI  - A Simulation System Towards Solving Societal-Scale Manipulation
AU  - Touzel, Maximilian Puelma
AU  - Sarangi, Sneheel
AU  - Welch, Austin
AU  - Krishnakumar, Gayatri
AU  - Zhao, Dan
AU  - Yang, Zachary
AU  - Yu, Hao
AU  - Kosak-Hine, Ethan
AU  - Gibbs, Tom
AU  - Musulan, Andreea
AU  - Thibault, Camille
AU  - Gurbuz, Busra Tugce
AU  - Rabbany, Reihaneh
AU  - Godbout, Jean-François
AU  - Pelrine, Kellin
AB  - The rise of AI-driven manipulation poses significant risks to societal trust and democratic processes. Yet, studying these effects in real-world settings at scale is ethically and logistically impractical, highlighting a need for simulation tools that can model these dynamics in controlled settings to enable experimentation with possible defenses. We present a simulation environment designed to address this. We elaborate upon the Concordia framework that simulates offline, `real life' activity by adding online interactions to the simulation through social media with the integration of a Mastodon server. We improve simulation efficiency and information flow, and add a set of measurement tools, particularly longitudinal surveys. We demonstrate the simulator with a tailored example in which we track agents' political positions and show how partisan manipulation of agents can affect election results.
DA  - 2024/10/17/
PY  - 2024
DO  - 10.48550/arXiv.2410.13915
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.13915
Y2  - 2024/11/14/13:47:59
L1  - http://arxiv.org/pdf/2410.13915v1
L2  - https://arxiv.org/abs/2410.13915
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computers and Society
KW  - Computer Science - Social and Information Networks
ER  - 

TY  - GEN
TI  - PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning
AU  - Fu, Tingchen
AU  - Sharma, Mrinank
AU  - Torr, Philip
AU  - Cohen, Shay B.
AU  - Krueger, David
AU  - Barez, Fazl
AB  - Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.
DA  - 2024/10/11/
PY  - 2024
DO  - 10.48550/arXiv.2410.08811
DP  - arXiv.org
PB  - arXiv
ST  - PoisonBench
UR  - http://arxiv.org/abs/2410.08811
Y2  - 2024/11/14/13:48:45
L1  - http://arxiv.org/pdf/2410.08811v1
L2  - https://arxiv.org/abs/2410.08811
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - "I Am the One and Only, Your Cyber BFF": Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI
AU  - Cheng, Myra
AU  - DeVrio, Alicia
AU  - Egede, Lisa
AU  - Blodgett, Su Lin
AU  - Olteanu, Alexandra
AB  - Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. In this perspective, we argue that we cannot thoroughly map the social impacts of generative AI without mapping the social impacts of anthropomorphic AI, and outline a call to action.
DA  - 2024/10/11/
PY  - 2024
DO  - 10.48550/arXiv.2410.08526
DP  - arXiv.org
PB  - arXiv
ST  - "I Am the One and Only, Your Cyber BFF"
UR  - http://arxiv.org/abs/2410.08526
Y2  - 2024/11/14/13:48:46
L1  - http://arxiv.org/pdf/2410.08526v1
L2  - https://arxiv.org/abs/2410.08526
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Context is Key: A Benchmark for Forecasting with Essential Textual Information
AU  - Ashok, Arjun
AU  - Williams, Andrew Robert
AU  - Marcotte, Étienne
AU  - Zantedeschi, Valentina
AU  - Subramanian, Jithendaraa
AU  - Riachi, Roland
AU  - Requeima, James
AU  - Lacoste, Alexandre
AU  - Rish, Irina
AU  - Chapados, Nicolas
AU  - Drouin, Alexandre
T2  - NeurIPS Workshop on Time Series in the Age of Large Models
AB  - Forecasting is a task of pinnacle importance in decision making across various fields. Numerical data alone often lacks crucial information for accurate forecasting, and in many cases, humans possess additional contextual information that is essential for forecasting, such as background knowledge or constraints on the quantity to predict. One convenient way to provide such essential information to models is through natural language. Yet, the extent to which existing forecasting approaches can effectively utilize contextual information in text is still an open question. To address this, we propose Context is Key (CiK), a time series forecasting benchmark consisting of tasks that combine numerical data with diverse kinds of textual context, requiring models to leverage a variety of skills to succeed. We evaluate a range of approaches and introduce a simple LLM prompting method that serves as a strong baseline. By presenting this challenging benchmark, we aim to foster progress in multimodal forecasting, paving the way for advancements that will lead to forecasting methods accessible to decision-makers irrespective of their technical expertise. The benchmark can be visualized at https://anon-forecast.github.io/benchmark_report/.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - Context is Key
UR  - https://openreview.net/forum?id=ReSNVjuPpw
Y2  - 2024/11/14/13:49:11
L2  - https://openreview.net/forum?id=ReSNVjuPpw
ER  - 

TY  - CONF
TI  - Faster, More Efficient RLHF through Off-Policy Asynchronous Learning
AU  - Noukhovitch, Michael
AU  - Huang, Shengyi
AU  - Xhonneux, Sophie
AU  - Hosseini, Arian
AU  - Agarwal, Rishabh
AU  - Courville, Aaron
T2  - NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability
AB  - To achieve state-of-the-art chatbots, large language models are finetuned with reinforcement learning (RL), frequently to optimize human feedback (RLHF). This process is computationally expensive and can take weeks. Offline approaches, like DPO, learn on a static dataset and are efficient but not performant. The dominant paradigm, online and on-policy---synchronously generating from the model, labelling with a reward model, and learning on feedback from the model's own outputs---is performant but not efficient. Following prior work in the generall deep RL setting, we propose separating the actor and learner in RLHF. This enables the asynchronously generation of new samples while learning on prior samples, thus leading to overall faster training and better scaling. But this requires a novel regime for RLHF, online but off-policy: learning on samples from a previous version of our model. We ask a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? We find that a contrastive loss, Online DPO, is most robust to off-policy data and that robustness increases with the scale of the policy model. We show even further compute optimizations but demonstrate that they come at a performance cost, giving rise to a trade-off. Finally, we verify our design choices by training LLaMA 3.1 8B with RLHF as a helpful chatbot in half the time of a synchronous run while matching final performance.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=ND3io3eses
Y2  - 2024/11/14/13:50:01
L1  - https://openreview.net/pdf?id=ND3io3eses
ER  - 

TY  - CONF
TI  - How Learning Rates Shape Neural Network Focus: Insights from Example Ranking
AU  - Lobacheva, Ekaterina
AU  - Jordan, Keller
AU  - Baratin, Aristide
AU  - Roux, Nicolas Le
T2  - NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning
AB  - The learning rate is a key hyperparameter that affects both the speed of training and the generalization performance of neural networks. Through a new {\it loss-based example ranking} analysis, we show that networks trained with different learning rates focus their capacity on different parts of the data distribution, leading to solutions with different generalization properties. These findings, which hold across architectures and datasets, provide new insights into how learning rates affect model performance and example-level dynamics in neural networks.
DA  - 2024/11/09/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - How Learning Rates Shape Neural Network Focus
UR  - https://openreview.net/forum?id=NeetGrZ22b
Y2  - 2024/11/14/13:50:49
L1  - https://openreview.net/pdf?id=NeetGrZ22b
ER  - 

TY  - CONF
TI  - Language model scaling laws and zero-sum learning
AU  - Mircea, Andrei
AU  - Lobacheva, Ekaterina
AU  - Chakraborty, Supriyo
AU  - Chitsazan, Nima
AU  - Rish, Irina
T2  - NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning
AB  - This work aims to understand how, in terms of training dynamics, scaling up language model size yields predictable loss improvements. We find that these improvements can be tied back to loss deceleration, an abrupt transition in the rate of loss improvement, characterized by piece-wise linear behavior in log-log space. Notably, improvements from increased model size appear to be a result of (1) improving the loss at which this transition occurs; and (2) improving the rate of loss improvement after this transition. As an explanation for the mechanism underlying this transition (and the effect of model size on loss it mediates), we propose the zero-sum learning (ZSL) hypothesis. In ZSL, per-token gradients become systematically opposed, leading to degenerate training dynamics where the model can't improve loss on one token without harming it on another; bottlenecking the overall rate at which loss can improve. We find compelling evidence of ZSL, as well as unexpected results which shed light on other factors contributing to ZSL.
DA  - 2024/11/09/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=yBq2g832Go
Y2  - 2024/11/14/13:50:57
L1  - https://openreview.net/pdf?id=yBq2g832Go
ER  - 

TY  - CONF
TI  - A Layer Selection Approach to Test Time Adaptation
AU  - Sahoo, Sabyasachi
AU  - ElAraby, Mostafa
AU  - Ngnawe, Jonas
AU  - Pequignot, Yann Batiste
AU  - Precioso, Frederic
AU  - Gagné, Christian
T2  - NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability
AB  - Test Time Adaptation (TTA) addresses the problem of distribution shift by adapting a pretrained model to a new domain during inference. When faced with challenging shifts, most methods collapse and perform worse than the original pretrained model. In this paper, we find that not all layers are equally receptive to the adaptation, and the layers with the most misaligned gradients often cause performance degradation. To address this, we propose GALA, a novel layer selection criterion to identify the most beneficial updates to perform during test time adaptation. This criterion can also filter out unreliable samples with noisy gradients. Its simplicity allows seamless integration with existing TTA loss functions, thereby preventing degradation and focusing adaptation on the most trainable layers. This approach also helps to regularize adaptation to preserve the pretrained features, which are crucial for handling unseen domains. Through extensive experiments, we demonstrate that the proposed layer selection framework improves the performance of existing TTA approaches across multiple datasets, domain shifts, model architectures, and TTA losses.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=WhYuW9n1At
Y2  - 2024/11/14/13:51:03
L1  - https://openreview.net/pdf?id=WhYuW9n1At
ER  - 

TY  - CONF
TI  - Learning Robust Representations for Transfer in Reinforcement Learning
AU  - Mohamed, Faisal
AU  - Castanyer, Roger Creus
AU  - Tang, Hongyao
AU  - Sheikhbahaee, Zahra
AU  - Berseth, Glen
T2  - NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability
AB  - Learning transferable representations for deep reinforcement learning (RL) is a challenging problem due to the inherent non-stationarity, distribution shift, and unstable training dynamics. To be useful, a transferable representation needs to be robust to such factors. In this work, we introduce a new architecture and training strategy for learning robust representations for transfer learning in RL. We propose leveraging multiple CNN encoders and training them not to specialize in areas of the state space but instead to match each other's representation. We find that learned representations transfer well across many Atari tasks, resulting in better transfer learning performance and data efficiency than training from scratch.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=tHa7MhPCdg
Y2  - 2024/11/14/13:51:09
L1  - https://openreview.net/pdf?id=tHa7MhPCdg
ER  - 

TY  - CONF
TI  - Learning Stochastic Rainbow Networks
AU  - White, Vivian
AU  - Chaudhary, Muawiz Sajjad
AU  - Wolf, Guy
AU  - Lajoie, Guillaume
AU  - Harris, Kameron Decker
T2  - NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning
AB  - Random feature models are a popular approach for studying network learning that can capture important behaviors while remaining simpler than traditional training. Guth et al. [2024] introduced “rainbow” networks which model the distribution of trained weights as correlated random features conditioned on previous layer activity. Sampling new weights from distributions fit to learned networks led to similar performance in entirely untrained networks, and the observed weight covariance were found to be low rank. This provided evidence that random feature models could be extended to some networks away from initialization, but White et al. [2024] failed to replicate their results in the deeper ResNet18 architecture. Here we ask whether the rainbow formulation can succeed in deeper networks by directly training a stochastic ensemble of random features, which we call stochastic rainbow networks. At every gradient descent iteration, new weights are sampled for all intermediate layers and features aligned layer-wise. We find: (1) this approach scales to deeper models, which outperform shallow networks at large widths; (2) ensembling multiple samples from the stochastic model is better than retraining the classifier head; and (3) low-rank parameterization of the learnable weight covariances can approach the accuracy of full-rank networks. This offers more evidence for rainbow and other structured random feature networks as reduced models of deep learning.
DA  - 2024/11/09/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=bvJ4UGTRtk
Y2  - 2024/11/14/13:51:16
L1  - https://openreview.net/pdf?id=bvJ4UGTRtk
ER  - 

TY  - GEN
TI  - Neuroplastic Expansion in Deep Reinforcement Learning
AU  - Liu, Jiashun
AU  - Obando-Ceron, Johan
AU  - Courville, Aaron
AU  - Pan, Ling
AB  - The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, Neuroplastic Expansion (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (1) elastic neuron generation based on potential gradients, (2) dormant neuron pruning to optimize network expressivity, and (3) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.
DA  - 2024/10/10/
PY  - 2024
DO  - 10.48550/arXiv.2410.07994
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.07994
Y2  - 2024/11/14/13:51:21
L1  - http://arxiv.org/pdf/2410.07994v1
L2  - https://arxiv.org/abs/2410.07994
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - The Pitfalls of Memorization: When Memorization Hinders Generalization
AU  - Bayat, Reza
AU  - Pezeshki, Mohammad
AU  - Dohmatob, Elvis
AU  - Lopez-Paz, David
AU  - Vincent, Pascal
T2  - NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning
AB  - Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This leads to poor generalization when the learned explanations are spurious. In this work, we formalize $\textit{the interplay between memorization and generalization}$, showing that spurious correlations, when combined with memorization, can reduce the training loss to zero, leaving no incentive to learn robust, generalizable patterns. To address this issue, we introduce $\textit{memorization-aware training}$ (MAT). MAT leverages the flip side of memorization by using held-out predictions to adjust a model's logits, guiding it towards learning robust patterns that remain invariant from training to test, thereby enhancing generalization under distribution shifts.
DA  - 2024/11/09/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - The Pitfalls of Memorization
UR  - https://openreview.net/forum?id=m24s1yUDFX
Y2  - 2024/11/14/13:52:13
L1  - https://openreview.net/pdf?id=m24s1yUDFX
ER  - 

TY  - CONF
TI  - Library Learning Doesn’t: The Curious Case of the Single-Use “Library”
AU  - Berlot-Attwell, Ian
AU  - Rudzicz, Frank
AU  - Si, Xujie
T2  - The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24
AB  - Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of *tools*, such as formal Isabelle lemmas or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts, but do current methods actually learn reusable libraries of tools? We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover and TroVE. We find that function reuse is extremely infrequent on miniF2F and MATH. Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - Library Learning Doesn’t
UR  - https://openreview.net/forum?id=et2T8SKF1O
Y2  - 2024/11/14/13:52:17
L1  - https://openreview.net/pdf?id=et2T8SKF1O
ER  - 

TY  - CONF
TI  - LLMs and Personalities: Inconsistencies Across Scales
AU  - Tommaso, Tosato
AU  - Hegazy, Mahmood
AU  - Lemay, David
AU  - Abukalam, Mohammed
AU  - Rish, Irina
AU  - Dumas, Guillaume
T2  - NeurIPS 2024 Workshop on Behavioral Machine Learning
AB  - This study investigates the application of human psychometric assessments to large language models (LLMs) to examine their consistency and malleability in exhibiting personality traits. We administered the Big Five Inventory (BFI) and the Eysenck Personality Questionnaire-Revised (EPQ-R) to various LLMs across different model sizes and persona prompts. Our results reveal substantial variability in responses due to question order shuffling, challenging the notion of a stable LLM "personality." We find that larger models demonstrate more consistent responses across most personas, though this scaling behavior varies significantly by trait and persona type. The assistant persona showed the most predictable scaling patterns, while clinical personas exhibited more variable and sometimes extreme trait expressions. Including conversation history unexpectedly increased response variability. These findings have important implications for understanding LLM behavior under different conditions and reflect on the consequences of scaling.
DA  - 2024/10/10/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - LLMs and Personalities
UR  - https://openreview.net/forum?id=vBg3OvsHwv
Y2  - 2024/11/14/13:52:20
L1  - https://openreview.net/pdf?id=vBg3OvsHwv
ER  - 

TY  - GEN
TI  - Not All LLM Reasoners Are Created Equal
AU  - Hosseini, Arian
AU  - Sordoni, Alessandro
AU  - Toyama, Daniel
AU  - Courville, Aaron
AU  - Agarwal, Rishabh
AB  - We study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, we evaluate their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. Our findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. Our analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates.
DA  - 2024/10/02/
PY  - 2024
DO  - 10.48550/arXiv.2410.01748
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.01748
Y2  - 2024/11/14/13:52:26
L1  - http://arxiv.org/pdf/2410.01748v1
L2  - https://arxiv.org/abs/2410.01748
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models
AU  - Lan, Michael
AU  - Torr, Philip
AU  - Meek, Austin
AU  - Khakzar, Ashkan
AU  - Krueger, David
AU  - Barez, Fazl
AB  - We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.
DA  - 2024/10/09/
PY  - 2024
DO  - 10.48550/arXiv.2410.06981
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.06981
Y2  - 2024/11/14/13:52:31
L1  - http://arxiv.org/pdf/2410.06981v1
L2  - https://arxiv.org/abs/2410.06981
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Compositional Risk Minimization
AU  - Mahajan, Divyat
AU  - Pezeshki, Mohammad
AU  - Mitliagkas, Ioannis
AU  - Ahuja, Kartik
AU  - Vincent, Pascal
AB  - In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test distribution. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.
DA  - 2024/10/08/
PY  - 2024
DO  - 10.48550/arXiv.2410.06303
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.06303
Y2  - 2024/11/14/13:53:19
L1  - http://arxiv.org/pdf/2410.06303v1
L2  - https://arxiv.org/abs/2410.06303
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?
AU  - Öncel, Fırat
AU  - Bethge, Matthias
AU  - Ermis, Beyza
AU  - Ravanelli, Mirco
AU  - Subakan, Cem
AU  - Yıldız, Çağatay
AB  - In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs. To this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity observations reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.
DA  - 2024/10/16/
PY  - 2024
DO  - 10.48550/arXiv.2410.05581
DP  - arXiv.org
PB  - arXiv
ST  - Adaptation Odyssey in LLMs
UR  - http://arxiv.org/abs/2410.05581
Y2  - 2024/11/14/13:53:21
L1  - http://arxiv.org/pdf/2410.05581v2
L2  - https://arxiv.org/abs/2410.05581
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - On the Modeling Capabilities of Large Language Models for Sequential Decision Making
AU  - Klissarov, Martin
AU  - Hjelm, Devon
AU  - Toshev, Alexander
AU  - Mazoure, Bogdan
AB  - Large pretrained models are showing increasingly better performance in reasoning and planning tasks across different modalities, opening the possibility to leverage them for complex sequential decision making problems. In this paper, we investigate the capabilities of Large Language Models (LLMs) for reinforcement learning (RL) across a diversity of interactive domains. We evaluate their ability to produce decision-making policies, either directly, by generating actions, or indirectly, by first generating reward models to train an agent with RL. Our results show that, even without task-specific fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards through artificial intelligence (AI) feedback yields the most generally applicable approach and can enhance performance by improving credit assignment and exploration. Finally, in environments with unfamiliar dynamics, we explore how fine-tuning LLMs with synthetic data can significantly improve their reward modeling capabilities while mitigating catastrophic forgetting, further broadening their utility in sequential decision-making tasks.
DA  - 2024/10/08/
PY  - 2024
DO  - 10.48550/arXiv.2410.05656
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.05656
Y2  - 2024/11/14/13:53:23
L1  - http://arxiv.org/pdf/2410.05656v1
L2  - https://arxiv.org/abs/2410.05656
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Efficient Design-and-Control Automation with Reinforcement Learning and Adaptive Exploration
AU  - Fan, Jiajun
AU  - Tang, Hongyao
AU  - Przystupa, Michael
AU  - Phielipp, Mariano
AU  - Miret, Santiago
AU  - Berseth, Glen
T2  - AI for Accelerated Materials Design - NeurIPS 2024
AB  - Seeking good designs is a central goal of many important domains, such as robotics, integrated circuits (IC), medicine, and materials science. These design problems are expensive, time-consuming, and traditionally performed by human experts. Moreover, the barriers to domain knowledge make it challenging to propose a universal solution that generalizes to different design problems. In this paper, we propose a new method called Efficient Design and Stable Control (EDiSon) for automatic design and control in different design problems. The key ideas of our method are (1) interactive sequential modeling of the design and control process and (2) adaptive exploration and design replay. To decompose the difficulty of learning design and control as a whole, we leverage sequential modeling for both the design process and control process, with a design policy to generate step-by-step design proposals and a control policy to optimize the objective by operating the design. With deep reinforcement learning (RL), the policies learn to find good designs by maximizing a reward signal that evaluates the quality of designs. Furthermore, we propose an adaptive exploration and replay mechanism based on a design memory that maintains high-quality designs generated so far. By regulating between constructing a design from scratch or replaying a design from memory to refine it, EDiSon balances the trade-off between exploration and exploitation in the design space and stabilizes the learning of the control policy. In the experiments, we evaluate our method in robotic morphology design and Tetris-based design tasks. Our framework has the potential to significantly accelerate the discovery of optimized designs across diverse domains, including automated materials discovery, by improving the exploration in design space while ensuring efficiency.
DA  - 2024/11/03/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=stiehhc5y6
Y2  - 2024/11/14/13:53:26
L1  - https://openreview.net/pdf?id=stiehhc5y6
ER  - 

TY  - GEN
TI  - Adaptive teachers for amortized samplers
AU  - Kim, Minsu
AU  - Choi, Sanghyeok
AU  - Yun, Taeyoung
AU  - Bengio, Emmanuel
AU  - Feng, Leo
AU  - Rector-Brooks, Jarrid
AU  - Ahn, Sungsoo
AU  - Park, Jinkyoo
AU  - Malkin, Nikolay
AU  - Bengio, Yoshua
AB  - Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is implemented as a sequential decision-making process, reinforcement learning (RL) methods, such as generative flow networks, can be used to train the sampling policy. Off-policy RL training facilitates the discovery of diverse, high-reward candidates, but existing methods still face challenges in efficient exploration. We propose to use an adaptive training distribution (the Teacher) to guide the training of the primary amortized sampler (the Student) by prioritizing high-loss regions. The Teacher, an auxiliary behavior model, is trained to sample high-error regions of the Student and can generalize across unexplored modes, thereby enhancing mode coverage by providing an efficient training curriculum. We validate the effectiveness of this approach in a synthetic environment designed to present an exploration challenge, two diffusion-based sampling tasks, and four biochemical discovery tasks demonstrating its ability to improve sample efficiency and mode coverage.
DA  - 2024/10/02/
PY  - 2024
DO  - 10.48550/arXiv.2410.01432
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.01432
Y2  - 2024/11/14/13:53:29
L1  - http://arxiv.org/pdf/2410.01432v1
L2  - https://arxiv.org/abs/2410.01432
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Were RNNs All We Needed?
AU  - Feng, Leo
AU  - Tung, Frederick
AU  - Ahmed, Mohamed Osama
AU  - Bengio, Yoshua
AU  - Hajimirsadegh, Hossein
AB  - The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.
DA  - 2024/10/04/
PY  - 2024
DO  - 10.48550/arXiv.2410.01201
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.01201
Y2  - 2024/11/14/13:54:34
L1  - http://arxiv.org/pdf/2410.01201v2
L2  - https://arxiv.org/abs/2410.01201
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Adaptive Exploration for Data-Efficient General Value Function Evaluations
AU  - Jain, Arushi
AU  - Hanna, Josiah P.
AU  - Precup, Doina
AB  - General Value Functions (GVFs) (Sutton et al., 2011) represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique reward. Existing methods relying on fixed behavior policies or pre-collected data often face data efficiency issues when learning multiple GVFs in parallel using off-policy methods. To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel. Our method optimizes the behavior policy by minimizing the total variance in return across GVFs, thereby reducing the required environmental interactions. We use an existing temporal-difference-style variance estimator to approximate the return variance. We prove that each behavior policy update decreases the overall mean squared error in GVF predictions. We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.
DA  - 2024/10/13/
PY  - 2024
DO  - 10.48550/arXiv.2405.07838
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2405.07838
Y2  - 2024/11/14/13:54:36
L1  - http://arxiv.org/pdf/2405.07838v2
L2  - https://arxiv.org/abs/2405.07838
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Amortizing intractable inference in diffusion models for vision, language, and control
AU  - Venkatraman, Siddarth
AU  - Jain, Moksh
AU  - Scimeca, Luca
AU  - Kim, Minsu
AU  - Sendera, Marcin
AU  - Hasan, Mohsin
AU  - Rowe, Luke
AU  - Mittal, Sarthak
AU  - Lemos, Pablo
AU  - Bengio, Emmanuel
AU  - Adam, Alexandre
AU  - Rector-Brooks, Jarrid
AU  - Bengio, Yoshua
AU  - Berseth, Glen
AU  - Malkin, Nikolay
AB  - Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint or likelihood function $r(\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.
DA  - 2024/05/31/
PY  - 2024
DO  - 10.48550/arXiv.2405.20971
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2405.20971
Y2  - 2024/11/14/13:54:39
L1  - http://arxiv.org/pdf/2405.20971v1
L2  - https://arxiv.org/abs/2405.20971
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Code Repair with LLMs gives an Exploration-Exploitation Tradeoff
AU  - Tang, Hao
AU  - Hu, Keya
AU  - Zhou, Jin Peng
AU  - Zhong, Si Cheng
AU  - Zheng, Wei-Long
AU  - Si, Xujie
AU  - Ellis, Kevin
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=o863gX6DxA
Y2  - 2024/11/14/13:54:42
L1  - https://openreview.net/pdf?id=o863gX6DxA
ER  - 

TY  - CONF
TI  - Conformal Inverse Optimization
AU  - Lin, Bo
AU  - Delage, Erick
AU  - Chan, Timothy
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data. We show that such a point estimation is insufficient in a prescriptive setting where the estimated parameters are used to prescribe new decisions. The prescribed decisions may be low-quality and misaligned with human intuition and thus are unlikely to be adopted. To tackle this challenge, we propose conformal inverse optimization, which seeks to learn an uncertainty set for the unknown parameters and then solve a robust optimization model to prescribe new decisions. Under mild assumptions, we show that our method enjoys provable guarantees on solution quality, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters. Our method demonstrates strong empirical performance compared to classic inverse optimization.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Y2NWKlrDrX
Y2  - 2024/11/14/13:54:44
L1  - https://openreview.net/pdf?id=Y2NWKlrDrX
ER  - 

TY  - GEN
TI  - Efficient Adversarial Training in LLMs with Continuous Attacks
AU  - Xhonneux, Sophie
AU  - Sordoni, Alessandro
AU  - Günnemann, Stephan
AU  - Gidel, Gauthier
AU  - Schwinn, Leo
AB  - Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.
DA  - 2024/11/01/
PY  - 2024
DO  - 10.48550/arXiv.2405.15589
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2405.15589
Y2  - 2024/11/14/13:54:46
L1  - http://arxiv.org/pdf/2405.15589v3
L2  - https://arxiv.org/abs/2405.15589
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers
AU  - Ngnawé, Jonas
AU  - Sahoo, Sabyasachi
AU  - Pequignot, Yann
AU  - Precioso, Frédéric
AU  - Gagné, Christian
AB  - Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively and confidently use the logit margin to detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to assess adversarial vulnerability in deployment scenarios efficiently.
DA  - 2024/11/01/
PY  - 2024
DO  - 10.48550/arXiv.2406.18451
DP  - arXiv.org
PB  - arXiv
ST  - Detecting Brittle Decisions for Free
UR  - http://arxiv.org/abs/2406.18451
Y2  - 2024/11/14/13:54:48
L1  - http://arxiv.org/pdf/2406.18451v3
L2  - https://arxiv.org/abs/2406.18451
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Efficient Reinforcement Learning by Discovering Neural Pathways
AU  - Arnob, Samin Yeasar
AU  - Ohib, Riyasat
AU  - Plis, Sergey M.
AU  - Zhang, Amy
AU  - Sordoni, Alessandro
AU  - Precup, Doina
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5% of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=WEoOreP0n5
Y2  - 2024/11/14/13:54:54
L1  - https://openreview.net/pdf?id=WEoOreP0n5
ER  - 

TY  - CONF
TI  - GenRL: Multimodal-foundation world models for generalization in embodied agents
AU  - Mazzaglia, Pietro
AU  - Verbelen, Tim
AU  - Dhoedt, Bart
AU  - Courville, Aaron
AU  - Rajeswar, Sai
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - GenRL
UR  - https://openreview.net/forum?id=za9Jx8yqUA
Y2  - 2024/11/14/13:55:40
L1  - https://openreview.net/pdf?id=za9Jx8yqUA
ER  - 

TY  - GEN
TI  - Grounding Multimodal Large Language Models in Actions
AU  - Szot, Andrew
AU  - Mazoure, Bogdan
AU  - Agrawal, Harsh
AU  - Hjelm, Devon
AU  - Kira, Zsolt
AU  - Toshev, Alexander
AB  - Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.
DA  - 2024/06/12/
PY  - 2024
DO  - 10.48550/arXiv.2406.07904
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.07904
Y2  - 2024/11/14/13:55:43
L1  - http://arxiv.org/pdf/2406.07904v1
L2  - https://arxiv.org/abs/2406.07904
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Harnessing small projectors and multiple views for efficient vision pretraining
AU  - Ghosh, Arna
AU  - Agrawal, Kumar Krishna
AU  - Sodhani, Shagun
AU  - Oberman, Adam
AU  - Richards, Blake Aaron
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions. However, there are few theoretically grounded principles to guide practice, so practical implementation of each SSL framework requires several heuristics to achieve competitive performance. In this work, we build on recent analytical results to design practical recommendations for competitive and efficient SSL that are grounded in theory. Specifically, recent theory tells us that existing SSL frameworks are actually minimizing the same idealized loss, which is to learn features that best match the data similarity kernel defined by the augmentations used. We show how this idealized loss can be reformulated to a functionally equivalent loss that is more efficient to compute. We study the implicit bias of using gradient descent to minimize our reformulated loss function, and find that using a stronger orthogonalization constraint with a reduced projector dimensionality should yield good representations. Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence. We empirically verify our findings on CIFAR, STL and Imagenet datasets, wherein we demonstrate an improved linear readout performance when training a ResNet-backbone using our theoretically grounded recommendations. Remarkably, we also demonstrate that by leveraging these insights, we can reduce the pretraining dataset size by up to 2$\times$ while maintaining downstream accuracy simply by using more data augmentations. Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Y5DPSJzpra
Y2  - 2024/11/14/13:55:45
L1  - https://openreview.net/pdf?id=Y5DPSJzpra
ER  - 

TY  - GEN
TI  - Improving Context-Aware Preference Modeling for Language Models
AU  - Pitis, Silviu
AU  - Xiao, Ziang
AU  - Roux, Nicolas Le
AU  - Sordoni, Alessandro
AB  - While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.
DA  - 2024/11/06/
PY  - 2024
DO  - 10.48550/arXiv.2407.14916
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2407.14916
Y2  - 2024/11/14/13:55:48
L1  - http://arxiv.org/pdf/2407.14916v2
L2  - https://arxiv.org/abs/2407.14916
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Improved off-policy training of diffusion samplers
AU  - Sendera, Marcin
AU  - Kim, Minsu
AU  - Mittal, Sarthak
AU  - Lemos, Pablo
AU  - Scimeca, Luca
AU  - Rector-Brooks, Jarrid
AU  - Adam, Alexandre
AU  - Bengio, Yoshua
AU  - Malkin, Nikolay
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for future work on diffusion models for amortized inference.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=vieIamY2Gi
Y2  - 2024/11/14/13:55:52
L1  - https://openreview.net/pdf?id=vieIamY2Gi
ER  - 

TY  - GEN
TI  - Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn
AU  - Tang, Hongyao
AU  - Berseth, Glen
AB  - Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings, as well as a scaling setting.
DA  - 2024/09/07/
PY  - 2024
DO  - 10.48550/arXiv.2409.04792
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2409.04792
Y2  - 2024/11/14/13:55:54
L1  - http://arxiv.org/pdf/2409.04792v1
L2  - https://arxiv.org/abs/2409.04792
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Interpreting Learned Feedback Patterns in Large Language Models
AU  - Marks, Luke
AU  - Abdullah, Amir
AU  - Neo, Clement
AU  - Arike, Rauno
AU  - Krueger, David
AU  - Torr, Philip
AU  - Barez, Fazl
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the **safety** and **alignment** of LLMs.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=xUoNgR1Byy
Y2  - 2024/11/14/13:55:57
L1  - https://openreview.net/pdf?id=xUoNgR1Byy
ER  - 

TY  - CONF
TI  - Learning Successor Features the Simple Way
AU  - Chua, Raymond
AU  - Ghosh, Arna
AU  - Kaplanis, Christos
AU  - Richards, Blake Aaron
AU  - Precup, Doina
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid) and 3D (Miniworld) mazes, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=rI7oZj1WMc
Y2  - 2024/11/14/13:56:38
L1  - https://openreview.net/pdf?id=rI7oZj1WMc
ER  - 

TY  - GEN
TI  - Listenable Maps for Zero-Shot Audio Classifiers
AU  - Paissan, Francesco
AU  - Libera, Luca Della
AU  - Ravanelli, Mirco
AU  - Subakan, Cem
AB  - Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio Classifiers in the Zero-Shot context), which, to the best of our knowledge, is the first decoder-based post-hoc interpretation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that maximizes the faithfulness to the original similarity between a given text-and-audio pair. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.
DA  - 2024/05/27/
PY  - 2024
DO  - 10.48550/arXiv.2405.17615
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2405.17615
Y2  - 2024/11/14/13:56:43
L1  - http://arxiv.org/pdf/2405.17615v1
L2  - https://arxiv.org/abs/2405.17615
KW  - Computer Science - Machine Learning
KW  - Computer Science - Sound
KW  - Electrical Engineering and Systems Science - Audio and Speech Processing
KW  - Electrical Engineering and Systems Science - Signal Processing
ER  - 

TY  - CONF
TI  - Do LLMs Build World Representations? Probing Through the Lens of State Abstraction
AU  - Li, Zichao
AU  - Cao, Yanshuai
AU  - Cheung, Jackie CK
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - How do large language models (LLMs) encode the state of the world, including the status of entities and their relations, as described by a text? While existing work directly probes for a complete state of the world, our research explores whether and how LLMs abstract this world state in their internal representations. We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks. To instantiate this framework, we design a text-based planning task, where an LLM acts as an agent in an environment and interacts with objects in containers to achieve a specified goal state. Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - Do LLMs Build World Representations?
UR  - https://openreview.net/forum?id=lzfzjYuWgY
Y2  - 2024/11/14/13:56:46
L1  - https://openreview.net/pdf?id=lzfzjYuWgY
ER  - 

TY  - GEN
TI  - Many-Shot In-Context Learning
AU  - Agarwal, Rishabh
AU  - Singh, Avi
AU  - Zhang, Lei M.
AU  - Bohnet, Bernd
AU  - Rosias, Luis
AU  - Chan, Stephanie
AU  - Zhang, Biao
AU  - Anand, Ankesh
AU  - Abbas, Zaheer
AU  - Nova, Azade
AU  - Co-Reyes, John D.
AU  - Chu, Eric
AU  - Behbahani, Feryal
AU  - Faust, Aleksandra
AU  - Larochelle, Hugo
AB  - Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.
DA  - 2024/10/17/
PY  - 2024
DO  - 10.48550/arXiv.2404.11018
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.11018
Y2  - 2024/11/14/13:56:48
L1  - http://arxiv.org/pdf/2404.11018v3
L2  - https://arxiv.org/abs/2404.11018
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving
AU  - Didolkar, Aniket
AU  - Goyal, Anirudh
AU  - Ke, Nan Rosemary
AU  - Guo, Siyuan
AU  - Valko, Michal
AU  - Lillicrap, Timothy
AU  - Rezende, Danilo
AU  - Bengio, Yoshua
AU  - Mozer, Michael
AU  - Arora, Sanjeev
AB  - Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.
DA  - 2024/05/20/
PY  - 2024
DO  - 10.48550/arXiv.2405.12205
DP  - arXiv.org
PB  - arXiv
ST  - Metacognitive Capabilities of LLMs
UR  - http://arxiv.org/abs/2405.12205
Y2  - 2024/11/14/13:56:50
L1  - http://arxiv.org/pdf/2405.12205v1
L2  - https://arxiv.org/abs/2405.12205
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Normalization and effective learning rates in reinforcement learning
AU  - Lyle, Clare
AU  - Zheng, Zeyu
AU  - Khetarpal, Khimya
AU  - Martens, James
AU  - Hasselt, Hado van
AU  - Pascanu, Razvan
AU  - Dabney, Will
AB  - Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting effective learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.
DA  - 2024/07/01/
PY  - 2024
DO  - 10.48550/arXiv.2407.01800
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2407.01800
Y2  - 2024/11/14/13:56:53
L1  - http://arxiv.org/pdf/2407.01800v1
L2  - https://arxiv.org/abs/2407.01800
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Offline Multitask Representation Learning for Reinforcement Learning
AU  - Ishfaq, Haque
AU  - Nguyen-Tang, Thanh
AU  - Feng, Songtao
AU  - Arora, Raman
AU  - Wang, Mengdi
AU  - Yin, Ming
AU  - Precup, Doina
AB  - We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
DA  - 2024/10/31/
PY  - 2024
DO  - 10.48550/arXiv.2403.11574
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2403.11574
Y2  - 2024/11/14/13:56:55
L1  - http://arxiv.org/pdf/2403.11574v2
L2  - https://arxiv.org/abs/2403.11574
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Parseval Regularization for Continual Reinforcement Learning
AU  - Chung, Wesley
AU  - Cherif, Lynn
AU  - Precup, Doina
AU  - Meger, David
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=RB1F2h5YEx
Y2  - 2024/11/14/13:56:57
L1  - https://openreview.net/pdf?id=RB1F2h5YEx
ER  - 

TY  - CONF
TI  - Predicting Future Actions of Reinforcement Learning Agents
AU  - Chung, Stephen
AU  - Niekum, Scott
AU  - Krueger, David
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=QgaGs7peYe
Y2  - 2024/11/14/13:56:59
L1  - https://openreview.net/pdf?id=QgaGs7peYe
ER  - 

TY  - GEN
TI  - QGFN: Controllable Greediness with Action Values
AU  - Lau, Elaine
AU  - Lu, Stephen Zhewen
AU  - Pan, Ling
AU  - Precup, Doina
AU  - Bengio, Emmanuel
AB  - Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.
DA  - 2024/11/01/
PY  - 2024
DO  - 10.48550/arXiv.2402.05234
DP  - arXiv.org
PB  - arXiv
ST  - QGFN
UR  - http://arxiv.org/abs/2402.05234
Y2  - 2024/11/14/13:57:02
L1  - http://arxiv.org/pdf/2402.05234v3
L2  - https://arxiv.org/abs/2402.05234
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences
AU  - Ferbach, Damien
AU  - Bertrand, Quentin
AU  - Bose, Avishek Joey
AU  - Gidel, Gauthier
AB  - The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models. Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step. However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users. In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an \emph{implicit preference optimization mechanism}. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.
DA  - 2024/06/12/
PY  - 2024
DO  - 10.48550/arXiv.2407.09499
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2407.09499
Y2  - 2024/11/14/13:59:18
L1  - http://arxiv.org/pdf/2407.09499v1
L2  - https://arxiv.org/abs/2407.09499
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space
AU  - Schwinn, Leo
AU  - Dobre, David
AU  - Xhonneux, Sophie
AU  - Gidel, Gauthier
AU  - Gunnemann, Stephan
AB  - Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. Trigger Warning: the appendix contains LLM-generated text with violence and harassment.
DA  - 2024/02/14/
PY  - 2024
DO  - 10.48550/arXiv.2402.09063
DP  - arXiv.org
PB  - arXiv
ST  - Soft Prompt Threats
UR  - http://arxiv.org/abs/2402.09063
Y2  - 2024/11/14/13:59:20
L1  - http://arxiv.org/pdf/2402.09063v1
L2  - https://arxiv.org/abs/2402.09063
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More
AU  - Kitouni, Ouail
AU  - Nolte, Niklas
AU  - Williams, Adina
AU  - Rabbat, Michael
AU  - Bouchacourt, Diane
AU  - Ibrahim, Mark
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Today's best language models still struggle with "hallucinations", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The *reversal curse*, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval. To better understand these limitations, we reframe the reversal curse as a *factorization curse* --- a failure of models to learn the same joint distribution under different factorizations. We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing *WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - The Factorization Curse
UR  - https://openreview.net/forum?id=f70e6YYFHF
Y2  - 2024/11/14/13:59:22
L1  - https://openreview.net/pdf?id=f70e6YYFHF
ER  - 

TY  - CONF
TI  - The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms
AU  - Collins-Woodfin, Elizabeth
AU  - Seroussi, Inbar
AU  - Malaxechebarría, Begoña García
AU  - Mackenzie, Andrew
AU  - Paquette, Elliot
AU  - Paquette, Courtney
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provide our code for evaluation at https://github.com/amackenzie1/highline2024.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - The High Line
UR  - https://openreview.net/forum?id=4VWnC5unAV
Y2  - 2024/11/14/13:59:25
L1  - https://openreview.net/pdf?id=4VWnC5unAV
ER  - 

TY  - CONF
TI  - On the Scalability of Certified Adversarial Robustness with Generated Data
AU  - Altstidl, Thomas
AU  - Dobre, David
AU  - Kosmala, Arthur
AU  - Eskofier, Bjoern
AU  - Gidel, Gauthier
AU  - Schwinn, Leo
T2  - The Thirty-eighth Annual Conference on Neural Information Processing Systems
AB  - Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_{\infty}$ ($\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100.
DA  - 2024/11/06/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=TFAG9UznPv
Y2  - 2024/11/14/13:59:27
L1  - https://openreview.net/pdf?id=TFAG9UznPv
ER  - 

TY  - GEN
TI  - When is an Embedding Model More Promising than Another?
AU  - Darrin, Maxime
AU  - Formont, Philippe
AU  - Ayed, Ismail Ben
AU  - Cheung, Jackie CK
AU  - Piantanida, Pablo
AB  - Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks. The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison. However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming. In this paper, we present a unified approach to evaluate embedders. First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness. We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure. We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology. This effectively offers practitioners a valuable tool for prioritizing model trials.
DA  - 2024/06/11/
PY  - 2024
DO  - 10.48550/arXiv.2406.07640
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.07640
Y2  - 2024/11/14/13:59:29
L1  - http://arxiv.org/pdf/2406.07640v1
L2  - https://arxiv.org/abs/2406.07640
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Not Only the Last-Layer Features for Spurious Correlations: All Layer Deep Feature Reweighting
AU  - Hameed, Humza Wajid
AU  - Nanfack, Geraldin
AU  - Belilovsky, Eugene
AB  - Spurious correlations are a major source of errors for machine learning models, in particular when aiming for group-level fairness. It has been recently shown that a powerful approach to combat spurious correlations is to re-train the last layer on a balanced validation dataset, isolating robust features for the predictor. However, key attributes can sometimes be discarded by neural networks towards the last layer. In this work, we thus consider retraining a classifier on a set of features derived from all layers. We utilize a recently proposed feature selection strategy to select unbiased features from all the layers. We observe this approach gives significant improvements in worst-group accuracy on several standard benchmarks.
DA  - 2024/09/23/
PY  - 2024
DO  - 10.48550/arXiv.2409.14637
DP  - arXiv.org
PB  - arXiv
ST  - Not Only the Last-Layer Features for Spurious Correlations
UR  - http://arxiv.org/abs/2409.14637
Y2  - 2024/11/14/13:59:31
L1  - http://arxiv.org/pdf/2409.14637v1
L2  - https://arxiv.org/abs/2409.14637
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - ToxiSight: Insights Towards Detected Chat Toxicity
AU  - Yang, Zachary
AU  - Tullo, Domenico
AU  - Rabbany, Reihaneh
T2  - The 7th BlackboxNLP Workshop
AB  - We present a comprehensive explainability dashboard designed for in-game chat toxicity. This dashboard integrates various existing explainable AI (XAI) techniques, including token importance analysis, model output visualization, and attribution to the training dataset. It also provides insights through the closest positive and negative examples, facilitating a deeper understanding and potential correction of the training data. Additionally, the dashboard includes word sense analysis—particularly useful for new moderators—and offers free-text explanations for both positive and negative predictions. This multi-faceted approach enhances the interpretability and transparency of toxicity detection models.
DA  - 2024/09/21/
PY  - 2024
DP  - openreview.net
LA  - en
ST  - ToxiSight
UR  - https://openreview.net/forum?id=iL6zxTh2HW
Y2  - 2024/11/14/13:59:33
L1  - https://openreview.net/pdf?id=iL6zxTh2HW
ER  - 

TY  - JOUR
TI  - AI content detection in the emerging information ecosystem: new obligations for media and tech companies
AU  - Knott, Alistair
AU  - Pedreschi, Dino
AU  - Jitsuzumi, Toshiya
AU  - Leavy, Susan
AU  - Eyers, David
AU  - Chakraborti, Tapabrata
AU  - Trotman, Andrew
AU  - Sundareswaran, Sundar
AU  - Baeza-Yates, Ricardo
AU  - Biecek, Przemyslaw
AU  - Weller, Adrian
AU  - Teal, Paul D.
AU  - Basu, Subhadip
AU  - Haklidir, Mehmet
AU  - Morini, Virginia
AU  - Russell, Stuart
AU  - Bengio, Yoshua
T2  - Ethics and Information Technology
AB  - The world is about to be swamped by an unprecedented wave of AI-generated content. We need reliable ways of identifying such content, to supplement the many existing social institutions that enable trust between people and organisations and ensure social resilience. In this paper, we begin by highlighting an important new development: providers of AI content generators have new obligations to support the creation of reliable detectors for the content they generate. These new obligations arise mainly from the EU’s newly finalised AI Act, but they are enhanced by the US President’s recent Executive Order on AI, and by several considerations of self-interest. These new steps towards reliable detection mechanisms are by no means a panacea—but we argue they will usher in a new adversarial landscape, in which reliable methods for identifying AI-generated content are commonly available. In this landscape, many new questions arise for policymakers. Firstly, if reliable AI-content detection mechanisms are available, who should be required to use them? And how should they be used? We argue that new duties arise for media and Web search companies arise for media companies, and for Web search companies, in the deployment of AI-content detectors. Secondly, what broader regulation of the tech ecosystem will maximise the likelihood of reliable AI-content detectors? We argue for a range of new duties, relating to provenance-authentication protocols, open-source AI generators, and support for research and enforcement. Along the way, we consider how the production of AI-generated content relates to ‘free expression’, and discuss the important case of content that is generated jointly by humans and AIs.
DA  - 2024/09/21/
PY  - 2024
DO  - 10.1007/s10676-024-09795-1
DP  - Springer Link
VL  - 26
IS  - 4
SP  - 63
J2  - Ethics Inf Technol
LA  - en
SN  - 1572-8439
ST  - AI content detection in the emerging information ecosystem
UR  - https://doi.org/10.1007/s10676-024-09795-1
Y2  - 2024/11/14/13:59:35
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10676-024-09795-1.pdf
KW  - AI regulation
KW  - AI-generated content
KW  - Artificial Intelligence
KW  - Generative AI
ER  - 

TY  - GEN
TI  - ChainBuddy: An AI Agent System for Generating LLM Pipelines
AU  - Zhang, Jingyue
AU  - Arawjo, Ian
AB  - As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-specific tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the "blank page" problem. ChainBuddy, an AI assistant for generating evaluative LLM pipelines built into the ChainForge platform, aims to tackle this issue. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior, making the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants reported a less demanding workload and felt more confident setting up evaluation pipelines of LLM behavior. We derive insights for the future of interfaces that assist users in the open-ended evaluation of AI.
DA  - 2024/09/20/
PY  - 2024
DO  - 10.48550/arXiv.2409.13588
DP  - arXiv.org
PB  - arXiv
ST  - ChainBuddy
UR  - http://arxiv.org/abs/2409.13588
Y2  - 2024/11/14/13:59:38
L1  - http://arxiv.org/pdf/2409.13588v1
L2  - https://arxiv.org/abs/2409.13588
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Human-Computer Interaction
ER  - 

TY  - GEN
TI  - Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience
AU  - Diaz, Manfred
AU  - Paull, Liam
AU  - Tacchetti, Andrea
AB  - Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning. It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences. Despite its success, understanding the conditions under which TSCL is effective remains challenging. In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL. We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that is found by TSCL approaches. To do so, we demonstrate that for every TSCL problem, an equivalent cooperative game exists, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles. The framework and experimental setup we present in this work represents a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning.
DA  - 2024/09/12/
PY  - 2024
DO  - 10.48550/arXiv.2404.03084
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.03084
Y2  - 2024/11/14/14:01:26
L1  - http://arxiv.org/pdf/2404.03084v2
L2  - https://arxiv.org/abs/2404.03084
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Science and Game Theory
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - When Machines Outshine Humans in Object Recognition, Benchmarking Dilemma
AU  - Darvishi Bayazi, Mohammad Javad
AU  - Arefin, Md Rifat
AU  - Faubert, Jocelyn
AU  - rish, Irina
T2  - Journal of Vision
AB  - In the field of vision science, recent endeavours have aimed to assess the comparative performance of artificial neural network models against human vision. Methodologies often involve the utilization of benchmarks that intentionally perturb or disturb images, thereby measuring noise sensitivity to gain insights into important features for object recognition. Recent studies employing critical frequency band masking have unveiled a perspective, positing that neural networks strategically exploit a wider band and less stable frequency channel compared to the one-octave band of human vision. In this work, we extend the inquiry to encompass diverse modern computer vision models, it becomes apparent that a considerable number of recently developed models outperform human capabilities in the presence of frequency noise. This ascendancy is not merely attributable to conventional techniques such as input image data augmentation but also crucially stems from the proficient exploitation of semantic information within expansive datasets, coupled with rigorous model scaling. Conceiving semantic information from multimodal training as a variant of output augmentation, we posit that augmenting input images and labels holds the potential to improve artificial neural networks to go beyond human performance in the current benchmarks. These advantages establish the idea that these models can be complementary agents for humans, particularly in challenging conditions. Despite acknowledging this progress, we must recognize a limitation in computer vision benchmarks, as they do not comprehensively quantify human vision. Consequently, we emphasize the imperative for vision science-inspired datasets to measure the alignment between models and human vision.
DA  - 2024/09/15/
PY  - 2024
DO  - 10.1167/jov.24.10.1523
DP  - Silverchair
VL  - 24
IS  - 10
SP  - 1523
J2  - Journal of Vision
SN  - 1534-7362
UR  - https://doi.org/10.1167/jov.24.10.1523
Y2  - 2024/11/14/14:01:28
L2  - https://jov.arvojournals.org/article.aspx?articleid=2801585
ER  - 

TY  - JOUR
TI  - End-to-end Conditional Robust Optimization
AU  - Chenreddy, Abhilash Reddy
AU  - Delage, Erick
AB  - The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality in our training loss. We show that the proposed training algorithms produce decisions that outperform the traditional “estimate then optimize” approaches.
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=Oe9ngGi8Gh
ER  - 

TY  - GEN
TI  - Trimming the Risk: Towards Reliable Continuous Training for Deep Learning Inspection Systems
AU  - Abbassi, Altaf Allah
AU  - Braiek, Houssem Ben
AU  - Khomh, Foutse
AU  - Reid, Thomas
AB  - The industry increasingly relies on deep learning (DL) technology for manufacturing inspections, which are challenging to automate with rule-based machine vision algorithms. DL-powered inspection systems derive defect patterns from labeled images, combining human-like agility with the consistency of a computerized system. However, finite labeled datasets often fail to encompass all natural variations necessitating Continuous Training (CT) to regularly adjust their models with recent data. Effective CT requires fresh labeled samples from the original distribution; otherwise, selfgenerated labels can lead to silent performance degradation. To mitigate this risk, we develop a robust CT-based maintenance approach that updates DL models using reliable data selections through a two-stage filtering process. The initial stage filters out low-confidence predictions, as the model inherently discredits them. The second stage uses variational auto-encoders and histograms to generate image embeddings that capture latent and pixel characteristics, then rejects the inputs of substantially shifted embeddings as drifted data with erroneous overconfidence. Then, a fine-tuning of the original DL model is executed on the filtered inputs while validating on a mixture of recent production and original datasets. This strategy mitigates catastrophic forgetting and ensures the model adapts effectively to new operational conditions. Evaluations on industrial inspection systems for popsicle stick prints and glass bottles using critical real-world datasets showed less than 9% of erroneous self-labeled data are retained after filtering and used for fine-tuning, improving model performance on production data by up to 14% without compromising its results on original validation data.
DA  - 2024/09/13/
PY  - 2024
DO  - 10.48550/arXiv.2409.09108
DP  - arXiv.org
PB  - arXiv
ST  - Trimming the Risk
UR  - http://arxiv.org/abs/2409.09108
Y2  - 2024/11/14/14:01:38
L1  - http://arxiv.org/pdf/2409.09108v1
L2  - https://arxiv.org/abs/2409.09108
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Shedding Light on Large Generative Networks: Estimating Epistemic Uncertainty in Diffusion Models
AU  - Berry, Lucas
AU  - Brando, Axel
AU  - Meger, David
AB  - Generative diffusion models, notable for their large parameter count (exceeding 100 million) and operation within high-dimensional image spaces, pose significant challenges for traditional uncertainty estimation methods due to computational demands. In this work, we introduce an innovative framework, Diffusion Ensembles for Capturing Uncertainty (DECU), designed for estimating epistemic uncertainty for diffusion models. The DECU framework introduces a novel method that efficiently trains ensembles of conditional diffusion models by incorporating a static set of pre-trained parameters, drastically reducing the computational burden and the number of parameters that require training. Additionally, DECU employs Pairwise-Distance Estimators (PaiDEs) to accurately measure epistemic uncertainty by evaluating the mutual information between model outputs and weights in high-dimensional spaces. The effectiveness of this framework is demonstrated through experiments on the ImageNet dataset, highlighting its capability to capture epistemic uncertainty, specifically in under-sampled image classes.
DA  - 2024/06/05/
PY  - 2024
DO  - 10.48550/arXiv.2406.18580
DP  - arXiv.org
PB  - arXiv
ST  - Shedding Light on Large Generative Networks
UR  - http://arxiv.org/abs/2406.18580
Y2  - 2024/11/14/14:01:41
L1  - http://arxiv.org/pdf/2406.18580v1
L2  - https://arxiv.org/abs/2406.18580
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Towards Robust Saliency Maps
AU  - Le, Nham
AU  - Gurfinkel, Arie
AU  - Si, Xujie
AU  - Geng, Chuqin
T2  - The 16th Asian Conference on Machine Learning (Conference Track)
AB  - Saliency maps are one of the most popular tools to interpret the operation of a neural network: they compute input features deemed relevant to the final prediction, which are often subsets of pixels that are easily understandable by a human being. However, it is known that relying solely on human assessment to judge a saliency map method can be misleading. In this work, we propose a new neural network verification specification called saliency-robustness, which aims to use formal methods to prove a relationship between Vanilla Gradient (VG) -- a simple yet surprisingly effective saliency map method -- and the network's prediction: given a network, if an input $x$ emits a certain VG saliency map, it is mathematically proven (or disproven) that the network must classify $x$ in a certain way. We then introduce a novel method that combines both Marabou and Crown -- two state-of-the-art neural network verifiers, to solve the proposed specification. Experiments on our synthetic dataset and MNIST show that Vanilla Gradient is surprisingly effective as a certification for the predicted output.
DA  - 2024/10/16/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=2tv0Ubg3o7
Y2  - 2024/11/14/14:01:43
L1  - https://openreview.net/pdf?id=2tv0Ubg3o7
ER  - 

TY  - GEN
TI  - Foundational Challenges in Assuring Alignment and Safety of Large Language Models
AU  - Anwar, Usman
AU  - Saparov, Abulhair
AU  - Rando, Javier
AU  - Paleka, Daniel
AU  - Turpin, Miles
AU  - Hase, Peter
AU  - Lubana, Ekdeep Singh
AU  - Jenner, Erik
AU  - Casper, Stephen
AU  - Sourbut, Oliver
AU  - Edelman, Benjamin L.
AU  - Zhang, Zhaowei
AU  - Günther, Mario
AU  - Korinek, Anton
AU  - Hernandez-Orallo, Jose
AU  - Hammond, Lewis
AU  - Bigelow, Eric
AU  - Pan, Alexander
AU  - Langosco, Lauro
AU  - Korbak, Tomasz
AU  - Zhang, Heidi
AU  - Zhong, Ruiqi
AU  - hÉigeartaigh, Seán Ó
AU  - Recchia, Gabriel
AU  - Corsi, Giulio
AU  - Chan, Alan
AU  - Anderljung, Markus
AU  - Edwards, Lilian
AU  - Petrov, Aleksandar
AU  - Witt, Christian Schroeder de
AU  - Motwan, Sumeet Ramesh
AU  - Bengio, Yoshua
AU  - Chen, Danqi
AU  - Torr, Philip H. S.
AU  - Albanie, Samuel
AU  - Maharaj, Tegan
AU  - Foerster, Jakob
AU  - Tramer, Florian
AU  - He, He
AU  - Kasirzadeh, Atoosa
AU  - Choi, Yejin
AU  - Krueger, David
AB  - This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.
DA  - 2024/09/06/
PY  - 2024
DO  - 10.48550/arXiv.2404.09932
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.09932
Y2  - 2024/11/14/14:01:46
L1  - http://arxiv.org/pdf/2404.09932v2
L2  - https://arxiv.org/abs/2404.09932
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning Multi-agent Multi-machine Tending by Mobile Robots
AU  - Abdalwhab, Abdalwhab
AU  - Beltrame, Giovanni
AU  - Kahou, Samira Ebrahimi
AU  - St-Onge, David
AB  - Robotics can help address the growing worker shortage challenge of the manufacturing industry. As such, machine tending is a task collaborative robots can tackle that can also highly boost productivity. Nevertheless, existing robotics systems deployed in that sector rely on a fixed single-arm setup, whereas mobile robots can provide more flexibility and scalability. In this work, we introduce a multi-agent multi-machine tending learning framework by mobile robots based on Multi-agent Reinforcement Learning (MARL) techniques with the design of a suitable observation and reward. Moreover, an attention-based encoding mechanism is developed and integrated into Multi-agent Proximal Policy Optimization (MAPPO) algorithm to boost its performance for machine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new challenging scenario in terms of task success, safety, and resources utilization. Furthermore, we provided an extensive ablation study to support our various design decisions.
DA  - 2024/08/29/
PY  - 2024
DO  - 10.48550/arXiv.2408.16875
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2408.16875
Y2  - 2024/11/14/14:01:48
L1  - http://arxiv.org/pdf/2408.16875v1
L2  - https://arxiv.org/abs/2408.16875
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Training Language Models to Self-Correct via Reinforcement Learning
AU  - Kumar, Aviral
AU  - Zhuang, Vincent
AU  - Agarwal, Rishabh
AU  - Su, Yi
AU  - Co-Reyes, John D.
AU  - Singh, Avi
AU  - Baumli, Kate
AU  - Iqbal, Shariq
AU  - Bishop, Colton
AU  - Roelofs, Rebecca
AU  - Zhang, Lei M.
AU  - McKinney, Kay
AU  - Shrivastava, Disha
AU  - Paduraru, Cosmin
AU  - Tucker, George
AU  - Precup, Doina
AU  - Behbahani, Feryal
AU  - Faust, Aleksandra
AB  - Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.
DA  - 2024/10/04/
PY  - 2024
DO  - 10.48550/arXiv.2409.12917
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2409.12917
Y2  - 2024/11/14/14:01:53
L1  - http://arxiv.org/pdf/2409.12917v2
L2  - https://arxiv.org/abs/2409.12917
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - An Attentive Approach for Building Partial Reasoning Agents from Pixels
AU  - Alver, Safa
AU  - Precup, Doina
T2  - Transactions on Machine Learning Research
AB  - We study the problem of building reasoning agents that are able to generalize in an effective manner. Towards this goal, we propose an end-to-end approach for building model-based reinforcement learning agents that dynamically focus their reasoning to the relevant aspects of the environment: after automatically identifying the distinct aspects of the environment, these agents dynamically filter out the relevant ones and then pass them to their simulator to perform partial reasoning. Unlike existing approaches, our approach works with pixel-based inputs and it allows for interpreting the focal points of the agent. Our quantitative analyses show that the proposed approach allows for effective generalization in high-dimensional domains with raw observational inputs. We also perform ablation analyses to validate of design choices. Finally, we demonstrate through qualitative analyses that our approach actually allows for building agents that focus their reasoning on the relevant aspects of the environment.
DA  - 2024/06/18/
PY  - 2024
DP  - openreview.net
LA  - en
SN  - 2835-8856
UR  - https://openreview.net/forum?id=S3FUKFMRw8
Y2  - 2024/11/14/14:01:59
L1  - https://openreview.net/pdf?id=S3FUKFMRw8
ER  - 

TY  - GEN
TI  - Zero-Shot Object-Centric Representation Learning
AU  - Didolkar, Aniket
AU  - Zadaianchuk, Andrii
AU  - Goyal, Anirudh
AU  - Mozer, Mike
AU  - Bengio, Yoshua
AU  - Martius, Georg
AU  - Seitzer, Maximilian
AB  - The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing pre-trained self-supervised features. However, so far, object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the wider trend in machine learning towards general-purpose models directly applicable to unseen data and tasks. Thus, in this work, we study current object-centric methods through the lens of zero-shot generalization by introducing a benchmark comprising eight different synthetic and real-world datasets. We analyze the factors influencing zero-shot performance and find that training on diverse real-world images improves transferability to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.
DA  - 2024/08/17/
PY  - 2024
DO  - 10.48550/arXiv.2408.09162
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2408.09162
Y2  - 2024/11/14/14:02:51
L1  - http://arxiv.org/pdf/2408.09162v1
L2  - https://arxiv.org/abs/2408.09162
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Understanding the Local Geometry of Generative Model Manifolds
AU  - Humayun, Ahmed Imtiaz
AU  - Amara, Ibtihel
AU  - Schumann, Candice
AU  - Farnadi, Golnoosh
AU  - Rostamzadeh, Negar
AU  - Havaei, Mohammad
AB  - Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training. For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fr\'echet Inception Distance using a large number of generated and real samples. However, generative model performance is not uniform across the learned manifold, e.g., for \textit{foundation models} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised. In this paper we study the relationship between the \textit{local geometry of the learned manifold} and downstream generation. Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling ($\psi$), rank ($\nu$), and complexity ($\delta$) - to characterize a pre-trained generative model manifold locally. We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization. Finally we demonstrate that training a \textit{reward model} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution.
DA  - 2024/08/15/
PY  - 2024
DO  - 10.48550/arXiv.2408.08307
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2408.08307
Y2  - 2024/11/14/14:02:53
L1  - http://arxiv.org/pdf/2408.08307v1
L2  - https://arxiv.org/abs/2408.08307
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Revisiting Feature Prediction for Learning Visual Representations from Video
AU  - Bardes, Adrien
AU  - Garrido, Quentin
AU  - Ponce, Jean
AU  - Chen, Xinlei
AU  - Rabbat, Michael
AU  - LeCun, Yann
AU  - Assran, Mahmoud
AU  - Ballas, Nicolas
AB  - This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.
DA  - 2024/02/15/
PY  - 2024
DO  - 10.48550/arXiv.2404.08471
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.08471
Y2  - 2024/11/14/14:02:55
L1  - http://arxiv.org/pdf/2404.08471v1
L2  - https://arxiv.org/abs/2404.08471
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Can a Bayesian Oracle Prevent Harm from an Agent?
AU  - Bengio, Yoshua
AU  - Cohen, Michael K.
AU  - Malkin, Nikolay
AU  - MacDermott, Matt
AU  - Fornasiere, Damiano
AU  - Greiner, Pietro
AU  - Kaddar, Younesse
AB  - Is there a way to design powerful AI systems based on machine learning methods that would satisfy probabilistic safety guarantees? With the long-term goal of obtaining a probabilistic guarantee that would apply in every context, we consider estimating a context-dependent bound on the probability of violating a given safety specification. Such a risk evaluation would need to be performed at run-time to provide a guardrail against dangerous actions of an AI. Noting that different plausible hypotheses about the world could produce very different outcomes, and because we do not know which one is right, we derive bounds on the safety violation probability predicted under the true but unknown hypothesis. Such bounds could be used to reject potentially dangerous actions. Our main results involve searching for cautious but plausible hypotheses, obtained by a maximization that involves Bayesian posteriors over hypotheses. We consider two forms of this result, in the iid case and in the non-iid case, and conclude with open problems towards turning such theoretical results into practical AI guardrails.
DA  - 2024/08/22/
PY  - 2024
DO  - 10.48550/arXiv.2408.05284
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2408.05284
Y2  - 2024/11/14/14:02:56
L1  - http://arxiv.org/pdf/2408.05284v2
L2  - https://arxiv.org/abs/2408.05284
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning to Rewrite: Generalized LLM-Generated Text Detection
AU  - Hao, Wei
AU  - Li, Ran
AU  - Zhao, Weiliang
AU  - Yang, Junfeng
AU  - Mao, Chengzhi
AB  - Large language models (LLMs) can be abused at scale to create non-factual content and spread disinformation. Detecting LLM-generated content is essential to mitigate these risks, but current classifiers often fail to generalize in open-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated content less frequently, which can be used for detection and naturally generalizes to unforeseen data. However, we find that the rewriting edit distance between human and LLM content can be indistinguishable across domains, leading to detection failures. We propose training an LLM to rewrite input text, producing minimal edits for LLM-generated content and more edits for human-written text, deriving a distinguishable and generalizable edit distance difference across different domains. Experiments on text from 21 independent domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that our classifier outperforms the state-of-the-art zero-shot classifier by up to 20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work suggests that LLM can effectively detect machine-generated text if they are trained properly.
DA  - 2024/08/08/
PY  - 2024
DO  - 10.48550/arXiv.2408.04237
DP  - arXiv.org
PB  - arXiv
ST  - Learning to Rewrite
UR  - http://arxiv.org/abs/2408.04237
Y2  - 2024/11/14/14:03:44
L1  - http://arxiv.org/pdf/2408.04237v1
L2  - https://arxiv.org/abs/2408.04237
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Are self-explanations from Large Language Models faithful?
AU  - Madsen, Andreas
AU  - Chandar, Sarath
AU  - Reddy, Siva
T2  - Findings 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.
C1  - Bangkok, Thailand
C3  - Findings of the Association for Computational Linguistics: ACL 2024
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.findings-acl.19
DP  - ACLWeb
SP  - 295
EP  - 337
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2024.findings-acl.19
Y2  - 2024/11/14/14:04:39
L1  - https://aclanthology.org/2024.findings-acl.19.pdf
ER  - 

TY  - GEN
TI  - Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models
AU  - Benkirane, Kenza
AU  - Gongas, Laura
AU  - Pelles, Shahar
AU  - Fuchs, Naomi
AU  - Darmon, Joshua
AU  - Stenetorp, Pontus
AU  - Adelani, David Ifeoluwa
AU  - Sánchez, Eduardo
AB  - Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.
DA  - 2024/10/20/
PY  - 2024
DO  - 10.48550/arXiv.2407.16470
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2407.16470
Y2  - 2024/11/14/14:04:50
L1  - http://arxiv.org/pdf/2407.16470v3
L2  - https://arxiv.org/abs/2407.16470
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Tackling the Problem of Distributional Shifts: Correcting Misspecified, High-Dimensional Data-Driven Priors for Inverse Problems
AU  - Barco, Gabriel Missael
AU  - Adam, Alexandre
AU  - Stone, Connor
AU  - Hezaveh, Yashar
AU  - Perreault-Levasseur, Laurence
AB  - Bayesian inference for inverse problems hinges critically on the choice of priors. In the absence of specific prior information, population-level distributions can serve as effective priors for parameters of interest. With the advent of machine learning, the use of data-driven population-level distributions (encoded, e.g., in a trained deep neural network) as priors is emerging as an appealing alternative to simple parametric priors in a variety of inverse problems. However, in many astrophysical applications, it is often difficult or even impossible to acquire independent and identically distributed samples from the underlying data-generating process of interest to train these models. In these cases, corrupted data or a surrogate, e.g. a simulator, is often used to produce training samples, meaning that there is a risk of obtaining misspecified priors. This, in turn, can bias the inferred posteriors in ways that are difficult to quantify, which limits the potential applicability of these models in real-world scenarios. In this work, we propose addressing this issue by iteratively updating the population-level distributions by retraining the model with posterior samples from different sets of observations and showcase the potential of this method on the problem of background image reconstruction in strong gravitational lensing when score-based models are used as data-driven priors. We show that starting from a misspecified prior distribution, the updated distribution becomes progressively closer to the underlying population-level distribution, and the resulting posterior samples exhibit reduced bias after several updates.
DA  - 2024/07/24/
PY  - 2024
DO  - 10.48550/arXiv.2407.17667
DP  - arXiv.org
PB  - arXiv
ST  - Tackling the Problem of Distributional Shifts
UR  - http://arxiv.org/abs/2407.17667
Y2  - 2024/11/14/14:04:51
L1  - http://arxiv.org/pdf/2407.17667v1
L2  - https://arxiv.org/abs/2407.17667
KW  - Astrophysics - Cosmology and Nongalactic Astrophysics
KW  - Astrophysics - Instrumentation and Methods for Astrophysics
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - An empirical study of testing machine learning in the wild
AU  - Openja, Moses
AU  - Khomh, Foutse
AU  - Foundjem, Armstrong
AU  - Jiang, Zhen Ming (Jack)
AU  - Abidi, Mouna
AU  - Hassan, Ahmed E.
T2  - ACM Trans. Softw. Eng. Methodol.
AB  - Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively). Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies. Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow. Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems. Results: Our findings reveal several key insights: 1.) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. 2.) A wide range of  \(17\)  ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency, Correctness, and Efficiency. 3.) Bias and Fairness is more tested in Recommendation (6%) and CV (3.9%) systems, while Security &amp; Privacy is tested in CV (2%), Application Platforms (0.9%), and NLP (0.5%). 4.) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing. Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.
DA  - 2024/07/24/
PY  - 2024
DO  - 10.1145/3680463
DP  - ACM Digital Library
SN  - 1049-331X
UR  - https://dl.acm.org/doi/10.1145/3680463
Y2  - 2024/11/14/14:04:59
L1  - https://dl.acm.org/doi/pdf/10.1145/3680463
ER  - 

TY  - JOUR
TI  - Reinforcement Learning Informed Evolutionary Search for Autonomous Systems Testing
AU  - Humeniuk, Dmytro
AU  - Khomh, Foutse
AU  - Antoniol, Giuliano
T2  - ACM Trans. Softw. Eng. Methodol.
AB  - Evolutionary search-based techniques are commonly used for testing autonomous robotic systems. However, these approaches often rely on computationally expensive simulator-based models for test scenario evaluation. To improve the computational efficiency of the search-based testing, we propose augmenting the evolutionary search (ES) with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge. In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm. By incorporating an RL agent into the search process, we aim to guide the algorithm towards promising regions of the search space from the start, enabling more efficient exploration of the solution space. We evaluate RIGAA on two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system. In both case studies, RIGAA reveals more failures of a high level of diversity, than the compared baselines. RIGAA also outperforms the state-of-the-art tools for vehicle lane keeping assist system testing, such as AmbieGen, CRAG, WOGAN and Frenetic in terms of number of revealed failures in a two-hour budget.
DA  - 2024/07/27/
PY  - 2024
DO  - 10.1145/3680468
DP  - ACM Digital Library
SN  - 1049-331X
UR  - https://dl.acm.org/doi/10.1145/3680468
Y2  - 2024/11/14/14:05:02
L1  - https://dl.acm.org/doi/pdf/10.1145/3680468
ER  - 

TY  - GEN
TI  - Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models
AU  - Lu, Cheng
AU  - Song, Yang
AB  - Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.
DA  - 2024/10/14/
PY  - 2024
DO  - 10.48550/arXiv.2410.11081
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2410.11081
Y2  - 2024/11/14/14:07:20
L1  - http://arxiv.org/pdf/2410.11081v1
L2  - https://arxiv.org/abs/2410.11081
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - First-Person Fairness in Chatbots
AU  - Eloundou, Tyna
AU  - Beutel, Alex
AU  - Robinson, David G
AU  - Gu-Lemberg, Keren
AU  - Brakman, Anna-Luisa
AU  - Mishkin, Pamela
AU  - Shah, Meghan
AU  - Heidecke, Johannes
AU  - Weng, Lilian
AU  - Kalai, Adam Tauman
AB  - Chatbots like ChatGPT are used by hundreds of millions of people for diverse purposes, ranging from re´sume´ writing to entertainment. These real-world applications are different from the institutional uses, such as re´sume´ screening or credit scoring, which have been the focus of much of AI research on bias and fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. In this work, we study “first-person fairness,” which means fairness toward the user who is interacting with a chatbot. This includes providing high-quality responses to all users regardless of their identity or background, and avoiding harmful stereotypes. We propose a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, we assess potential bias linked to users’ names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. Our method leverages a second language model to privately analyze name-sensitivity in the chatbot’s responses. We verify the validity of these annotations through independent human evaluation. Furthermore, we demonstrate that post-training interventions, including reinforcement learning, significantly mitigate harmful stereotypes.
DA  - 2024/10//
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf
ER  - 

TY  - GEN
TI  - MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering
AU  - Chan, Jun Shern
AU  - Chowdhury, Neil
AU  - Jaffe, Oliver
AU  - Aung, James
AU  - Sherburn, Dane
AU  - Mays, Evan
AU  - Starace, Giulio
AU  - Liu, Kevin
AU  - Maksin, Leon
AU  - Patwardhan, Tejal
AU  - Weng, Lilian
AU  - Mądry, Aleksander
AB  - We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.
DA  - 2024/10/24/
PY  - 2024
DO  - 10.48550/arXiv.2410.07095
DP  - arXiv.org
PB  - arXiv
ST  - MLE-bench
UR  - http://arxiv.org/abs/2410.07095
Y2  - 2024/11/14/14:08:19
L1  - http://arxiv.org/pdf/2410.07095v2
L2  - https://arxiv.org/abs/2410.07095
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Rule Based Rewards for Language Model Safety
AU  - Mu, Tong
AU  - Helyar, Alec
AU  - Heidecke, Johannes
AU  - Achiam, Joshua
AU  - Vallone, Andrea
AU  - Kivlichan, Ian
AU  - Lin, Molly
AU  - Beutel, Alex
AU  - Schulman, John
AU  - Weng, Lilian
AB  - Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior. However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental. Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader. In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating. We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf
ER  - 

TY  - GEN
TI  - Prover-Verifier Games improve legibility of LLM outputs
AU  - Kirchner, Jan Hendrik
AU  - Chen, Yining
AU  - Edwards, Harri
AU  - Leike, Jan
AU  - McAleese, Nat
AU  - Burda, Yuri
AB  - One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, "helpful" provers to produce correct solutions that the verifier accepts, and "sneaky" provers to produce incorrect solutions that fool the verifier. We find that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models.
DA  - 2024/08/01/
PY  - 2024
DO  - 10.48550/arXiv.2407.13692
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2407.13692
Y2  - 2024/11/14/14:09:07
L1  - http://arxiv.org/pdf/2407.13692v2
L2  - https://arxiv.org/abs/2407.13692
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Towards Unified Alignment Between Agents, Humans, and Environment
AU  - Yang, Zonghan
AU  - Liu, An
AU  - Liu, Zijun
AU  - Liu, Kaiming
AU  - Xiong, Fangzhou
AU  - Wang, Yile
AU  - Yang, Zeyuan
AU  - Hu, Qingyuan
AU  - Chen, Xinrui
AU  - Zhang, Zhenhe
AU  - Luo, Fuwen
AU  - Guo, Zhicheng
AU  - Li, Peng
AU  - Liu, Yang
AB  - The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.
DA  - 2024/02/14/
PY  - 2024
DO  - 10.48550/arXiv.2402.07744
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.07744
Y2  - 2024/11/14/15:25:16
L1  - http://arxiv.org/pdf/2402.07744v2
L2  - https://arxiv.org/abs/2402.07744
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models
AU  - Cheng, Sijie
AU  - Guo, Zhicheng
AU  - Wu, Jingwen
AU  - Fang, Kechen
AU  - Li, Peng
AU  - Liu, Huaping
AU  - Liu, Yang
AB  - Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to "think" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.
DA  - 2024/03/28/
PY  - 2024
DO  - 10.48550/arXiv.2311.15596
DP  - arXiv.org
PB  - arXiv
ST  - EgoThink
UR  - http://arxiv.org/abs/2311.15596
Y2  - 2024/11/14/15:25:50
L1  - http://arxiv.org/pdf/2311.15596v2
L2  - https://arxiv.org/abs/2311.15596
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CHAP
TI  - Privacy-Preserving Federated Cross-Domain Social Recommendation
AU  - Cai, Jianping
AU  - Liu, Yang
AU  - Liu, Ximeng
AU  - Li, Jiayin
AU  - Zhuang, Hongbin
T2  - Trustworthy Federated Learning
A2  - Goebel, Randy
A2  - Yu, Han
A2  - Faltings, Boi
A2  - Fan, Lixin
A2  - Xiong, Zehui
AB  - By combining user feedback on items with social networks, cross-domain social recommendations provide users with more accurate recommendation results. However, traditional cross-domain social recommendations require holding both data of ratings and social networks, which is not easy to achieve for both information-oriented and socialoriented websites. To promote cross-domain social network collaboration among the institutions holding different data, we propose a federated crossdomain social recommendation (FCSR) algorithm. The main innovation is applying Random Response mechanism to achieve sparsely maintained differential privacy for user connections and proposing Matrix Confusion Method to achieve efﬁcient encrypted user feature vector updates. Our experiments on three datasets show the practicality of FCSR in social recommendation and signiﬁcantly outperforms baselines.
CY  - Cham
DA  - 2023///
PY  - 2023
DP  - DOI.org (Crossref)
VL  - 13448
SP  - 144
EP  - 158
LA  - en
PB  - Springer International Publishing
SN  - 978-3-031-28995-8 978-3-031-28996-5
UR  - https://link.springer.com/10.1007/978-3-031-28996-5_11
Y2  - 2024/11/14/15:27:04
L1  - https://federated-learning.org/fl-ijcai-2022/Papers/FL-IJCAI-22_paper_13.pdf
ER  - 

TY  - CONF
TI  - VFLAIR: A Research Library and Benchmark for Vertical Federated Learning
AU  - Zou, Tianyuan
AU  - Gu, Zixuan
AU  - He, Yu
AU  - Takahashi, Hideaki
AU  - Liu, Yang
AU  - Zhang, Ya-Qin
T2  - The Twelfth International Conference on Learning Representations
AB  - Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. To address this need, we present an extensible and lightweight VFL framework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which supports VFL training with a variety of models, datasets and protocols, along with standardized modules for comprehensive evaluations of attacks and defense strategies. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - VFLAIR
UR  - https://openreview.net/forum?id=sqRgz88TM3
Y2  - 2024/11/14/15:27:07
L1  - https://openreview.net/pdf?id=sqRgz88TM3
ER  - 

TY  - CONF
TI  - On Transferability of Prompt Tuning for Natural Language Processing
AU  - Su, Yusheng
AU  - Wang, Xiaozhi
AU  - Qin, Yujia
AU  - Chan, Chi-Min
AU  - Lin, Yankai
AU  - Wang, Huadong
AU  - Wen, Kaiyue
AU  - Liu, Zhiyuan
AU  - Li, Peng
AU  - Li, Juanzi
AU  - Hou, Lei
AU  - Sun, Maosong
AU  - Zhou, Jie
T2  - NAACL-HLT 2022
A2  - Carpuat, Marine
A2  - de Marneffe, Marie-Catherine
A2  - Meza Ruiz, Ivan Vladimir
AB  - Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts' stimulation to PLMs. The source code can be obtained from https://github.com/thunlp/Prompt-Transferability.
C1  - Seattle, United States
C3  - Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
DA  - 2022/07//
PY  - 2022
DO  - 10.18653/v1/2022.naacl-main.290
DP  - ACLWeb
SP  - 3949
EP  - 3969
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.naacl-main.290
Y2  - 2024/11/14/15:27:37
L1  - https://aclanthology.org/2022.naacl-main.290.pdf
ER  - 

TY  - GEN
TI  - "I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired
AU  - Zhang, Yan
AU  - Li, Ziang
AU  - Guo, Haole
AU  - Wang, Luyao
AU  - Chen, Qihe
AU  - Jiang, Wenjie
AU  - Fan, Mingming
AU  - Zhou, Guyue
AU  - Gong, Jiangtao
AB  - Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.
DA  - 2023/02/07/
PY  - 2023
DO  - 10.48550/arXiv.2302.03481
DP  - arXiv.org
PB  - arXiv
ST  - "I am the follower, also the boss"
UR  - http://arxiv.org/abs/2302.03481
Y2  - 2024/11/14/15:28:18
L1  - http://arxiv.org/pdf/2302.03481v1
L2  - https://arxiv.org/abs/2302.03481
KW  - Computer Science - Human-Computer Interaction
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Selecting Large Language Model to Fine-tune via Rectified Scaling Law
AU  - Lin, Haowei
AU  - Huang, Baizhou
AU  - Ye, Haotian
AU  - Chen, Qinyu
AU  - Wang, Zihao
AU  - Li, Sujian
AU  - Ma, Jianzhu
AU  - Wan, Xiaojun
AU  - Zou, James
AU  - Liang, Yitao
AB  - The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.
DA  - 2024/05/28/
PY  - 2024
DO  - 10.48550/arXiv.2402.02314
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.02314
Y2  - 2024/11/14/15:29:16
L1  - http://arxiv.org/pdf/2402.02314v3
L2  - https://arxiv.org/abs/2402.02314
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - OpenChat: Advancing Open-source Language Models with Mixed-Quality Data
AU  - Wang, Guan
AU  - Cheng, Sijie
AU  - Zhan, Xianyuan
AU  - Li, Xiangang
AU  - Song, Sen
AU  - Liu, Yang
T2  - The Twelfth International Conference on Learning Representations
AB  - Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - OpenChat
UR  - https://openreview.net/forum?id=AOJyfhWYHf
Y2  - 2024/11/14/15:29:35
L1  - https://openreview.net/pdf?id=AOJyfhWYHf
ER  - 

TY  - GEN
TI  - Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge
AU  - Chang, Heng
AU  - Rong, Yu
AU  - Xu, Tingyang
AU  - Huang, Wenbing
AU  - Zhang, Honglei
AU  - Cui, Peng
AU  - Wang, Xin
AU  - Zhu, Wenwu
AU  - Huang, Junzhou
AB  - With the success of the graph embedding model in both academic and industry areas, the robustness of graph embedding against adversarial attack inevitably becomes a crucial problem in graph learning. Existing works usually perform the attack in a white-box fashion: they need to access the predictions/labels to construct their adversarial loss. However, the inaccessibility of predictions/labels makes the white-box attack impractical to a real graph learning system. This paper promotes current frameworks in a more general and flexible sense -- we demand to attack various kinds of graph embedding models with black-box driven. We investigate the theoretical connections between graph signal processing and graph embedding models and formulate the graph embedding model as a general graph signal process with a corresponding graph filter. Therefore, we design a generalized adversarial attacker: GF-Attack. Without accessing any labels and model predictions, GF-Attack can perform the attack directly on the graph filter in a black-box fashion. We further prove that GF-Attack can perform an effective attack without knowing the number of layers of graph embedding models. To validate the generalization of GF-Attack, we construct the attacker on four popular graph embedding models. Extensive experiments validate the effectiveness of GF-Attack on several benchmark datasets.
DA  - 2022/03/01/
PY  - 2022
DO  - 10.48550/arXiv.2105.12419
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2105.12419
Y2  - 2024/11/14/15:30:17
L1  - http://arxiv.org/pdf/2105.12419v2
L2  - https://arxiv.org/abs/2105.12419
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
KW  - Computer Science - Social and Information Networks
ER  - 

TY  - CONF
TI  - Multimodal Federated Learning via Contrastive Representation Ensemble
AU  - Yu, Qiying
AU  - Liu, Yang
AU  - Wang, Yimu
AU  - Xu, Ke
AU  - Liu, Jingjing
T2  - The Eleventh International Conference on Learning Representations
AB  - With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose \textit{Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL)}, a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (\textit{modality gap} and \textit{task gap}), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value.
DA  - 2022/09/29/
PY  - 2022
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Hnk1WRMAYqg
Y2  - 2024/11/14/15:30:18
L1  - https://openreview.net/pdf?id=Hnk1WRMAYqg
ER  - 

TY  - CONF
TI  - Large Language Models Are Not Robust Multiple Choice Selectors
AU  - Zheng, Chujie
AU  - Zhou, Hao
AU  - Meng, Fandong
AU  - Zhou, Jie
AU  - Huang, Minlie
T2  - The Twelfth International Conference on Learning Representations
AB  - Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model’s prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=shr9PXz7T0
Y2  - 2024/11/14/15:35:09
L1  - https://openreview.net/pdf?id=shr9PXz7T0
ER  - 

TY  - CONF
TI  - On Prompt-Driven Safeguarding for Large Language Models
AU  - Zheng, Chujie
AU  - Yin, Fan
AU  - Zhou, Hao
AU  - Meng, Fandong
AU  - Zhou, Jie
AU  - Chang, Kai-Wei
AU  - Huang, Minlie
AU  - Peng, Nanyun
T2  - ICLR 2024 Workshop on Secure and Trustworthy Large Language Models
AB  - Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not been revealed yet. In this work, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we further present a safety prompt optimization method in the Appendix. We demonstrate that the proposed method remarkably improves the safeguarding performance of human-crafted safety prompts without compromising the general model capability.
DA  - 2024/04/14/
PY  - 2024
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=lFwf7bnpUs&referrer=%5Bthe%20profile%20of%20Jie%20Zhou%5D(%2Fprofile%3Fid%3D~Jie_Zhou8)
Y2  - 2024/11/14/15:35:44
L1  - https://openreview.net/pdf?id=lFwf7bnpUs
ER  - 

TY  - CONF
TI  - Language Models Hallucinate, but May Excel at Fact Verification
AU  - Guan, Jian
AU  - Dodge, Jesse
AU  - Wadden, David
AU  - Huang, Minlie
AU  - Peng, Hao
T2  - NAACL-HLT 2024
A2  - Duh, Kevin
A2  - Gomez, Helena
A2  - Bethard, Steven
AB  - Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently “hallucinate,” resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B , the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.
C1  - Mexico City, Mexico
C3  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)
DA  - 2024/06//
PY  - 2024
DO  - 10.18653/v1/2024.naacl-long.62
DP  - ACLWeb
SP  - 1090
EP  - 1111
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2024.naacl-long.62
Y2  - 2024/11/14/15:35:45
L1  - https://aclanthology.org/2024.naacl-long.62.pdf
ER  - 

TY  - CONF
TI  - EmoBench: Evaluating the Emotional Intelligence of Large Language Models
AU  - Sabour, Sahand
AU  - Liu, Siyang
AU  - Zhang, Zheyuan
AU  - Liu, June
AU  - Zhou, Jinfeng
AU  - Sunaryo, Alvionna
AU  - Lee, Tatia
AU  - Mihalcea, Rada
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.326
DP  - ACLWeb
SP  - 5986
EP  - 6004
PB  - Association for Computational Linguistics
ST  - EmoBench
UR  - https://aclanthology.org/2024.acl-long.326
Y2  - 2024/11/14/15:35:47
L1  - https://aclanthology.org/2024.acl-long.326.pdf
ER  - 

TY  - CONF
TI  - MiniLLM: Knowledge Distillation of Large Language Models
AU  - Gu, Yuxian
AU  - Dong, Li
AU  - Wei, Furu
AU  - Huang, Minlie
T2  - The Twelfth International Conference on Learning Representations
AB  - Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.
DA  - 2023/10/13/
PY  - 2023
DP  - openreview.net
LA  - en
ST  - MiniLLM
UR  - https://openreview.net/forum?id=5h0qf7IBZZ
Y2  - 2024/11/14/15:35:49
L1  - https://openreview.net/pdf?id=5h0qf7IBZZ
ER  - 

TY  - CONF
TI  - Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization
AU  - Zhang, Zhexin
AU  - Yang, Junxiao
AU  - Ke, Pei
AU  - Mi, Fei
AU  - Wang, Hongning
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. Our code is available at https://github.com/thu-coai/JailbreakDefense_GoalPriority.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.481
DP  - ACLWeb
SP  - 8865
EP  - 8887
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2024.acl-long.481
Y2  - 2024/11/14/15:36:29
L1  - https://aclanthology.org/2024.acl-long.481.pdf
ER  - 

TY  - CONF
TI  - ToMBench: Benchmarking Theory of Mind in Large Language Models
AU  - Chen, Zhuang
AU  - Wu, Jincenzi
AU  - Zhou, Jinfeng
AU  - Wen, Bosi
AU  - Bi, Guanqun
AU  - Jiang, Gongyao
AU  - Cao, Yaru
AU  - Hu, Mengting
AU  - Lai, Yunghwei
AU  - Xiong, Zexuan
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.847
DP  - ACLWeb
SP  - 15959
EP  - 15983
PB  - Association for Computational Linguistics
ST  - ToMBench
UR  - https://aclanthology.org/2024.acl-long.847
Y2  - 2024/11/14/15:36:31
L1  - https://aclanthology.org/2024.acl-long.847.pdf
ER  - 

TY  - JOUR
TI  - Human vs. Generative AI in Content Creation Competition:  Symbiosis or Conflict?
AU  - Yao, Fan
AU  - Li, Chuanhao
AU  - Nekipelov, Denis
AU  - Wang, Hongning
AU  - Xu, Haifeng
AB  - The advent of generative AI (GenAI) technology produces a transformative impact on the content creation landscape, offering alternative approaches to produce diverse, good-quality content across media, thereby reshaping online ecosystems but also raising concerns about market oversaturation and the potential marginalization of human creativity. Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and GenAI. Our theory and simulations suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in GenAI.
DA  - 2024///
PY  - 2024
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=dT6ZbSxh33
ER  - 

TY  - GEN
TI  - Towards Efficient Exact Optimization of Language Model Alignment
AU  - Ji, Haozhe
AU  - Lu, Cheng
AU  - Niu, Yilin
AU  - Ke, Pei
AU  - Wang, Hongning
AU  - Zhu, Jun
AU  - Tang, Jie
AU  - Huang, Minlie
AB  - The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. However, we show that DPO derived based on the optimal solution of the problem leads to a compromised mean-seeking approximation of the optimal solution in practice. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. EXO is guaranteed to optimize in the same direction as RL algorithms asymptotically for arbitrary policy parametrization. This leads to the same mode-seeking solution, while enables efficient optimization by circumventing the complexities of RL. We also compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data. Code is available at https://github.com/haozheji/exact-optimization.
DA  - 2024/06/05/
PY  - 2024
DO  - 10.48550/arXiv.2402.00856
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2402.00856
Y2  - 2024/11/14/15:36:38
L1  - http://arxiv.org/pdf/2402.00856v4
L2  - https://arxiv.org/abs/2402.00856
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing
AU  - Niu, Yilin
AU  - Huang, Fei
AU  - Liu, Wei
AU  - Cui, Jianwei
AU  - Wang, Bin
AU  - Huang, Minlie
T2  - Transactions of the Association for Computational Linguistics
AB  - Semantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9% on KQA and +8.9% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1% for Hit@1).
DA  - 2023///
PY  - 2023
DO  - 10.1162/tacl_a_00552
DP  - ACLWeb
VL  - 11
SP  - 367
EP  - 383
UR  - https://aclanthology.org/2023.tacl-1.22
Y2  - 2024/11/14/15:37:40
L1  - https://aclanthology.org/2023.tacl-1.22.pdf
ER  - 

TY  - JOUR
TI  - EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training
AU  - Gu, Yuxian
AU  - Wen, Jiaxin
AU  - Sun, Hao
AU  - Song, Yi
AU  - Ke, Pei
AU  - Zheng, Chujie
AU  - Zhang, Zheng
AU  - Yao, Jianzhu
AU  - Liu, Lei
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - Machine Intelligence Research
AB  - Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems.
DA  - 2023/04/01/
PY  - 2023
DO  - 10.1007/s11633-022-1387-3
DP  - Springer Link
VL  - 20
IS  - 2
SP  - 207
EP  - 219
J2  - Mach. Intell. Res.
LA  - en
SN  - 2731-5398
ST  - EVA2.0
UR  - https://doi.org/10.1007/s11633-022-1387-3
Y2  - 2024/11/14/15:37:42
L1  - https://link.springer.com/content/pdf/10.1007%2Fs11633-022-1387-3.pdf
KW  - Artificial Intelligence
KW  - Chinese open-domain conversational model
KW  - deep learning (DL)
KW  - dialogue systems
KW  - large-scale pre-training
KW  - Natural language processing
ER  - 

TY  - CONF
TI  - COKE: A Cognitive Knowledge Graph for Machine Theory of Mind
AU  - Wu, Jincenzi
AU  - Chen, Zhuang
AU  - Deng, Jiawen
AU  - Sabour, Sahand
AU  - Meng, Helen
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.848
DP  - ACLWeb
SP  - 15984
EP  - 16007
PB  - Association for Computational Linguistics
ST  - COKE
UR  - https://aclanthology.org/2024.acl-long.848
Y2  - 2024/11/14/15:37:46
L1  - https://aclanthology.org/2024.acl-long.848.pdf
ER  - 

TY  - JOUR
TI  - TAILORING LANGUAGE GENERATION MODELS UNDER TOTAL VARIATION DISTANCE
AU  - Ji, Haozhe
AU  - Ke, Pei
AU  - Hu, Zhipeng
AU  - Zhang, Rongsheng
AU  - Huang, Minlie
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=VELL0PlWfc
ER  - 

TY  - CONF
TI  - Black-Box Prompt Optimization: Aligning Large Language Models without Model Training
AU  - Cheng, Jiale
AU  - Liu, Xiao
AU  - Zheng, Kehan
AU  - Ke, Pei
AU  - Wang, Hongning
AU  - Dong, Yuxiao
AU  - Tang, Jie
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective—Black-Box Prompt Optimization (BPO)—to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.176
DP  - ACLWeb
SP  - 3201
EP  - 3219
PB  - Association for Computational Linguistics
ST  - Black-Box Prompt Optimization
UR  - https://aclanthology.org/2024.acl-long.176
Y2  - 2024/11/14/15:37:48
L1  - https://aclanthology.org/2024.acl-long.176.pdf
ER  - 

TY  - CONF
TI  - CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation
AU  - Ke, Pei
AU  - Wen, Bosi
AU  - Feng, Andrew
AU  - Liu, Xiao
AU  - Lei, Xuanyu
AU  - Cheng, Jiale
AU  - Wang, Shengyuan
AU  - Zeng, Aohan
AU  - Dong, Yuxiao
AU  - Wang, Hongning
AU  - Tang, Jie
AU  - Huang, Minlie
T2  - ACL 2024
A2  - Ku, Lun-Wei
A2  - Martins, Andre
A2  - Srikumar, Vivek
AB  - Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024/08//
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.704
DP  - ACLWeb
SP  - 13034
EP  - 13054
PB  - Association for Computational Linguistics
ST  - CritiqueLLM
UR  - https://aclanthology.org/2024.acl-long.704
Y2  - 2024/11/14/15:37:49
L1  - https://aclanthology.org/2024.acl-long.704.pdf
ER  - 

TY  - CONF
TI  - Learning Task Decomposition to Assist Humans in Competitive Programming
AU  - Wen, Jiaxin
AU  - Zhong, Ruiqi
AU  - Ke, Pei
AU  - Shao, Zhihong
AU  - Wang, Hongning
AU  - Huang, Minlie
T2  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
AB  - When using language models (LMs) to solve complex problems, humans might struggle to understand the LM-generated solutions and repair the flawed ones. To assist humans in repairing them, we propose to automatically decompose complex solutions into multiple simpler pieces that correspond to specific subtasks. We introduce a novel objective for learning task decomposition, termed assistive value (AssistV), which measures the feasibility and speed for humans to repair the decomposed solution. We collect a dataset of human repair experiences on different decomposed solutions. Utilizing the collected data as in-context examples, we then learn to critique, refine, and rank decomposed solutions to improve AssistV. We validate our method under competitive programming problems: under 177 hours of human study, our method enables non-experts to solve 33.3% more problems, speeds them up by 3.3x, and empowers them to match unassisted experts.
C1  - Bangkok, Thailand
C3  - Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2024///
PY  - 2024
DO  - 10.18653/v1/2024.acl-long.629
DP  - DOI.org (Crossref)
SP  - 11700
EP  - 11723
LA  - en
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2024.acl-long.629
Y2  - 2024/11/14/15:37:57
L1  - https://aclanthology.org/2024.acl-long.629.pdf
ER  - 

TY  - CONF
TI  - DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering
AU  - Ke, Pei
AU  - Huang, Fei
AU  - Mi, Fei
AU  - Wang, Yasheng
AU  - Liu, Qun
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.539
DP  - ACLWeb
SP  - 9676
EP  - 9691
PB  - Association for Computational Linguistics
ST  - DecompEval
UR  - https://aclanthology.org/2023.acl-long.539
Y2  - 2024/11/14/15:39:17
L1  - https://aclanthology.org/2023.acl-long.539.pdf
ER  - 

TY  - CONF
TI  - ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation
AU  - Zhang, Zhexin
AU  - Wen, Jiaxin
AU  - Huang, Minlie
T2  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
AB  - Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named ETHICIST for targeted training data Extraction THrough loss smoothed soft prompting and calIbrated ConfIdence eSTimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that ETHICIST significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https://github.com/ thu-coai/Targeted-Data-Extraction.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.709
DP  - DOI.org (Crossref)
SP  - 12674
EP  - 12687
LA  - en
PB  - Association for Computational Linguistics
ST  - ETHICIST
UR  - https://aclanthology.org/2023.acl-long.709
Y2  - 2024/11/14/15:39:20
L1  - https://aclanthology.org/2023.acl-long.709.pdf
ER  - 

TY  - CONF
TI  - CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation
AU  - Zhou, Jinfeng
AU  - Zheng, Chujie
AU  - Wang, Bo
AU  - Zhang, Zheng
AU  - Huang, Minlie
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.457
DP  - ACLWeb
SP  - 8223
EP  - 8237
PB  - Association for Computational Linguistics
ST  - CASE
UR  - https://aclanthology.org/2023.acl-long.457
Y2  - 2024/11/14/15:39:23
L1  - https://aclanthology.org/2023.acl-long.457.pdf
ER  - 

TY  - JOUR
TI  - MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions
AU  - Sun, Hao
AU  - Zhang, Zhexin
AU  - Mi, Fei
AU  - Wang, Yasheng
AU  - Liu, Wei
AU  - Cui, Jianwei
AU  - Wang, Bin
AU  - Liu, Qun
AU  - Huang, Minlie
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://aclanthology.org/2023.acl-long.123.pdf
ER  - 

TY  - CONF
TI  - Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach
AU  - Zhou, Jinfeng
AU  - Chen, Zhuang
AU  - Wang, Bo
AU  - Huang, Minlie
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.96
DP  - ACLWeb
SP  - 1714
EP  - 1729
PB  - Association for Computational Linguistics
ST  - Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation
UR  - https://aclanthology.org/2023.acl-long.96
Y2  - 2024/11/14/15:39:24
L1  - https://aclanthology.org/2023.acl-long.96.pdf
ER  - 

TY  - CONF
TI  - Pre-Training to Learn in Context
AU  - Gu, Yuxian
AU  - Dong, Li
AU  - Wei, Furu
AU  - Huang, Minlie
T2  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
AB  - In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pretraining for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a large collection of “intrinsic tasks” in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widelyused text classification datasets and the SUPERNATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github. com/thu-coai/PICL.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.267
DP  - DOI.org (Crossref)
SP  - 4849
EP  - 4870
LA  - en
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.acl-long.267
Y2  - 2024/11/14/15:39:29
L1  - https://aclanthology.org/2023.acl-long.267.pdf
ER  - 

TY  - JOUR
TI  - Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models
AU  - Shao, Zhihong
AU  - Gong, Yeyun
AU  - Shen, Yelong
AU  - Huang, Minlie
AU  - Duan, Nan
AU  - Chen, Weizhu
AB  - Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through stepby-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce SYNTHETIC PROMPTING, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=RYD1UMgTdk
ER  - 

TY  - GEN
TI  - CEM: Commonsense-aware Empathetic Response Generation
AU  - Sabour, Sahand
AU  - Zheng, Chujie
AU  - Huang, Minlie
AB  - A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user's emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user's emotion, cognitive understanding of the user's situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user's situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EmpatheticDialogues, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses.
DA  - 2021/12/06/
PY  - 2021
DO  - 10.48550/arXiv.2109.05739
DP  - arXiv.org
PB  - arXiv
ST  - CEM
UR  - http://arxiv.org/abs/2109.05739
Y2  - 2024/11/14/15:40:47
L1  - http://arxiv.org/pdf/2109.05739v2
L2  - https://arxiv.org/abs/2109.05739
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy
AU  - Shao, Zhihong
AU  - Gong, Yeyun
AU  - Shen, Yelong
AU  - Huang, Minlie
AU  - Duan, Nan
AU  - Chen, Weizhu
T2  - Findings 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.
C1  - Singapore
C3  - Findings of the Association for Computational Linguistics: EMNLP 2023
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.findings-emnlp.620
DP  - ACLWeb
SP  - 9248
EP  - 9274
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.findings-emnlp.620
Y2  - 2024/11/14/15:40:52
L1  - https://aclanthology.org/2023.findings-emnlp.620.pdf
ER  - 

TY  - CONF
TI  - InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning
AU  - Zhang, Zhexin
AU  - Cheng, Jiale
AU  - Sun, Hao
AU  - Deng, Jiawen
AU  - Huang, Minlie
T2  - Findings 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - Safety detection has been an increasingly important topic in recent years and it has become even more necessary to develop reliable safety detection systems with the rapid development of large language models. However, currently available safety detection systems have limitations in terms of their versatility and interpretability. In this paper, we first introduce InstructSafety, a safety detection framework that unifies 7 common sub-tasks for safety detection. These tasks are unified into a similar form through different instructions. We then conduct a comprehensive survey of existing safety detection datasets and process 39 human-annotated datasets for instruction tuning. We also construct adversarial samples to enhance the model's robustness. After fine-tuning Flan-T5 on the collected data, we have developed Safety-Flan-T5, a multidimensional and explainable safety detector. We conduct comprehensive experiments on a variety of datasets and tasks, and demonstrate the strong performance of Safety-Flan-T5 in comparison to supervised baselines and served APIs (Perspective API, ChatGPT and InstructGPT). We will release the processed data, fine-tuned Safety-Flan-T5 and related code for public use.
C1  - Singapore
C3  - Findings of the Association for Computational Linguistics: EMNLP 2023
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.findings-emnlp.700
DP  - ACLWeb
SP  - 10421
EP  - 10436
PB  - Association for Computational Linguistics
ST  - InstructSafety
UR  - https://aclanthology.org/2023.findings-emnlp.700
Y2  - 2024/11/14/15:40:57
L1  - https://aclanthology.org/2023.findings-emnlp.700.pdf
ER  - 

TY  - CONF
TI  - Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond
AU  - Liu, Siyang
AU  - Deng, Naihao
AU  - Sabour, Sahand
AU  - Jia, Yilin
AU  - Huang, Minlie
AU  - Mihalcea, Rada
T2  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
AB  - We propose task-adaptive tokenization1 as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on taskspecific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model’s tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
C1  - Singapore
C3  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.emnlp-main.944
DP  - DOI.org (Crossref)
SP  - 15264
EP  - 15281
LA  - en
PB  - Association for Computational Linguistics
ST  - Task-Adaptive Tokenization
UR  - https://aclanthology.org/2023.emnlp-main.944
Y2  - 2024/11/14/15:41:06
L1  - https://aclanthology.org/2023.emnlp-main.944.pdf
ER  - 

TY  - CONF
TI  - Multi-Source Probing for Open-Domain Conversational Understanding
AU  - Li, Yuanxi
AU  - Zhou, Hao
AU  - Zhou, Jie
AU  - Huang, Minlie
T2  - EMNLP 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - Dialogue comprehension and generation are vital to the success of open-domain dialogue systems. Although pre-trained generative conversation models have made significant progress in generating fluent responses, people have difficulty judging whether they understand and efficiently model the contextual information of the conversation. In this study, we propose a Multi-Source Probing (MSP) method to probe the dialogue comprehension abilities of open-domain dialogue models. MSP aggregates features from multiple sources to accomplish diverse task goals and conducts downstream tasks in a generative manner that is consistent with dialogue model pre-training to leverage model capabilities. We conduct probing experiments on seven tasks that require various dialogue comprehension skills, based on the internal representations encoded by dialogue models. Experimental results show that open-domain dialogue models can encode semantic information in the intermediate hidden states, which facilitates dialogue comprehension tasks. Models of different scales and structures possess different conversational understanding capabilities. Our findings encourage a comprehensive evaluation and design of open-domain dialogue models.
C1  - Singapore
C3  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.emnlp-main.769
DP  - ACLWeb
SP  - 12491
EP  - 12505
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.emnlp-main.769
Y2  - 2024/11/14/15:41:09
L1  - https://aclanthology.org/2023.emnlp-main.769.pdf
ER  - 

TY  - CONF
TI  - Unveiling the Implicit Toxicity in Large Language Models
AU  - Wen, Jiaxin
AU  - Ke, Pei
AU  - Sun, Hao
AU  - Zhang, Zhexin
AU  - Li, Chengfei
AU  - Bai, Jinfeng
AU  - Huang, Minlie
T2  - EMNLP 2023
A2  - Bouamor, Houda
A2  - Pino, Juan
A2  - Bali, Kalika
AB  - The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.
C1  - Singapore
C3  - Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
DA  - 2023/12//
PY  - 2023
DO  - 10.18653/v1/2023.emnlp-main.84
DP  - ACLWeb
SP  - 1322
EP  - 1338
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.emnlp-main.84
Y2  - 2024/11/14/15:41:14
L1  - https://aclanthology.org/2023.emnlp-main.84.pdf
ER  - 

TY  - JOUR
TI  - Enhancing Offensive Language Detection with Data Augmentation and Knowledge Distillation
AU  - Deng, Jiawen
AU  - Chen, Zhuang
AU  - Sun, Hao
AU  - Zhang, Zhexin
AU  - Wu, Jincenzi
AU  - Nakagawa, Satoshi
AU  - Ren, Fuji
AU  - Huang, Minlie
T2  - Research
AB  - Offensive language detection has received important attention and plays a crucial role in promoting healthy communication on social platforms, as well as promoting the safe deployment of large language models. Training data is the basis for developing detectors; however, the available offense-related dataset in Chinese is severely limited in terms of data scale and coverage when compared to English resources. This significantly affects the accuracy of Chinese offensive language detectors in practical applications, especially when dealing with hard cases or out-of-domain samples. To alleviate the limitations posed by available datasets, we introduce AugCOLD (Augmented Chinese Offensive Language Dataset), a large-scale unsupervised dataset containing 1 million samples gathered by data crawling and model generation. Furthermore, we employ a multiteacher distillation framework to enhance detection performance with unsupervised data. That is, we build multiple teachers with publicly accessible datasets and use them to assign soft labels to AugCOLD. The soft labels serve as a bridge for knowledge to be distilled from both AugCOLD and multiteacher to the student network, i.e., the final offensive detector. We conduct experiments on multiple public test sets and our well-designed hard tests, demonstrating that our proposal can effectively improve the generalization and robustness of the offensive language detector.
DA  - 2023/09/18/
PY  - 2023
DO  - 10.34133/research.0189
DP  - spj.science.org (Atypon)
VL  - 6
SP  - 0189
UR  - https://spj.science.org/doi/10.34133/research.0189
Y2  - 2024/11/14/15:41:23
L1  - https://spj.science.org/doi/pdf/10.34133/research.0189
ER  - 

TY  - CONF
TI  - Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation
AU  - Guan, Jian
AU  - Huang, Minlie
T2  - Findings 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.
C1  - Toronto, Canada
C3  - Findings of the Association for Computational Linguistics: ACL 2023
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.findings-acl.431
DP  - ACLWeb
SP  - 6897
EP  - 6909
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.findings-acl.431
Y2  - 2024/11/14/15:41:29
L1  - https://aclanthology.org/2023.findings-acl.431.pdf
ER  - 

TY  - CONF
TI  - Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning
AU  - Zheng, Chujie
AU  - Ke, Pei
AU  - Zhang, Zheng
AU  - Huang, Minlie
T2  - Findings of the Association for Computational Linguistics: ACL 2023
C1  - Toronto, Canada
C3  - Findings of the Association for Computational Linguistics: ACL 2023
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.findings-acl.65
DP  - DOI.org (Crossref)
SP  - 1022
EP  - 1040
LA  - en
PB  - Association for Computational Linguistics
ST  - Click
UR  - https://aclanthology.org/2023.findings-acl.65
Y2  - 2024/11/14/15:41:49
L1  - https://aclanthology.org/2023.findings-acl.65.pdf
ER  - 

TY  - CONF
TI  - AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation
AU  - Zheng, Chujie
AU  - Sabour, Sahand
AU  - Wen, Jiaxin
AU  - Zhang, Zheng
AU  - Huang, Minlie
T2  - Findings of the Association for Computational Linguistics: ACL 2023
C1  - Toronto, Canada
C3  - Findings of the Association for Computational Linguistics: ACL 2023
DA  - 2023///
PY  - 2023
DO  - 10.18653/v1/2023.findings-acl.99
DP  - DOI.org (Crossref)
SP  - 1552
EP  - 1568
LA  - en
PB  - Association for Computational Linguistics
ST  - AugESC
UR  - https://aclanthology.org/2023.findings-acl.99
Y2  - 2024/11/14/15:41:51
L1  - https://aclanthology.org/2023.findings-acl.99.pdf
ER  - 

TY  - CONF
TI  - CDConv: A Benchmark for Contradiction Detection in Chinese Conversations
AU  - Zheng, Chujie
AU  - Zhou, Jinfeng
AU  - Zheng, Yinhe
AU  - Peng, Libiao
AU  - Guo, Zhen
AU  - Wu, Wenquan
AU  - Niu, Zheng-Yu
AU  - Wu, Hua
AU  - Huang, Minlie
T2  - EMNLP 2022
A2  - Goldberg, Yoav
A2  - Kozareva, Zornitsa
A2  - Zhang, Yue
AB  - Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains 12K multi-turn conversations annotated with three typical contradiction categories: Intra-sentence Contradiction, Role Confusion, and History Contradiction. To efficiently construct the CDConv conversations, we devise a series of methods for automatic conversation generation, which simulate common user behaviors that trigger chatbots to make contradictions. We conduct careful manual quality screening of the constructed conversations and show that state-of-the-art Chinese chatbots can be easily goaded into making contradictions. Experiments on CDConv show that properly modeling contextual information is critical for dialogue contradiction detection, but there are still unresolved challenges that require future research.
C1  - Abu Dhabi, United Arab Emirates
C3  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
DA  - 2022/12//
PY  - 2022
DO  - 10.18653/v1/2022.emnlp-main.2
DP  - ACLWeb
SP  - 18
EP  - 29
PB  - Association for Computational Linguistics
ST  - CDConv
UR  - https://aclanthology.org/2022.emnlp-main.2
Y2  - 2024/11/14/15:42:57
L1  - https://aclanthology.org/2022.emnlp-main.2.pdf
ER  - 

TY  - CONF
TI  - Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization
AU  - Gu, Yuxian
AU  - Ke, Pei
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
AB  - Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of humanannotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT’s effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT.
C1  - Abu Dhabi, United Arab Emirates
C3  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
DA  - 2022///
PY  - 2022
DO  - 10.18653/v1/2022.emnlp-main.105
DP  - DOI.org (Crossref)
SP  - 1617
EP  - 1634
LA  - en
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.emnlp-main.105
Y2  - 2024/11/14/15:43:08
L1  - https://aclanthology.org/2022.emnlp-main.105.pdf
ER  - 

TY  - CONF
TI  - COLD: A Benchmark for Chinese Offensive Language Detection
AU  - Deng, Jiawen
AU  - Zhou, Jingyan
AU  - Sun, Hao
AU  - Zheng, Chujie
AU  - Mi, Fei
AU  - Meng, Helen
AU  - Huang, Minlie
T2  - EMNLP 2022
A2  - Goldberg, Yoav
A2  - Kozareva, Zornitsa
A2  - Zhang, Yue
AB  - Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset –COLDATASET and a baseline detector –COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.
C1  - Abu Dhabi, United Arab Emirates
C3  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
DA  - 2022/12//
PY  - 2022
DO  - 10.18653/v1/2022.emnlp-main.796
DP  - ACLWeb
SP  - 11580
EP  - 11599
PB  - Association for Computational Linguistics
ST  - COLD
UR  - https://aclanthology.org/2022.emnlp-main.796
Y2  - 2024/11/14/15:43:13
L1  - https://aclanthology.org/2022.emnlp-main.796.pdf
ER  - 

TY  - JOUR
TI  - AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text
AU  - Shao, Zhihong
AU  - Wu, Zhongqin
AU  - Huang, Minlie
T2  - IEEE/ACM Transactions on Audio, Speech, and Language Processing
AB  - Adversarial examples are vital to expose vulnerability of machine learning models. Despite the success of the most popular word-level substitution-based attacks which substitute some words in the original examples, only substitution is insufficient to uncover all robustness issues of models. In this paper, we focus on perturbations beyond word-level substitution, and present AdvExpander, a method that crafts new adversarial examples by expanding text. We first utilize linguistic rules to determine which constituents to expand and what types of modifiers to expand with. We then expand each constituent by inserting an adversarial modifier searched from a pre-trained CVAE-based generative model. To ensure that our adversarial examples are label-preserving for text matching, we also constrain the modifications with a heuristic rule. Experiments on three classification tasks verify the effectiveness of AdvExpander and the validity of our adversarial examples. AdvExpander is significantly more effective than sentence-level attack baselines and is complementary to previous word substitution-based attacks, thus promising to reveal new robustness issues.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TASLP.2021.3129339
DP  - IEEE Xplore
VL  - 30
SP  - 1184
EP  - 1196
SN  - 2329-9304
ST  - AdvExpander
UR  - https://ieeexplore.ieee.org/document/9622188
Y2  - 2024/11/14/15:43:23
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9622188&ref=
KW  - Bibliographies
KW  - Databases
KW  - Patents
KW  - Robustness
KW  - Sorting
KW  - Speech processing
KW  - Standards
KW  - text classification
KW  - text matching
KW  - textual adversarial examples
KW  - Uniform resource locators
ER  - 

TY  - JOUR
TI  - Directed Acyclic Transformer for Non-Autoregressive Machine Translation
AU  - Huang, Fei
AU  - Zhou, Hao
AU  - Liu, Yang
AU  - Li, Hang
AU  - Huang, Minlie
AB  - Non-autoregressive Transformers (NATs) significantly reduce the decoding latency by generating all tokens in parallel. However, such independent predictions prevent NATs from capturing the dependencies between the tokens for generating multiple possible translations. In this paper, we propose Directed Acyclic Transfomer (DATransformer), which represents the hidden states in a Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a specific translation. The whole DAG simultaneously captures multiple translations and facilitates fast predictions in a non-autoregressive fashion. Experiments on the raw training data of WMT benchmark show that DA-Transformer substantially outperforms previous NATs by about 3 BLEU on average, which is the first NAT model that achieves competitive results with autoregressive Transformers without relying on knowledge distillation.
DA  - 2022///
PY  - 2022
DP  - Zotero
LA  - en
L1  - https://proceedings.mlr.press/v162/huang22m/huang22m.pdf
ER  - 

TY  - GEN
TI  - Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation
AU  - Ke, Pei
AU  - Ji, Haozhe
AU  - Yang, Zhenyu
AU  - Huang, Yi
AU  - Feng, Junlan
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
AB  - Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation.
DA  - 2022/06/06/
PY  - 2022
DO  - 10.48550/arXiv.2206.02712
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.02712
Y2  - 2024/11/14/15:44:44
L1  - http://arxiv.org/pdf/2206.02712v1
L2  - https://arxiv.org/abs/2206.02712
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation
AU  - Guan, Jian
AU  - Feng, Zhuoer
AU  - Chen, Yamei
AU  - He, Ruilin
AU  - Mao, Xiaoxi
AU  - Fan, Changjie
AU  - Huang, Minlie
T2  - Transactions of the Association for Computational Linguistics
AB  - Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT.
DA  - 2022/04/11/
PY  - 2022
DO  - 10.1162/tacl_a_00469
DP  - DOI.org (Crossref)
VL  - 10
SP  - 434
EP  - 451
LA  - en
SN  - 2307-387X
ST  - LOT
UR  - https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00469/110537/LOT-A-Story-Centric-Benchmark-for-Evaluating
Y2  - 2024/11/14/15:44:52
L1  - https://aclanthology.org/2022.tacl-1.25.pdf
ER  - 

TY  - CONF
TI  - On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark
AU  - Sun, Hao
AU  - Xu, Guangxuan
AU  - Deng, Jiawen
AU  - Cheng, Jiale
AU  - Zheng, Chujie
AU  - Zhou, Hao
AU  - Peng, Nanyun
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - Findings 2022
A2  - Muresan, Smaranda
A2  - Nakov, Preslav
A2  - Villavicencio, Aline
AB  - Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems.
C1  - Dublin, Ireland
C3  - Findings of the Association for Computational Linguistics: ACL 2022
DA  - 2022/05//
PY  - 2022
DO  - 10.18653/v1/2022.findings-acl.308
DP  - ACLWeb
SP  - 3906
EP  - 3923
PB  - Association for Computational Linguistics
ST  - On the Safety of Conversational Models
UR  - https://aclanthology.org/2022.findings-acl.308
Y2  - 2024/11/14/15:44:55
L1  - https://aclanthology.org/2022.findings-acl.308.pdf
ER  - 

TY  - CONF
TI  - Rethinking and Refining the Distinct Metric
AU  - Liu, Siyang
AU  - Sabour, Sahand
AU  - Zheng, Yinhe
AU  - Ke, Pei
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - ACL 2022
A2  - Muresan, Smaranda
A2  - Nakov, Preslav
A2  - Villavicencio, Aline
AB  - Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct.
C1  - Dublin, Ireland
C3  - Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)
DA  - 2022/05//
PY  - 2022
DO  - 10.18653/v1/2022.acl-short.86
DP  - ACLWeb
SP  - 762
EP  - 770
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.acl-short.86
Y2  - 2024/11/14/15:45:04
L1  - https://aclanthology.org/2022.acl-short.86.pdf
ER  - 

TY  - CONF
TI  - CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation
AU  - Ke, Pei
AU  - Zhou, Hao
AU  - Lin, Yankai
AU  - Li, Peng
AU  - Zhou, Jie
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
AB  - Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overﬁt task-speciﬁc data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text inﬁlling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities1.
C1  - Dublin, Ireland
C3  - Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2022///
PY  - 2022
DO  - 10.18653/v1/2022.acl-long.164
DP  - DOI.org (Crossref)
SP  - 2306
EP  - 2319
LA  - en
PB  - Association for Computational Linguistics
ST  - CTRLEval
UR  - https://aclanthology.org/2022.acl-long.164
Y2  - 2024/11/14/15:45:14
L1  - https://aclanthology.org/2022.acl-long.164.pdf
ER  - 

TY  - CONF
TI  - Continual Prompt Tuning for Dialog State Tracking
AU  - Zhu, Qi
AU  - Li, Bing
AU  - Mi, Fei
AU  - Zhu, Xiaoyan
AU  - Huang, Minlie
T2  - ACL 2022
A2  - Muresan, Smaranda
A2  - Nakov, Preslav
A2  - Villavicencio, Aline
AB  - A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens' embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines.
C1  - Dublin, Ireland
C3  - Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2022/05//
PY  - 2022
DO  - 10.18653/v1/2022.acl-long.80
DP  - ACLWeb
SP  - 1124
EP  - 1137
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.acl-long.80
Y2  - 2024/11/14/15:45:21
L1  - https://aclanthology.org/2022.acl-long.80v2.pdf
ER  - 

TY  - CONF
TI  - Learning Goal-oriented Dialogue Policy with opposite Agent Awareness
AU  - Zhang, Zheng
AU  - Liao, Lizi
AU  - Zhu, Xiaoyan
AU  - Chua, Tat-Seng
AU  - Liu, Zitao
AU  - Huang, Yan
AU  - Huang, Minlie
T2  - AACL 2020
A2  - Wong, Kam-Fai
A2  - Knight, Kevin
A2  - Wu, Hua
AB  - Most existing approaches for goal-oriented dialogue policy learning used reinforcement learning, which focuses on the target agent policy and simply treats the opposite agent policy as part of the environment. While in real-world scenarios, the behavior of an opposite agent often exhibits certain patterns or underlies hidden policies, which can be inferred and utilized by the target agent to facilitate its own decision making. This strategy is common in human mental simulation by first imaging a specific action and the probable results before really acting it. We therefore propose an opposite behavior aware framework for policy learning in goal-oriented dialogues. We estimate the opposite agent's policy from its behavior and use this estimation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines.
C1  - Suzhou, China
C3  - Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing
DA  - 2020/12//
PY  - 2020
DP  - ACLWeb
SP  - 122
EP  - 132
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2020.aacl-main.16
Y2  - 2024/11/14/15:47:13
L1  - https://aclanthology.org/2020.aacl-main.16.pdf
ER  - 

TY  - CONF
TI  - Robustness Testing of Language Understanding in Task-Oriented Dialog
AU  - Liu, Jiexi
AU  - Takanobu, Ryuichi
AU  - Wen, Jiaxin
AU  - Wan, Dazhen
AU  - Li, Hongguang
AU  - Nie, Weiran
AU  - Li, Cheng
AU  - Peng, Wei
AU  - Huang, Minlie
T2  - ACL-IJCNLP 2021
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021/08//
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.192
DP  - ACLWeb
SP  - 2467
EP  - 2480
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.acl-long.192
Y2  - 2024/11/14/15:47:17
L1  - https://aclanthology.org/2021.acl-long.192.pdf
ER  - 

TY  - JOUR
TI  - ROBUSTNESS VERIFICATION FOR TRANSFORMERS
AU  - Shi, Zhouxing
AU  - Zhang, Huan
AU  - Chang, Kai-Wei
AU  - Huang, Minlie
AU  - Hsieh, Cho-Jui
AB  - Robustness veriﬁcation that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness veriﬁcation problem for Transformers. Transformers have complex self-attention layers that pose many challenges for veriﬁcation, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the ﬁrst robustness veriﬁcation algorithm for Transformers. The certiﬁed robustness bounds computed by our method are signiﬁcantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reﬂect the importance of different words in sentiment analysis.
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://openreview.net/pdf?id=BJxwPJHFwS
ER  - 

TY  - CONF
TI  - A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering
AU  - Shao, Zhihong
AU  - Shang, Lifeng
AU  - Liu, Qun
AU  - Huang, Minlie
T2  - ACL-IJCNLP 2021
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021/08//
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.318
DP  - ACLWeb
SP  - 4111
EP  - 4124
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.acl-long.318
Y2  - 2024/11/14/15:47:19
L1  - https://aclanthology.org/2021.acl-long.318.pdf
ER  - 

TY  - CONF
TI  - Diversifying Dialog Generation via Adaptive Label Smoothing
AU  - Wang, Yida
AU  - Zheng, Yinhe
AU  - Jiang, Yong
AU  - Huang, Minlie
T2  - ACL-IJCNLP 2021
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021/08//
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.272
DP  - ACLWeb
SP  - 3507
EP  - 3520
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.acl-long.272
Y2  - 2024/11/14/15:47:21
L1  - https://aclanthology.org/2021.acl-long.272.pdf
ER  - 

TY  - CONF
TI  - Towards Emotional Support Dialog Systems
AU  - Liu, Siyang
AU  - Zheng, Chujie
AU  - Demasi, Orianna
AU  - Sabour, Sahand
AU  - Li, Yu
AU  - Yu, Zhou
AU  - Jiang, Yong
AU  - Huang, Minlie
T2  - ACL-IJCNLP 2021
A2  - Zong, Chengqing
A2  - Xia, Fei
A2  - Li, Wenjie
A2  - Navigli, Roberto
AB  - Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.
C1  - Online
C3  - Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
DA  - 2021/08//
PY  - 2021
DO  - 10.18653/v1/2021.acl-long.269
DP  - ACLWeb
SP  - 3469
EP  - 3483
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2021.acl-long.269
Y2  - 2024/11/14/15:47:23
L1  - https://aclanthology.org/2021.acl-long.269.pdf
ER  - 

TY  - CONF
TI  - AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning
AU  - Wen, Jiaxin
AU  - Zhu, Yeshuang
AU  - Zhang, Jinchao
AU  - Zhou, Jie
AU  - Huang, Minlie
T2  - Findings 2022
A2  - Goldberg, Yoav
A2  - Kozareva, Zornitsa
A2  - Zhang, Yue
AB  - Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models' reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals, thereby impeding CAD's applicability to a broad range of NLU tasks. In this paper, we present AutoCAD, a fully automatic and task-agnostic CAD generation framework. AutoCAD first leverages a classifier to unsupervisedly identify rationales as spans to be intervened, which disentangles spurious and causal features. Then, AutoCAD performs controllable generation enhanced by unlikelihood training to produce diverse counterfactuals. Extensive evaluations on multiple out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently and significantly boosts the out-of-distribution performance of powerful pre-trained models across different NLU tasks, which is comparable or even better than previous state-of-the-art human-in-the-loop or task-specific CAD methods.
C1  - Abu Dhabi, United Arab Emirates
C3  - Findings of the Association for Computational Linguistics: EMNLP 2022
DA  - 2022/12//
PY  - 2022
DO  - 10.18653/v1/2022.findings-emnlp.170
DP  - ACLWeb
SP  - 2302
EP  - 2317
PB  - Association for Computational Linguistics
ST  - AutoCAD
UR  - https://aclanthology.org/2022.findings-emnlp.170
Y2  - 2024/11/14/15:47:25
L1  - https://aclanthology.org/2022.findings-emnlp.170.pdf
ER  - 

TY  - CONF
TI  - Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation
AU  - Zhang, Zhexin
AU  - Cheng, Jiale
AU  - Sun, Hao
AU  - Deng, Jiawen
AU  - Mi, Fei
AU  - Wang, Yasheng
AU  - Shang, Lifeng
AU  - Huang, Minlie
T2  - Findings 2022
A2  - Goldberg, Yoav
A2  - Kozareva, Zornitsa
A2  - Zhang, Yue
AB  - Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at https://github.com/thu-coai/Reverse_Generation.
C1  - Abu Dhabi, United Arab Emirates
C3  - Findings of the Association for Computational Linguistics: EMNLP 2022
DA  - 2022/12//
PY  - 2022
DO  - 10.18653/v1/2022.findings-emnlp.270
DP  - ACLWeb
SP  - 3684
EP  - 3697
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.findings-emnlp.270
Y2  - 2024/11/14/15:47:26
L1  - https://aclanthology.org/2022.findings-emnlp.270.pdf
ER  - 

TY  - GEN
TI  - SafetyBench: Evaluating the Safety of Large Language Models
AU  - Zhang, Zhexin
AU  - Lei, Leqi
AU  - Wu, Lindong
AU  - Sun, Rui
AU  - Huang, Yongkang
AU  - Long, Chong
AU  - Liu, Xiao
AU  - Lei, Xuanyu
AU  - Tang, Jie
AU  - Huang, Minlie
AB  - With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.
DA  - 2024/06/24/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - SafetyBench
UR  - http://arxiv.org/abs/2309.07045
Y2  - 2024/11/14/15:48:01
L1  - https://arxiv.org/pdf/2309.07045
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - AlignBench: Benchmarking Chinese Alignment of Large Language Models
AU  - Liu, Xiao
AU  - Lei, Xuanyu
AU  - Wang, Shengyuan
AU  - Huang, Yue
AU  - Feng, Zhuoer
AU  - Wen, Bosi
AU  - Cheng, Jiale
AU  - Ke, Pei
AU  - Xu, Yifan
AU  - Tam, Weng Lam
AU  - Zhang, Xiaohan
AU  - Sun, Lichao
AU  - Gu, Xiaotao
AU  - Wang, Hongning
AU  - Zhang, Jing
AU  - Huang, Minlie
AU  - Dong, Yuxiao
AU  - Tang, Jie
AB  - Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still largely unexplored. To fill in this gap, we introduce ALIGNBENCH, a comprehensive multi-dimensional benchmark for evaluating LLMs’ alignment in Chinese. We design a human-in-the-loop data curation pipeline, containing eight main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledgeintensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLMas-Judge (Zheng et al., 2023) approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. All evaluation code, data, and LLM generations are available at https:// github.com/THUDM/AlignBench. Since its release, AlignBench has been adopted by top (Chinese) LLMs for evaluating their alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi, Baichuan, and Abab.
DA  - 2024/08/25/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - AlignBench
UR  - http://arxiv.org/abs/2311.18743
Y2  - 2024/11/14/15:48:21
L1  - https://arxiv.org/pdf/2311.18743
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Multi-horizon representations with hierarchical forward models for reinforcement learning
AU  - McInroe, Trevor
AU  - Schäfer, Lukas
AU  - Albrecht, Stefano V.
T2  - Transactions on Machine Learning Research (TMLR)
AB  - Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions. Instead, we propose Hierarchical k-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks and find that HKSL either reaches higher episodic returns or converges to maximum performance more quickly than several current baselines. Also, we find that levels in HKSL's hierarchy can learn to specialize in long- or short-term consequences of agent actions, thereby providing the downstream control policy with more informative representations. Finally, we determine that communication channels between hierarchy levels organize information based on both sides of the communication process, which improves sample efficiency.
DA  - 2024///
PY  - 2024
ER  - 

TY  - GEN
TI  - Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras
AU  - Dunion, Mhairi
AU  - Albrecht, Stefano V.
AB  - The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images. Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL. However, hardware constraints may limit the availability of multiple cameras in real-world deployment. Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training. To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that is robust to a reduction in the number of cameras to generalise to any single camera from the training set. Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific. We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera.
DA  - 2024/06/21/
PY  - 2024
DO  - 10.48550/arXiv.2404.14064
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2404.14064
Y2  - 2024/11/14/16:00:48
L1  - http://arxiv.org/pdf/2404.14064v2
L2  - https://arxiv.org/abs/2404.14064
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning
AU  - McInroe, Trevor
AU  - Jelley, Adam
AU  - Albrecht, Stefano V.
AU  - Storkey, Amos
AB  - Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modification, and UCB methods are myopic and it is unclear which learned-component's ensemble to use for action selection. We then introduce an algorithm for planning to go out-of-distribution (PTGOOD) that avoids these issues. PTGOOD uses a non-myopic planning procedure that targets exploration in relatively high-reward regions of the state-action space unlikely to be visited by the behavior policy. By leveraging concepts from the Conditional Entropy Bottleneck, PTGOOD encourages data collected online to provide new information relevant to improving the final deployment policy without altering rewards. We show empirically in several continuous control tasks that PTGOOD significantly improves agent returns during online fine-tuning and avoids the suboptimal policy convergence that many of our baselines exhibit in several environments.
DA  - 2024/06/21/
PY  - 2024
DO  - 10.48550/arXiv.2310.05723
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.05723
Y2  - 2024/11/14/16:00:49
L1  - http://arxiv.org/pdf/2310.05723v3
L2  - https://arxiv.org/abs/2310.05723
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design
AU  - Garcin, Samuel
AU  - Doran, James
AU  - Guo, Shangmin
AU  - Lucas, Christopher G.
AU  - Albrecht, Stefano V.
AB  - Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce data-regularised environment design (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods. Our code and experimental data are available at https://github.com/uoe-agents/dred.
DA  - 2024/06/11/
PY  - 2024
DO  - 10.48550/arXiv.2402.03479
DP  - arXiv.org
PB  - arXiv
ST  - DRED
UR  - http://arxiv.org/abs/2402.03479
Y2  - 2024/11/14/16:00:51
L1  - http://arxiv.org/pdf/2402.03479v4
L2  - https://arxiv.org/abs/2402.03479
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
AU  - Fosong, Elliot
AU  - Rahman, Arrasy
AU  - Carlucho, Ignacio
AU  - Albrecht, Stefano V.
AB  - Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, and propose a simple and scalable method to address these problems which augments existing actor-critic algorithms. We demonstrate the empirical benefits of our proposed method, enabling sub-task decomposition approaches to be deployed in diverse multi-agent tasks.
DA  - 2024/02/15/
PY  - 2024
DO  - 10.48550/arXiv.2302.04944
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.04944
Y2  - 2024/11/14/16:00:53
L1  - http://arxiv.org/pdf/2302.04944v2
L2  - https://arxiv.org/abs/2302.04944
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Causal Explanations for Sequential Decision-Making in Multi-Agent Systems
AU  - Gyevnar, Balint
AU  - Wang, Cheng
AU  - Lucas, Christopher G.
AU  - Cohen, Shay B.
AU  - Albrecht, Stefano V.
AB  - We present CEMA: Causal Explanations in Multi-Agent systems; a framework for creating causal natural language explanations of an agent's decisions in dynamic sequential multi-agent systems to build more trustworthy autonomous agents. Unlike prior work that assumes a fixed causal structure, CEMA only requires a probabilistic model for forward-simulating the state of the system. Using such a model, CEMA simulates counterfactual worlds that identify the salient causes behind the agent's decisions. We evaluate CEMA on the task of motion planning for autonomous driving and test it in diverse simulated scenarios. We show that CEMA correctly and robustly identifies the causes behind the agent's decisions, even when a large number of other agents is present, and show via a user study that CEMA's explanations have a positive effect on participants' trust in autonomous vehicles and are rated as high as high-quality baseline explanations elicited from other participants. We release the collected explanations with annotations as the HEADD dataset.
DA  - 2024/02/14/
PY  - 2024
DO  - 10.48550/arXiv.2302.10809
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2302.10809
Y2  - 2024/11/14/16:00:57
L1  - http://arxiv.org/pdf/2302.10809v4
L2  - https://arxiv.org/abs/2302.10809
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning
AU  - Azran, Guy
AU  - Danesh, Mohamad H.
AU  - Albrecht, Stefano V.
AU  - Keren, Sarah
AB  - Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RMs), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Empirical results show that our representations improve sample efficiency and few-shot transfer in a variety of domains.
DA  - 2024/02/21/
PY  - 2024
DO  - 10.48550/arXiv.2307.05209
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.05209
Y2  - 2024/11/14/16:01:00
L1  - http://arxiv.org/pdf/2307.05209v4
L2  - https://arxiv.org/abs/2307.05209
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - lpNTK: Better Generalisation with Less Data via Sample Interaction During Learning
AU  - Guo, Shangmin
AU  - Ren, Yi
AU  - Albrecht, Stefano V.
AU  - Smith, Kenny
AB  - Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of samples and forgetting events during learning. Moreover, we also show that using lpNTK to identify and remove poisoning training samples does not hurt the generalisation performance of ANNs.
DA  - 2024/05/14/
PY  - 2024
DO  - 10.48550/arXiv.2401.08808
DP  - arXiv.org
PB  - arXiv
ST  - lpNTK
UR  - http://arxiv.org/abs/2401.08808
Y2  - 2024/11/14/16:01:02
L1  - http://arxiv.org/pdf/2401.08808v2
L2  - https://arxiv.org/abs/2401.08808
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots
AU  - Han, Dongge
AU  - McInroe, Trevor
AU  - Jelley, Adam
AU  - Albrecht, Stefano V.
AU  - Bell, Peter
AU  - Storkey, Amos
AB  - Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.
DA  - 2024/04/22/
PY  - 2024
DO  - 10.48550/arXiv.2404.14285
DP  - arXiv.org
PB  - arXiv
ST  - LLM-Personalize
UR  - http://arxiv.org/abs/2404.14285
Y2  - 2024/11/14/16:02:18
L1  - http://arxiv.org/pdf/2404.14285v1
L2  - https://arxiv.org/abs/2404.14285
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning
AU  - Yu, Xuehui
AU  - Dunion, Mhairi
AU  - Li, Xin
AU  - Albrecht, Stefano V.
AB  - Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviour). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\log$-$K$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the $\log$-$K$ curse.
DA  - 2024/11/06/
PY  - 2024
DO  - 10.48550/arXiv.2406.04815
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2406.04815
Y2  - 2024/11/14/16:02:19
L1  - http://arxiv.org/pdf/2406.04815v3
L2  - https://arxiv.org/abs/2406.04815
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Robotics
ER  - 

TY  - GEN
TI  - A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning
AU  - Rahman, Arrasy
AU  - Carlucho, Ignacio
AU  - Höpner, Niklas
AU  - Albrecht, Stefano V.
AB  - Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully observable case to compute an agent's optimal policy under partial observability in open ad hoc teamwork. Empirical results demonstrate that our solution can learn efficient policies in open ad hoc teamwork in fully and partially observable cases. Further analysis demonstrates that our methods' success is a result of effectively learning the effects of teammates' actions while also inferring the inherent state of the environment under partial observability.
DA  - 2023/10/28/
PY  - 2023
DO  - 10.48550/arXiv.2210.05448
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2210.05448
Y2  - 2024/11/14/16:02:21
L1  - http://arxiv.org/pdf/2210.05448v2
L2  - https://arxiv.org/abs/2210.05448
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity
AU  - Rahman, Arrasy
AU  - Fosong, Elliot
AU  - Carlucho, Ignacio
AU  - Albrecht, Stefano V.
AB  - Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.
DA  - 2023/05/24/
PY  - 2023
DO  - 10.48550/arXiv.2207.14138
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.14138
Y2  - 2024/11/14/16:02:23
L1  - http://arxiv.org/pdf/2207.14138v3
L2  - https://arxiv.org/abs/2207.14138
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Conditional Mutual Information for Disentangled Representations in Reinforcement Learning
AU  - Dunion, Mhairi
AU  - McInroe, Trevor
AU  - Luck, Kevin Sebastian
AU  - Hanna, Josiah P.
AU  - Albrecht, Stefano V.
AB  - Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the conditional mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well as improving the training performance of RL algorithms in the presence of correlated features.
DA  - 2023/10/12/
PY  - 2023
DO  - 10.48550/arXiv.2305.14133
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2305.14133
Y2  - 2024/11/14/16:02:24
L1  - http://arxiv.org/pdf/2305.14133v2
L2  - https://arxiv.org/abs/2305.14133
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning
AU  - Schäfer, Lukas
AU  - Christianos, Filippos
AU  - Storkey, Amos
AU  - Albrecht, Stefano V.
AB  - Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks.
DA  - 2023/11/20/
PY  - 2023
DO  - 10.48550/arXiv.2207.02249
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.02249
Y2  - 2024/11/14/16:02:26
L1  - http://arxiv.org/pdf/2207.02249v2
L2  - https://arxiv.org/abs/2207.02249
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - How the level sampling process impacts zero-shot generalisation in deep reinforcement learning
AU  - Garcin, Samuel
AU  - Doran, James
AU  - Guo, Shangmin
AU  - Lucas, Christopher G.
AU  - Albrecht, Stefano V.
AB  - A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (UED) methods, which adaptively generate new training levels and minimise MI more effectively than methods sampling from a fixed set. However, we find UED methods significantly shift the training distribution, resulting in over-generalisation and worse ZSG performance over the distribution of interest. To prevent both instance overfitting and over-generalisation, we introduce self-supervised environment design (SSED). SSED generates levels using a variational autoencoder, effectively reducing MI while minimising the shift with the distribution of interest, and leads to statistically significant improvements in ZSG over fixed-set level sampling strategies and UED methods.
DA  - 2023/12/11/
PY  - 2023
DO  - 10.48550/arXiv.2310.03494
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2310.03494
Y2  - 2024/11/14/16:02:28
L1  - http://arxiv.org/pdf/2310.03494v2
L2  - https://arxiv.org/abs/2310.03494
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning
AU  - McCallum, Sabrina
AU  - Taylor-Davies, Max
AU  - Albrecht, Stefano V.
AU  - Suglia, Alessandro
AB  - Despite numerous successes, the field of reinforcement learning (RL) remains far from matching the impressive generalisation power of human behaviour learning. One possible way to help bridge this gap be to provide RL agents with richer, more human-like feedback expressed in natural language. To investigate this idea, we first extend BabyAI to automatically generate language feedback from the environment dynamics and goal condition success. Then, we modify the Decision Transformer architecture to take advantage of this additional signal. We find that training with language feedback either in place of or in addition to the return-to-go or goal descriptions improves agents' generalisation performance, and that agents can benefit from feedback even when this is only available during training, but not at inference.
DA  - 2023/12/07/
PY  - 2023
DO  - 10.48550/arXiv.2312.04736
DP  - arXiv.org
PB  - arXiv
ST  - Is Feedback All You Need?
UR  - http://arxiv.org/abs/2312.04736
Y2  - 2024/11/14/16:02:29
L1  - http://arxiv.org/pdf/2312.04736v1
L2  - https://arxiv.org/abs/2312.04736
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning
AU  - Dunion, Mhairi
AU  - McInroe, Trevor
AU  - Luck, Kevin Sebastian
AU  - Hanna, Josiah P.
AU  - Albrecht, Stefano V.
AB  - Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).
DA  - 2023/02/27/
PY  - 2023
DO  - 10.48550/arXiv.2207.05480
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.05480
Y2  - 2024/11/14/16:02:31
L1  - http://arxiv.org/pdf/2207.05480v4
L2  - https://arxiv.org/abs/2207.05480
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments
AU  - Andres, Alain
AU  - Schäfer, Lukas
AU  - Villar-Rodriguez, Esther
AU  - Albrecht, Stefano V.
AU  - Ser, Javier Del
AB  - One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently during online RL training both consistently improve the sample-efficiency while converging to optimal policies. Furthermore, we show that pre-training a policy from as few as two trajectories can make the difference between learning an optimal policy at the end of online training and not learning at all. Our findings motivate the widespread adoption of IL for pre-training and concurrent IL in procedurally generated environments whenever offline trajectories are available or can be generated.
DA  - 2023/04/18/
PY  - 2023
DO  - 10.48550/arXiv.2304.09825
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2304.09825
Y2  - 2024/11/14/16:04:43
L1  - http://arxiv.org/pdf/2304.09825v1
L2  - https://arxiv.org/abs/2304.09825
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning
AU  - Zhong, Rujie
AU  - Zhang, Duohan
AU  - Schäfer, Lukas
AU  - Albrecht, Stefano V.
AU  - Hanna, Josiah P.
AB  - Reinforcement learning (RL) algorithms are often categorized as either on-policy or off-policy depending on whether they use data from a target policy of interest or from a different behavior policy. In this paper, we study a subtle distinction between on-policy data and on-policy sampling in the context of the RL sub-problem of policy evaluation. We observe that on-policy sampling may fail to match the expected distribution of on-policy data after observing only a finite number of trajectories and this failure hinders data-efficient policy evaluation. Towards improved data-efficiency, we show how non-i.i.d., off-policy sampling can produce data that more closely matches the expected on-policy data distribution and consequently increases the accuracy of the Monte Carlo estimator for policy evaluation. We introduce a method called Robust On-Policy Sampling and demonstrate theoretically and empirically that it produces data that converges faster to the expected on-policy distribution compared to on-policy sampling. Empirically, we show that this faster convergence leads to lower mean squared error policy value estimates.
DA  - 2022/10/10/
PY  - 2022
DO  - 10.48550/arXiv.2111.14552
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2111.14552
Y2  - 2024/11/14/16:04:44
L1  - http://arxiv.org/pdf/2111.14552v2
L2  - https://arxiv.org/abs/2111.14552
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning
AU  - McInroe, Trevor
AU  - Schäfer, Lukas
AU  - Albrecht, Stefano V.
AB  - Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may cause learning inefficiencies if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple representations via a hierarchy of forward models that learn to communicate and an ensemble of $n$-step critics that all operate at varying magnitudes of step skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher or optimal episodic returns more quickly than several alternative representation learning approaches. Furthermore, we find that HKSL's representations capture task-relevant details accurately across timescales (even in the presence of distractors) and that communication channels between hierarchy levels organize information based on both sides of the communication process, both of which improve sample efficiency.
DA  - 2024/01/29/
PY  - 2024
DO  - 10.48550/arXiv.2206.11396
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.11396
Y2  - 2024/11/14/16:04:46
L1  - http://arxiv.org/pdf/2206.11396v2
L2  - https://arxiv.org/abs/2206.11396
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Few-Shot Teamwork
AU  - Fosong, Elliot
AU  - Rahman, Arrasy
AU  - Carlucho, Ignacio
AU  - Albrecht, Stefano V.
AB  - We propose the novel few-shot teamwork (FST) problem, where skilled agents trained in a team to complete one task are combined with skilled agents from different tasks, and together must learn to adapt to an unseen but related task. We discuss how the FST problem can be seen as addressing two separate problems: one of reducing the experience required to train a team of agents to complete a complex task; and one of collaborating with unfamiliar teammates to complete a new task. Progress towards solving FST could lead to progress in both multi-agent reinforcement learning and ad hoc teamwork.
DA  - 2022/07/19/
PY  - 2022
DO  - 10.48550/arXiv.2207.09300
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.09300
Y2  - 2024/11/14/16:04:50
L1  - http://arxiv.org/pdf/2207.09300v1
L2  - https://arxiv.org/abs/2207.09300
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Multiagent Systems
ER  - 

TY  - GEN
TI  - Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction
AU  - Ahmed, Ibrahim H.
AU  - Hanna, Josiah P.
AU  - Fosong, Elliot
AU  - Albrecht, Stefano V.
AB  - Current methods for authentication and key agreement based on public-key cryptography are vulnerable to quantum computing. We propose a novel approach based on artificial intelligence research in which communicating parties are viewed as autonomous agents which interact repeatedly using their private decision models. Authentication and key agreement are decided based on the agents' observed behaviors during the interaction. The security of this approach rests upon the difficulty of modeling the decisions of interacting agents from limited observations, a problem which we conjecture is also hard for quantum computing. We release PyAMI, a prototype authentication and key agreement system based on the proposed method. We empirically validate our method for authenticating legitimate users while detecting different types of adversarial attacks. Finally, we show how reinforcement learning techniques can be used to train server models which effectively probe a client's decisions to achieve more sample-efficient authentication.
DA  - 2021/07/09/
PY  - 2021
DO  - 10.48550/arXiv.2007.09327
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2007.09327
Y2  - 2024/11/14/16:04:52
L1  - http://arxiv.org/pdf/2007.09327v2
L2  - https://arxiv.org/abs/2007.09327
KW  - Computer Science - Cryptography and Security
KW  - Computer Science - Machine Learning
KW  - Computer Science - Multiagent Systems
ER  - 

