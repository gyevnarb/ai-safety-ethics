[
	{
		"id": "ganRiskDegreebasedSafe2016",
		"type": "article-journal",
		"abstract": "Semi-supervised learning has attracted much attention in machine learning field over the past decades and a number of algorithms are proposed to improve the performance by exploiting unlabeled data. However, unlabeled data may hurt performance of semi-supervised learning in some cases. It is instinctively expected to design a reasonable strategy to safety exploit unlabeled data. To address the problem, we introduce a safe semi-supervised learning by analyzing the different characteristics of unlabeled data in supervised and semi-supervised learning. Our intuition is that unlabeled data may be often risky in semi-supervised setting and the risk degree are different. Hence, we assign different risk degree to unlabeled data and the risk degree serve as a sieve to determine the exploiting way of unlabeled data. The unlabeled data with high risk should be exploited by supervised learning and the other should be used for semi-supervised learning. In particular, we utilize kernel minimum squared error (KMSE) and Laplacian regularized KMSE for supervised and semi-supervised learning, respectively. Experimental results on several benchmark datasets illustrate the performance of our algorithm is never inferior to that of KMSE and indicate the effectiveness and efficiency of our algorithm.",
		"archive_location": "WOS:000368167400006",
		"container-title": "INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS",
		"DOI": "10.1007/s13042-015-0416-8",
		"ISSN": "1868-8071",
		"issue": "1",
		"page": "85-94",
		"title": "A risk degree-based safe semi-supervised learning algorithm",
		"volume": "7",
		"author": [
			{
				"family": "Gan",
				"given": "HT"
			},
			{
				"family": "Luo",
				"given": "ZZ"
			},
			{
				"family": "Meng",
				"given": "M"
			},
			{
				"family": "Ma",
				"given": "YL"
			},
			{
				"family": "She",
				"given": "QS"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016",
					2
				]
			]
		}
	},
	{
		"id": "zhaoSafeSemisupervisedClassification2018",
		"type": "article-journal",
		"abstract": "In order to improve the performance of semi-supervised learning, a kind of safe semi-supervised classification algorithm based active learning sampling strategy is proposed. First, an active learning sampling method based on uncertainty and representativenes is designed. The weighted algorithm combining the uncertainty and representativenesss is used to select the unlabeled samples with rich information and representation, providing for semi-supervised learning. Second, a method of label prediction based on grouping verification is designed. Prelabeling is executed on unlabeled sample selected by active learning. The sample with pseudo-label is added into the labeled sample set to carry out grouping, training and testing. The corresponding errors of various pseudo-labels are calculated and the pseudo-label making the accuracy least is selected as the candidate label of the unlabeled sample. Third, a method of security verification is designed. Only the label making the accuracy lower than before is selected as the final label of the unlabeled sample to expand the number of labeled samples. Iterations are repeatedly executed until a certain precision is met. Finally, the classifier is trained using the final labeled set. The experiments are carried out on semi-supervised datasets and UCI datasets, and the results show that the proposed algorithms are effective.",
		"archive_location": "WOS:000451338400007",
		"container-title": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS",
		"DOI": "10.3233/JIFS-169722",
		"ISSN": "1064-1246",
		"issue": "4",
		"page": "4001-4010",
		"title": "Safe semi-supervised classification algorithm combined with active learning sampling strategy",
		"volume": "35",
		"author": [
			{
				"family": "Zhao",
				"given": "JH"
			},
			{
				"family": "Liu",
				"given": "N"
			},
			{
				"family": "Malov",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "carlsonProvablySafeArtificial2021",
		"type": "article-journal",
		"abstract": "Methods are currently lacking to prove artificial general intelligence (AGI) safety. An AGI 'hard takeoff' is possible, in which first generation AGI(1) rapidly triggers a succession of more powerful AGI(n) that differ dramatically in their computational capabilities (AGI(n) << AGI(n+1)). No proof exists that AGI will benefit humans or of a sound value-alignment method. Numerous paths toward human extinction or subjugation have been identified. We suggest that probabilistic proof methods are the fundamental paradigm for proving safety and value-alignment between disparately powerful autonomous agents. Interactive proof systems (IPS) describe mathematical communication protocols wherein a Verifier queries a computationally more powerful Prover and reduces the probability of the Prover deceiving the Verifier to any specified low probability (e.g., 2(-100)). IPS procedures can test AGI behavior control systems that incorporate hard-coded ethics or value-learning methods. Mapping the axioms and transformation rules of a behavior control system to a finite set of prime numbers allows validation of 'safe' behavior via IPS number-theoretic methods. Many other representations are needed for proving various AGI properties. Multi-prover IPS, program-checking IPS, and probabilistically checkable proofs further extend the paradigm. In toto, IPS provides a way to reduce AGI(n) <-> AGI(n+1) interaction hazards to an acceptably low level.",
		"archive_location": "WOS:000738051700001",
		"container-title": "PHILOSOPHIES",
		"DOI": "10.3390/philosophies6040083",
		"ISSN": "2409-9287",
		"issue": "4",
		"title": "Provably Safe Artificial General Intelligence via Interactive Proofs",
		"volume": "6",
		"author": [
			{
				"family": "Carlson",
				"given": "K"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "johnsonMetacognitionArtificialIntelligence2022",
		"type": "article-journal",
		"abstract": "Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human-machine teaming operations. In parallel, the increasing volume, velocity, variety, ve-racity, value, and variability of data is confounding the complexity of these new systems - creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.",
		"archive_location": "WOS:000792913700003",
		"container-title": "SAFETY SCIENCE",
		"DOI": "10.1016/j.ssci.2022.105743",
		"ISSN": "0925-7535",
		"title": "Metacognition for artificial intelligence system safety-An approach to safe and desired behavior",
		"volume": "151",
		"author": [
			{
				"family": "Johnson",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "gheraibiaSafetyAINovel2019",
		"type": "article-journal",
		"abstract": "Safety-critical systems are becoming larger and more complex to obtain a higher level of functionality. Hence, modeling and evaluation of these systems can be a difficult and error-prone task. Among existing safety models, Fault Tree Analysis (FTA) is one of the well-known methods in terms of easily understandable graphical structure. This study proposes a novel approach by using Machine Learning (ML) and real-time operational data to learn about the normal behavior of the system. Afterwards, if any abnormal situation arises with reference to the normal behavior model, the approach tries to find the explanation of the abnormality on the fault tree and then share the knowledge with the operator. If the fault tree fails to explain the situation, a number of different recommendations, including the potential repair of the fault tree, are provided based on the nature of the situation. A decision tree is utilized for this purpose. The effectiveness of the proposed approach is shown through a hypothetical example of an Aircraft Fuel Distribution System (AFDS).",
		"archive_location": "WOS:000563954900139",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2019.2941566",
		"ISSN": "2169-3536",
		"page": "135855-135869",
		"title": "Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence",
		"volume": "7",
		"author": [
			{
				"family": "Gheraibia",
				"given": "Y"
			},
			{
				"family": "Kabir",
				"given": "S"
			},
			{
				"family": "Aslansefat",
				"given": "K"
			},
			{
				"family": "Sorokos",
				"given": "I"
			},
			{
				"family": "Papadopoulos",
				"given": "Y"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "deyMultilayeredReviewSafety2021",
		"type": "article-journal",
		"abstract": "The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety. (C) 2021 Elsevier Inc. All rights reserved.",
		"archive_location": "WOS:000636371400004",
		"container-title": "JOURNAL OF SYSTEMS AND SOFTWARE",
		"DOI": "10.1016/j.jss.2021.110941",
		"ISSN": "0164-1212",
		"title": "Multilayered review of safety approaches for machine learning-based systems in the days of AI",
		"volume": "176",
		"author": [
			{
				"family": "Dey",
				"given": "S"
			},
			{
				"family": "Lee",
				"given": "SW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					6
				]
			]
		}
	},
	{
		"id": "rileyAssuredDeepMultiAgent2022",
		"type": "paper-conference",
		"abstract": "Using multi-agent reinforcement learning to find solutions to complex decision-making problems in shared environments has become standard practice in many scenarios. However, this is not the case in safety-critical scenarios, where the reinforcement learning process, which uses stochastic mechanisms, could lead to highly unsafe outcomes. We proposed a novel, safe multi-agent reinforcement learning approach named Assured Multi-Agent Reinforcement Learning (AMARL) to address this issue. Distinct from other safe multi-agent reinforcement learning approaches, AMARL utilises quantitative verification, a model checking technique that guarantees agent compliance of safety, performance, and non-functional requirements, both during and after the learning process. We have previously evaluated AMARL in patrolling domains with various multi-agent reinforcement learning algorithms for both homogeneous and heterogeneous systems. In this work we extend AMARL through the use of deep multi-agent reinforcement learning. This approach is particularly appropriate for systems in which the rewards are sparse and hence extends the applicability of AMARL. We evaluate our approach within a new search and collection domain which demonstrates promising results in safety standards and performance compared to algorithms not using AMARL.",
		"archive_location": "WOS:000876376200008",
		"DOI": "10.1007/978-3-031-10161-8_8",
		"event-title": "AGENTS AND ARTIFICIAL INTELLIGENCE, ICAART 2021",
		"ISBN": "0302-9743",
		"page": "158-180",
		"title": "Assured Deep Multi-Agent Reinforcement Learning for Safe Robotic Systems",
		"volume": "13251",
		"author": [
			{
				"family": "Riley",
				"given": "J"
			},
			{
				"family": "Calinescu",
				"given": "R"
			},
			{
				"family": "Paterson",
				"given": "C"
			},
			{
				"family": "Kudenko",
				"given": "D"
			},
			{
				"family": "Banks",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Rocha",
				"given": "AP"
			},
			{
				"family": "Steels",
				"given": "L"
			},
			{
				"family": "VanDenHerik",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schumegProposedVModelVerification2023",
		"type": "paper-conference",
		"abstract": "The Department of Defense strives to continuously develop and acquire systems that utilize novel technologies and methods for implementing new and complex mission requirements. One of the identified technologies with high impact and benefit to the Warfighter is the integration of Artificial Intelligence (AI) and Machine Learning (ML). Current AI models and methods have added layers of complexity to achieving a satisfactory level of verification and validation (V&V), possibly resulting in elevated risks with fewer mitigations. Regardless of the type of applications for AI technology within the DoD, the technology implementation must be verified, validated, and ultimately any residual risks accepted. This paper looks to introduce a V-model concept for Artificial Intelligence and Machine Learning, to include an outline of proposed activities that the development, assurance, and evaluation communities can follow. By following this proposed assessment, these organizations can increase their understanding and knowledge of the system, mitigating risk and helping to achieve justified confidence.",
		"archive_location": "WOS:001050787900009",
		"DOI": "10.1109/ICAA58325.2023.00017",
		"event-title": "2023 IEEE INTERNATIONAL CONFERENCE ON ASSURED AUTONOMY, ICAA",
		"ISBN": "979-8-3503-2601-7",
		"page": "61-66",
		"title": "Proposed V-Model for Verification, Validation, and Safety Activities for Artificial Intelligence",
		"author": [
			{
				"family": "Schumeg",
				"given": "B"
			},
			{
				"family": "Marotta",
				"given": "F"
			},
			{
				"family": "Werner",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouLearningLowDimensionalRepresentation2023",
		"type": "article-journal",
		"abstract": "For the safe application of reinforcement learning algorithms to high-dimensional nonlinear dynamical systems, a simplified system model is used to formulate a safe reinforcement learning (SRL) framework. Based on the simplified system model, a low-dimensional representation of the safe region is identified and used to provide safety estimates for learning algorithms. However, finding a satisfying simplified system model for complex dynamical systems usually requires a considerable amount of effort. To overcome this limitation, we propose a general data-driven approach that is able to efficiently learn a low-dimensional representation of the safe region. By employing an online adaptation method, the low-dimensional representation is updated using the feedback data to obtain more accurate safety estimates. The performance of the proposed approach for identifying the low-dimensional representation of the safe region is illustrated using the example of a quadcopter. The results demonstrate a more reliable and representative low-dimensional representation of the safe region compared with previous works, which extends the applicability of the SRL framework.",
		"archive_location": "WOS:000733505000001",
		"container-title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS",
		"DOI": "10.1109/TNNLS.2021.3106818",
		"ISSN": "2162-237X",
		"issue": "5",
		"page": "2513-2527",
		"title": "Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems",
		"volume": "34",
		"author": [
			{
				"family": "Zhou",
				"given": "ZH"
			},
			{
				"family": "Oguz",
				"given": "OS"
			},
			{
				"family": "Leibold",
				"given": "M"
			},
			{
				"family": "Buss",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					5
				]
			]
		}
	},
	{
		"id": "guissoumaContinuousSafetyAssessment2023",
		"type": "paper-conference",
		"abstract": "Over-The-Air (OTA) updates play an essential role in the lifecycle management of modern Cyber Physical Systems (CPSs). They are deployed in short time periods to fix bugs and introduce new features. However, an important part of these updates affects safety-critical functions, and thus, requires thorough verification and validation. Particular care must be taken when using machine learning algorithms, for which it is more difficult to test all conceivable corner cases during the development process. To prevent potential unforeseen misbehavior after deployment, we introduce a method for runtime evaluation of updates in shadow mode using contract specifications. The method focuses on supervised learning models and is embedded in a workflow for iterative training. This enables carrying out reliable field testing and obtaining a realistic evaluation of the planned updates before release. Finally, we evaluate our approach on a prototype Electronic Control Unit (ECU) implementing an automotive Lane Keep Assist (LKA) system.",
		"archive_location": "WOS:000990534100053",
		"DOI": "10.1109/ICSA-C57050.2023.00069",
		"event-title": "2023 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE COMPANION, ICSA-C",
		"ISBN": "2768-427X",
		"page": "301-308",
		"title": "Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode",
		"author": [
			{
				"family": "Guissouma",
				"given": "H"
			},
			{
				"family": "Zink",
				"given": "M"
			},
			{
				"family": "Sax",
				"given": "E"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "aksjonovSafetyCriticalDecisionMakingControl2023",
		"type": "article-journal",
		"abstract": "While machine-learning-based methods suffer from a lack of transparency, rule-based (RB) methods dominate safety-critical systems. Yet the RB approaches cannot compete with the first ones in robustness to multiple system requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, this article proposes a decision-making and control framework which profits from the advantages of both the RB and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. An RB switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized whenever the Learned one does not meet the safety constraint, and also directly participates in the Learned controller training. Decision-making and control in autonomous driving are chosen as the system case study, where an autonomous vehicle (AV) learns a multitask policy to safely execute an unprotected left turn. Multiple requirements (i.e., safety, efficiency, and comfort) are set to vehicle motion. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environments is successfully demonstrated.",
		"archive_location": "WOS:001072996800003",
		"container-title": "SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH",
		"DOI": "10.4271/10-07-03-0018",
		"ISSN": "2380-2162",
		"issue": "3",
		"page": "287-299",
		"title": "A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms",
		"volume": "7",
		"author": [
			{
				"family": "Aksjonov",
				"given": "A"
			},
			{
				"family": "Kyrki",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ganDualLearningBasedSafe2018",
		"type": "article-journal",
		"abstract": "In many real-world applications, labeled instances are generally limited and expensively collected, while the most instances are unlabeled and the amount is often sufficient. Therefore, semi supervised learning (SSL) has attracted much attention, since it is an effective tool to discover the unlabeled instances. However, how to safely make use of the unlabeled instances is an emerging and interesting problem in SSL. Hence, we propose DuAL Learning-based sAfe Semi-supervised learning (DALLAS), which employs dual learning to estimate the safety or risk of the unlabeled instances. To realize the safe exploitation of the unlabeled instances, our basic idea is to use supervised learning (SL) to analyze the risk of the unlabeled instances. First, DALLAS utilizes a primal model obtained by dual learning to classify each unlabeled instance and then uses a dual model to reconstruct the unlabeled instances according to the obtained classification results. The risk can be measured by analyzing the reconstruction error and predictions of the original and reconstructed unlabeled instances. If the error is small and the predictions are equal, the unlabeled instance may be safe. Otherwise, the instance may be risky and its output should be approach to be that obtained by SL. Finally, we embed a risk-based regularization term into SSL. Hence, the outputs of our algorithm are a tradeoff between those of SL and SSL. In particular, we utilize respectively regularized least squares (RLS) and Laplacian RLS for SL and SSL. To verify the effectiveness of the proposed safe mechanism in DALLAS, we carry out a series of experiments on several data sets by the comparison with the state-of-the-art supervised, semi-supervised, and safe semi-supervised learning methods and the results demonstrate that DALLAS can effectively reduce the risk of the unlabeled instances.",
		"archive_location": "WOS:000425688200021",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2017.2784406",
		"ISSN": "2169-3536",
		"page": "2615-2621",
		"title": "Dual Learning-Based Safe Semi-Supervised Learning",
		"volume": "6",
		"author": [
			{
				"family": "Gan",
				"given": "HT"
			},
			{
				"family": "Li",
				"given": "ZH"
			},
			{
				"family": "Fan",
				"given": "YL"
			},
			{
				"family": "Luo",
				"given": "ZZ"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "yangModelFreeSafeReinforcement2023",
		"type": "article-journal",
		"abstract": "Safety is a critical concern when applying reinforcement learning (RL) to real-world control tasks. However, existing safe RL works either only consider expected safety constraint violations and fail to maintain safety guarantees, or use overly conservative safety certificate tools borrowed from safe control theory, which sacrifices reward optimization and relies on analytic system models. This letter proposes a model-free safe RL algorithm that achieves near-zero constraint violations with high rewards. Our key idea is to jointly learn a policy and a neural barrier certificate under stepwise state constraint setting. The barrier certificate is learned in a model-free manner by minimizing the violations of appropriate barrier properties on transition data collected by the policy. We extend the single-step invariant property of the barrier certificate to a multi-step version and construct the corresponding multi-step invariant loss. This loss balances the bias and variance of the barrier certificate and enhances both the safety and performance of the policy. The policy is optimized under the constraint of the multi-step invariant property using the Lagrangian method. We optimize the policy in a model-free manner by introducing an importance sampling weight in the constraint. We test our algorithm on multiple problems, including classic control tasks, robot collision avoidance, and autonomous driving. Results show that our algorithm achieves near-zero constraint violations and high performance compared to the baselines. Moreover, the learned barrier certificates successfully identify the feasible regions on multiple tasks.",
		"archive_location": "WOS:000923839100012",
		"container-title": "IEEE ROBOTICS AND AUTOMATION LETTERS",
		"DOI": "10.1109/LRA.2023.3238656",
		"ISSN": "2377-3766",
		"issue": "3",
		"page": "1295-1302",
		"title": "Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate",
		"volume": "8",
		"author": [
			{
				"family": "Yang",
				"given": "YJ"
			},
			{
				"family": "Jiang",
				"given": "YX"
			},
			{
				"family": "Liu",
				"given": "YC"
			},
			{
				"family": "Chen",
				"given": "JY"
			},
			{
				"family": "Li",
				"given": "SE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "huntVerifiablySafeExploration2021",
		"type": "paper-conference",
		"abstract": "Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We characterize safety constraints in terms of a refinement relation on Markov decision processes - rather than directly constraining the reinforcement learning algorithm so that it only takes safe actions, we instead refine the environment so that only safe actions are defined in the environment's transition structure. This has pragmatic system design benefits and, more importantly, provides a clean conceptual setting in which we are able to prove important safety and efficiency properties. These allow us to transform the constrained optimization problem of acting safely in the original environment into an unconstrained optimization in a refined environment.",
		"archive_location": "WOS:000932821700017",
		"DOI": "10.1145/3447928.3456653",
		"event-title": "HSCC2021: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (PART OF CPS-IOT WEEK)",
		"ISBN": "978-1-4503-8339-4",
		"title": "Verifiably Safe Exploration for End-to-End Reinforcement Learning",
		"author": [
			{
				"family": "Hunt",
				"given": "N"
			},
			{
				"family": "Fulton",
				"given": "N"
			},
			{
				"family": "Magliacane",
				"given": "S"
			},
			{
				"family": "Hoang",
				"given": "TN"
			},
			{
				"family": "Das",
				"given": "S"
			},
			{
				"family": "Solar-Lezama",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shethProcessKnowledgeInfusedAI2022",
		"type": "article-journal",
		"abstract": "AI has seen wide adoption for automating tasks in several domains. However, AI's use in high-value, sensitive, or safety-critical applications such as self-management for personalized health or personalized nutrition has been challenging. These require that the AI system follows guidelines or well-defined processes set by experts, community, or standards. We characterize these as process knowledge (PK). For example, to diagnose the severity of depression, the AI system should incorporate PK that is part of the clinical decision-making process, such as the Patient Health Questionnaire (PHQ-9). Likewise, a nutritionist's knowledge and dietary guidelines are needed to create food plans for diabetic patients. Furthermore, the BlackBox nature of purely data-reliant statistical AI systems falls short in providing user-understandable explanations, such as what a clinician would need to ensure and document compliance with medical guidelines before relying on a recommendation. Using the examples of mental health and cooking recipes for diabetic patients, we show why, what, and how to incorporate PK along with domain knowledge in machine learning. We discuss methods for infusing PK and present performance evaluation metrics. Support for safety and user-level explainability of the PK-infused learning improves confidence and trust in the AI system.",
		"archive_location": "WOS:000853843100019",
		"container-title": "IEEE INTERNET COMPUTING",
		"DOI": "10.1109/MIC.2022.3182349",
		"ISSN": "1089-7801",
		"issue": "5",
		"page": "76-84",
		"title": "Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety",
		"volume": "26",
		"author": [
			{
				"family": "Sheth",
				"given": "A"
			},
			{
				"family": "Gaur",
				"given": "M"
			},
			{
				"family": "Roy",
				"given": "K"
			},
			{
				"family": "Venkataraman",
				"given": "R"
			},
			{
				"family": "Khandelwal",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					1
				]
			]
		}
	},
	{
		"id": "mazouchiConflictAwareSafeReinforcement2022",
		"type": "article-journal",
		"abstract": "In this paper, a data-driven conflict-aware safe reinforcement learning (CAS-RL) algorithm is presented for control of autonomous systems. Existing safe RL results with predefined performance functions and safe sets can only provide safety and performance guarantees for a single environment or circumstance. By contrast, the presented CAS-RL algorithm provides safety and performance guarantees across a variety of circumstances that the system might encounter. This is achieved by utilizing a bilevel learning control architecture: A higher metacognitive layer leverages a data-driven receding-horizon attentional controller (RHAC) to adapt relative attention to different system's safety and performance requirements, and, a lower-layer RL controller designs control actuation signals for the system. The presented RHAC makes its meta decisions based on the reaction curve of the lower-layer RL controller using a meta-model or knowledge. More specifically, it leverages a prediction meta-model (PMM) which spans the space of all future meta trajectories using a given finite number of past meta trajectories. RHAC will adapt the system's aspiration towards performance metrics (e.g., performance weights) as well as safety boundaries to resolve conflicts that arise as mission scenarios develop. This will guarantee safety and feasibility (i.e., performance boundness) of the lower-layer RL-based control solution. It is shown that the interplay between the RHAC and the lower-layer RL controller is a bilevel optimization problem for which the leader (RHAC) operates at a lower rate than the follower (RL-based controller) and its solution guarantees feasibility and safety of the control solution. The effectiveness of the proposed framework is verified through a simulation example.",
		"archive_location": "WOS:000735515700010",
		"container-title": "IEEE-CAA JOURNAL OF AUTOMATICA SINICA",
		"DOI": "10.1109/JAS.2021.1004353",
		"ISSN": "2329-9266",
		"issue": "3",
		"page": "466-481",
		"title": "Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework",
		"volume": "9",
		"author": [
			{
				"family": "Mazouchi",
				"given": "M"
			},
			{
				"family": "Nageshrao",
				"given": "S"
			},
			{
				"family": "Modares",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "perkinsLyapunovDesignSafe2003",
		"type": "article-journal",
		"abstract": "Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.",
		"archive_location": "WOS:000184926200009",
		"container-title": "JOURNAL OF MACHINE LEARNING RESEARCH",
		"DOI": "10.1162/jmlr.2003.3.4-5.803",
		"ISSN": "1532-4435",
		"issue": "4-5",
		"page": "803-832",
		"title": "Lyapunov design for safe reinforcement learning",
		"volume": "3",
		"author": [
			{
				"family": "Perkins",
				"given": "TJ"
			},
			{
				"family": "Barto",
				"given": "AG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2003",
					5,
					15
				]
			]
		}
	},
	{
		"id": "kohjimaCensoredMarkovDecision2020",
		"type": "paper-conference",
		"abstract": "The importance of safe reinforcement learning (safe RL) is widely recognized for enhancing real world systems. In this study, we construct the censored Markov decision process (CeMDP), a new Markov Decision Process (MDP) framework that describes the interaction of environment, learner and external systems, e.g., human intervention or pre-designed controller for emergency response. We also theoretically analyze the relation of CeMDP to existing frameworks such as the semi-Markov decision process, MDP with Option (OMDP) and standard MDP; the analysis clarifies that CeMDP is a special case of OMDP and can, with environment redefinition, be represented by MDP. This finding allows us to design planning and reinforcement learning algorithms for CeMDP. We confirm the validity of the theory and algorithms by numerical experiments.",
		"archive_location": "WOS:000717663402145",
		"event-title": "2020 59TH IEEE CONFERENCE ON DECISION AND CONTROL (CDC)",
		"ISBN": "0743-1546",
		"page": "3623-3630",
		"title": "Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration with External Systems",
		"author": [
			{
				"family": "Kohjima",
				"given": "M"
			},
			{
				"family": "Takahashi",
				"given": "M"
			},
			{
				"family": "Toda",
				"given": "H"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "moradiExploringFaultParameter2020",
		"type": "paper-conference",
		"abstract": "Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.",
		"archive_location": "WOS:000853340600018",
		"DOI": "10.1109/DSN-W50199.2020.00028",
		"event-title": "50TH ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS WORKSHOPS (DSN-W 2020)",
		"ISBN": "978-1-7281-7263-7",
		"page": "102-109",
		"title": "Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection",
		"author": [
			{
				"family": "Moradi",
				"given": "M"
			},
			{
				"family": "Oakes",
				"given": "BJ"
			},
			{
				"family": "Saraoglu",
				"given": "M"
			},
			{
				"family": "Morozov",
				"given": "A"
			},
			{
				"family": "Janschek",
				"given": "K"
			},
			{
				"family": "Denil",
				"given": "J"
			},
			{
				"literal": "IEEE Comp Soc"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhuContactSafeReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.",
		"archive_location": "WOS:000908368202013",
		"DOI": "10.1109/IROS47612.2022.9981185",
		"event-title": "2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)",
		"ISBN": "2153-0858",
		"page": "2476-2482",
		"title": "A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation",
		"author": [
			{
				"family": "Zhu",
				"given": "X"
			},
			{
				"family": "Kang",
				"given": "SC"
			},
			{
				"family": "Chen",
				"given": "JY"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangSafeSelfRecoverableReinforcement2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) holds the promise of autonomous robots because it can adapt to dynamic or unknown environments by automatically learning optimal control policies from the interactions between robots and environments. However, the interactions can be unsafe to both robots and environments during the learning phase, which hinders the practical deployment of RL. Some safe RL methods have been proposed to improve the learning safety by using external or prior knowledge to guide safe actions, but it is difficult to assume having this knowledge in practical applications, especially in unknown environments. More importantly, considering failures are unavoidable in practice, current safe RL lacks the capability of recovering to safe states from failures so that the learning cannot be continued and finished. To solve these problems, we propose a safe and self-recoverable reinforcement learning framework that can predict and prohibit other unsafe actions based on known, explored unsafe actions during the exploration process, and can self-recover to a safe state when a failure occurs. The maze navigation simulation results show that our approach can not only significantly reduce the number of failures but also accelerate the convergence of reinforcement learning.",
		"archive_location": "WOS:000932071603168",
		"event-title": "2022 41ST CHINESE CONTROL CONFERENCE (CCC)",
		"ISBN": "2161-2927",
		"page": "3878-3883",
		"title": "A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots",
		"author": [
			{
				"family": "Wang",
				"given": "WQ"
			},
			{
				"family": "Zhou",
				"given": "X"
			},
			{
				"family": "Xu",
				"given": "BL"
			},
			{
				"family": "Lu",
				"given": "ML"
			},
			{
				"family": "Zhang",
				"given": "YX"
			},
			{
				"family": "Gu",
				"given": "YH"
			}
		],
		"editor": [
			{
				"family": "Li",
				"given": "Z"
			},
			{
				"family": "Sun",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sezenerInferringHumanValues2015",
		"type": "paper-conference",
		"abstract": "Aligning goals of superintelligent machines with human values is one of the ways to pursue safety in AGI systems. To achieve this, it is first necessary to learn what human values are. However, human values are incredibly complex and cannot easily be formalized by hand. In this work, we propose a general framework to estimate the values of a human given its behavior.",
		"archive_location": "WOS:000363479400016",
		"DOI": "10.1007/978-3-319-21365-1_16",
		"event-title": "ARTIFICIAL GENERAL INTELLIGENCE (AGI 2015)",
		"ISBN": "0302-9743",
		"page": "152-155",
		"title": "Inferring Human Values for Safe AGI Design",
		"volume": "9205",
		"author": [
			{
				"family": "Sezener",
				"given": "CE"
			}
		],
		"editor": [
			{
				"family": "Bieger",
				"given": "J"
			},
			{
				"family": "Goertzel",
				"given": "B"
			},
			{
				"family": "Potapov",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "fayollasSafeOpsConceptContinuous2020",
		"type": "paper-conference",
		"abstract": "Improved safety is one of the key benefits expected from autonomous vehicles. This can only be achieved if the autonomous vehicles are guaranteed to be safe enough. This paper proposes a potential approach contributing to this safety improvement: it describes and investigates \"SafeOps\", a concept of \"continuous safety\", based on the DevOps approach, unifying development and operations. DevOps consists in a set of practices intended to reduce the time between committing a change to a system and the change being deployed into production, while ensuring high quality. DevOps benefits to system development and delivery by enabling software continuous delivery, faster changes management with faster issues resolution, and improved reliability. SafeOps key principle is to monitor the system in operation and to use this information for validating and certifying a certain safety assurance level. Following this approach, a system could be compliant to a first safety assurance level when it's first delivered and compliant to higher ones when validated in operation.",
		"archive_location": "WOS:000630473500010",
		"DOI": "10.1109/EDCC51268.2020.00020",
		"event-title": "2020 16TH EUROPEAN DEPENDABLE COMPUTING CONFERENCE (EDCC 2020)",
		"ISBN": "978-1-7281-8936-9",
		"page": "65-68",
		"title": "SafeOps: a concept of continuous safety",
		"author": [
			{
				"family": "Fayollas",
				"given": "C"
			},
			{
				"family": "Bonnin",
				"given": "H"
			},
			{
				"family": "Flebus",
				"given": "O"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhuSafetyPerformanceWhy2022",
		"type": "paper-conference",
		"abstract": "The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.",
		"archive_location": "WOS:001062775200014",
		"DOI": "10.1145/3551349.3556906",
		"event-title": "PROCEEDINGS OF THE 37TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING, ASE 2022",
		"ISBN": "1527-1366",
		"title": "Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment",
		"author": [
			{
				"family": "Zhu",
				"given": "J"
			},
			{
				"family": "Wang",
				"given": "LY"
			},
			{
				"family": "Han",
				"given": "X"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bossensExplicitExploreExploit2023",
		"type": "article-journal",
		"abstract": "In reinforcement learning (RL), an agent must explore an initially unknown environment in order to learn a desired behaviour. When RL agents are deployed in real world environments, safety is of primary concern. Constrained Markov decision processes (CMDPs) can provide long-term safety constraints; however, the agent may violate the constraints in an effort to explore its environment. This paper proposes a model-based RL algorithm called Explicit Explore, Exploit, or Escape (E-4), which extends the Explicit Explore or Exploit (E-3) algorithm to a robust CMDP setting. E-4 explicitly separates exploitation, exploration, and escape CMDPs, allowing targeted policies for policy improvement across known states, discovery of unknown states, as well as safe return to known states. E-4 robustly optimises these policies on the worst-case CMDP from a set of CMDP models consistent with the empirical observations of the deployment environment. Theoretical results show that E-4 finds a near-optimal constraint-satisfying policy in polynomial time whilst satisfying safety constraints throughout the learning process. We then discuss E-4 as a practical algorithmic framework, including robust-constrained offline optimisation algorithms, the design of uncertainty sets for the transition dynamics of unknown states, and how to further leverage empirical observations and prior knowledge to relax some of the worst-case assumptions underlying the theory.",
		"archive_location": "WOS:000814467900003",
		"container-title": "MACHINE LEARNING",
		"DOI": "10.1007/s10994-022-06201-z",
		"ISSN": "0885-6125",
		"issue": "3",
		"page": "817-858",
		"title": "Explicit Explore, Exploit, or Escape (E4): near-optimal safety-constrained reinforcement learning in polynomial time",
		"volume": "112",
		"author": [
			{
				"family": "Bossens",
				"given": "DM"
			},
			{
				"family": "Bishop",
				"given": "N"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "yavasRealWorldReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Lane-change decision-making for vehicles is a challenging task for many reasons, including traffic rules, safety, and the stochastic nature of driving. Because of its success in solving complex problems, deep reinforcement learning (DRL) has been suggested for addressing these issues. However, the studies on DRL to date have gone no further than validation in simulation and failed to address what are arguably the most critical issues, namely, the mismatch between simulation and reality, human-likeness, and safety. This paper introduces a real-world DRL framework for decision-making to design safe and human-like agents that can operate in the real world without extra tuning. We propose a new learning paradigm for DRL integrated with Real2Sim transfer, which comprises training, validation, and testing phases. The approach involves two simulator environments with different levels of fidelity, which are parameterized via real-world data. Within the framework, a large amount of randomized experience is generated with a low-fidelity simulator, whereupon the learned skills are validated regularly in a high-fidelity simulator to avoid overfitting. Finally, in the testing phase, the agent is examined concerning safety and human-like decision-making. Extensive simulation and real-world evaluations show the superiority of the proposed approach. To the best of the authors' knowledge, this is the first application of DRL lane-changing policy in the real world.",
		"archive_location": "WOS:001040652400001",
		"container-title": "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS",
		"DOI": "10.1109/TITS.2023.3292981",
		"ISSN": "1524-9050",
		"title": "A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making",
		"author": [
			{
				"family": "Yavas",
				"given": "MU"
			},
			{
				"family": "Kumbasar",
				"given": "T"
			},
			{
				"family": "Ure",
				"given": "NK"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					25
				]
			]
		}
	},
	{
		"id": "fultonVerifiablySafeOffModel2019",
		"type": "paper-conference",
		"abstract": "The desire to use reinforcement learning in safety-critical settings has inspired a recent interest in formal methods for learning algorithms. Existing formal methods for learning and optimization primarily consider the problem of constrained learning or constrained optimization. Given a single correct model and associated safety constraint, these approaches guarantee efficient learning while provably avoiding behaviors outside the safety constraint. Acting well given an accurate environmental model is an important pre-requisite for safe learning, but is ultimately insufficient for systems that operate in complex heterogeneous environments. This paper introduces verification-preserving model updates, the first approach toward obtaining formal safety guarantees for reinforcement learning in settings where multiple possible environmental models must be taken into account. Through a combination of inductive data and deductive proving with design-time model updates and runtime model falsification, we provide a first approach toward obtaining formal safety proofs for autonomous systems acting in heterogeneous environments.",
		"archive_location": "WOS:000681166500028",
		"DOI": "10.1007/978-3-030-17462-0_28",
		"event-title": "TOOLS AND ALGORITHMS FOR THE CONSTRUCTION AND ANALYSIS OF SYSTEMS, PT I",
		"ISBN": "0302-9743",
		"page": "413-430",
		"title": "Verifiably Safe Off-Model Reinforcement Learning",
		"volume": "11427",
		"author": [
			{
				"family": "Fulton",
				"given": "N"
			},
			{
				"family": "Platzer",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Vojnar",
				"given": "T"
			},
			{
				"family": "Zhang",
				"given": "L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "wenConstrainedCrossEntropyMethod2021",
		"type": "article-journal",
		"abstract": "We study a safe reinforcement learning problem, in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The key idea is to transform the original constrained optimization problem into an unconstrained one with a surrogate objective. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then, we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show the performance of the proposed algorithm in two simulation examples. In a constrained linear-quadratic regulator example, we observe that the algorithm converges to the global optimum with high probability. In a 2-D navigation example, we find that the algorithm effectively learns feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.",
		"archive_location": "WOS:000668858300014",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2020.3015931",
		"ISSN": "0018-9286",
		"issue": "7",
		"page": "3123-3137",
		"title": "Constrained Cross-Entropy Method for Safe Reinforcement Learning",
		"volume": "66",
		"author": [
			{
				"family": "Wen",
				"given": "M"
			},
			{
				"family": "Topcu",
				"given": "U"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					7
				]
			]
		}
	},
	{
		"id": "beckerAuditingTestingAI2022",
		"type": "paper-conference",
		"abstract": "This paper describes a framework that can be used to assess and analyze AI systems in terms of risk. The framework addresses the structure and components of AI systems at five layers and allows taking a holistic view of AI systems while focusing on specific aspects, such as discrimination or data.",
		"archive_location": "WOS:000870272800020",
		"DOI": "10.1007/978-3-031-06018-2_20",
		"event-title": "DIGITAL HUMAN MODELING AND APPLICATIONS IN HEALTH, SAFETY, ERGONOMICS AND RISK MANAGEMENT: HEALTH, OPERATIONS MANAGEMENT, AND DESIGN, PT II",
		"ISBN": "0302-9743",
		"page": "283-292",
		"title": "Auditing and Testing AI - A Holistic Framework",
		"volume": "13320",
		"author": [
			{
				"family": "Becker",
				"given": "N"
			},
			{
				"family": "Waltl",
				"given": "B"
			}
		],
		"editor": [
			{
				"family": "Duffy",
				"given": "VG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "phanNeuralSimplexArchitecture2020",
		"type": "paper-conference",
		"abstract": "We present the Neural Simplex Architecture (NSA), a new approach to runtime assurance that provides safety guarantees for neural controllers (obtained e.g. using reinforcement learning) of autonomous and other complex systems without unduly sacrificing performance. NSA is inspired by the Simplex control architecture of Sha et al., but with some significant differences. In the traditional approach, the advanced controller (AC) is treated as a black box; when the decision module switches control to the baseline controller (BC), the BC remains in control forever. There is relatively little work on switching control back to the AC, and there are no techniques for correcting the AC's behavior after it generates a potentially unsafe control input that causes a failover to the BC. Our NSA addresses both of these limitations. NSA not only provides safety assurances in the presence of a possibly unsafe neural controller, but can also improve the safety of such a controller in an online setting via retraining, without overly degrading its performance. To demonstrate NSA's benefits, we have conducted several significant case studies in the continuous control domain. These include a target-seeking ground rover navigating an obstacle field, and a neural controller for an artificial pancreas system.",
		"archive_location": "WOS:000890074700006",
		"DOI": "10.1007/978-3-030-55754-6_6",
		"event-title": "NASA FORMAL METHODS (NFM 2020)",
		"ISBN": "0302-9743",
		"page": "97-114",
		"title": "Neural Simplex Architecture",
		"volume": "12229",
		"author": [
			{
				"family": "Phan",
				"given": "DT"
			},
			{
				"family": "Grosu",
				"given": "R"
			},
			{
				"family": "Jansen",
				"given": "N"
			},
			{
				"family": "Paoletti",
				"given": "N"
			},
			{
				"family": "Smolka",
				"given": "SA"
			},
			{
				"family": "Stoller",
				"given": "SD"
			}
		],
		"editor": [
			{
				"family": "Lee",
				"given": "R"
			},
			{
				"family": "Jha",
				"given": "S"
			},
			{
				"family": "Mavridou",
				"given": "A"
			},
			{
				"family": "Giannakopoulou",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lvDeepSafeReinforcement2021",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) is used more and more in robot navigation, however the safety of RL is usually not guaranteed. To improve the safety in the end-to-end mapless navigation using deep reinforcement learning (DRL), we propose a deep safe RL approach which uses a safe RL algorithm called Constrained Policy Optimization (CPO) and design the Actor-Critic-Safety (ACS) architecture to apply CPO. We use the Social Force Pedestrian Simulator based on social force model to simulate the dynamic environment with pedestrians in Gazebo. Experiment results show that the proposed approach can obviously increase the success rate and reduce the collision rate, which means the safety in navigation is improved. The planned path is almost as good as by ROS move base which needs to build a map of environment first. What's more, the model trained in static environment is able to generalize to unseen dynamic environment with pedestrians without any fine tuning and behaves well.",
		"archive_location": "WOS:000812286900248",
		"DOI": "10.1109/ROBIO54168.2021.9739251",
		"event-title": "2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE-ROBIO 2021)",
		"ISBN": "978-1-6654-0535-5",
		"page": "1520-1525",
		"title": "A Deep Safe Reinforcement Learning Approach for Mapless Navigation",
		"author": [
			{
				"family": "Lv",
				"given": "SH"
			},
			{
				"family": "Li",
				"given": "YJ"
			},
			{
				"family": "Liu",
				"given": "Q"
			},
			{
				"family": "Gao",
				"given": "JQ"
			},
			{
				"family": "Pang",
				"given": "XZ"
			},
			{
				"family": "Chen",
				"given": "ML"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barzaminiMultilevelSemanticWeb2022",
		"type": "article-journal",
		"abstract": "Machine Learning (ML) algorithms are widely used in building software-intensive systems, including safety-critical ones. Unlike traditional software components, Machine-Learned Components (MLC)s, software components built using ML algorithms, learn their specifications through generalizing the common features that they find in a limited set of collected examples. While this inductive nature overcomes the limitations of programming hard-to-specify concepts, the same feature becomes problematic for verifying safety in ML-based software systems. One reason is that, due to MLCs data-driven nature, there is often no set of explicitly written and pre-defined specifications, against which the MLC can be verified. In this regard, we propose to partially specify hard-to-specify domain concepts, which MLCs tend to classify, instead of fully relying on their inductive learning ability from arbitrarily-collected datasets. In this paper, we propose a semi-automated approach to construct a multi-level semantic web to partially outline the hard-to-specify, yet crucial, domain concept \"pedestrian\" in automotive domain. We evaluate the applicability of the generated semantic web in two ways: first, with a reference to the web, we augment a pedestrian dataset for a missing feature, wheelchair, to show training a state-of-the-art ML-based object detector on the augmented dataset improves its accuracy in detecting pedestrians; second, we evaluate the coverage of the generated semantic web based on multiple state-of-the-art pedestrian and human datasets.",
		"archive_location": "WOS:000740138800001",
		"container-title": "REQUIREMENTS ENGINEERING",
		"DOI": "10.1007/s00766-021-00366-0",
		"ISSN": "0947-3602",
		"issue": "2",
		"page": "161-182",
		"title": "A multi-level semantic web for hard-to-specify domain concept, Pedestrian, in ML-based software",
		"volume": "27",
		"author": [
			{
				"family": "Barzamini",
				"given": "H"
			},
			{
				"family": "Shahzad",
				"given": "M"
			},
			{
				"family": "Alhoori",
				"given": "H"
			},
			{
				"family": "Rahimi",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					6
				]
			]
		}
	},
	{
		"id": "cowen-riversSAMBASafeModelbased2022",
		"type": "article-journal",
		"abstract": "In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.",
		"archive_location": "WOS:000738430100001",
		"container-title": "MACHINE LEARNING",
		"DOI": "10.1007/s10994-021-06103-6",
		"ISSN": "0885-6125",
		"issue": "1",
		"page": "173-203",
		"title": "SAMBA: safe model-based & active reinforcement learning",
		"volume": "111",
		"author": [
			{
				"family": "Cowen-Rivers",
				"given": "AI"
			},
			{
				"family": "Palenicek",
				"given": "D"
			},
			{
				"family": "Moens",
				"given": "V"
			},
			{
				"family": "Abdullah",
				"given": "MA"
			},
			{
				"family": "Sootla",
				"given": "A"
			},
			{
				"family": "Wang",
				"given": "J"
			},
			{
				"family": "Bou-Ammar",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					1
				]
			]
		}
	},
	{
		"id": "polymenakosSafePolicySearch2019",
		"type": "paper-conference",
		"abstract": "We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.",
		"archive_location": "WOS:000474345000180",
		"event-title": "AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS",
		"ISBN": "978-1-4503-6309-9",
		"page": "1565-1573",
		"title": "Safe Policy Search Using Gaussian Process Models",
		"author": [
			{
				"family": "Polymenakos",
				"given": "K"
			},
			{
				"family": "Abate",
				"given": "A"
			},
			{
				"family": "Roberts",
				"given": "S"
			},
			{
				"literal": "Assoc Comp Machinery"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "treacyMechanismsConstraintsUnderpinning2021",
		"type": "paper-conference",
		"abstract": "The unpredictability of artificial intelligence (AI) services and products pose major ethical concerns for multinational companies as evidenced by the prevalence of unfair, biased, and discriminate AI systems. Examples including Amazon's recruiting tool, Facebook's biased ads, and racially biased healthcare risk algorithms have raised fundamental questions about what these systems should be used for, the inherent risks they possess, and how they can be mitigated. Unfortunately, these failures not only serve to highlight the lack of regulation in AI development, but it also reveals how organisations are struggling to alleviate the dangers associated with this technology. We argue that to successfully implement ethical AI applications, developers need a deeper understanding of not only the implications of misuse, but also a grounded approach in their conception. Judgement studies were therefore conducted with experts from data science backgrounds who identified six performance areas, resulting in a theoretical framework for the development of ethically aligned AI systems. This framework also reveals that these performance areas require specific mechanisms which must be acted upon to ensure that an AI system implements and meets ethical requirements throughout its lifecycle. The findings also outline several constraints which present challenges in the manifestation of these elements. By implementing this framework, organisations can contribute to an elevated trust between technology and people resulting in significant implications for both IS research and practice. This framework will further allow organisations to take a positive and proactive approach in ensuring they are best prepared for the ethical implications associated with the development, deployment and use of AI systems.",
		"archive_location": "WOS:000838033200024",
		"DOI": "10.34190/EAIR.21.005",
		"event-title": "PROCEEDINGS OF THE 3RD EUROPEAN CONFERENCE ON THE IMPACT OF ARTIFICIAL INTELLIGENCE AND ROBOTICS (ECIAIR 2021)",
		"ISBN": "978-1-914587-23-8",
		"page": "183-191",
		"title": "Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of key Performance Areas",
		"author": [
			{
				"family": "Treacy",
				"given": "S"
			}
		],
		"editor": [
			{
				"family": "Matos",
				"given": "F"
			},
			{
				"family": "Salavisa",
				"given": "I"
			},
			{
				"family": "Serrao",
				"given": "C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenBinFIEfficientFault2019",
		"type": "paper-conference",
		"abstract": "As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors.\nIn this work, we propose Biel, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.",
		"archive_location": "WOS:000545976800069",
		"DOI": "10.1145/3295500.3356177",
		"event-title": "PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS",
		"ISBN": "978-1-4503-6229-0",
		"title": "BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems",
		"author": [
			{
				"family": "Chen",
				"given": "ZT"
			},
			{
				"family": "Li",
				"given": "GP"
			},
			{
				"family": "Pattabiraman",
				"given": "K"
			},
			{
				"family": "DeBardeleben",
				"given": "N"
			},
			{
				"literal": "Assoc Comp Machinery"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "rauschAutoencoderBasedSemanticNovelty2021",
		"type": "article-journal",
		"abstract": "Many autonomous systems, such as driverless taxis, perform safety-critical functions. Autonomous systems employ artificial intelligence (AI) techniques, specifically for environmental perception. Engineers cannot completely test or formally verify AI-based autonomous systems. The accuracy of AI-based systems depends on the quality of training data. Thus, novelty detection, that is, identifying data that differ in some respect from the data used for training, becomes a safety measure for system development and operation. In this study, we propose a new architecture for autoencoder-based semantic novelty detection with two innovations: architectural guidelines for a semantic autoencoder topology and a semantic error calculation as novelty criteria. We demonstrate that such a semantic novelty detection outperforms autoencoder-based novelty detection approaches known from the literature by minimizing false negatives.",
		"archive_location": "WOS:000719108200001",
		"container-title": "APPLIED SCIENCES-BASEL",
		"DOI": "10.3390/app11219881",
		"ISSN": "2076-3417",
		"issue": "21",
		"title": "Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems",
		"volume": "11",
		"author": [
			{
				"family": "Rausch",
				"given": "A"
			},
			{
				"family": "Sedeh",
				"given": "AM"
			},
			{
				"family": "Zhang",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "liBridgingModelbasedSafety2022",
		"type": "paper-conference",
		"abstract": "Bridging model-based safety and model-free reinforcement learning (RL) for dynamic robots is appealing since model-based methods are able to provide formal safety guarantees, while RL-based methods are able to exploit the robot agility by learning from the full-order system dynamics. However, current approaches to tackle this problem are mostly restricted to simple systems. In this paper, we propose a new method to combine model-based safety with model-free reinforcement learning by explicitly finding a low-dimensional model of the system controlled by a RL policy and applying stability and safety guarantees on that simple model. We use a complex bipedal robot Cassie, which is a high dimensional nonlinear system with hybrid dynamics and underactuation, and its RL-based walking controller as an example. We show that a low-dimensional dynamical model is sufficient to capture the dynamics of the closed-loop system. We demonstrate that this model is linear, asymptotically stable, and is decoupled across control input in all dimensions. We further exemplify that such linearity exists even when using different RL control policies. Such results point out an interesting direction to understand the relationship between RL and optimal control: whether RL tends to linearize the nonlinear system during training in some cases. Furthermore, we illustrate that the found linear model is able to provide guarantees by safety-critical optimal control framework, e.g., Model Predictive Control with Control Barrier Functions, on an example of autonomous navigation using Cassie while taking advantage of the agility provided by the RL-based controller.",
		"archive_location": "WOS:000827625700033",
		"event-title": "ROBOTICS: SCIENCE AND SYSTEM XVIII",
		"ISBN": "2330-7668",
		"title": "Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models",
		"author": [
			{
				"family": "Li",
				"given": "ZY"
			},
			{
				"family": "Zeng",
				"given": "J"
			},
			{
				"family": "Thirugnanam",
				"given": "A"
			},
			{
				"family": "Sreenath",
				"given": "K"
			}
		],
		"editor": [
			{
				"family": "Hauser",
				"given": "K"
			},
			{
				"family": "Shell",
				"given": "D"
			},
			{
				"family": "Huang",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "modaresSafeReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "This article presents a data-driven safe reinforcement learning (RL) algorithm for discrete-time nonlinear systems. A data-driven safety certifier is designed to intervene with the actions of the RL agent to ensure both safety and stability of its actions. This is in sharp contrast to existing model-based safety certifiers that can result in convergence to an undesired equilibrium point or conservative interventions that jeopardize the performance of the RL agent. To this end, the proposed method directly learns a robust safety certifier while completely bypassing the identification of the system model. The nonlinear system is modeled using linear parameter varying (LPV) systems with polytopic disturbances. To prevent the requirement for learning an explicit model of the LPV system, data-based $\\lambda$ -contractivity conditions are first provided for the closed-loop system to enforce robust invariance of a prespecified polyhedral safe set and the system's asymptotic stability. These conditions are then leveraged to directly learn a robust data-based gain-scheduling controller by solving a convex program. A significant advantage of the proposed direct safe learning over model-based certifiers is that it completely resolves conflicts between safety and stability requirements while assuring convergence to the desired equilibrium point. Data-based safety certification conditions are then provided using Minkowski functions. They are then used to seemingly integrate the learned backup safe gain-scheduling controller with the RL controller. Finally, we provide a simulation example to verify the effectiveness of the proposed approach.",
		"archive_location": "WOS:000973264800001",
		"container-title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS",
		"DOI": "10.1109/TNNLS.2023.3264815",
		"ISSN": "2162-237X",
		"title": "Safe Reinforcement Learning via a Model-Free Safety Certifier",
		"author": [
			{
				"family": "Modares",
				"given": "A"
			},
			{
				"family": "Sadati",
				"given": "N"
			},
			{
				"family": "Esmaeili",
				"given": "B"
			},
			{
				"family": "Yaghmaie",
				"given": "FA"
			},
			{
				"family": "Modares",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					13
				]
			]
		}
	},
	{
		"id": "fischerSamplingbasedInverseReinforcement2021",
		"type": "paper-conference",
		"abstract": "Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.",
		"archive_location": "WOS:000755125500079",
		"DOI": "10.1109/IROS51168.2021.9636672",
		"event-title": "2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)",
		"ISBN": "2153-0858",
		"page": "791-798",
		"title": "Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints",
		"author": [
			{
				"family": "Fischer",
				"given": "J"
			},
			{
				"family": "Eyberg",
				"given": "C"
			},
			{
				"family": "Werling",
				"given": "M"
			},
			{
				"family": "Lauer",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kimEvaluatingCorrectnessReinforcement2022",
		"type": "paper-conference",
		"abstract": "Deep learning is used for decision making and functional control in various fields, such as autonomous systems. However, rather than being developed by logical design, deep learning models are trained by itself through learning data. Moreover, only reward values are used to evaluate its performance, which does not provide enough information that the model learned properly. This paper proposes a new method to assess the correctness of reinforcement learning, considering other properties of the learning algorithm. The proposed method is applied for the evaluation of ActorCritic Algorithms, and correctness-related insights of the algorithm are confirmed through experiments.",
		"archive_location": "WOS:000855059600065",
		"DOI": "10.1109/ICUFN55119.2022.9829571",
		"event-title": "2022 THIRTEENTH INTERNATIONAL CONFERENCE ON UBIQUITOUS AND FUTURE NETWORKS (ICUFN)",
		"ISBN": "2165-8528",
		"page": "320-325",
		"title": "Evaluating Correctness of Reinforcement Learning based on Actor-Critic Algorithm",
		"author": [
			{
				"family": "Kim",
				"given": "Y"
			},
			{
				"family": "Hussain",
				"given": "M"
			},
			{
				"family": "Suh",
				"given": "JW"
			},
			{
				"family": "Hong",
				"given": "JE"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lecerfAutomaticallyLearningFallback2022",
		"type": "paper-conference",
		"abstract": "When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.\nIn this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.\nWe apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.",
		"archive_location": "WOS:001053939400033",
		"DOI": "10.1145/3529399.3529432",
		"event-title": "PROCEEDINGS OF 2022 7TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING TECHNOLOGIES, ICMLT 2022",
		"ISBN": "978-1-4503-9574-8",
		"page": "209-215",
		"title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios",
		"author": [
			{
				"family": "Lecerf",
				"given": "UUL"
			},
			{
				"family": "Yemdji-Tchassi",
				"given": "CCY"
			},
			{
				"family": "Aubert",
				"given": "SSA"
			},
			{
				"family": "Michiardi",
				"given": "PPM"
			},
			{
				"literal": "ACM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bazzanAligningIndividualCollective2019",
		"type": "article-journal",
		"abstract": "In complex socio-technical systems it is not easy to find a balance between the welfare state (i.e., a state where the overall performance of a system is optimal) and a situation in which individual components act selfishly to optimize their own utilities. This is even harder when individuals compete for scarce resources. In order to deal with this, some forms of biasing the optimization process have been proposed. However, mostly, such approaches only work for cooperative scenarios. When resources are scarce, the components of the system compete for them, thus approaches designed for cooperative systems are not necessarily appropriate. In the present paper an approach is proposed, which is based on a synergy between: (i) a global optimization process in which the system authority employs metaheuristics, and (ii) reinforcement learning processes that run at each component or agent. Both the agents and the system authority exchange solutions that are incorporated by the other party. The contributions of the proposed approach are twofold: a general scheme for such synergy is given and its benefits are shown in scenarios related to selfish routing, a typical load balance problem in a complex socio-technical system.",
		"archive_location": "WOS:000459524300003",
		"container-title": "ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE",
		"DOI": "10.1016/j.engappai.2018.12.003",
		"ISSN": "0952-1976",
		"page": "23-33",
		"title": "Aligning individual and collective welfare in complex socio-technical systems by combining metaheuristics and reinforcement learning",
		"volume": "79",
		"author": [
			{
				"family": "Bazzan",
				"given": "ALC"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					3
				]
			]
		}
	},
	{
		"id": "maConservativeAdaptivePenalty2022",
		"type": "paper-conference",
		"abstract": "Reinforcement Learning (RL) agents in the real world must satisfy safety constraints in addition to maximizing a reward objective. Model-based RL algorithms hold promise for reducing unsafe real-world actions: they may synthesize policies that obey all constraints using simulated samples from a learned model. However, imperfect models can result in real-world constraint violations even for actions that are predicted to satisfy all constraints. We propose Conservative and Adaptive Penalty (CAP), a model-based safe RL framework that accounts for potential modeling errors by capturing model uncertainty and adaptively exploiting it to balance the reward and the cost objectives. First, CAP inflates predicted costs using an uncertainty-based penalty. Theoretically, we show that policies that satisfy this conservative cost constraint are guaranteed to also be feasible in the true environment. We further show that this guarantees the safety of all intermediate solutions during RL training. Further, CAP adaptively tunes this penalty during training using true cost feedback from the environment. We evaluate this conservative and adaptive penalty-based approach for model-based safe RL extensively on state and image-based environments. Our results demonstrate substantial gains in sample-efficiency while incurring fewer violations than prior safe RL algorithms.",
		"archive_location": "WOS:000893636205058",
		"event-title": "THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE",
		"ISBN": "2159-5399",
		"page": "5404-5412",
		"title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
		"author": [
			{
				"family": "Ma",
				"given": "YJ"
			},
			{
				"family": "Shen",
				"given": "A"
			},
			{
				"family": "Bastani",
				"given": "O"
			},
			{
				"family": "Jayaraman",
				"given": "D"
			},
			{
				"literal": "Assoc Advancement Artificial Intelligence"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "massianiSafeValueFunctions2023",
		"type": "article-journal",
		"abstract": "Safety constraints and optimality are important but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning (RL) to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces an SVF. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.",
		"archive_location": "WOS:000979661300009",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2022.3200948",
		"ISSN": "0018-9286",
		"issue": "5",
		"page": "2743-2757",
		"title": "Safe Value Functions",
		"volume": "68",
		"author": [
			{
				"family": "Massiani",
				"given": "PF"
			},
			{
				"family": "Heim",
				"given": "S"
			},
			{
				"family": "Solowjow",
				"given": "F"
			},
			{
				"family": "Trimpe",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					5
				]
			]
		}
	},
	{
		"id": "fernandezFunctionalSafetyCompliance2021",
		"type": "article-journal",
		"abstract": "Autonomous systems execute complex tasks to perceive the environment and take self-aware decisions with limited human interaction. This autonomy is commonly achieved with the support of machine learning algorithms. The nature of these algorithms, that need to process large data volumes, poses high-performance demands on the underlying hardware. As a result, the embedded critical real-time domain is adopting increasingly powerful processors that combine multi-core processors with accelerators such as GPUs. The resulting hardware and software complexity makes it difficult to demonstrate that the system will run safely and reliably. This is the main objective of functional safety standards, such as IEC 61508 or ISO 26262, that deal with the avoidance, detection and control of hardware or software errors. In this paper, we adopt those measures for the safe inference of machine learning libraries on multi-core devices, two topics that are not explicitly covered in the current version of standards. To this end, we adapt the matrix-matrix multiplication function, a central element of existing machine learning libraries, according to the recommendations of functional safety standards. The paper makes the following contributions: (i) adoption of recommended programming practices for the avoidance of programming errors in the matrix-matrix multiplication, (ii) inclusion of diagnostic mechanisms based on widely used checksums to control runtime errors, and (iii) evaluation of the impact of previous measures in terms of performance and a quantification of the achieved diagnostic coverage. For this purpose, we implement the diagnostic mechanisms on one of the ARM R5 cores of a Zynq UltraScale+ multi-processor system-on-chip and we then adapt them to an Intel i7 processor with native code employing vectorization for the sake of performance.",
		"archive_location": "WOS:000712050800002",
		"container-title": "JOURNAL OF SYSTEMS ARCHITECTURE",
		"DOI": "10.1016/j.sysarc.2021.102298",
		"ISSN": "1383-7621",
		"title": "Towards functional safety compliance of matrix-matrix multiplication for machine learning-based autonomous systems",
		"volume": "121",
		"author": [
			{
				"family": "Fernández",
				"given": "J"
			},
			{
				"family": "Perez",
				"given": "J"
			},
			{
				"family": "Agirre",
				"given": "I"
			},
			{
				"family": "Allende",
				"given": "I"
			},
			{
				"family": "Abella",
				"given": "J"
			},
			{
				"family": "Cazorla",
				"given": "FJ"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "umbrelloBeneficialArtificialIntelligence2019",
		"type": "article-journal",
		"abstract": "This paper argues that the Value Sensitive Design (VSD) methodology provides a principled approach to embedding common values into AI systems both early and throughout the design process. To do so, it draws on an important case study: the evidence and final report of the UK Select Committee on Artificial Intelligence. This empirical investigation shows that the different and often disparate stakeholder groups that are implicated in AI design and use share some common values that can be used to further strengthen design coordination efforts. VSD is shown to be both able to distill these common values as well as provide a framework for stakeholder coordination.",
		"archive_location": "WOS:000697668400005",
		"container-title": "BIG DATA AND COGNITIVE COMPUTING",
		"DOI": "10.3390/bdcc3010005",
		"ISSN": "2504-2289",
		"issue": "1",
		"title": "Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach",
		"volume": "3",
		"author": [
			{
				"family": "Umbrello",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					3
				]
			]
		}
	},
	{
		"id": "mindomAssessingSafetyReinforcement2021",
		"type": "paper-conference",
		"abstract": "The increasing adoption of Reinforcement Learning in safety-critical systems domains such as autonomous vehicles, health, and aviation raises the need for ensuring their safety. Existing safety mechanisms such as adversarial training, adversarial detection, and robust learning are not always adapted to all disturbances in which the agent is deployed. Those disturbances include moving adversaries whose behavior can be unpredictable by the agent, and as a matter of fact harmful to its learning. Ensuring the safety of critical systems also requires methods that give formal guarantees on the behaviour of the agent evolving in a perturbed environment. It is therefore necessary to propose new solutions adapted to the learning challenges faced by the agent. In this paper, first we generate adversarial agents that exhibit flaws in the agent's policy by presenting moving adversaries. Secondly, We use reward shaping and a modified Q-learning algorithm as defense mechanisms to improve the agent's policy when facing adversarial perturbations. Finally, probabilistic model checking is employed to evaluate the effectiveness of both mechanisms. We have conducted experiments on a discrete grid world with a single agent facing non-learning and learning adversaries. Our results show a diminution in the number of collisions between the agent and the adversaries. Probabilistic model checking provides lower and upper probabilistic bounds regarding the agent's safety in the adversarial environment.",
		"archive_location": "WOS:000814747000027",
		"DOI": "10.1109/QRS54544.2021.00037",
		"event-title": "2021 IEEE 21ST INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS 2021)",
		"ISBN": "2693-9185",
		"page": "260-269",
		"title": "On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods",
		"author": [
			{
				"family": "Mindom",
				"given": "PSN"
			},
			{
				"family": "Nikanjam",
				"given": "A"
			},
			{
				"family": "Khomh",
				"given": "F"
			},
			{
				"family": "Mullins",
				"given": "J"
			},
			{
				"literal": "IEEE COMP SOC"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "vaccariEXplainableReliableAdversarial2022",
		"type": "article-journal",
		"abstract": "Machine learning (ML) algorithms are nowadays widely adopted in different contexts to perform autonomous decisions and predictions. Due to the high volume of data shared in the recent years, ML algorithms are more accurate and reliable since training and testing phases are more precise. An important concept to analyze when defining ML algorithms concerns adversarial machine learning attacks. These attacks aim to create manipulated datasets to mislead ML algorithm decisions. In this work, we propose new approaches able to detect and mitigate malicious adversarial machine learning attacks against a ML system. In particular, we investigate the Carlini-Wagner (CW), the fast gradient sign method (FGSM) and the Jacobian based saliency map (JSMA) attacks. The aim of this work is to exploit detection algorithms as countermeasures to these attacks. Initially, we performed some tests by using canonical ML algorithms with a hyperparameters optimization to improve metrics. Then, we adopt original reliable AI algorithms, either based on eXplainable AI (Logic Learning Machine) or Support Vector Data Description (SVDD). The obtained results show how the classical algorithms may fail to identify an adversarial attack, while the reliable AI methodologies are more prone to correctly detect a possible adversarial machine learning attack. The evaluation of the proposed methodology was carried out in terms of good balance between FPR and FNR on real world application datasets: Domain Name System (DNS) tunneling, Vehicle Platooning and Remaining Useful Life (RUL). In addition, a statistical analysis was performed to improve the robustness of the trained models, including evaluating their performance in terms of runtime and memory consumption.",
		"archive_location": "WOS:000842742900001",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2022.3197299",
		"ISSN": "2169-3536",
		"page": "83949-83970",
		"title": "eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics",
		"volume": "10",
		"author": [
			{
				"family": "Vaccari",
				"given": "I"
			},
			{
				"family": "Carlevaro",
				"given": "A"
			},
			{
				"family": "Narteni",
				"given": "S"
			},
			{
				"family": "Cambiaso",
				"given": "E"
			},
			{
				"family": "Mongelli",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "vakkuriECCOLAMethodImplementing2021",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners. (C) 2021 The Author(s). Published by Elsevier Inc.",
		"archive_location": "WOS:000704056400002",
		"container-title": "JOURNAL OF SYSTEMS AND SOFTWARE",
		"DOI": "10.1016/j.jss.2021.111067",
		"ISSN": "0164-1212",
		"title": "ECCOLA - A method for implementing ethically aligned AI systems",
		"volume": "182",
		"author": [
			{
				"family": "Vakkuri",
				"given": "V"
			},
			{
				"family": "Kemell",
				"given": "KK"
			},
			{
				"family": "Jantunen",
				"given": "M"
			},
			{
				"family": "Halme",
				"given": "E"
			},
			{
				"family": "Abrahamsson",
				"given": "P"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "wabersichProbabilisticModelPredictive2022",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL) methods have demonstrated their efficiency in simulation. However, many of the applications for which RL offers great potential, such as autonomous driving, are also safety critical and require a certified closed-loop behavior in order to meet the safety specifications in the presence of physical constraints. This article introduces a concept called probabilistic model predictive safety certification (PMPSC), which can be combined with any RL algorithm and provides provable safety certificates in terms of state and input chance constraints for potentially large-scale systems. The certificate is realized through a stochastic tube that safely connects the current system state with a terminal set of states that is known to be safe. A novel formulation allows a recursively feasible real-time computation of such probabilistic tubes, despite the presence of possibly unbounded disturbances. A design procedure for PMPSC relying on Bayesian inference and recent advances in probabilistic set invariance is presented. Using a numerical car simulation, the method and its design procedure are illustrated by enhancing an RL algorithm with safety certificates.",
		"archive_location": "WOS:000735567400016",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2021.3049335",
		"ISSN": "0018-9286",
		"issue": "1",
		"page": "176-188",
		"title": "Probabilistic Model Predictive Safety Certification for Learning-Based Control",
		"volume": "67",
		"author": [
			{
				"family": "Wabersich",
				"given": "KJ"
			},
			{
				"family": "Hewing",
				"given": "L"
			},
			{
				"family": "Carron",
				"given": "A"
			},
			{
				"family": "Zeilinger",
				"given": "MN"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					1
				]
			]
		}
	},
	{
		"id": "jinStabilityCertifiedReinforcementLearning2020",
		"type": "article-journal",
		"abstract": "We investigate the important problem of certifying stability of reinforcement learning policies when interconnected with nonlinear dynamical systems. We show that by regulating the partial gradients of policies, strong guarantees of robust stability can be obtained based on a proposed semidefinite programming feasibility problem. The method is able to certify a large set of stabilizing controllers by exploiting problem-specific structures; furthermore, we analyze and establish its (non)conservatism. Empirical evaluations on two decentralized control tasks, namely multi-flight formation and power system frequency regulation, demonstrate that the reinforcement learning agents can have high performance within the stability-certified parameter space and also exhibit stable learning behaviors in the long run.",
		"archive_location": "WOS:000616290100001",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2020.3045114",
		"ISSN": "2169-3536",
		"page": "229086-229100",
		"title": "Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective",
		"volume": "8",
		"author": [
			{
				"family": "Jin",
				"given": "M"
			},
			{
				"family": "Lavaei",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhuoDeepUnsupervisedConvolutional2017",
		"type": "paper-conference",
		"abstract": "In multimedia analysis, the task of domain adaptation is to adapt the feature representation learned in the source domain with rich label information to the target domain with less or even no label information. Significant research endeavors have been devoted to aligning the feature distributions between the source and the target domains in the top fully connected layers based on unsupervised DNN-based models. However, the domain adaptation has been arbitrarily constrained near the output ends of the DNN models, which thus brings about inadequate knowledge transfer in DNN-based domain adaptation process, especially near the input end. We develop an attention transfer process for convolutional domain adaptation. The domain discrepancy, measured in correlation alignment loss, is minimized on the second order correlation statistics of the attention maps for both source and target domains. Then we propose Deep Unsupervised Convolutional Domain Adaptation (DUCDA) method, which jointly minimizes the supervised classification loss of labeled source data and the unsupervised correlation alignment loss measured on both convolutional layers and fully connected layers. The multi-layer domain adaptation process collaborately reinforces each individual domain adaptation component, and significantly enhances the generalization ability of the CNN models. Extensive cross-domain object classification experiments show DUCDA outperforms other state-of-the-art approaches, and validate the promising power of DUCDA towards large scale real world application.",
		"archive_location": "WOS:000482109500031",
		"DOI": "10.1145/3123266.3123292",
		"event-title": "PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17)",
		"ISBN": "978-1-4503-4906-2",
		"page": "261-269",
		"title": "Deep Unsupervised Convolutional Domain Adaptation",
		"author": [
			{
				"family": "Zhuo",
				"given": "JB"
			},
			{
				"family": "Wang",
				"given": "SH"
			},
			{
				"family": "Zhang",
				"given": "WG"
			},
			{
				"family": "Huang",
				"given": "QM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "badeaHaveBreakMaking2022",
		"type": "paper-conference",
		"abstract": "The Multi-valued Action Reasoning System (MARS) is an automated value-based ethical decision-making model for agents in Artificial Intelligence (AI). Given a set of available actions and an underlying moral paradigm, by employing MARS one can identify the ethically preferred action. It can be used to implement and model different ethical theories, different moral paradigms, as well as combinations of such, in the context of automated practical reasoning and normative decision analysis. It can also be used to model moral dilemmas and discover the moral paradigms that result in the desired outcomes therein. In this paper we give a condensed description of MARS, explain its uses, and comparatively place it in the existing literature.",
		"archive_location": "WOS:000922637500031",
		"DOI": "10.1007/978-3-031-21441-7_31",
		"event-title": "ARTIFICIAL INTELLIGENCE XXXIX, AI 2022",
		"ISBN": "0302-9743",
		"page": "359-366",
		"title": "Have a Break from Making Decisions, Have a MARS: The Multi-valued Action Reasoning System",
		"volume": "13652",
		"author": [
			{
				"family": "Badea",
				"given": "C"
			}
		],
		"editor": [
			{
				"family": "Bramer",
				"given": "M"
			},
			{
				"family": "Stahl",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "grossArchitecturalPatternsHandling2022",
		"type": "paper-conference",
		"abstract": "Data-driven models (DDM) based on machine learning and other AI techniques play an important role in the perception of increasingly autonomous systems. Due to the merely implicit definition of their behavior mainly based on the data used for training, DDM outputs are subject to uncertainty. This poses a challenge with respect to the realization of safety-critical perception tasks by means of DDMs. A promising approach to tackling this challenge is to estimate the uncertainty in the current situation during operation and adapt the system behavior accordingly. In previous work, we focused on runtime estimation of uncertainty and discussed approaches for handling uncertainty estimations. In this paper, we present additional architectural patterns for handling uncertainty. Furthermore, we evaluate the four patterns qualitatively and quantitatively with respect to safety and performance gains. For the quantitative evaluation, we consider a distance controller for vehicle platooning where performance gains are measured by considering how much the distance can be reduced in different operational situations. We conclude that the consideration of context information concerning the driving situation makes it possible to accept more or less uncertainty depending on the inherent risk of the situation, which results in performance gains.",
		"archive_location": "WOS:000871734000019",
		"DOI": "10.1007/978-3-031-14835-4_19",
		"event-title": "COMPUTER SAFETY, RELIABILITY, AND SECURITY, SAFECOMP 2022",
		"ISBN": "0302-9743",
		"page": "284-297",
		"title": "Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception",
		"volume": "13414",
		"author": [
			{
				"family": "Gross",
				"given": "J"
			},
			{
				"family": "Adler",
				"given": "R"
			},
			{
				"family": "Kläs",
				"given": "M"
			},
			{
				"family": "Reich",
				"given": "J"
			},
			{
				"family": "Jöckel",
				"given": "L"
			},
			{
				"family": "Gansch",
				"given": "R"
			}
		],
		"editor": [
			{
				"family": "Trapp",
				"given": "M"
			},
			{
				"family": "Saglietti",
				"given": "F"
			},
			{
				"family": "Spislander",
				"given": "M"
			},
			{
				"family": "Bitsch",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dzambicArchitecturalPatternsIntegrating2021",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.",
		"archive_location": "WOS:000931946300036",
		"DOI": "10.1145/3489449.3490014",
		"event-title": "PROCEEDINGS OF THE EUROPEAN CONFERENCE ON PATTERN LANGUAGES OF PROGRAMS 2021, EUROPLOP 2021",
		"ISBN": "978-1-4503-8997-6",
		"title": "Architectural Patterns for Integrating AI Technology into Safety-Critical Systems",
		"author": [
			{
				"family": "Dzambic",
				"given": "M"
			},
			{
				"family": "Dobaj",
				"given": "J"
			},
			{
				"family": "Seidl",
				"given": "M"
			},
			{
				"family": "Macher",
				"given": "G"
			},
			{
				"literal": "ACM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abellaSAFEXPLAINSafeExplainable2023",
		"type": "paper-conference",
		"abstract": "Deep Learning (DL) techniques are at the heart of most future advanced software functions in Critical Autonomous AI-based Systems (CAIS), where they also represent a major competitive factor. Hence, the economic success of CAIS industries (e.g., automotive, space, railway) depends on their ability to design, implement, qualify, and certify DL-based software products under bounded effort/cost. However, there is a fundamental gap between Functional Safety (FUSA) requirements on CAIS and the nature of DL solutions. This gap stems from the development process of DL libraries and affects high-level safety concepts such as (1) explainability and traceability, (2) suitability for varying safety requirements, (3) FUSA-compliant implementations, and (4) real-time constraints. As a matter of fact, the data-dependent and stochastic nature of DL algorithms clashes with current FUSA practice, which instead builds on deterministic, verifiable, and pass/fail test-based software. The SAFEXPLAIN project tackles these challenges and targets by providing a flexible approach to allow the certification - hence adoption - of DL-based solutions in CAIS building on: (1) DL solutions that provide end-to-end traceability, with specific approaches to explain whether predictions can be trusted and strategies to reach (and prove) correct operation, in accordance to certification standards; (2) alternative and increasingly sophisticated design safety patterns for DL with varying criticality and fault tolerance requirements; (3) DL library implementations that adhere to safety requirements; and (4) computing platform configurations, to regain determinism, and probabilistic timing analyses, to handle the remaining non-determinism.",
		"archive_location": "WOS:001027444200173",
		"event-title": "2023 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION, DATE",
		"ISBN": "1530-1591",
		"title": "SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI",
		"author": [
			{
				"family": "Abella",
				"given": "J"
			},
			{
				"family": "Perez",
				"given": "J"
			},
			{
				"family": "Englund",
				"given": "C"
			},
			{
				"family": "Zonooz",
				"given": "B"
			},
			{
				"family": "Giordana",
				"given": "G"
			},
			{
				"family": "Donzella",
				"given": "C"
			},
			{
				"family": "Cazorla",
				"given": "FJ"
			},
			{
				"family": "Mezzetti",
				"given": "E"
			},
			{
				"family": "Serra",
				"given": "I"
			},
			{
				"family": "Brando",
				"given": "A"
			},
			{
				"family": "Agirre",
				"given": "I"
			},
			{
				"family": "Eizaguirre",
				"given": "F"
			},
			{
				"family": "Bui",
				"given": "TH"
			},
			{
				"family": "Arani",
				"given": "E"
			},
			{
				"family": "Sarfraz",
				"given": "F"
			},
			{
				"family": "Balasubramaniam",
				"given": "A"
			},
			{
				"family": "Badar",
				"given": "A"
			},
			{
				"family": "Bloise",
				"given": "I"
			},
			{
				"family": "Feruglio",
				"given": "L"
			},
			{
				"family": "Cinelli",
				"given": "I"
			},
			{
				"family": "Brighenti",
				"given": "D"
			},
			{
				"family": "Cunial",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pfrommerReduceHandicapPerformance2023",
		"type": "paper-conference",
		"abstract": "The safety validation of AI and ML-based systems is challenging, as (i) analytical validation needs to include the interaction with a complex and stochastic physical environment and (ii) empirical validation needs to observe very long time-horizons to get enough \"statistical signal\" for the typically very low safety-related incident rate. This paper proposes an approach that amplifies the empirical evidence by introducing a handicap that reduces the system performance-making safety-related failures empirically more visible in a controlled environment-and gradually removing the handicap so that the convergence to the final incident rate can be estimated. Two numerical case studies are used to support and exemplify the approach.",
		"archive_location": "WOS:001066089800056",
		"DOI": "10.1109/INDIN51400.2023.10218017",
		"event-title": "2023 IEEE 21ST INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, INDIN",
		"ISBN": "1935-4576",
		"title": "Reduce the Handicap: Performance Estimation for AI Systems Safety Certification",
		"author": [
			{
				"family": "Pfrommer",
				"given": "J"
			},
			{
				"family": "Poyer",
				"given": "M"
			},
			{
				"family": "Kiroriwal",
				"given": "S"
			}
		],
		"editor": [
			{
				"family": "Dorksen",
				"given": "H"
			},
			{
				"family": "Scanzio",
				"given": "S"
			},
			{
				"family": "Jasperneite",
				"given": "J"
			},
			{
				"family": "Wisniewski",
				"given": "L"
			},
			{
				"family": "Man",
				"given": "KF"
			},
			{
				"family": "Sauter",
				"given": "T"
			},
			{
				"family": "Seno",
				"given": "L"
			},
			{
				"family": "Trsek",
				"given": "H"
			},
			{
				"family": "Vyatkin",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "poenaru-olaruRetrainAISystems2023",
		"type": "paper-conference",
		"abstract": "Deployed machine learning systems often suffer from accuracy degradation over time generated by constant data shifts, also known as concept drift. Therefore, these systems require regular maintenance, in which the machine learning model needs to be adapted to concept drift. The literature presents plenty of model adaptation techniques. The most common technique is periodically executing the whole training pipeline with all the data gathered until a particular point in time, yielding a massive energy footprint. In this paper, we propose a research path that uses concept drift detection and adaptation to enable sustainable AI systems.",
		"archive_location": "WOS:001041741400003",
		"DOI": "10.1109/GREENS59328.2023.00009",
		"event-title": "2023 IEEE/ACM 7TH INTERNATIONAL WORKSHOP ON GREEN AND SUSTAINABLE SOFTWARE, GREENS",
		"ISBN": "979-8-3503-1238-6",
		"page": "17-18",
		"title": "Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques",
		"author": [
			{
				"family": "Poenaru-Olaru",
				"given": "L"
			},
			{
				"family": "Sallou",
				"given": "J"
			},
			{
				"family": "Cruz",
				"given": "L"
			},
			{
				"family": "Rellermeyer",
				"given": "JS"
			},
			{
				"family": "Deursen",
				"given": "A",
				"non-dropping-particle": "van"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "okawaAutomaticExplorationProcess2020",
		"type": "paper-conference",
		"abstract": "In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation. Copyright (C) 2020 The Authors.",
		"archive_location": "WOS:000652592500257",
		"DOI": "10.1016/j.ifacol.2020.12.2198",
		"event-title": "IFAC PAPERSONLINE",
		"ISBN": "2405-8963",
		"note": "issue: 2",
		"page": "1588-1595",
		"title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction",
		"volume": "53",
		"author": [
			{
				"family": "Okawa",
				"given": "Y"
			},
			{
				"family": "Sasaki",
				"given": "T"
			},
			{
				"family": "Iwane",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dobbeHardChoicesArtificial2021",
		"type": "article-journal",
		"abstract": "As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured. (C) 2021 The Authors. Published by Elsevier B.V.",
		"archive_location": "WOS:000697026000010",
		"container-title": "ARTIFICIAL INTELLIGENCE",
		"DOI": "10.1016/j.artint.2021.103555",
		"ISSN": "0004-3702",
		"title": "Hard choices in artificial intelligence",
		"volume": "300",
		"author": [
			{
				"family": "Dobbe",
				"given": "R"
			},
			{
				"family": "Gilbert",
				"given": "TK"
			},
			{
				"family": "Mintz",
				"given": "Y"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "carlucciMultiDIALDomainAlignment2021",
		"type": "article-journal",
		"abstract": "One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach.",
		"archive_location": "WOS:000714203900021",
		"container-title": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
		"DOI": "10.1109/TPAMI.2020.3001338",
		"ISSN": "0162-8828",
		"issue": "12",
		"page": "4441-4452",
		"title": "MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation",
		"volume": "43",
		"author": [
			{
				"family": "Carlucci",
				"given": "FM"
			},
			{
				"family": "Porzi",
				"given": "L"
			},
			{
				"family": "Caputo",
				"given": "B"
			},
			{
				"family": "Ricci",
				"given": "E"
			},
			{
				"family": "Bulo",
				"given": "SR"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					1
				]
			]
		}
	},
	{
		"id": "otteInterpretableSemiparametricRegression2014",
		"type": "article-journal",
		"abstract": "Unreliable extrapolation of data-driven models hinders their applicability not only in safety-related domains. The paper discusses how model interpretability and uncertainty estimates can address this problem. A new semi-parametric approach is proposed for providing an interpretable model with improved accuracy by combining a symbolic regression model with a residual Gaussian Process. While the learned symbolic model is highly interpretable the residual model usually is not. However, by limiting the output of the residual model to a defined range a worst-case guarantee can be given in the sense that the maximal deviation from the symbolic model is always below a defined limit. The limitation of the residual model can include the uncertainty estimate of the Gaussian Process, thus giving the residual model more impact in high-confidence regions. When ranking the accuracy and interpretability of several different approaches on the SARCOS data benchmark the proposed combination yields the best result",
		"archive_location": "WOS:000340982800001",
		"container-title": "NEUROCOMPUTING",
		"DOI": "10.1016/j.neucom.2013.11.042",
		"ISSN": "0925-2312",
		"page": "1-6",
		"title": "Interpretable semi-parametric regression models with defined error bounds",
		"volume": "143",
		"author": [
			{
				"family": "Otte",
				"given": "C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014",
					11,
					2
				]
			]
		}
	},
	{
		"id": "burtonAddressingUncertaintySafety2023",
		"type": "article-journal",
		"abstract": "There is increasing interest in the application of machine learning (ML) technologies to safety-critical cyber-physical systems, with the promise of increased levels of autonomy due to their potential for solving complex perception and planning tasks. However, demonstrating the safety of ML is seen as one of the most challenging hurdles to their widespread deployment for such applications. In this paper we explore the factors which make the safety assurance of ML such a challenging task. In particular we address the impact of uncertainty on the confidence in ML safety assurance arguments. We show how this uncertainty is related to complexity in the ML models as well as the inherent complexity of the tasks that they are designed to implement. Based on definitions of uncertainty as well as an exemplary assurance argument structure, we examine typical weaknesses in the argument and how these can be addressed. The analysis combines an understanding of causes of insufficiencies in ML models with a systematic analysis of the types of asserted context, asserted evidence and asserted inference within the assurance argument. This leads to a systematic identification of requirements on the assurance argument structure as well as supporting evidence. We conclude that a combination of qualitative arguments combined with quantitative evidence are required to build a robust argument for safety-related properties of ML functions that is continuously refined to reduce residual and emerging uncertainties in the arguments after the function has been deployed into the target environment.",
		"archive_location": "WOS:000971580900001",
		"container-title": "FRONTIERS IN COMPUTER SCIENCE",
		"DOI": "10.3389/fcomp.2023.1132580",
		"ISSN": "2624-9898",
		"title": "Addressing uncertainty in the safety assurance of machine-learning",
		"volume": "5",
		"author": [
			{
				"family": "Burton",
				"given": "S"
			},
			{
				"family": "Herd",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					6
				]
			]
		}
	},
	{
		"id": "douthwaiteEstablishingVerificationValidation2017",
		"type": "paper-conference",
		"abstract": "The assurance of autonomous systems and the technologies that drive them is a major research challenge in the safety-critical systems engineering domain. The nature of many of these Machine Learning (ML) and Artificial Intelligence (AI) approaches raises a number of additional, technology-specific assurance concerns. One such approach is the Bayesian Network (BN) probabilistic modelling framework. Bayesian Networks and the family of modelling techniques they belong to form the basis of many AI applications. However, little research has been conducted into the assurance of BN-based systems for use in safety-critical applications. This paper explores some of the key distinctions between BN-based software-intensive systems and conventional software systems. It introduces a modelling framework that explicitly captures BN-based system-specific considerations and facilitates both the communication of assurance concerns between safety practitioners and system stakeholders, and the subsequent safety analysis of the system itself. It demonstrates how this approach can be used to develop specific verification and validation objectives for a BN-based system in a medical application.",
		"archive_location": "WOS:000418465000065",
		"DOI": "10.1109/ISSREW.2017.60",
		"event-title": "2017 IEEE 28TH INTERNATIONAL SYMPOSIUM ON SOFTWARE RELIABILITY ENGINEERING WORKSHOPS (ISSREW 2017)",
		"ISBN": "2375-821X",
		"page": "302-309",
		"title": "Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks",
		"author": [
			{
				"family": "Douthwaite",
				"given": "M"
			},
			{
				"family": "Kelly",
				"given": "T"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "khanNoHarmNovel2023",
		"type": "article-journal",
		"abstract": "Given the impact artificial intelligence (AI)-based medical technologies (hardware devices, software programs, and mobile apps) can have on society, debates regarding the principles behind their development and deployment are emerging. Using the biopsychosocial model applied in psychiatry and other fields of medicine as our foundation, we propose a novel 3-step framework to guide industry developers of AI-based medical tools as well as health care regulatory agencies on how to decide if a product should be launched-a \"Go or No-Go\" approach. More specifically, our novel framework places stakeholders' (patients, health care professionals, industry, and government institutions) safety at its core by asking developers to demonstrate the biological-psychological (impact on physical and mental health), economic, and social value of their AI tool before it is launched. We also introduce a novel cost-effective, time-sensitive, and safety-oriented mixed quantitative and qualitative clinical phased trial approach to help industry and government health care regulatory agencies test and deliberate on whether to launch these AI-based medical technologies. To our knowledge, our biological-psychological, economic, and social (BPES) framework and mixed method phased trial approach are the first to place the Hippocratic Oath of \"Do No Harm\" at the center of developers', implementers', regulators', and users' mindsets when determining whether an AI-based medical technology is safe to launch. Moreover, as the welfare of AI users and developers becomes a greater concern, our framework's novel safety feature will allow it to complement existing and future AI reporting guidelines.",
		"archive_location": "WOS:001007075800001",
		"container-title": "JOURNAL OF MEDICAL INTERNET RESEARCH",
		"DOI": "10.2196/43386",
		"ISSN": "1438-8871",
		"title": "A \"Do No Harm\" Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework",
		"volume": "25",
		"author": [
			{
				"family": "Khan",
				"given": "WU"
			},
			{
				"family": "Seto",
				"given": "E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					5
				]
			]
		}
	},
	{
		"id": "carlanAutomatingSafetyArgument2022",
		"type": "paper-conference",
		"abstract": "The need to make sense of complex input data within a vast variety of unpredictable scenarios has been a key driver for the use of machine learning (ML), for example in Automated Driving Systems (ADS). Such systems are usually safety-critical, and therefore they need to be safety assured. In order to consider the results of the safety assurance activities (scoping uncovering previously unknown hazardous scenarios), a continuous approach to arguing safety is required, whilst iteratively improving ML-specific safety-relevant properties, such as robustness and prediction certainty. Such a continuous safety life cycle will only be practical with an efficient and effective approach to analyzing the impact of system changes on the safety case. In this paper, we propose a semi-automated approach for accurately identifying the impact of changes on safety arguments. We focus on arguments that reason about the sufficiency of the data used for the development of ML components. The approach qualitatively and quantitatively analyses the impact of changes in the input space of the considered ML component on other artifacts created during the execution of the safety life cycle, such as datasets and performance requirements and makes recommendations to safety engineers for handling the identified impact. We implement the proposed approach in a model-based safety engineering environment called FASTEN, and we demonstrate its application for an ML-based pedestrian detection component of an ADS.",
		"archive_location": "WOS:000965064800005",
		"DOI": "10.1109/PRDC55274.2022.00019",
		"event-title": "2022 IEEE 27TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING (PRDC)",
		"ISBN": "1555-094X",
		"page": "43-53",
		"title": "Automating Safety Argument Change Impact Analysis for Machine Learning Components",
		"author": [
			{
				"family": "Carlan",
				"given": "C"
			},
			{
				"family": "Gauerhof",
				"given": "L"
			},
			{
				"family": "Gallina",
				"given": "B"
			},
			{
				"family": "Burton",
				"given": "S"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "elfwingParallelRewardPunishment2017",
		"type": "paper-conference",
		"abstract": "An important issue in reinforcement learning systems for autonomous agents is whether it makes sense to have separate systems for predicting rewards and punishments. In robotics, learning and control are typically achieved by a single controller, with punishments coded as negative rewards. However in biological systems, some evidence suggests that the brain has a separate system for punishment. Although this may in part be due to biological constraints of implementing negative quantities, it raises the question as to whether there is any computational rationale for keeping reward and punishment prediction operationally distinct. Here we outline a basic argument supporting this idea, based on the proposition that learning best-case predictions (as in Q-learning) does not always achieve the safest behaviour. We introduce a modified RL scheme involving a new algorithm which we call 'MaxPain' - which back-ups worst-case predictions in parallel, and then scales the two predictions in a multi-attribute RL policy. i.e. independently learning 'what to do' as well as 'what not to do' and then combining this information. We show how this scheme can improve performance in benchmark RL environments, including a grid-world experiment and a delayed version of the mountain car experiment. In particular, we demonstrate how early exploration and learning are substantially improved, leading to much 'safer' behaviour. In conclusion, the results illustrate the importance of independent punishment prediction in RL, and provide a testable framework for better understanding punishment (such as pain) and avoidance in humans, in both health and disease.",
		"archive_location": "WOS:000491967600019",
		"event-title": "2017 THE SEVENTH JOINT IEEE INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING AND EPIGENETIC ROBOTICS (ICDL-EPIROB)",
		"ISBN": "2161-9484",
		"page": "140-147",
		"title": "Parallel reward and punishment control in humans and robots: safe reinforcement learning using the MaxPain algorithm",
		"author": [
			{
				"family": "Elfwing",
				"given": "S"
			},
			{
				"family": "Seymour",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "badeaMoralityMachinesInterpretation2022",
		"type": "paper-conference",
		"abstract": "We present what we call the Interpretation Problem, whereby any rule in symbolic form is open to infinite interpretation in ways thatwemight disapprove of and argue that any attempt to build morality into machines is subject to it. We show how the Interpretation Problem in Artificial Intelligence is an illustration of Wittgenstein's general claim that no rule can contain the criteria for its own application, and that the risks created by this problem escalates in proportion to the degree to which a machine is causally connected to the world, in what we call the Law of Interpretative Exposure. Using games as an illustration, we attempt to define the structure of normative spaces and argue that any rule-following within a normative space is guided by values that are external to that space and which cannot themselves be represented as rules. In light of this, we categorise the types of mistakes an artificial moral agent could make into Mistakes of Intention and Instrumental Mistakes, and we proposeways of building morality into machines by getting them to interpret the rules we give in accordance with these external values, through explicit moral reasoning, the \"Show, not Tell\" paradigm, the adjustment of causal power and structure of the agent, and relational values, with the ultimate aim that the machine develop a virtuous character and that the impact of the Interpretation Problem is minimised.",
		"archive_location": "WOS:000922637500009",
		"DOI": "10.1007/978-3-031-21441-7_9",
		"event-title": "ARTIFICIAL INTELLIGENCE XXXIX, AI 2022",
		"ISBN": "0302-9743",
		"page": "124-137",
		"title": "Morality, Machines, and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents",
		"volume": "13652",
		"author": [
			{
				"family": "Badea",
				"given": "C"
			},
			{
				"family": "Artus",
				"given": "G"
			}
		],
		"editor": [
			{
				"family": "Bramer",
				"given": "M"
			},
			{
				"family": "Stahl",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kocakSafePredictMetaAlgorithmMachine2021",
		"type": "article-journal",
		"abstract": "SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, 1 - epsilon, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed epsilon. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate epsilon, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415.",
		"archive_location": "WOS:000607383300019",
		"container-title": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
		"DOI": "10.1109/TPAMI.2019.2932415",
		"ISSN": "0162-8828",
		"issue": "2",
		"page": "663-678",
		"title": "SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",
		"volume": "43",
		"author": [
			{
				"family": "Kocak",
				"given": "MA"
			},
			{
				"family": "Ramirez",
				"given": "D"
			},
			{
				"family": "Erkip",
				"given": "E"
			},
			{
				"family": "Shasha",
				"given": "DE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					1
				]
			]
		}
	},
	{
		"id": "tayStudyRealtimeArtificial1998",
		"type": "paper-conference",
		"abstract": "This paper highlights some of the important considerations when applying artificial intelligence techniques in real-time applications. There is a specific focus on real-time expert systems. The issues of reliability and safety in real-time control systems are also addressed. Important considerations like scheduling, real-time operating systems are also highlighted in designing real-time systems. Copyright (C) 1998 IFAC.",
		"archive_location": "WOS:000077333600018",
		"event-title": "ARTIFICIAL INTELLIGENCE IN REAL-TIME CONTROL 1997",
		"ISBN": "0962-9505",
		"page": "109-114",
		"title": "A study on real-time artificial intelligence",
		"author": [
			{
				"family": "Tay",
				"given": "EB"
			},
			{
				"family": "Gan",
				"given": "OP"
			},
			{
				"family": "Ho",
				"given": "WK"
			}
		],
		"editor": [
			{
				"family": "Rauch",
				"given": "HE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1998"
				]
			]
		}
	},
	{
		"id": "dacquistoConflictsEthicalLogical2020",
		"type": "article-journal",
		"abstract": "Artificial intelligence is nowadays a reality. Setting rules on the potential outcomes of intelligent machines, so that no surprise can be expected by humans from the behavior of those machines, is becoming a priority for policy makers. In its recent Communication \"Artificial Intelligence for Europe\" (EU Commission2018), for instance, the European Commission identifies the distinguishing trait of an intelligent machine in the presence of \"a certain degree of autonomy\" in decision making, in the light of the context. The crucial issue to be addressed is, therefore, whether it is possible to identify a set of rules for data use by intelligent machines so that the decision-making autonomy of machines can allow for humans' traditional informational self-determination (humans provide machines only with the data they decide to), as enshrined in many existing legal frameworks (including, for personal data protection, the EU's General Data Protection Regulation) (EU Parliament and Council2016) and can actually turn out to be further beneficial to individuals. Governing the autonomy of machines can be a very ambitious goal for humans since machines are geared first to the principles of formal logic and then-possibly-to ethical or legal principles. This introduces an unprecedented degree of complexity in how a norm should be engineered, which requires, in turn, an in-depth reflection in order to prevent conflicts between the legal and ethical principles underlying humans' civil coexistence and the rules of formal logic upon which the functioning of machines is based (EU Parliament2017).",
		"archive_location": "WOS:000548491100001",
		"container-title": "AI & SOCIETY",
		"DOI": "10.1007/s00146-019-00927-6",
		"ISSN": "0951-5666",
		"issue": "4",
		"page": "895-900",
		"title": "On conflicts between ethical and logical principles in artificial intelligence",
		"volume": "35",
		"author": [
			{
				"family": "D'Acquisto",
				"given": "G"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "zhangBarrierLyapunovFunctionBased2022",
		"type": "article-journal",
		"abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton-Jacobi-Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
		"archive_location": "WOS:000826063500001",
		"container-title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS",
		"DOI": "10.1109/TNNLS.2022.3186528",
		"ISSN": "2162-237X",
		"title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
		"author": [
			{
				"family": "Zhang",
				"given": "YX"
			},
			{
				"family": "Liang",
				"given": "XL"
			},
			{
				"family": "Li",
				"given": "DY"
			},
			{
				"family": "Ge",
				"given": "SZS"
			},
			{
				"family": "Gao",
				"given": "BZ"
			},
			{
				"family": "Chen",
				"given": "H"
			},
			{
				"family": "Lee",
				"given": "TH"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					12
				]
			]
		}
	},
	{
		"id": "boudiDeepReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.",
		"archive_location": "WOS:000950940200005",
		"container-title": "FORMAL ASPECTS OF COMPUTING",
		"DOI": "10.1145/3577204",
		"ISSN": "0934-5043",
		"issue": "1",
		"title": "A Deep Reinforcement Learning Framework with Formal Verification",
		"volume": "35",
		"author": [
			{
				"family": "Boudi",
				"given": "Z"
			},
			{
				"family": "Wakrime",
				"given": "AA"
			},
			{
				"family": "Toub",
				"given": "M"
			},
			{
				"family": "Haloua",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "terraSafetyVsEfficiency2020",
		"type": "paper-conference",
		"abstract": "The use of AI-based risk mitigation is increasing to provide safety in the areas of smart manufacturing, automated logistics etc, where the human-robot collaboration operations are in use. This paper presents our work on implementation of fuzzy logic system (FLS) and reinforcement learning (RL) to build risk mitigation modules for human-robot collaboration scenarios. Risk mitigation using FLS strategy is developed by manually defining the linguistic values, tuning the membership functions and generating the rules based on ISO/TS15066:2016. RL-based risk mitigation modules are developed using three different Qnetworks to estimate the Q-value function. Our purpose is twofold: to perform a comparative analysis of FLS and RL in terms of safety perspectives and further to evaluate the efficiency to accomplish the task. Our results present that all the proposed risk mitigation strategies improve the safety aspect by up to 26% as compared to a default setup where the robot is just relying on a navigation module without risk mitigation. The efficiency of using FLS model is maintained to the default setup, while the efficiency of using RL model is reduced by 26% from the default setup. We also compare the computation performance of risk mitigation between centralized and edge execution where the edge execution is 27.5 times faster than the centralized one.",
		"archive_location": "WOS:000591176900027",
		"DOI": "10.1109/iccar49639.2020.9108037",
		"event-title": "2020 6TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND ROBOTICS (ICCAR)",
		"ISBN": "2251-2446",
		"page": "151-160",
		"title": "Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics",
		"author": [
			{
				"family": "Terra",
				"given": "A"
			},
			{
				"family": "Riaz",
				"given": "H"
			},
			{
				"family": "Raizer",
				"given": "K"
			},
			{
				"family": "Hata",
				"given": "A"
			},
			{
				"family": "Inam",
				"given": "R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "maskaraDevelopingSaferAIconcepts2023",
		"type": "article-journal",
		"abstract": "With the rapid advancement of AI, there exists a possibility of rogue human actor(s) taking control of a potent AI system or an AI system redefining its objective function such that it presents an existential threat to mankind or severely curtails its freedom. Therefore, some suggest an outright ban on AI development while others profess international agreement on constraining specific types of AI. These approaches are untenable because countries will continue developing AI for national defense, regardless. Some suggest having an all-powerful benevolent one-AI that will act as an AI nanny. However, such an approach relies on the everlasting benevolence of one-AI, an untenable proposition. Furthermore, such an AI is itself subject to capture by a rogue actor. We present an alternative approach that uses existing mechanisms and time-tested economic concepts of competition and marginal analysis to limit centralization and integration of AI, rather than AI itself. Instead of depending on international consensus it relies on countries working in their best interests. We recommend that through regulation and subsidies countries promote independent development of competing AI technologies, especially those with decentralized architecture. The Sherman Antitrust Act can be used to limit the domain of an AI system, training module, or any of its components. This will increase the segmentation of potent AI systems and force technological incompatibility across systems. Finally, cross-border communication between AI-enabled systems should be restricted, something countries like China and the US are already inclined to do to serve their national interests. Our approach can ensure the availability of numerous sufficiently powerful AI systems largely disconnected from each other that can be called upon to identify and neutralize rogue systems when needed. This setup can provide sufficient deterrence to any rational human or AI system from attempting to exert undue control.",
		"archive_location": "WOS:001076383400001",
		"container-title": "AI & SOCIETY",
		"DOI": "10.1007/s00146-023-01778",
		"ISSN": "0951-5666",
		"title": "Developing safer AI-concepts from economics to the rescue",
		"author": [
			{
				"family": "Maskara",
				"given": "PK"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					2
				]
			]
		}
	},
	{
		"id": "cornelissenReflectionMachinesIncreasing2022",
		"type": "article-journal",
		"abstract": "Rapid developments in Artificial Intelligence are leading to an increasing human reliance on machine decision making. Even in collaborative efforts with Decision Support Systems (DSSs), where a human expert is expected to make the final decisions, it can be hard to keep the expert actively involved throughout the decision process. DSSs suggest their own solutions and thus invite passive decision making. To keep humans actively 'on' the decision-making loop and counter overreliance on machines, we propose a 'reflection machine' (RM). This system asks users questions about their decision strategy and thereby prompts them to evaluate their own decisions critically. We discuss what forms RMs can take and present a proof-of-concept implementation of a RM that can produce feedback on users' decisions in the medical and law domains. We show that the prototype requires very little domain knowledge to create reasonably intelligent critiquing questions. With this prototype, we demonstrate the technical feasibility to develop RMs and hope to pave the way for future research into their effectiveness and value.",
		"archive_location": "WOS:000781340300001",
		"container-title": "ETHICS AND INFORMATION TECHNOLOGY",
		"DOI": "10.1007/s10676-022-09645-y",
		"ISSN": "1388-1957",
		"issue": "2",
		"title": "Reflection machines: increasing meaningful human control over Decision Support Systems",
		"volume": "24",
		"author": [
			{
				"family": "Cornelissen",
				"given": "NAJ"
			},
			{
				"family": "Eerdt",
				"given": "RJM",
				"non-dropping-particle": "van"
			},
			{
				"family": "Schraffenberger",
				"given": "HK"
			},
			{
				"family": "Haselager",
				"given": "WFG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					6
				]
			]
		}
	},
	{
		"id": "sallamiSafetyRobustnessDeep2019",
		"type": "paper-conference",
		"abstract": "Embedding machine or deep learning software into safety-critical systems such as autonomous vehicles requires software verification and validation. Such software adds non traceable hazards to traditional hardware and sensors failures, not to mention attacks that fool the prediction of a DNN and hampers its robustness. Formal methods from computer science are now applied to deep neural networks to assess the local and global robustness of a given DNN. Typically static analysis with Abstract Interpretation or SAT solvers approaches are applied to neural networks and leverages the important progress of formal methods over the last decades. Such approaches estimate bounds on the perturbation of the inputs and formally guarantee the same DNN prediction within these bounds. However formal methods over DNN for image perception system have only been applied to simple image attacks (2D rotation, brightness). In this work, we extend the definition of Lower and Upper Bounds to assess the robustness of a DNN perception system against more generic attacks. We propose a general method to verify object recognition systems using Abstract Interpretation theory. Another major contribution is the adaptation of Upper and Lower Bounds with the abstract intervals to support more complex attacks. We consider the three following classes: convolutional attacks, occlusion attacks and geometrical transformations. For the last one, we generalize the geometrical transformations with displacements in the three-dimensional space.",
		"archive_location": "WOS:000651201400030",
		"DOI": "10.1007/978-3-030-36808-1_30",
		"event-title": "NEURAL INFORMATION PROCESSING (ICONIP 2019), PT IV",
		"ISBN": "1865-0929",
		"page": "274-286",
		"title": "Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks",
		"volume": "1142",
		"author": [
			{
				"family": "Sallami",
				"given": "MM"
			},
			{
				"family": "Ibn Khedher",
				"given": "M"
			},
			{
				"family": "Trabelsi",
				"given": "A"
			},
			{
				"family": "Kerboua-Benlarbi",
				"given": "S"
			},
			{
				"family": "Bettebghor",
				"given": "D"
			}
		],
		"editor": [
			{
				"family": "Gedeon",
				"given": "T"
			},
			{
				"family": "Wong",
				"given": "KW"
			},
			{
				"family": "Lee",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ranjbarSafetyMonitoringNeural2022",
		"type": "article-journal",
		"abstract": "Neural networks are currently suggested to be implemented in several different driving functions of autonomous vehicles. While showing promising results the drawback lies in the difficulty of safety verification and ensuring operation as intended. The aim of this paper is to increase safety when using neural networks, by proposing a monitoring framework based on novelty estimation of incoming driving data. The idea is to use unsupervised instance discrimination to learn a similarity measure across ego-vehicle camera images. By estimating a von Mises-Fisher distribution of expected ego-camera images they can be compared with unexpected novel images. A novelty measurement is inferred through the likelihood of test frames belonging to the expected distribution. The suggested method provides competitive results to several other novelty or anomaly detection algorithms on the CIFAR-10 and CIFAR-100 datasets. It also shows promising results on real world driving scenarios by distinguishing novel driving scenes from the training data of BDD100 k. Applied on the identical training-test data split, the method is also able to predict the performance profile of a segmentation network. Finally, examples are provided on how this method can be extended to find novel segments in images.",
		"archive_location": "WOS:000873905600027",
		"container-title": "IEEE TRANSACTIONS ON INTELLIGENT VEHICLES",
		"DOI": "10.1109/TIV.2022.3152084",
		"ISSN": "2379-8858",
		"issue": "3",
		"page": "711-721",
		"title": "Safety Monitoring of Neural Networks Using Unsupervised Feature Learning and Novelty Estimation",
		"volume": "7",
		"author": [
			{
				"family": "Ranjbar",
				"given": "A"
			},
			{
				"family": "Hornauer",
				"given": "S"
			},
			{
				"family": "Fredriksson",
				"given": "J"
			},
			{
				"family": "Yu",
				"given": "SX"
			},
			{
				"family": "Chan",
				"given": "CY"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					9
				]
			]
		}
	},
	{
		"id": "antikainenDeploymentModelExtend2021",
		"type": "paper-conference",
		"abstract": "There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOIA method for creating ethically aligned AI -systems. ECCOIA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOIA with a deployment model to drive the adoption of ECCOIA, as any method - no matter how good -is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given life-cycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition",
		"archive_location": "WOS:000788547300034",
		"DOI": "10.1109/REW53955.2021.00043",
		"event-title": "29TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE WORKSHOPS (REW 2021)",
		"ISBN": "978-1-6654-1898-0",
		"page": "230-235",
		"title": "A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA",
		"author": [
			{
				"family": "Antikainen",
				"given": "J"
			},
			{
				"family": "Agbese",
				"given": "M"
			},
			{
				"family": "Alanen",
				"given": "HK"
			},
			{
				"family": "Halme",
				"given": "E"
			},
			{
				"family": "Isomäki",
				"given": "H"
			},
			{
				"family": "Jantunen",
				"given": "M"
			},
			{
				"family": "Kemell",
				"given": "KK"
			},
			{
				"family": "Rousi",
				"given": "R"
			},
			{
				"family": "Vainio-Pekka",
				"given": "H"
			},
			{
				"family": "Vakkuri",
				"given": "V"
			}
		],
		"editor": [
			{
				"family": "Yue",
				"given": "T"
			},
			{
				"family": "Mirakhorli",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yuanJointDomainAdaptation2021",
		"type": "article-journal",
		"abstract": "Domain adaptation aims to improve the performance of the classifier in the target domain by reducing the difference between the two domains. Domain shifts usually exist in both marginal distribution and conditional distribution, and their relative importance varies with datasets. Moreover, there is an influence between marginal distribution distance and conditional distribution distance. However, joint domain adaptation approaches rarely consider those. Existing dynamic distribution alignment methods require a feature discriminator, and they need to train a subdomain discriminator for each class. Besides, they don't think about the interaction between the two distribution distances. In this article, we propose a dynamic joint domain adaptation approach, namely Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning (ADPL), to deal with the above problems. Both marginal distribution alignment and conditional distribution alignment can be implemented by adversarial learning. The dynamic algorithm can keep a balance between marginal and conditional distribution alignment with only two domain discriminators. In addition, the dynamic algorithm takes the influence between the two distribution distances into consideration. Compared with several advanced domain adaptation methods on both text and image datasets, all classification experiments and extensive comparison experiments demonstrate that ADPL has higher learning performance of classification and less running time. This reveals that ADPL outperforms the state-of-the-art domain adaptation approaches.",
		"archive_location": "WOS:000677536500017",
		"container-title": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE",
		"DOI": "10.1109/TETCI.2021.3055873",
		"ISSN": "2471-285X",
		"issue": "4",
		"page": "714-723",
		"title": "Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning",
		"volume": "5",
		"author": [
			{
				"family": "Yuan",
				"given": "YM"
			},
			{
				"family": "Li",
				"given": "YH"
			},
			{
				"family": "Zhu",
				"given": "ZL"
			},
			{
				"family": "Li",
				"given": "RX"
			},
			{
				"family": "Gu",
				"given": "XW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "choMaturityModelTrustworthy2023",
		"type": "article-journal",
		"abstract": "Recently, AI software has been rapidly growing and is widely used in various industrial domains, such as finance, medicine, robotics, and autonomous driving. Unlike traditional software, in which developers need to define and implement specific functions and rules according to requirements, AI software learns these requirements by collecting and training relevant data. For this reason, if unintended biases exist in the training data, AI software can create fairness and safety issues. To address this challenge, we propose a maturity model for ensuring trustworthy and reliable AI software, known as AI-MM, by considering common AI processes and fairness-specific processes within a traditional maturity model, SPICE (ISO/IEC 15504). To verify the effectiveness of AI-MM, we applied this model to 13 real-world AI projects and provide a statistical assessment on them. The results show that AI-MM not only effectively measures the maturity levels of AI projects but also provides practical guidelines for enhancing maturity levels.",
		"archive_location": "WOS:000977808400001",
		"container-title": "APPLIED SCIENCES-BASEL",
		"DOI": "10.3390/app13084771",
		"ISSN": "2076-3417",
		"issue": "8",
		"title": "A Maturity Model for Trustworthy AI Software Development",
		"volume": "13",
		"author": [
			{
				"family": "Cho",
				"given": "S"
			},
			{
				"family": "Kim",
				"given": "I"
			},
			{
				"family": "Kim",
				"given": "J"
			},
			{
				"family": "Woo",
				"given": "H"
			},
			{
				"family": "Shin",
				"given": "W"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4
				]
			]
		}
	},
	{
		"id": "boggustSharedInterestMeasuring2022",
		"type": "paper-conference",
		"abstract": "Saliency methods-techniques to identify the importance of input features on a model's output-are a common step in understanding neural network behavior. However, interpreting saliency requires tedious manual inspection to identify and aggregate patterns in model behavior, resulting in ad hoc or cherry-picked analysis. To address these concerns, we present Shared Interest: metrics for comparing model reasoning (via saliency) to human reasoning (via ground truth annotations). By providing quantitative descriptors, Shared Interest enables ranking, sorting, and aggregating inputs, thereby facilitating large-scale systematic analysis of model behavior. We use Shared Interest to identify eight recurring patterns in model behavior, such as cases where contextual features or a subset of ground truth features are most important to the model. Working with representative real-world users, we show how Shared Interest can be used to decide if a model is trustworthy, uncover issues missed in manual analyses, and enable interactive probing.",
		"archive_location": "WOS:000890212502034",
		"DOI": "10.1145/3491102.3501965",
		"event-title": "PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22)",
		"ISBN": "978-1-4503-9157-3",
		"title": "Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior",
		"author": [
			{
				"family": "Boggust",
				"given": "A"
			},
			{
				"family": "Hoover",
				"given": "B"
			},
			{
				"family": "Satyanarayan",
				"given": "A"
			},
			{
				"family": "Strobelt",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kshetrySafetyFaceUnknown2019",
		"type": "paper-conference",
		"abstract": "Most current machine learning algorithms make highly confident yet incorrect classifications when faced with unexpected test samples from an unknown distribution different from training; such epistemic uncertainty (unknown unknowns) can have catastrophic safety implications. In this conceptual paper, we propose a method to leverage engineering science knowledge to control epistemic uncertainty and maintain decision safety. The basic idea is an algorithm fusion approach that combines data-driven learned models with physical system knowledge, to operate between the extremes of purely data-driven classifiers and purely engineering science rules. This facilitates the safe operation of data-driven engineering systems, such as wastewater treatment plants.",
		"archive_location": "WOS:000482554008080",
		"event-title": "2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)",
		"ISBN": "1520-6149",
		"page": "8162-8166",
		"title": "Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-driven Engineering Systems",
		"author": [
			{
				"family": "Kshetry",
				"given": "N"
			},
			{
				"family": "Varshney",
				"given": "LR"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "nilsenRewardTamperingEvolutionary2023",
		"type": "article-journal",
		"abstract": "Reward tampering is a problem that will impact the trustworthiness of the powerful AI systems of the future. Reward Tampering describes the problem where AI agents bypass their intended objective, enabling unintended and potentially harmful behaviours. This paper investigates whether the creative potential of evolutionary algorithms could help ensure trustworthy solutions when facing this problem. The reason why evolutionary algorithms may help combat reward tampering is that they are able to find a diverse collection of different solutions to a problem within a single run, aiding the search for desirable solutions. Four different evolutionary algorithms were deployed in tasks illustrating the problem of reward tampering. The algorithms were designed with varying degrees of human expertise, measuring how human guidance influences the ability to discover trustworthy solutions. The results indicate that the algorithms' ability to find and preserve trustworthy solutions is very dependent on preserving diversity during the search. Algorithms searching for behavioural diversity showed to be the most effective against reward tampering. Human expertise also showed to improve the certainty and quality of safe solutions, but even with only a minimal degree of human expertise, domain-independent diversity management was found to discover safe solutions.",
		"archive_location": "WOS:001068097800001",
		"container-title": "GENETIC PROGRAMMING AND EVOLVABLE MACHINES",
		"DOI": "10.1007/s10710-023-09456-0",
		"ISSN": "1389-2576",
		"issue": "2",
		"title": "Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms",
		"volume": "24",
		"author": [
			{
				"family": "Nilsen",
				"given": "MK"
			},
			{
				"family": "Nygaard",
				"given": "TF"
			},
			{
				"family": "Ellefsen",
				"given": "KO"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "costonValidityPerspectiveEvaluating2023",
		"type": "paper-conference",
		"abstract": "Recent research increasingly brings to question the appropriateness of using predictive tools in complex, real-world tasks. While a growing body of work has explored ways to improve value alignment in these tools, comparatively less work has centered concerns around the fundamental justifiability of using these tools. This work seeks to center validity considerations in deliberations around whether and how to build data-driven algorithms in high-stakes domains. Toward this end, we translate key concepts from validity theory to predictive algorithms. We apply the lens of validity to re-examine common challenges in problem formulation and data issues that jeopardize the justifiability of using predictive algorithms and connect these challenges to the social science discourse around validity. Our interdisciplinary exposition clarifies how these concepts apply to algorithmic decision making contexts. We demonstrate how these validity considerations could distill into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data.",
		"archive_location": "WOS:001012311500040",
		"DOI": "10.1109/SaTML54575.2023.00050",
		"event-title": "2023 IEEE CONFERENCE ON SECURE AND TRUSTWORTHY MACHINE LEARNING, SATML",
		"ISBN": "978-1-6654-6299-0",
		"page": "690-704",
		"title": "A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms",
		"author": [
			{
				"family": "Coston",
				"given": "A"
			},
			{
				"family": "Kawakami",
				"given": "A"
			},
			{
				"family": "Zhu",
				"given": "HY"
			},
			{
				"family": "Holstein",
				"given": "K"
			},
			{
				"family": "Heidari",
				"given": "H"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "carlsonSafeArtificialGeneral2019",
		"type": "article-journal",
		"abstract": "Artificial general intelligence (AGI) progression metrics indicate AGI will occur within decades. No proof exists that AGI will benefit humans and not harm or eliminate humans. A set of logically distinct conceptual components is proposed that are necessary and sufficient to (1) ensure various AGI scenarios will not harm humanity, and (2) robustly align AGI and human values and goals. By systematically addressing pathways to malevolent AI we can induce the methods/axioms required to redress them. Distributed ledger technology (DLT, \"blockchain\") is integral to this proposal, e.g., \"smart contracts\" are necessary to address the evolution of AI that will be too fast for human monitoring and intervention. The proposed axioms: (1) Access to technology by market license. (2) Transparent ethics embodied in DLT. (3) Morality encrypted via DLT. (4) Behavior control structure with values at roots. (5) Individual bar-code identification of critical components. (6) Configuration Item (from business continuity/disaster recovery planning). (7) Identity verification secured via DLT. (8) \"Smart\" automated contracts based on DLT. (9) Decentralized applications-AI software modules encrypted via DLT. (10) Audit trail of component usage stored via DLT. (11) Social ostracism (denial of resources) augmented by DLT petitions. (12) Game theory and mechanism design.",
		"archive_location": "WOS:000697671000006",
		"container-title": "BIG DATA AND COGNITIVE COMPUTING",
		"DOI": "10.3390/bdcc3030040",
		"ISSN": "2504-2289",
		"issue": "3",
		"title": "Safe Artificial General Intelligence via Distributed Ledger Technology",
		"volume": "3",
		"author": [
			{
				"family": "Carlson",
				"given": "KW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					9
				]
			]
		}
	},
	{
		"id": "guoSafeDeepSemiSupervised2020",
		"type": "paper-conference",
		"abstract": "Deep semi-supervised learning (SSL) has been recently shown very effectively. However, its performance is seriously decreased when the class distribution is mismatched, among which a common situation is that unlabeled data contains some classes not seen in the labeled data. Efforts on this issue remain to be limited. This paper proposes a simple and effective safe deep SSL method to alleviate the harm caused by it. In theory, the result learned from the new method is never worse than learning from merely labeled data, and it is theoretically guaranteed that its generalization approaches the optimal in the order O(root dln(n)/n), even faster than the convergence rate in supervised learning associated with massive parameters. In the experiment of benchmark data, unlike the existing deep SSL methods which are no longer as good as supervised learning in 40% of unseen-class unlabeled data, the new method can still achieve performance gain in more than 60% of unseen-class unlabeled data. Moreover, the proposal is suitable for many deep SSL algorithms and can be easily extended to handle other cases of class distribution mismatch.",
		"archive_location": "WOS:000683178504002",
		"event-title": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 119",
		"ISBN": "2640-3498",
		"title": "Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data",
		"volume": "119",
		"author": [
			{
				"family": "Guo",
				"given": "LZ"
			},
			{
				"family": "Zhang",
				"given": "ZY"
			},
			{
				"family": "Jiang",
				"given": "Y"
			},
			{
				"family": "Li",
				"given": "YF"
			},
			{
				"family": "Zhou",
				"given": "ZH"
			}
		],
		"editor": [
			{
				"family": "Daume",
				"given": "H"
			},
			{
				"family": "Singh",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "heSafeStudentSafeDeep2022",
		"type": "paper-conference",
		"abstract": "Deep semi-supervised learning (SSL) methods aim to take advantage of abundant unlabeled data to improve the algorithm performance. In this paper, we consider the problem of safe SSL scenario where unseen-class instances appear in the unlabeled data. This setting is essential and commonly appears in a variety of real applications. One intuitive solution is removing these unseen-class instances after detecting them during the SSL process. Nevertheless, the performance of unseen-class identification is limited by the small number of labeled data and ignoring the availability of unlabeled data. To take advantage of these unseen-class data and ensure performance, we propose a safe SSL method called SAFE-STUDENT from the teacher-student view. Firstly, a new scoring function called energy-discrepancy (ED) is proposed to help the teacher model improve the security of instances selection. Then, a novel unseen-class label distribution learning mechanism mitigates the unseen-class perturbation by calibrating the unseen-class label distribution. Finally, we propose an iterative optimization strategy to facilitate teacher-student network learning. Extensive studies on several representative datasets show that SAFE-STUDENT remarkably outperforms the state-of-the-art, verifying the feasibility and robustness of our method in the under-explored problem.",
		"archive_location": "WOS:000870783000016",
		"DOI": "10.1109/CVPR52688.2022.01418",
		"event-title": "2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)",
		"ISBN": "1063-6919",
		"page": "14565-14574",
		"title": "Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class Unlabeled Data",
		"author": [
			{
				"family": "He",
				"given": "RD"
			},
			{
				"family": "Han",
				"given": "ZY"
			},
			{
				"family": "Lu",
				"given": "XK"
			},
			{
				"family": "Yin",
				"given": "YL"
			},
			{
				"literal": "IEEE COMP SOC"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mayerAdversarialFeatureDistribution2021",
		"type": "article-journal",
		"abstract": "Training deep neural networks with only a few labeled samples can lead to overfitting. This is problematic in semi-supervised learning where only a few labeled samples are available. In this paper, we show that a consequence of overfitting in SSL is feature distribution misalignment between labeled and unlabeled samples. Hence, we propose a new feature distribution alignment method. Our method is particularly effective when using only a small amount of labeled samples. We test our method on CIFAR-10, SVHN and LSUN. On SVHN we achieve a test error of 3.88% (250 labeled samples) and 3.39% (1000 labeled samples), which is close to the fully supervised model 2.89% (73k labeled samples). In comparison, the current SOTA achieves only 4.29% and 3.74%. On LSUN we achieve superior results than a state-of-the- art method even when using 100x less unlabeled samples (500 labeled samples). Finally, we provide a theoretical insight why feature distribution misalignment occurs and show that our method reduces it.",
		"archive_location": "WOS:000616091100012",
		"container-title": "COMPUTER VISION AND IMAGE UNDERSTANDING",
		"DOI": "10.1016/j.cviu.2020.103109",
		"ISSN": "1077-3142",
		"title": "Adversarial feature distribution alignment for semi-supervised learning",
		"volume": "202",
		"author": [
			{
				"family": "Mayer",
				"given": "C"
			},
			{
				"family": "Paul",
				"given": "M"
			},
			{
				"family": "Timofte",
				"given": "R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					1
				]
			]
		}
	},
	{
		"id": "duanRDAReciprocalDistribution2022",
		"type": "paper-conference",
		"abstract": "In this work, we propose Reciprocal Distribution Alignment (RDA) to address semi-supervised learning (SSL), which is a hyperparameter-free framework that is independent of confidence threshold and works with both the matched (conventionally) and the mismatched class distributions. Distribution mismatch is an often overlooked but more general SSL scenario where the labeled and the unlabeled data do not fall into the identical class distribution. This may lead to the model not exploiting the labeled data reliably and drastically degrade the performance of SSL methods, which could not be rescued by the traditional distribution alignment. In RDA, we enforce a reciprocal alignment on the distributions of the predictions from two classifiers predicting pseudo-labels and complementary labels on the unlabeled data. These two distributions, carrying complementary information, could be utilized to regularize each other without any prior of class distribution. Moreover, we theoretically show that RDA maximizes the input-output mutual information. Our approach achieves promising performance in SSL under a variety of scenarios of mismatched distributions, as well as the conventional matched SSL setting. Our code is available at: https://github.com/NJUyued/RDA4RobustSSL.",
		"archive_location": "WOS:000903586400031",
		"DOI": "10.1007/978-3-031-20056-4_31",
		"event-title": "COMPUTER VISION - ECCV 2022, PT XXX",
		"ISBN": "0302-9743",
		"page": "533-549",
		"title": "RDA: Reciprocal Distribution Alignment for Robust Semi-supervised Learning",
		"volume": "13690",
		"author": [
			{
				"family": "Duan",
				"given": "Y"
			},
			{
				"family": "Qi",
				"given": "L"
			},
			{
				"family": "Wang",
				"given": "L"
			},
			{
				"family": "Zhou",
				"given": "LP"
			},
			{
				"family": "Shi",
				"given": "YH"
			}
		],
		"editor": [
			{
				"family": "Avidan",
				"given": "S"
			},
			{
				"family": "Brostow",
				"given": "G"
			},
			{
				"family": "Cisse",
				"given": "M"
			},
			{
				"family": "Farinella",
				"given": "GM"
			},
			{
				"family": "Hassner",
				"given": "T"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuSafeMultiviewCotraining2022",
		"type": "paper-conference",
		"abstract": "Co-training is a popular disagreement-based semisupervised learning method. Learners of different views mutually select reliable unlabeled instances to augment the labeled dataset. Existing co-training style algorithms have cumbersome procedures for selecting confident instances. Furthermore, the pseudolabels assigned to selected unlabeled instances are not always reliable. In this paper, we propose a safe co-training regression algorithm for multi-view scenarios with two characteristics. An instance selection strategy based on the consistency assumption aims to improve the efficiency of selecting confident unlabeled instances. This strategy makes full use of the information provided by a committee to measure the confidence of unlabeled instances. A safe labeling technique in an ensemble manner is introduced to improve the quality of pseudo-labels. The safe pseudo-labels not only integrate information provided by the committee, but also take into account the part of the receiver. The results over twenty datasets prove the superiority of the proposed algorithm against other state-of-the-art semi-supervised regression algorithms.",
		"archive_location": "WOS:000967751000007",
		"DOI": "10.1109/DSAA54385.2022.10032437",
		"event-title": "2022 IEEE 9TH INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA)",
		"ISBN": "2472-1573",
		"page": "56-65",
		"title": "Safe Multi-view Co-training for Semi-supervised Regression",
		"author": [
			{
				"family": "Liu",
				"given": "LY"
			},
			{
				"family": "Huang",
				"given": "P"
			},
			{
				"family": "Min",
				"given": "F"
			}
		],
		"editor": [
			{
				"family": "Huang",
				"given": "JZ"
			},
			{
				"family": "Pan",
				"given": "Y"
			},
			{
				"family": "Hammer",
				"given": "B"
			},
			{
				"family": "Khan",
				"given": "MK"
			},
			{
				"family": "Xie",
				"given": "X"
			},
			{
				"family": "Cui",
				"given": "L"
			},
			{
				"family": "He",
				"given": "Y"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "heNotAllParameters2022",
		"type": "paper-conference",
		"abstract": "Deep semi-supervised learning (SSL) aims to utilize a sizeable unlabeled set to train deep networks, thereby reducing the dependence on labeled instances. However, the unlabeled set often carries unseen classes that cause the deep SSL algorithm to lose generalization. Previous works focus on the data level that they attempt to remove unseen class data or assign lower weight to them but could not eliminate their adverse effects on the SSL algorithm. Rather than focusing on the data level, this paper turns attention to the model parameter level. We find that only partial parameters are essential for seen-class classification, termed safe parameters. In contrast, the other parameters tend to fit irrelevant data, termed harmful parameters. Driven by this insight, we propose Safe Parameter Learning (SPL) to discover safe parameters and make the harmful parameters inactive, such that we can mitigate the adverse effects caused by unseen-class data. Specifically, we firstly design an effective strategy to divide all parameters in the pre-trained SSL model into safe and harmful ones. Then, we introduce a bi-level optimization strategy to update the safe parameters and kill the harmful parameters. Extensive experiments show that SPL outperforms the state-of-the-art SSL methods on all the benchmarks by a large margin. Moreover, experiments demonstrate that SPL can be integrated into the most popular deep SSL networks and be easily extended to handle other cases of class distribution mismatch.",
		"archive_location": "WOS:000893636206110",
		"event-title": "THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE",
		"ISBN": "2159-5399",
		"page": "6874-6883",
		"title": "Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch",
		"author": [
			{
				"family": "He",
				"given": "RD"
			},
			{
				"family": "Han",
				"given": "ZY"
			},
			{
				"family": "Yang",
				"given": "Y"
			},
			{
				"family": "Yin",
				"given": "YL"
			},
			{
				"literal": "Assoc Advancement Artificial Intelligence"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "baeSafeSemisupervisedLearning2022",
		"type": "article-journal",
		"abstract": "Semi-supervised learning attempts to use a large set of unlabeled data to increase the pre-diction accuracy of machine learning models when the amount of labeled data is limited. However, in realistic cases, unlabeled data may worsen performance because they contain out-of-distribution (OOD) data that differ from the labeled data. To address this issue, safe semi-supervised deep learning has recently been presented. This study suggests a new safe semi-supervised algorithm that uses an uncertainty-aware Bayesian neural network. Our proposed method, safe uncertainty-based consistency training (SafeUC), uses Bayesian uncertainty to minimize the harmful effects caused by unlabeled OOD examples. The pro-posed method improves the model's generalization performance by regularizing the net-work for consistency against uncertain noise. Moreover, to avoid uncertain prediction results, the proposed method includes a practical inference tip based on a well -calibrated uncertainty. The effectiveness of the proposed method is demonstrated in the experimental results on CIFAR-10 and SVHN by showing that it achieved state-of-the-art performance for all semi-supervised learning tasks with OOD data presence rates.(c) 2022 Elsevier Inc. All rights reserved.",
		"archive_location": "WOS:000863219500003",
		"container-title": "INFORMATION SCIENCES",
		"DOI": "10.1016/j.ins.2022.08.094",
		"ISSN": "0020-0255",
		"page": "453-464",
		"title": "Safe semi-supervised learning using a bayesian neural network",
		"volume": "612",
		"author": [
			{
				"family": "Bae",
				"given": "J"
			},
			{
				"family": "Lee",
				"given": "MJ"
			},
			{
				"family": "Kim",
				"given": "SB"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					10
				]
			]
		}
	},
	{
		"id": "stankoRiskaverseDistributionalReinforcement2019",
		"type": "paper-conference",
		"abstract": "Conditional Value-at-Risk (CVaR) is a well-known measure of risk that has been directly equated to robustness, an important component of Artificial Intelligence (AI) safety. In this paper we focus on optimizing CVaR in the context of Reinforcement Learning (RL), as opposed to the usual risk-neutral expectation. As a first original contribution, we improve the CVaR Value Iteration algorithm (Chow et al., 2015) in a way that reduces computational complexity of the original algorithm from polynomial to linear time. Secondly, we propose a sampling version of CVaR Value Iteration we call CVaR Q-learning. We also derive a distributional policy improvement algorithm, and later use it as a heuristic for extracting the optimal policy from the converged CVaR Q-learning algorithm. Finally, to show the scalability of our method, we propose an approximate Q-learning algorithm by reformulating the CVaR Temporal Difference update rule as a loss function which we later use in a deep learning context. All proposed methods are experimentally analyzed, including the Deep CVaR Q-learning agent which learns how to avoid risk from raw pixels.",
		"archive_location": "WOS:000571773900044",
		"DOI": "10.5220/0008175604120423",
		"event-title": "IJCCI: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE",
		"ISBN": "978-989-758-384-1",
		"page": "412-423",
		"title": "Risk-averse Distributional Reinforcement Learning: A CVaR Optimization Approach",
		"author": [
			{
				"family": "Stanko",
				"given": "S"
			},
			{
				"family": "Macek",
				"given": "K"
			}
		],
		"editor": [
			{
				"family": "Merelo",
				"given": "JJ"
			},
			{
				"family": "Garibaldi",
				"given": "J"
			},
			{
				"family": "Barranco",
				"given": "AL"
			},
			{
				"family": "Madani",
				"given": "K"
			},
			{
				"family": "Warwick",
				"given": "K"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "selimSafeReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) algorithms can achieve state-of-the-art performance in decision-making and continuous control tasks. However, applying RL algorithms on safety-critical systems still needs to be well justified due to the exploration nature of many RL algorithms, especially when the model of the robot and the environment are unknown. To address this challenge, we propose a data-driven safety layer that acts as a filter for unsafe actions. The safety layer uses a data-driven predictive controller to enforce safety guarantees for RL policies during training and after deployment. The RL agent proposes an action that is verified by computing the data-driven reachability analysis. If there is an intersection between the reachable set of the robot using the proposed action, we call the data-driven predictive controller to find the closest safe action to the proposed unsafe action. The safety layer penalizes the RL agent if the proposed action is unsafe and replaces it with the closest safe one. In the simulation, we show that our method outperforms state-of-the-art safe RL methods on the robotics navigation problem for a Turtlebot 3 in Gazebo and a quadrotor in Unreal Engine 4 (UE4).",
		"archive_location": "WOS:000972628300008",
		"DOI": "10.1109/ICCSPA55860.2022.10018994",
		"event-title": "2022 5TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS, SIGNAL PROCESSING, AND THEIR APPLICATIONS (ICCSPA)",
		"ISBN": "2377-682X",
		"title": "Safe Reinforcement Learning using Data-Driven Predictive Control",
		"author": [
			{
				"family": "Selim",
				"given": "M"
			},
			{
				"family": "Alanwar",
				"given": "A"
			},
			{
				"family": "El-Kharashi",
				"given": "MW"
			},
			{
				"family": "Abbas",
				"given": "HM"
			},
			{
				"family": "Johansson",
				"given": "KH"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangSafeIncompleteLabel2022",
		"type": "article-journal",
		"abstract": "Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance. (C) 2021 Elsevier Ltd. All rights reserved.",
		"archive_location": "WOS:000742689700004",
		"container-title": "PATTERN RECOGNITION",
		"DOI": "10.1016/j.patcog.2021.108518",
		"ISSN": "0031-3203",
		"title": "Safe incomplete label distribution learning",
		"volume": "125",
		"author": [
			{
				"family": "Zhang",
				"given": "J"
			},
			{
				"family": "Tao",
				"given": "H"
			},
			{
				"family": "Luo",
				"given": "TJ"
			},
			{
				"family": "Hou",
				"given": "CP"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "luoLearningBarrierCertificates2021",
		"type": "paper-conference",
		"abstract": "Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS),which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates are learned via adversarial training and ensure the policy's safety assuming calibrated learned dynamics. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.",
		"archive_location": "WOS:000922928205035",
		"event-title": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)",
		"ISBN": "1049-5258",
		"title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations",
		"volume": "34",
		"author": [
			{
				"family": "Luo",
				"given": "YP"
			},
			{
				"family": "Ma",
				"given": "TY"
			}
		],
		"editor": [
			{
				"family": "Ranzato",
				"given": "M"
			},
			{
				"family": "Beygelzimer",
				"given": "A"
			},
			{
				"family": "Dauphin",
				"given": "Y"
			},
			{
				"family": "Liang",
				"given": "PS"
			},
			{
				"family": "Vaughan",
				"given": "JW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "murugesanFormalMethodsAssisted2019",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) is emerging as a powerful machine learning paradigm to develop autonomous safety critical systems; RL enables the systems to learn optimal control strategies by interacting with the environment. However, there is also widespread apprehension to deploying such systems in the real world since rigorously ensuring if they had learned safe strategies by interacting with an environment that is representative of the real world remains a challenge. Hence, there is a surge of interest to establish safety-focused RL techniques.\nIn this paper, we present a safety-assured training approach that augments standard RL with formal analysis and simulation technology. The benefits of coupling these techniques is three-fold: the formal analysis tools (SMT solvers) guide the system to learn strategies that rigorously uphold specified safety properties; the sophisticated simulators provide a wide-range of quantifiable, realistic learning environments; the adequacy of the safety properties can be assessed as agent explores complex environments. We illustrate this approach using a Flappy Bird game.",
		"archive_location": "WOS:000657973800022",
		"DOI": "10.1007/978-3-030-20652-9_22",
		"event-title": "NASA FORMAL METHODS (NFM 2019)",
		"ISBN": "0302-9743",
		"page": "333-340",
		"title": "Formal Methods Assisted Training of Safe Reinforcement Learning Agents",
		"volume": "11460",
		"author": [
			{
				"family": "Murugesan",
				"given": "A"
			},
			{
				"family": "Moghadamfalahi",
				"given": "M"
			},
			{
				"family": "Chattopadhyay",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Badger",
				"given": "JM"
			},
			{
				"family": "Rozier",
				"given": "KY"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yangSafetyconstrainedReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Safety is critical to broadening the real-world use of reinforcement learning. Modeling the safety aspects using a safety-cost signal separate from the reward and bounding the expected safety-cost is becoming standard practice, since it avoids the problem of finding a good balance between safety and performance. However, it can be risky to set constraints only on the expectation neglecting the tail of the distribution, which might have prohibitively large values. In this paper, we propose a method called Worst-Case Soft Actor Critic for safe RL that approximates the distribution of accumulated safety-costs to achieve risk control. More specifically, a certain level of conditional Value-at-Risk from the distribution is regarded as a safety constraint, which guides the change of adaptive safety weights to achieve a trade-off between reward and safety. As a result, we can compute policies whose worst-case performance satisfies the constraints. We investigate two ways to estimate the safety-cost distribution, namely a Gaussian approximation and a quantile regression algorithm. On the one hand, the Gaussian approximation is simple and easy to implement, but may underestimate the safety cost, on the other hand, the quantile regression leads to a more conservative behavior. The empirical analysis shows that the quantile regression method achieves excellent results in complex safety-constrained environments, showing good risk control.",
		"archive_location": "WOS:000814940000002",
		"container-title": "MACHINE LEARNING",
		"DOI": "10.1007/s10994-022-06187-8",
		"ISSN": "0885-6125",
		"issue": "3",
		"page": "859-887",
		"title": "Safety-constrained reinforcement learning with a distributional safety critic",
		"volume": "112",
		"author": [
			{
				"family": "Yang",
				"given": "QS"
			},
			{
				"family": "Simao",
				"given": "TD"
			},
			{
				"family": "Tindemans",
				"given": "SH"
			},
			{
				"family": "Spaan",
				"given": "MTJ"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "mengIntegratingSafetyConstraints2023",
		"type": "article-journal",
		"abstract": "The ability to resist interference is the key to the widespread application of reinforcement learning. Although adversarial training is a promising method for robust promotion, stan-dard adversarial training leads to unstable results or performance deterioration due to the presence of perturbation. To address the problem, a robust reinforcement learning method which integrates safety constraints that are modelled by environment termination condi-tions into adversarial training is proposed, where safety constraints are adopted to restrict agent's actions and guide the training process. For better modelling the robust reinforce-ment learning problem, a modified constrained Markov Decision Process (MDP) that con-siders perturbation for robust reinforcement learning, named Constrained Markov Decision Process (CMDP) with Perturbation (CMDPP) is also introduced. The proposed safe robust reinforcement learning method based on CMDPP utilizes the penalty function to solve CMDP and generates perturbation from the gradient of state for adversarial training. Tests on the robustness of the proposed method under several attack methods and evalu-ation of generalization through changing environment dynamics were carried out on the OpenAI gym and Roboschool environments. The results demonstrate that our method not only has a better performance confronting the attack but also has a higher generaliza-tion capability with reference to the changing environment dynamics.(c) 2022 Elsevier Inc. All rights reserved.",
		"archive_location": "WOS:000901771900018",
		"container-title": "INFORMATION SCIENCES",
		"DOI": "10.1016/j.ins.2022.11.051",
		"ISSN": "0020-0255",
		"page": "310-323",
		"title": "Integrating safety constraints into adversarial training for robust deep reinforcement learning",
		"volume": "619",
		"author": [
			{
				"family": "Meng",
				"given": "JL"
			},
			{
				"family": "Zhu",
				"given": "F"
			},
			{
				"family": "Ge",
				"given": "YY"
			},
			{
				"family": "Zhao",
				"given": "PY"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					1
				]
			]
		}
	},
	{
		"id": "gabourieLearningDomainInvariantEmbedding2019",
		"type": "paper-conference",
		"abstract": "We address the problem of unsupervised domain adaptation (UDA) by learning a cross-domain agnostic embedding space, where the distance between the probability distributions of the two source and target visual domains is minimized. We use the output space of a shared cross-domain deep encoder to model the embedding space and use the Sliced-Wasserstein Distance (SWD) to measure and minimize the distance between the embedded distributions of two source and target domains to enforce the embedding to be domain-agnostic. Additionally, we use the source domain labeled data to train a deep classifier from the embedding space to the label space to enforce the embedding space to be discriminative. As a result of this training scheme, we provide an effective solution to train the deep classification network on the source domain such that it will generalize well on the target domain, where only unlabeled training data is accessible. To mitigate the challenge of class matching, we also align corresponding classes in the embedding space by using high confidence pseudo-labels for the target domain, i.e. assigning the class for which the source classifier has a high prediction probability. We provide experimental results on UDA benchmark tasks to demonstrate that our method is effective and leads to state-of-the-art performance.",
		"archive_location": "WOS:000535355700051",
		"DOI": "10.1109/allerton.2019.8919960",
		"event-title": "2019 57TH ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING (ALLERTON)",
		"ISBN": "2474-0195",
		"page": "352-359",
		"title": "Learning a Domain-Invariant Embedding for Unsupervised Domain Adaptation Using Class-Conditioned Distribution Alignment",
		"author": [
			{
				"family": "Gabourie",
				"given": "AJ"
			},
			{
				"family": "Rostami",
				"given": "M"
			},
			{
				"family": "Pope",
				"given": "PE"
			},
			{
				"family": "Kolouri",
				"given": "S"
			},
			{
				"family": "Kim",
				"given": "K"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liuSafeOfflineReinforcement2022",
		"type": "paper-conference",
		"abstract": "Recently, offline reinforcement learning has gained increasing attention. However, the safety of offline reinforcement learning has been ignored. It poses a significant challenge to learn a safe and high-performance policy from a fixed dataset that contains unsafe or unexpected state-action pairs without interacting with the environment. Since the unsafe state-action pairs are usually sparse in the behavior data collected by humans, it is difficult to effectively model information about unsafe behaviors. This paper utilized the hierarchical reinforcement learning framework to alleviate the sparsity issue by modeling unsafe behaviors with hierarchical policies. Specifically, a high-level policy determines a prospective state, and a low-level policy takes action to reach the specified goal state. The training objective of the high-level policy is to improve the expected reward that the low-level policy collects when it moves toward the goal state and reduce the number of unsafe actions. We further develop data processing methods to provide training data for the high-level policy and the low-level policy. Evaluation experiments about performance and safety are conducted in simulation environments that return the rewards and unsafe costs obtained by agents during the interaction. Experimental results demonstrate that the proposed algorithm can choose safe actions while maintaining high performance.",
		"archive_location": "WOS:000870701000030",
		"DOI": "10.1007/978-3-031-05936-0_30",
		"event-title": "ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PAKDD 2022, PT II",
		"ISBN": "0302-9743",
		"page": "380-391",
		"title": "Safe Offline Reinforcement Learning Through Hierarchical Policies",
		"volume": "13281",
		"author": [
			{
				"family": "Liu",
				"given": "SF"
			},
			{
				"family": "Sun",
				"given": "SL"
			}
		],
		"editor": [
			{
				"family": "Gama",
				"given": "J"
			},
			{
				"family": "Li",
				"given": "T"
			},
			{
				"family": "Yu",
				"given": "Y"
			},
			{
				"family": "Chen",
				"given": "E"
			},
			{
				"family": "Zheng",
				"given": "Y"
			},
			{
				"family": "Teng",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "picotAdversarialRobustnessFisherRao2023",
		"type": "article-journal",
		"abstract": "Adversarial robustness has become a topic of growing interest in machine learning since it was observed that neural networks tend to be brittle. We propose an information-geometric formulation of adversarial defense and introduce Fire, a new Fisher-Rao regularization for the categorical cross-entropy loss, which is based on the geodesic distance between the softmax outputs corresponding to natural and perturbed input features. Based on the information-geometric properties of the class of softmax distributions, we derive an explicit characterization of the Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some interesting properties as well as connections with standard regularization metrics. Furthermore, we verify on a simple linear and Gaussian model, that all Pareto-optimal points in the accuracy-robustness region can be reached by Fire while other state-of-the-art methods fail. Empirically, we evaluate the performance of various classifiers trained with the proposed loss on standard datasets, showing up to a simultaneous 1% of improvement in terms of clean and robust performances while reducing the training time by 20% over the best-performing methods.",
		"archive_location": "WOS:000934990500001",
		"container-title": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
		"DOI": "10.1109/TPAMI.2022.3174724",
		"ISSN": "0162-8828",
		"issue": "3",
		"page": "2698-2710",
		"title": "Adversarial Robustness Via Fisher-Rao Regularization",
		"volume": "45",
		"author": [
			{
				"family": "Picot",
				"given": "M"
			},
			{
				"family": "Messina",
				"given": "F"
			},
			{
				"family": "Boudiaf",
				"given": "M"
			},
			{
				"family": "Labeau",
				"given": "F"
			},
			{
				"family": "Ayed",
				"given": "IB"
			},
			{
				"family": "Piantanida",
				"given": "P"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					1
				]
			]
		}
	},
	{
		"id": "fraserTrainSmallDeploy2020",
		"type": "paper-conference",
		"abstract": "In order to 'train small, deploy big', agent control policies must be transplanted from one trained agent into a larger set of agents for deployment. Given that compute resources and training time generally scale with the number of agents, this approach to generating swarm control policies may be favourable for larger swarms. However, in order for this process to be successful, the agent control policy must be indistinct to the agent on which it is trained so that it can perform as required in its new host agent. Through extensive simulation of a cooperative multi-agent navigation task, it is shown that this indistinctness of agent policies, and therefore the success of the associated learned solution of the transplanted swarm, is dependent upon the way in which an agent views the world: absolute or relative. As a corollary to, and in contrary to naive intuition of, this result, we show that homogeneous agent capability is not enough to guarantee policy indistinctness. The article also discusses what general conditions may be required in order to enforce policy indistinctness.",
		"archive_location": "WOS:001061406300021",
		"DOI": "10.1007/978-3-030-64984-5_21",
		"event-title": "AI 2020: ADVANCES IN ARTIFICIAL INTELLIGENCE",
		"ISBN": "2945-9133",
		"page": "269-280",
		"title": "Train Small, Deploy Big: Do Relative World Views Permit Swarm-Safety During Policy Transplantation for Multi-Agent Reinforcement Learning Problems?",
		"volume": "12576",
		"author": [
			{
				"family": "Fraser",
				"given": "B"
			},
			{
				"family": "Laurito",
				"given": "G"
			}
		],
		"editor": [
			{
				"family": "Gallagher",
				"given": "M"
			},
			{
				"family": "Moustafa",
				"given": "N"
			},
			{
				"family": "Lakshika",
				"given": "E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dongSafeBatchConstrained2023",
		"type": "article-journal",
		"abstract": "Batch-constrained reinforcement learning constrains the learned policy to be close to the behavior policy, which holds a tremendous promise for alleviating the distributional shift in offline reinforcement learning. Existing batch-constrained techniques rely on perturbation models to adjust the actions generated from the generative model to maximize the estimated value function. However, the perturbation model deviates from the distribution of the offline datasets and introduces a new distribution drift problem, which affects the performance of the learned policies. In addition, since offline reinforcement learning cannot learn by trial and error, the final policies are often prone to failure in reality or make unsafe decisions when trained with a noisy or small size dataset. To address the above issues, this paper employs constrained generative adversarial network to generate actions with given states. Specifically, we train the generator to maximize the estimated value and constrain the state-action pairs to follow the dataset distribution. The perturbation model is trained to maximize the probability of the perturbed actions belonging to the dataset and minimize the likelihood of taking dangerous actions. Moreover, we utilize safety critics to predict the risk of the actions under a state. Experimental results show that the proposed method is effective and can choose safe actions while maintaining a high performance in offline settings.",
		"archive_location": "WOS:000962845200001",
		"container-title": "INFORMATION SCIENCES",
		"DOI": "10.1016/j.ins.2023.03.108",
		"ISSN": "0020-0255",
		"page": "259-270",
		"title": "Safe batch constrained deep reinforcement learning with generative adversarial network",
		"volume": "634",
		"author": [
			{
				"family": "Dong",
				"given": "WB"
			},
			{
				"family": "Liu",
				"given": "SF"
			},
			{
				"family": "Sun",
				"given": "SL"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "osinenkoActorCriticFrameworkOnline2023",
		"type": "article-journal",
		"abstract": "Online actor-critic reinforcement learning is concerned with training an agent on-the-fly via dynamic interaction with the environment. Due to the specifics of the application, it is not generally possible to perform long pre-training, as it is commonly done in off-line, tabular or Monte-Carlo mode. Such applications may be found more frequently in industry, rather than in pure digital fields, such as cloud services, video games, database management, etc., where reinforcement learning has been demonstrating success. Stability of the closed-loop of the agent plus the environment is a major challenge here, and not only in terms of the environment safety and integrity, but also in terms of sparing resources on failed training episodes. In this paper, we tackle the problem of environment stability under an actor-critic reinforcement learning agent by integration of the Lyapunov stability theory tools. Under the presented approach, the closed-loop stability is secured in all episodes without pre-training. It was observed in a case study with a mobile robot that the suggested agent could always successfully achieve the control goal, while significantly reducing the cost. While many approaches may be exploited for mobile robot control, we suggest that the experiments showed the promising potential of actor-critic reinforcement learning agents based on Lyapunov-like constraints. The presented methodology may be utilized in safety-critical, industrial applications where stability is necessary.",
		"archive_location": "WOS:001061771000001",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2023.3306070",
		"ISSN": "2169-3536",
		"page": "89188-89204",
		"title": "An Actor-Critic Framework for Online Control With Environment Stability Guarantee",
		"volume": "11",
		"author": [
			{
				"family": "Osinenko",
				"given": "P"
			},
			{
				"family": "Yaremenko",
				"given": "G"
			},
			{
				"family": "Malaniya",
				"given": "G"
			},
			{
				"family": "Bolychev",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "leeAdversarialTrainingJoint2020",
		"type": "paper-conference",
		"abstract": "Deep neural networks tend to be erroneous when the training and test distribution differ. Especially, neural classifiers are brittle to adversarial examples, and highly overconfident to out-of-distribution examples. Hybrid modeling of generative and discriminative distribution shown to be effective for out-of-distribution detection, but is not robust to adversarial attacks. Otherwise, defense methods for adversarial attacks cannot distinguish out-of-distribution examples. In this work, we present a hybrid model that can deal with both adversarial and out-of-distribution examples. Our method is built upon the joint energy based model and adversarial training. Through experiments on CIFAR-10 dataset, we show that our method has state-of-the-art performanced among hybrid models. Furthermore, we show that our model exhibits more perceptually-aligned feature than other methods, by showing the gradient sensitivity map with newly proposed score function.",
		"archive_location": "WOS:000681746000004",
		"DOI": "10.23919/iccas50221.2020.9268406",
		"event-title": "2020 20TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS)",
		"ISBN": "2093-7121",
		"page": "17-21",
		"title": "Adversarial Training on Joint Energy Based Model for Robust Classification and Out-of-Distribution Detection",
		"author": [
			{
				"family": "Lee",
				"given": "K"
			},
			{
				"family": "Yang",
				"given": "H"
			},
			{
				"family": "Oh",
				"given": "SY"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "reimannSafeDSDomainSpecific2023",
		"type": "paper-conference",
		"abstract": "Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide.\nIn this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python libraries, and automatically generate suitable stubs.\nMoreover, Safe-DS complements textual DS pipelines with a graphical representation that eases safe development by preventing syntax errors. The seamless synchronization of textual and graphic view lets developers always choose the one best suited for their skills and current task.\nWe think that Safe-DS can make DS development easier, faster, and more reliable, significantly reducing development costs.",
		"archive_location": "WOS:001032816400013",
		"DOI": "10.1109/ICSE-NIER58687.2023.00019",
		"event-title": "2023 IEEE/ACM 45TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING-NEW IDEAS AND EMERGING RESULTS, ICSE-NIER",
		"ISBN": "2832-7624",
		"page": "72-77",
		"title": "Safe-DS: A Domain Specific Language to Make Data Science Safe",
		"author": [
			{
				"family": "Reimann",
				"given": "L"
			},
			{
				"family": "Kniesel-Wünsche",
				"given": "G"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "harlandAIApologyInteractive2023",
		"type": "article-journal",
		"abstract": "For an Artificially Intelligent (AI) system to maintain alignment between human desires and its behaviour, it is important that the AI account for human preferences. This paper proposes and empirically evaluates the first approach to aligning agent behaviour to human preference via an apologetic framework. In practice, an apology may consist of an acknowledgement, an explanation and an intention for the improvement of future behaviour. We propose that such an apology, provided in response to recognition of undesirable behaviour, is one way in which an AI agent may both be transparent and trustworthy to a human user. Furthermore, that behavioural adaptation as part of apology is a viable approach to correct against undesirable behaviours. The Act-Assess-Apologise framework potentially could address both the practical and social needs of a human user, to recognise and make reparations against prior undesirable behaviour and adjust for the future. Applied to a dual-auxiliary impact minimisation problem, the apologetic agent had a near perfect determination and apology provision accuracy in several non-trivial configurations. The agent subsequently demonstrated behaviour alignment with success that included up to complete avoidance of the impacts described by these objectives in some scenarios.",
		"archive_location": "WOS:000973380900004",
		"container-title": "NEURAL COMPUTING & APPLICATIONS",
		"DOI": "10.1007/s00521-023-08586-x",
		"ISSN": "0941-0643",
		"issue": "23",
		"page": "16917-16930",
		"title": "AI apology: interactive multi-objective reinforcement learning for human-aligned AI",
		"volume": "35",
		"author": [
			{
				"family": "Harland",
				"given": "H"
			},
			{
				"family": "Dazeley",
				"given": "R"
			},
			{
				"family": "Nakisa",
				"given": "B"
			},
			{
				"family": "Cruz",
				"given": "F"
			},
			{
				"family": "Vamplew",
				"given": "P"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					8
				]
			]
		}
	},
	{
		"id": "kunduDetectingFunctionalSafety2022",
		"type": "paper-conference",
		"abstract": "With the ubiquitous deployment of Deep Neural Networks (DNNs) in low latency mission critical applications, there has been an extensive proliferation of custom-built AI inference accelerators at the edge. Drastic technology scaling in recent years has made these circuits highly vulnerable to faults due to various reasons like aging, latent defects, single event upsets, etc. Such faults are highly detrimental to the classification accuracy of the AI accelerator, leading to the critical Functional Safety (FuSa) violation, when used in mission-critical applications. In order to detect such violations in mission mode, we analyze the efficiency of a software-based self test scheme that employs functional test patterns, akin to instances in the application dataset. Such patterns are either selected from the dataset of the DNN, or generated from scratch utilizing the concept of Generative Adversarial Networks (GANs). When evaluated on state-of-the-art DNNs on multivariate exhaustive datasets, the GAN generated test patterns significantly improve FuSa violation detection coverage by up to 130.28%, compared to the selected test patterns, thereby accomplishing efficient testing of the AI accelerator, online, in mission mode.",
		"archive_location": "WOS:000865857100024",
		"DOI": "10.1109/IOLTS56730.2022.9897702",
		"event-title": "2022 IEEE 28TH INTERNATIONAL SYMPOSIUM ON ON-LINE TESTING AND ROBUST SYSTEM DESIGN (IOLTS 2022)",
		"ISBN": "1942-9398",
		"title": "Detecting Functional Safety Violations in Online AI Accelerators",
		"author": [
			{
				"family": "Kundu",
				"given": "S"
			},
			{
				"family": "Basu",
				"given": "K"
			}
		],
		"editor": [
			{
				"family": "Savino",
				"given": "A"
			},
			{
				"family": "Rech",
				"given": "P"
			},
			{
				"family": "DiCarlo",
				"given": "S"
			},
			{
				"family": "Gizopoulos",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "turchettaSafeExplorationInteractive2019",
		"type": "paper-conference",
		"abstract": "In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.",
		"archive_location": "WOS:000534424302084",
		"event-title": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)",
		"ISBN": "1049-5258",
		"title": "Safe Exploration for Interactive Machine Learning",
		"volume": "32",
		"author": [
			{
				"family": "Turchetta",
				"given": "M"
			},
			{
				"family": "Berkenkamp",
				"given": "F"
			},
			{
				"family": "Krause",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Wallach",
				"given": "H"
			},
			{
				"family": "Larochelle",
				"given": "H"
			},
			{
				"family": "Beygelzimer",
				"given": "A"
			},
			{
				"family": "Buc",
				"given": "F",
				"non-dropping-particle": "d'Alche-"
			},
			{
				"family": "Fox",
				"given": "E"
			},
			{
				"family": "Garnett",
				"given": "R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ramakrishnanDiscoveringBlindSpots2018",
		"type": "paper-conference",
		"abstract": "Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.",
		"archive_location": "WOS:000468231300121",
		"event-title": "PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18)",
		"ISBN": "978-1-4503-5649-7",
		"page": "1017-1025",
		"title": "Discovering Blind Spots in Reinforcement Learning",
		"author": [
			{
				"family": "Ramakrishnan",
				"given": "R"
			},
			{
				"family": "Kamar",
				"given": "E"
			},
			{
				"family": "Dey",
				"given": "D"
			},
			{
				"family": "Shah",
				"given": "J"
			},
			{
				"family": "Horvitz",
				"given": "E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "liuSafeReinforcementLearning2023a",
		"type": "paper-conference",
		"abstract": "Safety is a fundamental property for the realworld deployment of robotic platforms. Any control policy should avoid dangerous actions that could harm the environment, humans, or the robot itself. In reinforcement learning (RL), safety is crucial when exploring a new environment to learn a new skill. This paper introduces a new formulation of safe exploration for robotic RL in the tangent space of the constraint manifold that effectively transforms the action space of the RL agent for always respecting safety constraints locally. We show how to apply this approach to a wide range of robotic platforms and how to define safety constraints that represent dynamic articulated objects like humans in the context of robotic RL. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a TIAGo++ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.",
		"archive_location": "WOS:001048371102021",
		"DOI": "10.1109/ICRA48891.2023.10161548",
		"event-title": "2023 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2023)",
		"ISBN": "1050-4729",
		"page": "9449-9456",
		"title": "Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction",
		"author": [
			{
				"family": "Liu",
				"given": "PZ"
			},
			{
				"family": "Zhang",
				"given": "K"
			},
			{
				"family": "Tateo",
				"given": "D"
			},
			{
				"family": "Jauhri",
				"given": "S"
			},
			{
				"family": "Hu",
				"given": "ZY"
			},
			{
				"family": "Peters",
				"given": "J"
			},
			{
				"family": "Chalvatzaki",
				"given": "G"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhaoDetectingOperationalAdversarial2021",
		"type": "paper-conference",
		"abstract": "The utilisation of Deep Learning (DL) raises new challenges regarding its dependability in critical applications. Sound verification and validation methods are needed to assure the safe and reliable use of DL. However, state-of-the-art debug testing methods on DL that aim at detecting adversarial examples (AEs) ignore the operational profile, which statistically depicts the software's future operational use. This may lead to very modest effectiveness on improving the software's delivered reliability, as the testing budget is likely to be wasted on detecting AEs that are unrealistic or encountered very rarely in real-life operation. In this paper, we first present the novel notion of \"operational AEs\" which are AEs that have relatively high chance to be seen in future operation. Then an initial design of a new DL testing method to efficiently detect \"operational AEs\" is provided, as well as some insights on our prospective research plan.",
		"archive_location": "WOS:000701459900003",
		"DOI": "10.1109/DSN-S52858.2021.00013",
		"event-title": "51ST ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS - SUPPLEMENTAL VOL (DSN 2021)",
		"ISBN": "1530-0889",
		"page": "5-6",
		"title": "Detecting Operational Adversarial Examples for Reliable Deep Learning",
		"author": [
			{
				"family": "Zhao",
				"given": "XY"
			},
			{
				"family": "Huang",
				"given": "W"
			},
			{
				"family": "Schewe",
				"given": "S"
			},
			{
				"family": "Dong",
				"given": "Y"
			},
			{
				"family": "Huang",
				"given": "XW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "freieslebenGeneralizationTheoryRobustness2023",
		"type": "article-journal",
		"abstract": "The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept. © 2023, The Author(s).",
		"archive": "Scopus",
		"container-title": "Synthese",
		"DOI": "10.1007/s11229-023-04334-9",
		"issue": "4",
		"title": "Beyond generalization: a theory of robustness in machine learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172812290&doi=10.1007%2fs11229-023-04334-9&partnerID=40&md5=7e9f1df24e78804b758b8df25867fe87",
		"volume": "202",
		"author": [
			{
				"family": "Freiesleben",
				"given": "T."
			},
			{
				"family": "Grote",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "tunaTENETNewHybrid2023",
		"type": "article-journal",
		"abstract": "Deep neural network (DNN) models are widely renowned for their resistance to random perturbations. However, researchers have found out that these models are indeed extremely vulnerable to deliberately crafted and seemingly imperceptible perturbations of the input, referred to as adversarial examples. Adversarial attacks have the potential to substantially compromise the security of DNN-powered systems and posing high risks especially in the areas where security is a top priority. Numerous studies have been conducted in recent years to defend against these attacks and to develop more robust architectures resistant to adversarial threats. In this study, we propose a new architecture and enhance a recently proposed technique by which we can restore adversarial samples back to their original class manifold. We leverage the use of several uncertainty metrics obtained from Monte Carlo dropout (MC Dropout) estimates of the model together with the model’s own loss function and combine them with the use of defensive distillation technique to defend against these attacks. We have experimentally evaluated and verified the efficacy of our approach on MNIST (Digit), MNIST (Fashion) and CIFAR10 datasets. In our experiments, we showed that our proposed method reduces the attack’s success rate lower than 5% without compromising clean accuracy. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE.",
		"archive": "Scopus",
		"container-title": "International Journal of Information Security",
		"DOI": "10.1007/s10207-023-00675-1",
		"issue": "4",
		"page": "987-1004",
		"title": "TENET: a new hybrid network architecture for adversarial defense",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150178910&doi=10.1007%2fs10207-023-00675-1&partnerID=40&md5=5f8f82fcebf7de7f98e293b8a6974614",
		"volume": "22",
		"author": [
			{
				"family": "Tuna",
				"given": "O.F."
			},
			{
				"family": "Catak",
				"given": "F.O."
			},
			{
				"family": "Eskil",
				"given": "M.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "deyMultilayeredCollaborativeFramework2023",
		"type": "paper-conference",
		"abstract": "In the days of AI, data-centric machine learning (ML) models are increasingly used in various complex systems. While many researchers are focusing on specifying ML-specific performance requirements, not enough guideline is provided to engineer the data requirements systematically involving diverse stakeholders. Lack of written agreement about the training data, collaboration bottlenecks, lack of data validation framework, etc. are posing new challenges to ensuring training data fitness for safety-critical ML components. To reduce these gaps, we propose a multi-layered framework that helps to perceive and elicit data requirements. We provide a template for verifiable data requirements specifications. Moreover, we show how such requirements can facilitate an evidence-driven assessment of the training data quality based on the experts' judgments about the satisfaction of the requirements. We use Dempster Shafer's theory to combine experts' subjective opinions in the process. A preliminary case study on the CityPersons dataset for the pedestrian detection feature of autonomous cars shows the usefulness of the proposed framework for data requirements understanding and the confidence assessment of the dataset.  © 2023 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3555776.3577647",
		"event-title": "Proceedings of the ACM Symposium on Applied Computing",
		"page": "1404-1413",
		"title": "A Multi-layered Collaborative Framework for Evidence-driven Data Requirements Engineering for Machine Learning-based Safety-critical Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162869276&doi=10.1145%2f3555776.3577647&partnerID=40&md5=f371778982c09e76b7bdfe7aeb0c546f",
		"author": [
			{
				"family": "Dey",
				"given": "S."
			},
			{
				"family": "Lee",
				"given": "S.-W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenRobustFederatedLearning2023",
		"type": "article-journal",
		"abstract": "Federated learning (FL) is a communication-efficient machine learning paradigm to leverage distributed data at the network edge. Nevertheless, FL usually fails to train a high-quality model from the networks, where the edge nodes collect noisy labeled data. To tackle this challenge, this paper focuses on developing an innovative robust FL. We consider two kinds of networks with different data distribution. Firstly, we design a reweighted FL under a full-data network, where all edge nodes are equipped with both numerous noisy labeled dataset and small clean dataset. The key idea is that edge devices learn to assign the local weights of loss functions in noisy labeled dataset, and cooperate with central server to update global weights. Secondly, we consider a part-data network where some edge nodes exclude clean dataset, and can not compute the weights locally. The broadcasting of the global weights is added to help those edge nodes without clean dataset to reweight their noisy loss functions. Both designs have a convergence rate of O(1/T2). Simulation results illustrate that the both proposed training processes improve the prediction accuracy due to the proper weights assignments of noisy loss function. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Network Science and Engineering",
		"DOI": "10.1109/TNSE.2022.3227287",
		"issue": "3",
		"page": "1501-1511",
		"title": "Robust Federated Learning With Noisy Labeled Data Through Loss Function Correction",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144783924&doi=10.1109%2fTNSE.2022.3227287&partnerID=40&md5=3a73f67de23a187c9afb49c45465f69b",
		"volume": "10",
		"author": [
			{
				"family": "Chen",
				"given": "L."
			},
			{
				"family": "Ang",
				"given": "F."
			},
			{
				"family": "Chen",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "robinetteSelfPreservingGeneticAlgorithms2023",
		"type": "paper-conference",
		"abstract": "Self-Preserving Genetic Algorithms (SPGA) combine the evolutionary strategy of a genetic algorithm with safety assurance methods commonly implemented in safe reinforcement learning (SRL), a branch of reinforcement learning (RL) that accounts for safety in the exploration and decision-making process of the agent. Safe learning approaches are especially important in safety-critical environments, where failure to account for the safety of the controlled system could result in the loss of millions of dollars in hardware or bodily harm to people working nearby, as is true of many cyber-physical systems. While SRL is a viable approach to safe learning, there are many challenges that must be taken into consideration when training agents, such as sample efficiency, stability, and exploration—an issue that is easily addressed by the evolutionary strategy of a genetic algorithm. By combining GAs with the safety mechanisms used with SRL, SPGA offers a safe learning alternative that is able to explore large areas of the solution space, addressing SRL’s challenge of exploration. This work implements SPGA with both action masking and run time assurance safety strategies to evolve safe controllers for three types of discrete action space environments applicable to cyber physical systems (control, routing, and operations) and under various safety conditions. Training and testing evaluation metrics are compared with results from SRL trained controllers to validate results. SPGA and SRL controllers are trained across 5 random seeds and evaluated on 500 episodes to calculate average wall time to train, average expected return, and percentage of safe action evaluation metrics. SPGA achieves comparable reward and safety performance results with significantly improved training efficiency (55x faster on average), demonstrating the effectiveness of this safe learning approach. © ICCPS 2023. All rights reserved.",
		"archive": "Scopus",
		"DOI": "10.1145/3576841.3585936",
		"event-title": "ICCPS 2023 - Proceedings of the 2023 ACM/IEEE 14th International Conference on Cyber-Physical Systems with CPS-IoT Week 2023",
		"page": "110-119",
		"title": "Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167866179&doi=10.1145%2f3576841.3585936&partnerID=40&md5=01b2dc767bb29f9262c5ca95bfd80333",
		"author": [
			{
				"family": "Robinette",
				"given": "P.K."
			},
			{
				"family": "Hamilton",
				"given": "N.P."
			},
			{
				"family": "Johnson",
				"given": "T.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "feldkampExplainableAIExplainable2023",
		"type": "paper-conference",
		"abstract": "Evaluating robustness is an important goal in simulation-based analysis. Robustness is achieved when the controllable factors of a system are adjusted in such a way that any possible variance in uncontrollable factors (noise) has minimal impact on the variance of the desired output. The optimization of system robustness using simulation is a dedicated and well-established research direction. However, once a simulation model is available, there is a lot of potential to learn more about the inherent relationships in the system, especially regarding its robustness. Data farming offers the possibility to explore large design spaces using smart experiment design, high performance computing, automated analysis, and interactive visualization. Sophisticated machine learning methods excel at recognizing and modelling the relation between large amounts of simulation input and output data. However, investigating and analyzing this modelled relationship can be very difficult, since most modern machine learning methods like neural networks or random forests are opaque black boxes. Explainable Artificial Intelligence (XAI) can help to peak into this black box, helping us to explore and learn about relations between simulation input and output. In this paper, we introduce a concept for using Data Farming, machine learning and XAI to investigate and understand system robustness of a given simulation model. © 2023 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3573900.3591114",
		"event-title": "ACM International Conference Proceeding Series",
		"page": "96-106",
		"title": "From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163894026&doi=10.1145%2f3573900.3591114&partnerID=40&md5=1d1bd8540275e7e02e822b4891aad1dc",
		"author": [
			{
				"family": "Feldkamp",
				"given": "N."
			},
			{
				"family": "Strassburger",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gholampourAdversarialRobustnessPhishing2023",
		"type": "paper-conference",
		"abstract": "Developing robust detection models against phishing emails has long been the main concern of the cyber defense community. Currently, public phishing/legitimate datasets lack adversarial email examples which keeps the detection models vulnerable. To address this problem, we developed an augmented phishing/legitimate email dataset, utilizing different adversarial text attack techniques. Next, the models were retrained with the adversarial dataset. Results showed that accuracy and F1 score of the models improved under subsequent attacks. In another experiment, synthetic phishing emails were generated using a fine-tuned GPT-2 model. The detection model was retrained with a newly formed synthetic dataset. Subsequently, we observed that the accuracy and robustness of the model did not improve significantly under black box attack methods. In the last experiment, we proposed a defensive technique to classify adversarial examples to their true labels using a K-Nearest Neighbor approach with 94% accuracy in our prediction.  © 2023 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3579987.3586567",
		"event-title": "IWSPA 2023 - Proceedings of the 9th ACM International Workshop on Security and Privacy Analytics",
		"page": "67-76",
		"title": "Adversarial Robustness of Phishing Email Detection Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159123101&doi=10.1145%2f3579987.3586567&partnerID=40&md5=a1c834530dbd2c55d36ba67001fdbe55",
		"author": [
			{
				"family": "Gholampour",
				"given": "P.M."
			},
			{
				"family": "Verma",
				"given": "R.M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "soremekunBackdoorAttacksDefense2023",
		"type": "article-journal",
		"abstract": "The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. Notably, the state-of-the-art projected gradient descent (PGD) -based training method has been shown to be universally and reliably effective in defending against adversarial inputs. This robustness approach uses PGD as a reliable and universal “first-order adversary”. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we study how to inject and defend against backdoor attacks for robust models trained using PGD-based robust optimisation. We demonstrate that these models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect such backdoor-infected models via a detection technique called AEGIS. Specifically, given a robust Deep Neural Network (DNN) that is trained using PGD-based first-order adversarial training approach, AEGIS uses feature clustering to effectively detect whether such DNNs are backdoor-infected or clean. In our evaluation of several visible and hidden backdoor triggers on major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects PGD-trained robust DNNs infected with backdoors. AEGIS detects such backdoor-infected models with 91.6% accuracy (11 out of 12 tested models), without any false positives. Furthermore, AEGIS detects the targeted class in the backdoor-infected model with a reasonably low (11.1%) false positive rate. Our investigation reveals that salient features of adversarially robust DNNs could be promising to break the stealthy nature of backdoor attacks. © 2023 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Computers and Security",
		"DOI": "10.1016/j.cose.2023.103101",
		"title": "Towards Backdoor Attacks and Defense in Robust Machine Learning Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147089205&doi=10.1016%2fj.cose.2023.103101&partnerID=40&md5=082a38fe3e54aee9a4c98eb655378cfa",
		"volume": "127",
		"author": [
			{
				"family": "Soremekun",
				"given": "E."
			},
			{
				"family": "Udeshi",
				"given": "S."
			},
			{
				"family": "Chattopadhyay",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mauriRobustMLModel2023",
		"type": "article-journal",
		"abstract": "In this paper, we improve the robustness of Machine Learning (ML) classifiers against training-time attacks by linking the risk of training data being tampered with to the redundancy in the ML model's design needed to prevent it. Our defense mechanism is directly applicable to classifiers' training data, without any knowledge of the specific ML model to be hardened. First, we compute the training data proximity to class separation surfaces, identified via a reference linear model. Each data point is associated with a risk index, which is used to partition the training set by an unsupervised technique. Then, we train a learner for each partition and combine the learners' output in an ensemble. Our method treats the protected ML classifier as a black box and is inherently robust to transfer attacks. Experiments show that, for data poisoning rates between 6 and 25 percent of the training set, our method is more robust compared to benchmarks and to a monolithic version of the model trained on the whole training set. Our results make a convincing case for adopting training set partitioning and ensemble generation as a stage of ML models' development and deployment lifecycle. © 2023 The Author(s)",
		"archive": "Scopus",
		"container-title": "Information Sciences",
		"DOI": "10.1016/j.ins.2023.03.085",
		"page": "122-140",
		"title": "Robust ML model ensembles via risk-driven anti-clustering of training data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150025882&doi=10.1016%2fj.ins.2023.03.085&partnerID=40&md5=85e7c63d37ea275829e2bc1b82c9fb9f",
		"volume": "633",
		"author": [
			{
				"family": "Mauri",
				"given": "L."
			},
			{
				"family": "Apolloni",
				"given": "B."
			},
			{
				"family": "Damiani",
				"given": "E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangEvaluatingModelFreeReinforcement2023",
		"type": "paper-conference",
		"abstract": "Safety comes first in many real-world applications involving autonomous agents. Despite a large number of reinforcement learning (RL) methods focusing on safety-critical tasks, there is still a lack of high-quality evaluation of those algorithms that adheres to safety constraints at each decision step under complex and unknown dynamics. In this paper, we revisit prior work in this scope from the perspective of state-wise safe RL and categorize them as projection-based, recovery-based, and optimization-based approaches, respectively. Furthermore, we propose Unrolling Safety Layer (USL), a joint method that combines safety optimization and safety projection. This novel technique explicitly enforces hard constraints via the deep unrolling architecture and enjoys structural advantages in navigating the trade-off between reward improvement and constraint satisfaction. To facilitate further research in this area, we reproduce related algorithms in a unified pipeline and incorporate them into SafeRL-Kit, a toolkit that provides off-the-shelf interfaces and evaluation utilities for safety-critical tasks. We then perform a comparative study of the involved algorithms on six benchmarks ranging from robotic control to autonomous driving. The empirical results provide an insight into their applicability and robustness in learning zero-cost-return policies without task-dependent handcrafting. The project page is available at https://sites.google.com/view/saferlkit. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
		"page": "15313-15321",
		"title": "Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167964284&partnerID=40&md5=792e1228a61ef0eed74f6cb077dba25d",
		"volume": "37",
		"author": [
			{
				"family": "Zhang",
				"given": "L."
			},
			{
				"family": "Zhang",
				"given": "Q."
			},
			{
				"family": "Shen",
				"given": "L."
			},
			{
				"family": "Yuan",
				"given": "B."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Tao",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liCurricularRobustReinforcement2023",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL), one of three branches of machine learning, aims for autonomous learning and is now greatly driving the artificial intelligence development, especially in autonomous distributed systems, such as cooperative Boston Dynamics robots. However, robust RL has been a challenging problem of reliable aspects due to the gap between laboratory simulation and real world. Existing efforts have been made to approach this problem, such as performing random environmental perturbations in the learning process. However, one cannot guarantee to train with a positive perturbation as bad ones might bring failures to RL. In this work, we treat robust RL as a multi-task RL problem, and propose a curricular robust RL approach. We first present a generative adversarial network (GAN) based task generation model to iteratively output new tasks at the appropriate level of difficulty for the current policy. Furthermore, with these progressive tasks, we can realize curricular learning and finally obtain a robust policy. Extensive experiments in multiple environments demonstrate that our method improves the training stability and is robust to differences in training/test conditions.  © 1996-2012 Tsinghua University Press.",
		"archive": "Scopus",
		"container-title": "Tsinghua Science and Technology",
		"DOI": "10.26599/TST.2021.9010076",
		"issue": "1",
		"page": "27-38",
		"title": "Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135316788&doi=10.26599%2fTST.2021.9010076&partnerID=40&md5=394a312cdebd40ed1f5ed1d35cf53987",
		"volume": "28",
		"author": [
			{
				"family": "Li",
				"given": "Y."
			},
			{
				"family": "Tian",
				"given": "Y."
			},
			{
				"family": "Tong",
				"given": "E."
			},
			{
				"family": "Niu",
				"given": "W."
			},
			{
				"family": "Xiang",
				"given": "Y."
			},
			{
				"family": "Chen",
				"given": "T."
			},
			{
				"family": "Wu",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "deyImperativeActionMasking2023",
		"type": "paper-conference",
		"abstract": "Reinforcement Learning (RL) needs sufficient exploration to learn an optimal policy. However, exploratory actions could lead the learning agent to safety hazards, not necessarily in the next state but in the future. Therefore, it is essential to evaluate each action beforehand to ensure safety. The exploratory actions and the actions proposed by the RL agent could also be unsafe during training and in the deployment phase. In this work, we have proposed the Imperative Action Masking Framework, a Graph-Plan-based method considering a finite and small look ahead to assess the safety of actions from the current state. This information is used to construct action masks on the run, filtering out the unsafe actions proposed by the RL agent (including the exploitative ones). The Graph-Plan-based method makes our framework interpretable, while the finite and small look ahead makes the proposed method scalable for larger environments. However, considering the finite and small look ahead comes with a cost of overlooking safety beyond the look ahead. We have done a comparative study against the probabilistic safety shield in Pacman and Warehouse environments approach. Our framework has produced better results in terms of both safety and reward. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40878-6_8",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "130-142",
		"title": "Imperative Action Masking for Safe Exploration in Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172194779&doi=10.1007%2f978-3-031-40878-6_8&partnerID=40&md5=d76cfee6e9ec5905292cddffea4cd7cb",
		"volume": "14127 LNAI",
		"author": [
			{
				"family": "Dey",
				"given": "S."
			},
			{
				"family": "Bhat",
				"given": "S."
			},
			{
				"family": "Dasgupta",
				"given": "P."
			},
			{
				"family": "Dey",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diemertSafetyIntegrityLevels2023",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) and Machine Learning (ML) technologies are rapidly being adopted to perform safety-related tasks in critical systems. These AI-based systems pose significant challenges, particularly regarding their assurance. Existing safety approaches defined in internationally recognized standards such as ISO 26262, DO-178C, UL 4600, EN 50126, and IEC 61508 do not provide detailed guidance on how to assure AI-based systems. For conventional (non-AI) systems, these standards adopt a ‘Level of Rigor’ (LoR) approach, where increasingly demanding engineering activities are required as risk associated with the system increases. This paper proposes an extension to existing LoR approaches, which considers the complexity of the task(s) being performed by an AI-based component. Complexity is assessed in terms of input entropy and output non-determinism, and then combined with the allocated Safety Integrity Level (SIL) to produce an AI-SIL. That AI-SIL may be used to identify appropriate measures and techniques for the development and verification of the system. The proposed extension is illustrated by examples from the automotive, aviation, and medical industries. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40953-0_34",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "397-409",
		"title": "Safety Integrity Levels for Artificial Intelligence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172417895&doi=10.1007%2f978-3-031-40953-0_34&partnerID=40&md5=5d73b325a3baac32df8f86740108eda6",
		"volume": "14182 LNCS",
		"author": [
			{
				"family": "Diemert",
				"given": "S."
			},
			{
				"family": "Millet",
				"given": "L."
			},
			{
				"family": "Groves",
				"given": "J."
			},
			{
				"family": "Joyce",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zwaneSafeTrajectorySampling2023",
		"type": "paper-conference",
		"abstract": "Model-based reinforcement learning aims to learn a policy to solve a target task by leveraging a learned dynamics model. This approach, paired with principled handling of uncertainty allows for data-efficient policy learning in robotics. However, the physical environment has feasibility and safety constraints that need to be incorporated into the policy before it is safe to execute on a real robot. In this work, we study how to enforce the aforementioned constraints in the context of model-based reinforcement learning with probabilistic dynamics models. In particular, we investigate how trajectories sampled from the learned dynamics model can be used on a real robot, while fulfilling user-specified safety requirements. We present a model-based reinforcement learning approach using Gaussian processes where safety constraints are taken into account without simplifying Gaussian assumptions on the predictive state distributions. We evaluate the proposed approach on different continuous control tasks with varying complexity and demonstrate how our safe trajectory-sampling approach can be directly used on a real robot without violating safety constraints.  © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/CASE56687.2023.10260496",
		"event-title": "IEEE International Conference on Automation Science and Engineering",
		"title": "Safe Trajectory Sampling in Model-Based Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174425795&doi=10.1109%2fCASE56687.2023.10260496&partnerID=40&md5=edfccd70835d83e66fac61314c7dd926",
		"volume": "2023-August",
		"author": [
			{
				"family": "Zwane",
				"given": "S."
			},
			{
				"family": "Hadjivelichkov",
				"given": "D."
			},
			{
				"family": "Luo",
				"given": "Y."
			},
			{
				"family": "Bekiroglu",
				"given": "Y."
			},
			{
				"family": "Kanoulas",
				"given": "D."
			},
			{
				"family": "Deisenroth",
				"given": "M.P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kammConceptDynamicRobust2023",
		"type": "paper-conference",
		"abstract": "With the increasing amount of available and connected data sources, industrial automation applications such as condition monitoring of a production machine can be improved by considering various data. To gain insights from this data and make it useable, heterogeneous data has to be analyzed intensively. Limited machine learning approaches exist in industrial automation and manufacturing for analyzing data acquired from multiple sources. In this paper, first, a suitable concept for handling heterogeneous data from integration to analysis is presented as well as a multi-layer architecture for the concept's realization. The architecture encapsulates functionalities into the different layers and allows easy extendability and modifiability. Afterwards, a context modeling approach for managing heterogeneous data and existing approaches and algorithms for analyzing this data robustly and dynamically analyzing it are presented. © 2023 Elsevier B.V.. All rights reserved.",
		"archive": "Scopus",
		"DOI": "10.1016/j.procir.2023.06.061",
		"event-title": "Procedia CIRP",
		"page": "354-359",
		"title": "A Concept for Dynamic and Robust Machine Learning with Context Modeling for Heterogeneous Manufacturing Data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173582100&doi=10.1016%2fj.procir.2023.06.061&partnerID=40&md5=93caaf62c655e13f5089c9dd4ee26c65",
		"volume": "118",
		"author": [
			{
				"family": "Kamm",
				"given": "S."
			},
			{
				"family": "Sahlab",
				"given": "N."
			},
			{
				"family": "Jazdi",
				"given": "N."
			},
			{
				"family": "Weyrich",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "moRobustDataSampling2023",
		"type": "article-journal",
		"abstract": "How to sample training/validation data is an important question for machine learning models, especially when the dataset is heterogeneous and skewed. In this paper, we propose a data sampling method that robustly selects training/validation data. We formulate the training/validation data sampling process as a two-player game: a trainer aims to sample training data so as to minimize the test error, while a validator adversarially samples validation data that can increase the test error. Robust sampling is achieved at the game equilibrium. To accelerate the searching process, we adopt reinforcement learning aided Monte Carlo trees search (MCTS). We apply our method to a car-following modeling problem, a complicated scenario with heterogeneous and random human driving behavior. Real-world data, the Next Generation SIMulation (NGSIM), is used to validate this method, and experiment results demonstrate the sampling robustness and thereby the model out-of-sample performance. © 2023 by the authors.",
		"archive": "Scopus",
		"container-title": "Games",
		"DOI": "10.3390/g14010013",
		"issue": "1",
		"title": "Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148599130&doi=10.3390%2fg14010013&partnerID=40&md5=1befbf74e2c83028ea4123204f7abeea",
		"volume": "14",
		"author": [
			{
				"family": "Mo",
				"given": "Z."
			},
			{
				"family": "Di",
				"given": "X."
			},
			{
				"family": "Shi",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kirchheimDeepAnomalyDetection2023",
		"type": "paper-conference",
		"abstract": "Machine learning models tend to only make reliable predictions for inputs that are similar to the training data. Consequentially, anomaly detection, which can be used to detect unusual inputs, is critical for ensuring the safety of machine learning agents operating in open environments. In this work, we identify and discuss several limitations of current anomaly detection methods, such as their weak performance on tasks that require abstract reasoning, the inability to integrate background knowledge, and the opaqueness that undermines their trustworthiness in critical applications. Furthermore, we propose an architecture for anomaly detection models that aims to integrate structured knowledge representations to address these limitations. Our hypothesis is that this approach can improve performance and robustness, reduce the required resources (such as data and computation), and provide a higher degree of transparency. As a result, our work contributes to the increased safety of machine learning systems. Our code is publicly available. (https://github.com/kkirchheim/sumnist )",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40953-0_32",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "382-389",
		"title": "Towards Deep Anomaly Detection with Structured Knowledge Representations",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172421520&doi=10.1007%2f978-3-031-40953-0_32&partnerID=40&md5=e50642e6d72d42491a1f6b39cdb01a4a",
		"volume": "14182 LNCS",
		"author": [
			{
				"family": "Kirchheim",
				"given": "K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shiNearOptimalAlgorithmSafe2023",
		"type": "paper-conference",
		"abstract": "In many applications of Reinforcement Learning (RL), it is critically important that the algorithm performs safely, such that instantaneous hard constraints are satisfied at each step, and unsafe states and actions are avoided. However, existing algorithms for “safe” RL are often designed under constraints that either require expected cumulative costs to be bounded or assume all states are safe. Thus, such algorithms could violate instantaneous hard constraints and traverse unsafe states (and actions) in practice. Hence, in this paper, we develop the first near-optimal safe RL algorithm for episodic Markov Decision Processes with unsafe states and actions under instantaneous hard constraints and the linear mixture model. It achieves a regret (Equation presented) that nearly matches the state-of-the-art regret in the setting with only unsafe actions and that in the unconstrained setting, and is safe at each step, where d is the feature-mapping dimension, K is the number of episodes, H is the episode length, and ∆c is a safety-related parameter. We also provide a lower bound (Equation presented), which indicates that the dependency on ∆c is necessary. Further, both our algorithm design and regret analysis involve several novel ideas, which may be of independent interest. © 2023 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "31243-31268",
		"title": "A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174407935&partnerID=40&md5=e136ba097637fe4a30217c735b53fb7b",
		"volume": "202",
		"author": [
			{
				"family": "Shi",
				"given": "M."
			},
			{
				"family": "Liang",
				"given": "Y."
			},
			{
				"family": "Shroff",
				"given": "N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenCurriculumLearningbasedFuzzy2023",
		"type": "article-journal",
		"abstract": "To improve the robustness of SVM models to noise and outliers, fuzzy support vector machine (FSVM) has been proposed. However, many existing FSVM models have limitations such as their dependence on assumptions, limited optimization, and unreasonable handling of noise. To address these problems, we propose a novel approach called curriculum learning-based FSVM. Our approach employs a curriculum-learning strategy, where model initially learns easy samples to avoid noise interference and obtain a good initial solution, before proceeding to learn all samples, including hard ones. To distinguish between easy and hard samples, we introduce an adaptive density-based clustering model and it is extended to kernel feature space. Moreover, we propose a slack variable-based fuzzy membership function to evaluate the importance of samples. Additionally, our model adaptively adapts the importance of samples based on feedback during the learning process. Finally, our experimental results on popular benchmarks demonstrate that our proposed model outperforms existing competitors in terms of accuracy and robustness. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Fuzzy Systems",
		"DOI": "10.1109/TFUZZ.2023.3319170",
		"page": "1-15",
		"title": "Curriculum learning-based fuzzy support vector machine",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173338514&doi=10.1109%2fTFUZZ.2023.3319170&partnerID=40&md5=228b8e7997aa7b33157da00f29d0e3a6",
		"author": [
			{
				"family": "Chen",
				"given": "B."
			},
			{
				"family": "Gao",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "J."
			},
			{
				"family": "Weng",
				"given": "W."
			},
			{
				"family": "Huang",
				"given": "J."
			},
			{
				"family": "Fan",
				"given": "Y."
			},
			{
				"family": "Lan",
				"given": "W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jiProbabilisticCounterexampleGuidance2023",
		"type": "paper-conference",
		"abstract": "Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method’s effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-43835-6_22",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "311-328",
		"title": "Probabilistic Counterexample Guidance for Safer Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174226630&doi=10.1007%2f978-3-031-43835-6_22&partnerID=40&md5=b3d687148a4fbe03b6bafbf99bae2612",
		"volume": "14287 LNCS",
		"author": [
			{
				"family": "Ji",
				"given": "X."
			},
			{
				"family": "Filieri",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "caiDiscrepancyAwareFramework2023",
		"type": "article-journal",
		"abstract": "Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this article, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a discrepancy aware framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Industrial Informatics",
		"DOI": "10.1109/TII.2023.3318302",
		"page": "1-10",
		"title": "A Discrepancy Aware Framework for Robust Anomaly Detection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174819201&doi=10.1109%2fTII.2023.3318302&partnerID=40&md5=d67ed69e94513df294a49a8610cfedcb",
		"author": [
			{
				"family": "Cai",
				"given": "Y."
			},
			{
				"family": "Liang",
				"given": "D."
			},
			{
				"family": "Luo",
				"given": "D."
			},
			{
				"family": "He",
				"given": "X."
			},
			{
				"family": "Yang",
				"given": "X."
			},
			{
				"family": "Bai",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dagdanovSelfImprovingSafetyPerformance2023",
		"type": "paper-conference",
		"abstract": "In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL. © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICRA48891.2023.10160883",
		"event-title": "Proceedings - IEEE International Conference on Robotics and Automation",
		"page": "5631-5637",
		"title": "Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168711151&doi=10.1109%2fICRA48891.2023.10160883&partnerID=40&md5=dc9e3afea2258e7f3a6a741b55b96a88",
		"volume": "2023-May",
		"author": [
			{
				"family": "Dagdanov",
				"given": "R."
			},
			{
				"family": "Durmus",
				"given": "H."
			},
			{
				"family": "Ure",
				"given": "N.K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhaoProbabilisticSafeguardReinforcement2023",
		"type": "paper-conference",
		"abstract": "Safety is one of the biggest concerns to applying reinforcement learning (RL) to the physical world. In its core part, it is challenging to ensure RL agents persistently satisfy a hard state constraint without white-box or black-box dynamics models. This paper presents an integrated model learning and safe control framework to safeguard any RL agent, where the environment dynamics are learned as Gaussian processes. The proposed theory provides (i) a novel method to construct an offline dataset for model learning that best achieves safety requirements; (ii) a design rule to construct the safety index to ensure the existence of safe control under control limits; (iii) a probablistic safety guarantee (i.e. probabilistic forward invariance) when the model is learned using the aforementioned dataset. Simulation results show that our framework achieves almost zero safety violation on various continuous control tasks. © 2023 W. Zhao, T. He & C. Liu.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "783-796",
		"title": "Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164508271&partnerID=40&md5=52100e0b70a3b6e78be9f039cde62fc6",
		"volume": "211",
		"author": [
			{
				"family": "Zhao",
				"given": "W."
			},
			{
				"family": "He",
				"given": "T."
			},
			{
				"family": "Liu",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangRobustExtremeLearning2023",
		"type": "article-journal",
		"abstract": "The extreme learning machine (ELM) algorithm is advantageous to regression modeling owing to its simple structure, fast computation, and good generalization performance. However, the existing ELM algorithm uses an l2 -norm loss function, which is sensitive to outliers and has low robustness. In addition, some existing robust loss functions are not sufficiently flexible to accurately estimate the relationship between sample points and loss values, resulting in unsatisfactory ELM performance. To address these problems, this study established a robust ELM (ALFELM) algorithm. First, an adaptive loss function with two tunable hyperparameters was introduced; the function can be transformed into several robust loss functions by varying the parameters. It overcomes the limitations of fixed robust loss functions. Then, the Bayesian optimization strategy was used to determine the optimal parameters of the loss function. Furthermore, the classical iterative reweighted least squares method was used to solve for output weights, with a weight function corresponding to the loss function and a regularization parameter to prevent overfitting. Finally, the proposed method was tested using several artificial and benchmark datasets, and its effectiveness was verified for a real engineering case. The results indicated that the proposed ALFELM algorithm is more robust and accurate compared with other methods, especially for a large number of outliers. In addition, the algorithm can be used to establish effective regression models for actual processes. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Neural Processing Letters",
		"DOI": "10.1007/s11063-023-11340-y",
		"title": "A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164156087&doi=10.1007%2fs11063-023-11340-y&partnerID=40&md5=6ff6b65fab5ec5db4a239067c063e3ac",
		"author": [
			{
				"family": "Zhang",
				"given": "F."
			},
			{
				"family": "Chen",
				"given": "S."
			},
			{
				"family": "Hong",
				"given": "Z."
			},
			{
				"family": "Shan",
				"given": "B."
			},
			{
				"family": "Xu",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuHumanGuidedReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL) is a promising approach in unmanned ground vehicles (UGVs) applications, but limited computing resource makes it challenging to deploy a well-behaved RL strategy with sophisticated neural networks. Meanwhile, the training of RL on navigation tasks is difficult, which requires a carefully-designed reward function and a large number of interactions, yet RL navigation can still fail due to many corner cases. This shows the limited intelligence of current RL methods, thereby prompting us to rethink combining RL with human intelligence. In this paper, a human-guided RL framework is proposed to improve RL performance both during learning in the simulator and deployment in the real world. The framework allows humans to intervene in RL&#x0027;s control progress and provide demonstrations as needed, thereby improving RL&#x0027;s capabilities. An innovative human-guided RL algorithm is proposed that utilizes a series of mechanisms to improve the effectiveness of human guidance, including human-guided learning objective, prioritized human experience replay, and human intervention-based reward shaping. Our RL method is trained in simulation and then transferred to the real world, and we develop a denoised representation for domain adaptation to mitigate the simulation-to-real gap. Our method is validated through simulations and real-world experiments to navigate UGVs in diverse and dynamic environments based only on tiny neural networks and image inputs. Our method performs better in goal-reaching and safety than existing learning- and model-based navigation approaches and is robust to changes in input features and ego kinetics. Furthermore, our method allows small-scale human demonstrations to be used to improve the trained RL agent and learn expected behaviors online. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"DOI": "10.1109/TPAMI.2023.3314762",
		"page": "1-15",
		"title": "Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171531773&doi=10.1109%2fTPAMI.2023.3314762&partnerID=40&md5=213ee3846632eadf5c500e5900fd77df",
		"author": [
			{
				"family": "Wu",
				"given": "J."
			},
			{
				"family": "Zhou",
				"given": "Y."
			},
			{
				"family": "Yang",
				"given": "H."
			},
			{
				"family": "Huang",
				"given": "Z."
			},
			{
				"family": "Lv",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangLearningNoisyLabels2023",
		"type": "article-journal",
		"abstract": "Numerous researches have proved that deep neural networks (DNNs) can fit almost everything even given data with noisy labels, and result in poor generalization performance. However, recent studies suggest that DNNs tend to gradually memorize the data, moving from correct data to mislabeled data. Inspired by this finding, we propose a novel method named <italic>Dynamic Loss Thresholding (DLT)</italic>. During the training process, DLT records the loss value of each sample and calculates dynamic loss thresholds. Specifically, DLT compares the loss value of each sample with the current loss threshold. Samples with smaller losses can be considered as clean samples with higher probability and vice versa. Then, DLT discards the potentially corrupted labels and further leverages self-training semi-supervised learning techniques. Experiments on CIFAR-10/100, WebVision and Clothing1M demonstrate substantial improvements over recent state-of-the-art methods. In addition, we investigate two real-world problems. Firstly, we propose a novel approach to estimate the noise rates of datasets based on the loss difference between the early and late training stages of DNNs. Secondly, we explore the effect of hard samples (which are difficult to be distinguished) on the process of learning from noisy labels. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Knowledge and Data Engineering",
		"DOI": "10.1109/TKDE.2023.3313604",
		"page": "1-14",
		"title": "Learning From Noisy Labels Via Dynamic Loss Thresholding",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171528884&doi=10.1109%2fTKDE.2023.3313604&partnerID=40&md5=e14cea8b8cc944e7e9d83a3bca018bdf",
		"author": [
			{
				"family": "Yang",
				"given": "H."
			},
			{
				"family": "Jin",
				"given": "Y."
			},
			{
				"family": "Li",
				"given": "Z."
			},
			{
				"family": "Wang",
				"given": "D."
			},
			{
				"family": "Geng",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "samarasingheCounterfactualLearningEnhancing2023",
		"type": "article-journal",
		"abstract": "Resilience in autonomous agent systems is about having the capacity to anticipate, respond to, adapt to, and recover from adverse and dynamic conditions in complex environments. It is associated with the intelligence possessed by the agents to preserve the functionality or to minimize the impact on functionality through a transformation, reconfiguration, or expansion performed across the system. Enhancing the resilience of systems could pave way toward higher autonomy allowing them to tackle intricate dynamic problems. The state-of-the-art systems have mostly focussed on improving the redundancy of the system, adopting decentralized control architectures, and utilizing distributed sensing capabilities. While machine learning approaches for efficient distribution and allocation of skills and tasks have enhanced the potential of these systems, they are still limited when presented with dynamic environments. To move beyond the current limitations, this paper advocates incorporating counterfactual learning models for agents to enable them with the ability to predict possible future conditions and adjust their behavior. Counterfactual learning is a topic that has recently been gaining attention as a model-agnostic and post-hoc technique to improve explainability in machine learning models. Using counterfactual causality can also help gain insights into unforeseen circumstances and make inferences about the probability of desired outcomes. We propose that this can be used in agent systems as a means to guide and prepare them to cope with unanticipated environmental conditions. This supplementary support for adaptation can enable the design of more intelligent and complex autonomous agent systems to address the multifaceted characteristics of real-world problem domains. Copyright © 2023 Samarasinghe.",
		"archive": "Scopus",
		"container-title": "Frontiers in Artificial Intelligence",
		"DOI": "10.3389/frai.2023.1212336",
		"title": "Counterfactual learning in enhancing resilience in autonomous agent systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167886649&doi=10.3389%2ffrai.2023.1212336&partnerID=40&md5=b8c838d98f3d1ffd5dc99ec1d8cba075",
		"volume": "6",
		"author": [
			{
				"family": "Samarasinghe",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "al-malikiImprovedReliabilityDeep2023",
		"type": "article-journal",
		"abstract": "Deep neural networks have shown vulnerability to well-designed inputs called adversarial examples. Researchers in industry and academia have proposed many adversarial example defense techniques. However, they offer partial but not full robustness. Thus, complementing them with another layer of protection is a must, especially for mission-critical applications. This article proposes a novel online selection and relabeling algorithm (OSRA) that opportunistically utilizes a limited number of crowdsourced workers to maximize the machine learning (ML) system&#x0027;s robustness. The OSRA strives to use crowdsourced workers effectively by selecting the most suspicious inputs and moving them to the crowdsourced workers to be validated and corrected. As a result, the impact of adversarial examples gets reduced, and accordingly, the ML system becomes more robust. We also proposed a heuristic threshold selection method that contributes to enhancing the prediction system&#x0027;s reliability. We empirically validated our proposed algorithm and found that it can efficiently and optimally utilize the allocated budget for crowdsourcing. It is also effectively integrated with a state-of-the-art black box defense technique, resulting in a more robust system. Simulation results show that the OSRA can outperform a random selection algorithm by 60&#x0025; and achieve comparable performance to an optimal offline selection benchmark. They also show that OSRA&#x0027;s performance has a positive correlation with system robustness. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Reliability",
		"DOI": "10.1109/TR.2023.3298685",
		"page": "1-16",
		"title": "Toward Improved Reliability of Deep Learning Based Systems Through Online Relabeling of Potential Adversarial Attacks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168260380&doi=10.1109%2fTR.2023.3298685&partnerID=40&md5=49a3ddb8a11f11d30bdb6f3d9a3c89bd",
		"author": [
			{
				"family": "Al-Maliki",
				"given": "S."
			},
			{
				"family": "Bouanani",
				"given": "F.E."
			},
			{
				"family": "Ahmad",
				"given": "K."
			},
			{
				"family": "Abdallah",
				"given": "M."
			},
			{
				"family": "Hoang",
				"given": "D.T."
			},
			{
				"family": "Niyato",
				"given": "D."
			},
			{
				"family": "Al-Fuqaha",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuDiscrimLossUniversalLoss2023",
		"type": "article-journal",
		"abstract": "Given data with label noise (i.e., incorrect data), deep neural networks would gradually memorize the label noise and impair model performance. To relieve this issue, curriculum learning is proposed to improve model performance and generalization by ordering training samples in a meaningful (e.g., easy to hard) sequence. Previous work takes incorrect samples as generic hard ones without discriminating between hard samples (i.e., hard samples in correct data) and incorrect samples. Indeed, a model should learn from hard samples to promote generalization rather than overfit to incorrect ones. In this paper, we address this problem by appending a novel loss function <italic>DiscrimLoss</italic>, on top of the existing task loss. Its main effect is to automatically and stably estimate the importance of easy samples and difficult samples (including hard and incorrect samples) at the early stages of training to improve the model performance. Then, during the following stages, DiscrimLoss is dedicated to discriminating between hard and incorrect samples to improve the model generalization. Such a training strategy can be formulated dynamically in a self-supervised manner, effectively mimicking the main principle of curriculum learning. Experiments on image classification, image regression, text sequence regression, and event relation reasoning demonstrate the versatility and effectiveness of our method, particularly in the presence of diversified noise levels. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Multimedia",
		"DOI": "10.1109/TMM.2023.3290477",
		"page": "1-12",
		"title": "DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163478229&doi=10.1109%2fTMM.2023.3290477&partnerID=40&md5=304b809823e78a2e0f99c1366b6f2143",
		"author": [
			{
				"family": "Wu",
				"given": "T."
			},
			{
				"family": "Ding",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "H."
			},
			{
				"family": "Gao",
				"given": "J."
			},
			{
				"family": "Tang",
				"given": "M."
			},
			{
				"family": "Du",
				"given": "L."
			},
			{
				"family": "Qin",
				"given": "B."
			},
			{
				"family": "Liu",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangJointAdversarialDomain2023",
		"type": "article-journal",
		"abstract": "Generative adversarial networks as a powerful technique is also used in domain adaptation (DA) problem. Existing adversarial DA methods mainly conduct domain-wise alignment to alleviate marginal distribution shift between the two domains, while it may damage latent discriminative structure hidden in data feature space and cause negative transfer accordingly. To handle this problem, we propose a joint adversarial domain adaptation method with structural graph alignment to minimize joint distribution bias by further realizing class-wise matching (conditional distribution shift) based on a simple sampling strategy except for the domain-wise alignment, and validate that simultaneously considering these two types of shift can approximately reduce the joint distribution bias. To explore transferable structural information and realize more sufficient transfer for DA problem, we propose to align structural graphs between the two domains which is also based on a simple sampling strategy. Notably, the structural graph describes the relationship between each two samples and it is computed on two domains. As such, we can learn new feature representation of the two domains which are more discriminative and transferable to benefit a cross-domain classification task desirably. Finally, we design a number of experiments to evaluate our approach on four public cross-domain benchmark datasets including standard and large-scale ones, and empirical results show that the proposed model can outperform compared state-of-the-art methods. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Network Science and Engineering",
		"DOI": "10.1109/TNSE.2023.3302574",
		"page": "1-10",
		"title": "Joint Adversarial Domain Adaptation With Structural Graph Alignment",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167778383&doi=10.1109%2fTNSE.2023.3302574&partnerID=40&md5=b1d6a78515a4f98172a249841a67ea69",
		"author": [
			{
				"family": "Wang",
				"given": "M."
			},
			{
				"family": "Chen",
				"given": "J."
			},
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "S."
			},
			{
				"family": "li",
				"given": "L."
			},
			{
				"family": "Su",
				"given": "H."
			},
			{
				"family": "Gong",
				"given": "Z."
			},
			{
				"family": "Wu",
				"given": "K."
			},
			{
				"family": "Chen",
				"given": "Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "luRobustVerifiablePrivacy2023",
		"type": "article-journal",
		"abstract": "Federated Learning (FL) safeguards user privacy by uploading gradients instead of raw data. However, inference attacks can reconstruct raw data using gradients uploaded by users in FL. To mitigate this issue, researchers have combined privacy computing techniques with FL. However, these tech-niques may not ensure the Byzantine robustness of aggregation or the integrity of the aggregated outcomes. Most current robust privacy FL methods assess differences between gradients and benchmarks in the direction, allowing adversaries to poison the aggregation against the magnitude. Furthermore, these methods cannot ensure the integrity of the aggregation results. To over-come these challenges, this study proposes a novel algorithm, Robust and Verifiable Privacy Federated Learning(RVPFL), which can more effectively eliminate the poisoning attack of the opponent by measuring the direction and magnitude of the gradient in the ciphertext state. The proposed algorithm guarantees the integrity of server aggregation results while safe-guarding user privacy. In this study, comprehensive theoretical analysis and experimental validation of RVPFL are conducted to demonstrate its superiority. The proposed RVPFL algorithm solves the Byzantine robustness problem of aggregation and the integrity problem of aggregation results, which helps to research and develop more robust and effective privacy-preserving federal learning techniques. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Artificial Intelligence",
		"DOI": "10.1109/TAI.2023.3309273",
		"page": "1-14",
		"title": "Robust and verifiable privacy federated learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169701947&doi=10.1109%2fTAI.2023.3309273&partnerID=40&md5=8dc0334ff83737a2d178d275de67ec8a",
		"author": [
			{
				"family": "Lu",
				"given": "Z."
			},
			{
				"family": "Lu",
				"given": "S."
			},
			{
				"family": "Tang",
				"given": "X."
			},
			{
				"family": "Wu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangDataBanzhafRobust2023",
		"type": "paper-conference",
		"abstract": "Data valuation has wide use cases in machine learning, including improving data quality and creating economic incentives for data sharing. This paper studies the robustness of data valuation to noisy model performance scores. Particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the Shapley value and the Leave-one-out error) to produce inconsistent data value rankings across different runs. To address this challenge, we introduce the concept of safety margin, which measures the robustness of a data value notion. We show that the Banzhaf value, a famous value notion that originated from cooperative game theory literature, achieves the largest safety margin among all semivalues (a class of value notions that satisfy crucial properties entailed by ML applications and include the famous Shapley value and Leave-one-out error). We propose an algorithm to efficiently estimate the Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation demonstrates that the Banzhaf value outperforms the existing semivalue-based data value notions on several ML tasks such as learning with weighted samples and noisy label detection. Overall, our study suggests that when the underlying ML algorithm is stochastic, the Banzhaf value is a promising alternative to the other semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality. Copyright © 2023 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "6388-6421",
		"title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165146592&partnerID=40&md5=a821d12295cd52facd652e9c94031f07",
		"volume": "206",
		"author": [
			{
				"family": "Wang",
				"given": "J.T."
			},
			{
				"family": "Jia",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "henrikssonOutofDistributionDetectionSupport2023",
		"type": "paper-conference",
		"abstract": "[Context and Motivation] The automotive industry is moving towards increased automation, where features such as automated driving systems typically include machine learning (ML), e.g. in the perception system. [Question/Problem] Ensuring safety for systems partly relying on ML is challenging. Different approaches and frameworks have been proposed, typically where the developer must define quantitative and/or qualitative acceptance criteria, and ensure the criteria are fulfilled using different methods to improve e.g., design, robustness and error detection. However, there is still a knowledge gap between quality methods and metrics employed in the ML domain and how such methods can contribute to satisfying the vehicle level safety requirements. [Principal Ideas/Results] In this paper, we argue the need for connecting available ML quality methods and metrics to the safety lifecycle and explicitly show their contribution to safety. In particular, we analyse Out-of-Distribution (OoD) detection, e.g., the frequency of novelty detection, and show its potential for multiple safety-related purposes. I.e., as (a) an acceptance criterion contributing to the decision if the software fulfills the safety requirements and hence is ready-for-release, (b) in operational design domain selection and expansion by including novelty samples into the training/development loop, and (c) as a run-time measure, e.g., if there is a sequence of novel samples, the vehicle should consider reaching a minimal risk condition. [Contribution] This paper describes the possibility to use OoD detection as a safety measure, and the potential contributions in different stages of the safety lifecycle. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-29786-1_16",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "233-242",
		"title": "Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152531710&doi=10.1007%2f978-3-031-29786-1_16&partnerID=40&md5=a7d664a89d78ed62ff983c2f646834a8",
		"volume": "13975 LNCS",
		"author": [
			{
				"family": "Henriksson",
				"given": "J."
			},
			{
				"family": "Ursing",
				"given": "S."
			},
			{
				"family": "Erdogan",
				"given": "M."
			},
			{
				"family": "Warg",
				"given": "F."
			},
			{
				"family": "Thorsén",
				"given": "A."
			},
			{
				"family": "Jaxing",
				"given": "J."
			},
			{
				"family": "Örsmark",
				"given": "O."
			},
			{
				"family": "Toftås",
				"given": "M.Ö."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "costaRobustLearningMethodology2023",
		"type": "article-journal",
		"abstract": "Robust learning is an important issue in Scientific Machine Learning (SciML). There are several works in the literature addressing this topic. However, there is an increasing demand for methods that can simultaneously consider all the different uncertainty components involved in SciML model identification. Hence, this work proposes a comprehensive methodology for uncertainty evaluation of the SciML that also considers several possible sources of uncertainties involved in the identification process. The uncertainties considered in the proposed method are the absence of a theory, causal models, sensitivity to data corruption or imperfection, and computational effort. Therefore, it is possible to provide an overall strategy for uncertainty-aware models in the SciML field. The methodology is validated through a case study developing a soft sensor for a polymerization reactor. The first step is to build the nonlinear model parameter probability distribution (PDF) by Bayesian inference. The second step is to obtain the machine learning model uncertainty by Monte Carlo simulations. In the first step, a PDF with 30,000 samples is built. In the second step, the uncertainty of the machine learning model is evaluated by sampling 10,000 values through Monte Carlo simulation. The results demonstrate that the identified soft sensors are robust to uncertainties, corroborating the consistency of the proposed approach. © 2022 by the authors.",
		"archive": "Scopus",
		"container-title": "Mathematics",
		"DOI": "10.3390/math11010074",
		"issue": "1",
		"title": "A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145902502&doi=10.3390%2fmath11010074&partnerID=40&md5=f0ff60278dc34ab43dc423f9f5cb51d3",
		"volume": "11",
		"author": [
			{
				"family": "Costa",
				"given": "E.A."
			},
			{
				"family": "Rebello",
				"given": "C.D.M."
			},
			{
				"family": "Fontana",
				"given": "M."
			},
			{
				"family": "Schnitman",
				"given": "L."
			},
			{
				"family": "Nogueira",
				"given": "I.B.D.R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jeddiMemoryaugmentedLyapunovbasedSafe2023",
		"type": "article-journal",
		"abstract": "Despite recent advances in safe reinforcement learning (RL), safety constraints are often violated at deployment; especially under extreme uncertainty in memory-based partially observable environments. To address these limitations, we propose a memory-augmented Lyapunov-based safe RL model. The primary contributions of our method include (i) an explicit memory module based on Transformers to process long time horizons of information feedback from the environment; (ii) a memory-augmented Lyapunov function to determine a safe set of policies, and (iii) an exploration module that identifies highly rewarding safe actions by characterizing the uncertainty in the environment. We evaluate the proposed model in reactive OpenAI Safety Gym and memory-based partially observable DMLab-30 environments. The results of these experiments show that the proposed method significantly outperforms state-of-the-art baselines. Specifically, our proposed method achieves the lowest constraint costs among the tested benchmarks, while delivering high returns. Moreover, we perform ablation studies that show significant contributions of the introduced Transformer-based encoder, memory-augmented Lyapunov functions, and the uncertainty-aware exploration module. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Artificial Intelligence",
		"DOI": "10.1109/TAI.2023.3238700",
		"page": "1-10",
		"title": "Memory-augmented Lyapunov-based safe reinforcement learning: end-to-end safety under uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147271121&doi=10.1109%2fTAI.2023.3238700&partnerID=40&md5=66191103fffb274acb1a9974b97f7d3a",
		"author": [
			{
				"family": "Jeddi",
				"given": "A.B."
			},
			{
				"family": "Dehghani",
				"given": "N.L."
			},
			{
				"family": "Shafieezadeh",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouRobustMeanFieldActorCritic2023",
		"type": "article-journal",
		"abstract": "Multiagent deep reinforcement learning (DRL) makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The mean-field actor-critic (MFAC) reinforcement learning is well-known in the multiagent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust MFAC (RoMFAC) reinforcement learning that has two innovations: 1) a new objective function of training actors, composed of a policy gradient function that is related to the expected cumulative discount reward on sampled clean states and an action loss function that represents the difference between actions taken on clean and adversarial states and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a game model named a state-adversarial stochastic game (SASG). Despite the Nash equilibrium of SASG may not exist, adversarial perturbations to states in the RoMFAC are proven to be defensible based on SASG. Experimental results show that RoMFAC is robust against adversarial perturbations while maintaining its competitive performance in environments without perturbations. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2023.3278715",
		"page": "1-12",
		"title": "A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161516711&doi=10.1109%2fTNNLS.2023.3278715&partnerID=40&md5=a842a020dd2078511d253e34c9126787",
		"author": [
			{
				"family": "Zhou",
				"given": "Z."
			},
			{
				"family": "Liu",
				"given": "G."
			},
			{
				"family": "Zhou",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liuNovelCompositeGraph2023",
		"type": "article-journal",
		"abstract": "Graph neural networks (GNNs) have achieved great success in many fields due to their powerful capabilities of processing graph-structured data. However, most GNNs can only be applied to scenarios where graphs are known, but real-world data are often noisy or even do not have available graph structures. Recently, graph learning has attracted increasing attention in dealing with these problems. In this article, we develop a novel approach to improving the robustness of the GNNs, called composite GNN. Different from existing methods, our method uses composite graphs (C-graphs) to characterize both sample and feature relations. The C-graph is a unified graph that unifies these two kinds of relations, where edges between samples represent sample similarities, and each sample has a tree-based feature graph to model feature importance and combination preference. By jointly learning multiaspect C-graphs and neural network parameters, our method improves the performance of semisupervised node classification and ensures robustness. We conduct a series of experiments to evaluate the performance of our method and the variants of our method that only learn sample relations or feature relations. Extensive experimental results on nine benchmark datasets demonstrate that our proposed method achieves the best performance on almost all the datasets and is robust to feature noises. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2023.3268766",
		"page": "1-15",
		"title": "A Novel Composite Graph Neural Network",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160246263&doi=10.1109%2fTNNLS.2023.3268766&partnerID=40&md5=a81f79fc707d9e9221429e3840d4e9b2",
		"author": [
			{
				"family": "Liu",
				"given": "Z."
			},
			{
				"family": "Yang",
				"given": "J."
			},
			{
				"family": "Zhong",
				"given": "X."
			},
			{
				"family": "Wang",
				"given": "W."
			},
			{
				"family": "Chen",
				"given": "H."
			},
			{
				"family": "Chang",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangAdversarialRobustnessDeep2022",
		"type": "article-journal",
		"abstract": "Adversarial attacks, e.g., adversarial perturbations of the input and adversarial samples, pose significant challenges to machine learning and deep learning techniques, including interactive recommendation systems. The latent embedding space of those techniques makes adversarial attacks challenging to detect at an early stage. Recent advance in causality shows that counterfactual can also be considered one of the ways to generate the adversarial samples drawn from different distribution as the training samples. We propose to explore adversarial examples and attack agnostic detection on reinforcement learning (RL)-based interactive recommendation systems. We first craft different types of adversarial examples by adding perturbations to the input and intervening on the casual factors. Then, we augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our white-box detector trained with one crafting method has the generalization ability over several other crafting methods. Copyright © 2022 Wang, Cao, Chen, Yao, Wang and Sheng.",
		"archive": "Scopus",
		"container-title": "Frontiers in Big Data",
		"DOI": "10.3389/fdata.2022.822783",
		"title": "Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132621553&doi=10.3389%2ffdata.2022.822783&partnerID=40&md5=eaceb56da44a54961dfc1c1089624dea",
		"volume": "5",
		"author": [
			{
				"family": "Wang",
				"given": "S."
			},
			{
				"family": "Cao",
				"given": "Y."
			},
			{
				"family": "Chen",
				"given": "X."
			},
			{
				"family": "Yao",
				"given": "L."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Sheng",
				"given": "Q.Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhaRobustDoubleparallelExtreme2022",
		"type": "article-journal",
		"abstract": "To solve the problem of improving the regression accuracy and model stability of the extreme learning machine(ELM), a new approach based on an improved M-estimation optimized double-parallel extreme learning machine is proposed in this study, namely robust double-parallel extreme learning machine(RD-ELM). Firstly, RD-ELM is constructed with a double parallel forward structure, thus the information can be received from both hidden layer neurons and input layer neurons. Secondly, we use an improved M-estimation to calculate output weights of neural network by iteratively reweighted Least-Squares Estimation(LSE), with weights assigned by the least absolute residual estimation of the samples. Finally, we establish a regression prediction model utilized to test the goodness of fit in a SinC function and verify the regression ability in eight benchmark regression problems. Then the proposed method is applied to an actual operational condition of a power plant. Experimental results show that the proposed method can efficiently process the influence of outliers and noise with strong anti-jamming ability. Compared with other methods, RD-ELM has superior performance that is stronger robustness and better generalization performance in many benchmark data and practical experiments. © 2022 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Advanced Engineering Informatics",
		"DOI": "10.1016/j.aei.2022.101606",
		"title": "A robust double-parallel extreme learning machine based on an improved M-estimation algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128187488&doi=10.1016%2fj.aei.2022.101606&partnerID=40&md5=a4f5f40de5633708b4f9407e58292c83",
		"volume": "52",
		"author": [
			{
				"family": "Zha",
				"given": "L."
			},
			{
				"family": "Ma",
				"given": "K."
			},
			{
				"family": "Li",
				"given": "G."
			},
			{
				"family": "Fang",
				"given": "Q."
			},
			{
				"family": "Hu",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "padakandlaDataEfficientSafe2022",
		"type": "paper-conference",
		"abstract": "Applying reinforcement learning (RL) methods for real world applications poses multiple challenges - the foremost being safety of the physical system controlled by the learning agent and the learning efficiency. A RL agent learns to control a system by exploring available actions. In some operating states, when the RL agent exercises an exploratory action, the system may enter unsafe operation, which can lead to safety hazards both for the system as well as for humans supervising the system. RL algorithms thus need to respect these safety constraints and must do so with limited available information. In our work, we formulate this problem in the constrained off-policy setting that facilitates safe exploration by the RL agent. Further, we develop a sample efficient algorithm by adapting the cross-entropy method. The proposed algorithm’s safety performance is evaluated numerically on benchmark RL problems. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.",
		"archive": "Scopus",
		"event-title": "ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022",
		"title": "Data Efficient Safe Reinforcement Learning Algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586002&partnerID=40&md5=360c0b6fe4e7ad5c29d0651567b520d8",
		"author": [
			{
				"family": "Padakandla",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "xiongRobustTrustworthyMachine2022",
		"type": "article-journal",
		"abstract": "While Machine Learning (ML) technologies are widely adopted in many mission critical fields to support intelligent decision-making, concerns remain about system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness from a security engineering perspective, focusing on the problems in system threat analysis, design and evaluation faced in developing practical machine learning applications, in terms of robustness and user trust. Accordingly, we organize the presentation of this survey intended to facilitate the convey of the body of knowledge from this angle. We then describe a metamodel we created that represents the body of knowledge in a standard and visualized way. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process which extends and scales up the classic process. Finally, we propose the future research directions motivated by our findings. Our work differs itself from the existing surveys by (i) exploring the fundamental principles and best practices to support robust and trustworthy ML system development, and (ii) studying the interplay of robustness and user trust in the context of ML systems. We expect this survey provides a big picture for machine learning security practitioners. © 2022",
		"archive": "Scopus",
		"container-title": "Journal of Information Security and Applications",
		"DOI": "10.1016/j.jisa.2022.103121",
		"title": "Towards a robust and trustworthy machine learning system development: An engineering perspective",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124314035&doi=10.1016%2fj.jisa.2022.103121&partnerID=40&md5=fa8bf24ca832c86ff0ff8108655259bf",
		"volume": "65",
		"author": [
			{
				"family": "Xiong",
				"given": "P."
			},
			{
				"family": "Buffett",
				"given": "S."
			},
			{
				"family": "Iqbal",
				"given": "S."
			},
			{
				"family": "Lamontagne",
				"given": "P."
			},
			{
				"family": "Mamun",
				"given": "M."
			},
			{
				"family": "Molyneaux",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "weiModelSelectionApproach2022",
		"type": "paper-conference",
		"abstract": "We develop a model selection approach to tackle reinforcement learning with adversarial corruption in both transition and reward. For finite-horizon tabular MDPs, without prior knowledge on the total amount of corruption, our algorithm achieves a regret bound of (Equation presented) where T is the number of episodes, C is the total amount of corruption, and ∆ is the reward gap between the best and the second-best policy. This is the first worst-case optimal bound achieved without knowledge of C, improving previous results of Lykouris et al. (2021); Chen et al. (2021b); Wu et al. (2021). For finite-horizon linear MDPs, we develop a computationally efficient algorithm with a regret bound of Oe(p(1 + C)T), and another computationally inefficient one with Oe(√T + C), improving the result of Lykouris et al. (2021) and answering an open question by Zhang et al. (2021b). Finally, our model selection framework can be easily applied to other settings including linear bandits, linear contextual bandits, and MDPs with general function approximation, leading to several improved or new results. © 2022 C.-Y. Wei, C. Dann & J. Zimmert.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "1043-1096",
		"title": "A Model Selection Approach for Corruption Robust Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163731154&partnerID=40&md5=be608eb3822f8bbe3c87072cb3e0d7fd",
		"volume": "167",
		"author": [
			{
				"family": "Wei",
				"given": "C.-Y."
			},
			{
				"family": "Dann",
				"given": "C."
			},
			{
				"family": "Zimmert",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ebrahimiAdversarialReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Empowered by the recent development in Ma-chine Learning (ML), signatureless ML-based malware detectors present promising performance in identifying unseen mal ware variants and zero days without requiring expensive dynamic malware analysis. However, it has been recently shown that ML-based malware detectors are vulnerable to adversarial malware attacks, in which an attacker modifies a known malware exe-cutable to trick the malware detector into recognizing the modi-fied variant as benign. Adversarial malware example generation has become an emerging area in adversarial ML that studies creating functionality-preserving adversarial malware variants. Advancements in this area have led to an eternal game between the adversary and defender. While the area has attracted much attention in the security community, a large body of these studies merely focuses on attack methods against ML-based malware detectors. There has been little work on understanding how these adversarial variants can be systematically used by the defender to strengthen the robustness of these detectors and stand ahead of the adversary. Latest efforts have led to emergence of adversarial learning. In this work, we propose a simple wargame approach to empirically conduct the adversarial minimax optimization underlying in the adversarial learning for improving the robustness of ML-based malware detectors. Our proposed approach employs adversarial malware variants generated from a reinforcement learning-based adversarial attack policy in a minimax game alternating between strengthening the attack policy and improving the detectors' robustness. We evaluated the effectiveness of our approach on a testbed with 33.2 GB working malware collected from VirusTotal. Despite the sub-optimal nature of our method, it was able to surprisingly enhance the robustness of three known open-source ML-based malware detectors (LGBM, MalConv, and NonNeg) against the adversarial malware variants by 4, 7, and 11 times, respectively.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICDMW58026.2022.00079",
		"event-title": "IEEE International Conference on Data Mining Workshops, ICDMW",
		"page": "567-576",
		"title": "An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148436341&doi=10.1109%2fICDMW58026.2022.00079&partnerID=40&md5=fa5afef0860ddc17f4b63e0796529524",
		"volume": "2022-November",
		"author": [
			{
				"family": "Ebrahimi",
				"given": "M.R."
			},
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Chai",
				"given": "Y."
			},
			{
				"family": "Pacheco",
				"given": "J."
			},
			{
				"family": "Chen",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rodriguez-sotoBuildingMultiAgentEnvironments2022",
		"type": "paper-conference",
		"abstract": "This paper tackles the open problem of value alignment in multi-agent systems. In particular, we propose an approach to build an ethical environment that guarantees that all agents in the system learn to behave ethically while pursuing their individual objectives. Our contributions are founded in the framework of Multi-Objective Multi-Agent Reinforcement Learning. Firstly, we characterise a family of Multi-Objective Markov Games (MOMGs), the so-called ethical MOMGs, for which we can formally guarantee the learning of ethical behaviours. From these, we specify the process for building single-objective ethical environments that simplify the learning in the multi-agent system. Interestingly, our theoretical results for multi-agent environments generalise recent state-of-the-art results for single-agent environments. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.",
		"archive": "Scopus",
		"event-title": "ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022",
		"title": "Building Multi-Agent Environments with Theoretical Guarantees on the Learning of Ethical Policies",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584282&partnerID=40&md5=b592333309eaa9bbf060db65eaa1e353",
		"author": [
			{
				"family": "Rodriguez-Soto",
				"given": "M."
			},
			{
				"family": "Rodriguez-Aguilar",
				"given": "J.A."
			},
			{
				"family": "Lopez-Sanchez",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "qiHierarchicalHAZOPLikeSafety2022",
		"type": "paper-conference",
		"abstract": "Hazard and Operability Analysis (HAZOP) is a powerful safety analysis technique with a long history in industrial process control domain. With the increasing use of Machine Learning (ML) components in cyber physical systems—so called Learning-Enabled Systems (LESs), there is a recent trend of applying HAZOP-like analysis to LESs. While it shows a great potential to reserve the capability of doing sufficient and systematic safety analysis, there are new technical challenges raised by the novel characteristics of ML that require retrofit of the conventional HAZOP technique. In this regard, we present a new Hierarchical HAZOP-Like method for LESs (HILLS). To deal with the complexity of LESs, HILLS first does “divide and conquer” by stratifying the whole system into three levels, and then proceeds HAZOP on each level to identify (latent-)hazards, causes, security threats and mitigation (with new nodes and guide words). Finally, HILLS attempts at linking and propagating the causal relationship among those identified elements within and across the three levels via both qualitative and quantitative methods. We examine and illustrate the utility of HILLS by a case study on Autonomous Underwater Vehicles, with discussions on assumptions and extensions to real-world applications. HILLS, as a first HAZOP-like attempt on LESs that explicitly considers ML internal behaviours and its interactions with other components, not only uncovers the inherent difficulties of doing safety analysis for LESs, but also demonstrates a good potential to tackle them. © 2022 Copyright for this paper by its authors.",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139381061&partnerID=40&md5=80164f44c1c28764025cd0045f7ff701",
		"volume": "3215",
		"author": [
			{
				"family": "Qi",
				"given": "Y."
			},
			{
				"family": "Conmy",
				"given": "P.R."
			},
			{
				"family": "Huang",
				"given": "W."
			},
			{
				"family": "Zhao",
				"given": "X."
			},
			{
				"family": "Huang",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hiettMeaningfulMachineLearning2022",
		"type": "paper-conference",
		"abstract": "Applied research presented in this paper describes an approach to provide meaningful evaluation of the Machine Learning (ML) components in a Full Motion Video (FMV) Machine Learning Enabled System (MLES). The MLES itself is not discussed in the paper. We focus on the experimental activity that has been designed to provide confidence that the MLES, when fielded under dynamic and uncertain conditions, performance will not be undermined by a lack of ML robustness. For example, to real-world changes of the same scene under differing light conditions. The paper details the technical approach and how it is applied to data, across the overall experimental pipeline, consisting of a perturbation engine, test pipeline and metric production. Data is from a small imagery dataset and the results are shown and discussed as part of a proof of concept study. © 2022 SPIE.",
		"archive": "Scopus",
		"DOI": "10.1117/12.2638492",
		"event-title": "Proceedings of SPIE - The International Society for Optical Engineering",
		"title": "Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145198838&doi=10.1117%2f12.2638492&partnerID=40&md5=0fe9bf39ca17fb32f5989749b1568d7c",
		"volume": "12276",
		"author": [
			{
				"family": "Hiett",
				"given": "B."
			},
			{
				"family": "Boyd",
				"given": "P."
			},
			{
				"family": "Fletcher",
				"given": "C."
			},
			{
				"family": "Gowland",
				"given": "S."
			},
			{
				"family": "Sharp",
				"given": "J.H."
			},
			{
				"family": "Sloggett",
				"given": "D."
			},
			{
				"family": "Banks",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "peralesgomezMethodologyEvaluatingRobustness2022",
		"type": "article-journal",
		"abstract": "Anomaly Detection systems based on Machine and Deep learning are the most promising solutions to detect cyberattacks in the industry. However, these techniques are vulnerable to adversarial attacks that downgrade prediction performance. Several techniques have been proposed to measure the robustness of Anomaly Detection in the literature. However, they do not consider that, although a small perturbation in an anomalous sample belonging to an attack, i.e., Denial of Service, could cause it to be misclassified as normal while retaining its ability to damage, an excessive perturbation might also transform it into a truly normal sample, with no real impact on the industrial system. This paper presents a methodology to calculate the robustness of Anomaly Detection models in industrial scenarios. The methodology comprises four steps and uses a set of additional models called support models to determine if an adversarial sample remains anomalous. We carried out the validation using the Tennessee Eastman process, a simulated testbed of a chemical process. In such a scenario, we applied the methodology to both a Long-Short Term Memory (LSTM) neural network and 1-dimensional Convolutional Neural Network (1D-CNN) focused on detecting anomalies produced by different cyberattacks. The experiments showed that 1D-CNN is significantly more robust than LSTM for our testbed. Specifically, a perturbation of 60% (empirical robustness of 0.6) of the original sample is needed to generate adversarial samples for LSTM, whereas in 1D-CNN the perturbation required increases up to 111% (empirical robustness of 1.11).  © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2022.3224930",
		"page": "124582-124594",
		"title": "A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144075247&doi=10.1109%2fACCESS.2022.3224930&partnerID=40&md5=6d19be1d6aa20ba9d211e1995bbbca85",
		"volume": "10",
		"author": [
			{
				"family": "Perales Gomez",
				"given": "A.L."
			},
			{
				"family": "Maimo",
				"given": "L.F."
			},
			{
				"family": "Clemente",
				"given": "F.J.G."
			},
			{
				"family": "Morales",
				"given": "J.A.M."
			},
			{
				"family": "Celdran",
				"given": "A.H."
			},
			{
				"family": "Bovet",
				"given": "G."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gittensAdversarialPerspectiveAccuracy2022",
		"type": "article-journal",
		"abstract": "Model accuracy is the traditional metric employed in machine learning (ML) applications. However, privacy, fairness, and robustness guarantees are crucial as ML algorithms increasingly pervade our lives and play central roles in socially important systems. These four desiderata constitute the pillars of Trustworthy ML (TML) and may mutually inhibit or reinforce each other. It is necessary to understand and clearly delineate the trade-offs among these desiderata in the presence of adversarial attacks. However, threat models for the desiderata are different and the defenses introduced for each leads to further trade-offs in a multilateral adversarial setting (i.e., a setting attacking several pillars simultaneously). The first half of the paper reviews the state of the art in TML research, articulates known multilateral trade-offs, and identifies open problems and challenges in the presence of an adversary that may take advantage of such multilateral trade-offs. The fundamental shortcomings of statistical association-based TML are discussed, to motivate the use of causal methods to achieve TML. The second half of the paper, in turn, advocates the use of causal modeling in TML. Evidence is collected from across the literature that causal ML is well-suited to provide a unified approach to TML. Causal discovery and causal representation learning are introduced as essential stages of causal modeling, and a new threat model for causal ML is introduced to quantify the vulnerabilities introduced through the use of causal methods. The paper concludes with pointers to possible next steps in the development of a causal TML pipeline. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2022.3218715",
		"page": "120850-120865",
		"title": "An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141622799&doi=10.1109%2fACCESS.2022.3218715&partnerID=40&md5=9035238f9011623885d10467067c26b9",
		"volume": "10",
		"author": [
			{
				"family": "Gittens",
				"given": "A."
			},
			{
				"family": "Yener",
				"given": "B."
			},
			{
				"family": "Yung",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangRobustOfflineReinforcement2022",
		"type": "paper-conference",
		"abstract": "Offline deep reinforcement learning algorithms are still in developing. Some existing algorithms have shown that it is feasible to learn directly without using environmental interaction under the condition of sufficient datasets. In this paper, we combine an offline reinforcement learning method through behavior regularization with a robust offline reinforcement learning algorithm. Moreover, the algorithm is verified and analyzed with a high-quality but limited dataset. The experimental results show that it is feasible to combine the behavior regularization method with the robust offline reinforcement learning algorithm, to gain better performance under the condition of limited data compared with the baseline algorithms.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IAICT55358.2022.9887435",
		"event-title": "Proceedings of the 2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology, IAICT 2022",
		"page": "150-154",
		"title": "A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139201012&doi=10.1109%2fIAICT55358.2022.9887435&partnerID=40&md5=21027466550a155936800e0c9aabbafa",
		"author": [
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Gao",
				"given": "T."
			},
			{
				"family": "Mi",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sunFairRobustClassification2022",
		"type": "paper-conference",
		"abstract": "Robustness and fairness are two equally important issues for machine learning systems. Despite the active research on robustness and fairness of ML recently, these efforts focus on either fairness or robustness, but not both. To bridge this gap, in this paper, we design Fair and Robust Classification (FRoC) models that equip the classification models with both fairness and robustness. Meeting both fairness and robustness constraints is not trivial due to the tension between them. The trade-off between fairness, robustness, and model accuracy also introduces additional challenge. To address these challenges, we design two FRoC methods, namely FRoC-PRE that modifies the input data before model training, and FRoC-IN that modifies the model with an adversarial objective function to address both fairness and robustness during training. FRoC-IN is suitable to the settings where the users (e.g., ML service providers) only have the access to the model but not the original data, while FRoC-PRE works for the settings where the users (e.g., data owners) have the access to both data and a surrogate model that may have similar architecture as the target model. Our extensive experiments on real-world datasets demonstrate that both FRoC-IN and FRoC-PRE can achieve both fairness and robustness with insignificant accuracy loss of the target model. © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/EuroSP53844.2022.00030",
		"event-title": "Proceedings - 7th IEEE European Symposium on Security and Privacy, Euro S and P 2022",
		"page": "356-376",
		"title": "Towards Fair and Robust Classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134017587&doi=10.1109%2fEuroSP53844.2022.00030&partnerID=40&md5=671eb3c92941e21c487bcff140b27e8d",
		"author": [
			{
				"family": "Sun",
				"given": "H."
			},
			{
				"family": "Wu",
				"given": "K."
			},
			{
				"family": "Wang",
				"given": "T."
			},
			{
				"family": "Wang",
				"given": "W.H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "xiongHiSaRLHierarchicalFramework2022",
		"type": "paper-conference",
		"abstract": "We propose a two-level hierarchical framework for safe reinforcement learning in a complex environment. The high-level part is an adaptive planner, which aims at learning and generating safe and efficient paths for tasks with imperfect map information. The lower-level part contains a learning-based controller and its corresponding neural Lyapunov function, which characterizes the controller's stability property. This learned neural Lyapunov function serves two purposes. First, it will be part of the high-level heuristic for our planning algorithm. Second, it acts as a part of a runtime shield to guard the safety of the whole system. We use a robot navigation example to demonstrate that our framework can operate efficiently and safely in complex environments, even under adversarial attacks. Copyright © 2022 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382338&partnerID=40&md5=ac7bb4dc501b17e0342bcc2d1c031aca",
		"volume": "3087",
		"author": [
			{
				"family": "Xiong",
				"given": "Z."
			},
			{
				"family": "Agarwal",
				"given": "I."
			},
			{
				"family": "Jagannathan",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangDirichletProcessMixture2022",
		"type": "article-journal",
		"abstract": "While reinforcement learning (RL) algorithms are achieving state-of-the-art performance in various challenging tasks, they can easily encounter catastrophic forgetting or interference when faced with lifelong streaming information. In this article, we propose a scalable lifelong RL method that dynamically expands the network capacity to accommodate new knowledge while preventing past memories from being perturbed. We use a Dirichlet process mixture to model the nonstationary task distribution, which captures task relatedness by estimating the likelihood of task-to-cluster assignments and clusters the task models in a latent space. We formulate the prior distribution of the mixture as a Chinese restaurant process (CRP) that instantiates new mixture components as needed. The update and expansion of the mixture are governed by the Bayesian nonparametric framework with an expectation maximization (EM) procedure, which dynamically adapts the model complexity without explicit task boundaries or heuristics. Moreover, we use the domain randomization technique to train robust prior parameters for the initialization of each task model in the mixture; thus, the resulting model can better generalize and adapt to unseen tasks. With extensive experiments conducted on robot navigation and locomotion domains, we show that our method successfully facilitates scalable lifelong RL and outperforms relevant existing methods. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Cybernetics",
		"DOI": "10.1109/TCYB.2022.3170485",
		"page": "1-12",
		"title": "A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130466921&doi=10.1109%2fTCYB.2022.3170485&partnerID=40&md5=db9ee7534b0697b5af43aa33db661075",
		"author": [
			{
				"family": "Wang",
				"given": "Z."
			},
			{
				"family": "Chen",
				"given": "C."
			},
			{
				"family": "Dong",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liAdaptiveInterleavedReinforcement2022",
		"type": "article-journal",
		"abstract": "This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton-Jacobi-Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.  © 2012 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2020.3027653",
		"issue": "1",
		"page": "270-280",
		"title": "Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems with Unknown Uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122590061&doi=10.1109%2fTNNLS.2020.3027653&partnerID=40&md5=0519b95993dc7072c4748d7a8416a5be",
		"volume": "33",
		"author": [
			{
				"family": "Li",
				"given": "J."
			},
			{
				"family": "Ding",
				"given": "J."
			},
			{
				"family": "Chai",
				"given": "T."
			},
			{
				"family": "Lewis",
				"given": "F.L."
			},
			{
				"family": "Jagannathan",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liAssessingEnhancingAdversarial2022",
		"type": "article-journal",
		"abstract": "As predictive analytics increasingly applies supervised machine learning (SML) models to inform mission-critical decision-making, adversaries become incentivized to exploit the vulnerabilities of these SML models and mislead predictive analytics into erroneous decisions. Due to the limited understanding and awareness of such adversarial attacks, the predictive analytics knowledge and deployment need a principled technique for adversarial robustness assessment and enhancement. In this research, we leverage the technology threat avoidance theory as the kernel theory and propose a research framework for assessing and enhancing the adversarial robustness of predictive analytics applications. We instantiate the proposed framework by developing a robust text classification system, the ARText system. The proposed system is rigorously evaluated in comparison with benchmark methods on two tasks extensively enabled by SML: spam review detection and spam email detection, which then confirmed the utility and effectiveness of our ARText system. Results from numerous experiments revealed that our proposed framework could significantly enhance the adversarial robustness of predictive analytics applications. © 2022 Taylor & Francis Group, LLC.",
		"archive": "Scopus",
		"container-title": "Journal of Management Information Systems",
		"DOI": "10.1080/07421222.2022.2063549",
		"issue": "2",
		"page": "542-572",
		"title": "Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131375989&doi=10.1080%2f07421222.2022.2063549&partnerID=40&md5=03a5b535d2a12af5782faf5ad9d9b1a6",
		"volume": "39",
		"author": [
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Chai",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liRobustSupervisedSubspace2021",
		"type": "article-journal",
		"abstract": "This paper proposes a novel robust supervised subspace learning (RSSL) method for output-relevant prediction and detection against outliers. RSSL learns the robust subspaces by optimizing a joint problem over both the prediction of output and the reconstruction of input. To this end, the learned subspaces/data representations are informative, i.e., they are encapsulated with the critic information related to both the input and output, and thus can benefit the following tasks of output-related modeling and detection. Besides, we separate sparse items from the raw measurements to suppress the effects of outliers. An efficient optimization algorithm is designed to solve the optimization problem of RSSL. We further conduct post orthogonal decomposition upon the subspaces provided by RSSL so that the trimmed subspaces are more suitable for output-related detection. The efficacy of the proposed method is extensively verified by synthesis data and benchmark data. © 2021 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Journal of Process Control",
		"DOI": "10.1016/j.jprocont.2021.09.007",
		"page": "184-194",
		"title": "A robust supervised subspace learning approach for output-relevant prediction and detection against outliers",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116036069&doi=10.1016%2fj.jprocont.2021.09.007&partnerID=40&md5=0d74c08e6d482e68ae1a0bf1a20ff3cb",
		"volume": "106",
		"author": [
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Wang",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "seifiDatadrivenRobustOptimization2021",
		"type": "article-journal",
		"abstract": "The huge availability of data in the last decade has raised the opportunity for the better use of data in decision-making processes. The idea of using the existing data to achieve a more coherent reality solution has led to a branch of optimization called data-driven optimization. On the one hand, the presence of uncertain variables in these datasets makes it crucial to design robust optimization methods in this area. On the other hand, in many real-world problems, the closed-form of the objective function is not available and a meta-model based framework is necessary. Motivated by the above points, in this paper a Gaussian process is used in a Bayesian optimization framework to design a method that is consistent with the data in a predefined confidence level. The advantage of the proposed method is that it is computationally tractable in addition to being robust and independent of the objective function's form. As one of the applications of the proposed algorithm, hyper-parameter optimization for deep learning is investigated. The proposed method can help find the optimal hyper-parameters that are robust with respect to noise. © 2021 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Computers and Industrial Engineering",
		"DOI": "10.1016/j.cie.2021.107581",
		"title": "A data-driven robust optimization algorithm for black-box cases: An application to hyper-parameter optimization of machine learning algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112307597&doi=10.1016%2fj.cie.2021.107581&partnerID=40&md5=a6ef9ee5ae1cddf885674810e44be2c2",
		"volume": "160",
		"author": [
			{
				"family": "Seifi",
				"given": "F."
			},
			{
				"family": "Azizi",
				"given": "M.J."
			},
			{
				"family": "Akhavan Niaki",
				"given": "S.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "curiCombiningPessimismOptimism2021",
		"type": "paper-conference",
		"abstract": "In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness against worst-case situations. The robust RL framework addresses this challenge via a worst-case optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty, and efficiently explores both the agent and adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally, we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments. Copyright © 2021 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "2254-2264",
		"title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127231156&partnerID=40&md5=0803f12a48c9ee753414ffb80af7e9d2",
		"volume": "139",
		"author": [
			{
				"family": "Curi",
				"given": "S."
			},
			{
				"family": "Bogunovic",
				"given": "I."
			},
			{
				"family": "Krause",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wangRobustAutomatedMachine2021",
		"type": "article-journal",
		"abstract": "Developing a robust deep neural network (DNN) for a specific task is not only time-consuming but also requires lots of experienced human experts. In order to make deep neural networks easier to apply or even take the human experts out of the design of network architecture completely, a growing number of researches focus on robust automated machine learning (AutoML). In this paper, we investigated the robustness problem of AutoML systems based on contractive pseudoinverse learners. In our proposed method, deep neural networks were built with stacked contractive pseudoinverse learners (CPILer). Each CPILer has a Jacobian regularized reconstruction loss function and is trained with pseudoinverse learning algorithm. When sigmoid activation function is adopted in the hidden layer, the graph Laplace regularizer is derived from square Frobenius norm of the Jacobian matrix. This learning scheme not only speeds up the training process dramatically but also reduces the effort of hyperparameter tuning. In addition, the graph Laplace regularization can improve the robustness of the learning systems by reducing the sensibility to noise. An ensemble network architecture consisting of several sub-networks was designed to build the AutoML systems. The architecture hyperparameters of the system were determined in an automated way which could be considered as a data-driven way. The proposed method shown good performance in the experiments in terms of efficiency and accuracy, and outperformed the baseline methods on a series of benchmark data sets. The robustness improvement of our proposed method was also demonstrated in the experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Cognitive Computation",
		"DOI": "10.1007/s12559-021-09853-6",
		"issue": "3",
		"page": "724-735",
		"title": "A Robust Automated Machine Learning System with Pseudoinverse Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103065280&doi=10.1007%2fs12559-021-09853-6&partnerID=40&md5=e40b73e3515c9d6099b659efc424c45b",
		"volume": "13",
		"author": [
			{
				"family": "Wang",
				"given": "K."
			},
			{
				"family": "Guo",
				"given": "P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenFedEqualDefendingModel2021",
		"type": "paper-conference",
		"abstract": "With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks. © 2021 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/GLOBECOM46510.2021.9685082",
		"event-title": "2021 IEEE Global Communications Conference, GLOBECOM 2021 - Proceedings",
		"title": "FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127300983&doi=10.1109%2fGLOBECOM46510.2021.9685082&partnerID=40&md5=018f5ed6035aca25fa5d428aab92a59a",
		"author": [
			{
				"family": "Chen",
				"given": "L.-Y."
			},
			{
				"family": "Chiu",
				"given": "T.-C."
			},
			{
				"family": "Pang",
				"given": "A.-C."
			},
			{
				"family": "Cheng",
				"given": "L.-C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "xuLookYouLeap2021",
		"type": "paper-conference",
		"abstract": "Safety has become one of the main challenges of applying deep reinforcement learning to real world systems. Currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. In this paper, we propose MBHI, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both”local” and”non-local” catastrophes. An ensemble of supervised learners are trained in MBHI to imitate human blocking decisions. Similar to human decision-making process, MBHI will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. When the imagination encounters a catastrophe, MBHI will block the current action and use an efficient MPC method to output a safety policy. We evaluate our method on several safety tasks, and the results show that MBHI achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines. © 2021 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "332-341",
		"title": "Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146865238&partnerID=40&md5=b9693007970623e55035eef698de6e42",
		"volume": "164",
		"author": [
			{
				"family": "Xu",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "Z."
			},
			{
				"family": "Duan",
				"given": "G."
			},
			{
				"family": "Zhu",
				"given": "J."
			},
			{
				"family": "Bai",
				"given": "X."
			},
			{
				"family": "Tan",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "maAdaptiveRobustLearning2021",
		"type": "article-journal",
		"abstract": "In general, introducing robust distance metrics and loss functions in the learning process can improve the robustness of the algorithms. In this work, we first propose a new robust loss function called adaptive capped Lθε-loss. For different problems, we can choose different loss functions through adaptive parameter θ during the learning process. Secondly, we propose a new robust distance metric induced by correntropy (CIM) that is based on Laplacian kernel. The CIM contains first and higher-order moments from samples. Further, we demonstrate some important and interesting properties of the Lθε-loss and CIM, such as robustness, boundedness, nonconvexity, etc. Finally, we apply the to Lθε-loss and CIM to twin support vector machine (TWSVM) and develop an adaptive robust learning framework, namely adaptive robust twin support vector machine (ARTSVM). The proposed ARTSVM not only inherits the advantages of TWSVM but also improves the robustness of classification problems. A non-convex optimization method, DC (difference of convex functions) programming algorithm (DCA) is used to solve the proposed ARTSVM, and the convergence of the algorithm is proved theoretically. Experiments on multiple datasets show that the proposed ARTSVM is competitive with existing methods. © 2020",
		"archive": "Scopus",
		"container-title": "Knowledge-Based Systems",
		"DOI": "10.1016/j.knosys.2020.106536",
		"title": "Adaptive robust learning framework for twin support vector machine classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094912105&doi=10.1016%2fj.knosys.2020.106536&partnerID=40&md5=c95bdc9b36362521439723d98ce958c3",
		"volume": "211",
		"author": [
			{
				"family": "Ma",
				"given": "J."
			},
			{
				"family": "Yang",
				"given": "L."
			},
			{
				"family": "Sun",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wangOnlineRobustReinforcement2021",
		"type": "paper-conference",
		"abstract": "Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially, and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set, and design robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms, and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts (within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms. © 2021 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"page": "7193-7206",
		"title": "Online Robust Reinforcement Learning with Model Uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130775690&partnerID=40&md5=ac00e2eb7a717c085e21d9699fe68881",
		"volume": "9",
		"author": [
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Zou",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fengLearningSafelyApprove2021",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms in healthcare have the potential to continually learn from real-world data generated during healthcare delivery and adapt to dataset shifts. As such, regulatory bodies like the US FDA have begun discussions on how to autonomously approve modifications to algorithms. Current proposals evaluate algorithmic modifications via hypothesis testing and control a definition of online approval error that only applies if the data is stationary over time, which is unlikely in practice. To this end, we investigate designing approval policies for modifications to ML algorithms in the presence of distributional shifts. Our key observation is that the approval policy most efficient at identifying and approving beneficial modifications varies across problem settings. So, rather than selecting a fixed approval policy a priori, we propose learning the best approval policy by searching over a family of approval strategies. We define a family of strategies that range in their level of optimism when approving modifications. To protect against settings where no version of the ML algorithm performs well, this family includes a pessimistic strategy that rescinds approval. We use the exponentially weighted averaging forecaster (EWAF) to learn the most appropriate strategy and derive tighter regret bounds assuming the distributional shifts are bounded. In simulation studies and empirical analyses, we find that wrapping approval strategies within EWAF is a simple yet effective approach to protect against distributional shifts without significantly slowing down approval of beneficial modifications.  © 2021 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3450439.3451864",
		"event-title": "ACM CHIL 2021 - Proceedings of the 2021 ACM Conference on Health, Inference, and Learning",
		"page": "164-173",
		"title": "Learning to safely approve updates to machine learning algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104096120&doi=10.1145%2f3450439.3451864&partnerID=40&md5=742aa4352dc79c83df5bdfb093ff02cb",
		"author": [
			{
				"family": "Feng",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pulawskiBDIDojoDevelopingRobust2021",
		"type": "paper-conference",
		"abstract": "The Belief-Desire-Intention (BDI) architecture is a widely-used model for developing multi-agent systems. BDI agents pursue their goals over time using a collection of plan recipes that are programmed by the developers. Thus, traditional BDI agents are limited in dealing with dynamic environments where uncertainties are not known beforehand, such as those introduced by adversarial forces. In this paper, we present the BDI-Dojo framework for developing robust BDI agents by training them using reinforcement learning against similarly learning-equipped adversarial agents. This adversarial training approach empowers BDI agents to become more resilient in uncertain, dynamic environments. © 2021 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ACSOS-C52956.2021.00066",
		"event-title": "Proceedings - 2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion, ACSOS-C 2021",
		"page": "257-262",
		"title": "BDI-Dojo: Developing robust BDI agents in evolving adversarial environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123394011&doi=10.1109%2fACSOS-C52956.2021.00066&partnerID=40&md5=1d727ac9b754c915d09d08c9493bfe5e",
		"author": [
			{
				"family": "Pulawski",
				"given": "S."
			},
			{
				"family": "Dam",
				"given": "H.K."
			},
			{
				"family": "Ghose",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangRobustMethodMeasure2021",
		"type": "article-journal",
		"abstract": "Because machine learning has been widely used in various domains, interpreting internal mechanisms and predictive results of models is crucial for further applications of complex machine learning models. However, the interpretability of complex machine learning models on biased data remains a difficult problem. When the important explanatory features of concerned data are highly influenced by contaminated distributions, particularly in risk-sensitive fields, such as self-driving vehicles and healthcare, it is crucial to provide a robust interpretation of complex models for users. The interpretation of complex models is often associated with analyzing model features by measuring feature importance. Therefore, this article proposes a novel method derived from high-dimensional model representation (HDMR) to measure feature importance. The proposed method can provide robust estimation when the input features follow contaminated distributions. Moreover, the method is model-agnostic, which can enhance its ability to compare different interpretations due to its generalizability. Experimental evaluations on artificial models and machine learning models show that the proposed method is more robust than the traditional method based on HDMR. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2021.3049412",
		"page": "7885-7893",
		"title": "A Robust Method to Measure the Global Feature Importance of Complex Prediction Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099261795&doi=10.1109%2fACCESS.2021.3049412&partnerID=40&md5=d2209d9951eb35633613b8bfcbdab3a2",
		"volume": "9",
		"author": [
			{
				"family": "Zhang",
				"given": "X."
			},
			{
				"family": "Wu",
				"given": "L."
			},
			{
				"family": "Li",
				"given": "Z."
			},
			{
				"family": "Liu",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mengDistantlySupervisedNamedEntity2021",
		"type": "paper-conference",
		"abstract": "We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins. © 2021 Association for Computational Linguistics",
		"archive": "Scopus",
		"event-title": "EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings",
		"page": "10367-10378",
		"title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121748854&partnerID=40&md5=0fb4ea97baff35f25aec98ee2a1ae5de",
		"author": [
			{
				"family": "Meng",
				"given": "Y."
			},
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Huang",
				"given": "J."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Ji",
				"given": "H."
			},
			{
				"family": "Han",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abdelkaderRobustProductionMachine2020",
		"type": "paper-conference",
		"abstract": "The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components. © 2020 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3324884.3415281",
		"event-title": "Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",
		"page": "1164-1166",
		"title": "Towards Robust Production Machine Learning Systems: Managing Dataset Shift",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099251225&doi=10.1145%2f3324884.3415281&partnerID=40&md5=b7f18b590ee0b52cf8df446932f366cc",
		"author": [
			{
				"family": "Abdelkader",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kowsherImpactlearningRobustMachine2020",
		"type": "paper-conference",
		"abstract": "The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth. © 2020 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3411174.3411185",
		"event-title": "ACM International Conference Proceeding Series",
		"page": "9-13",
		"title": "Impact-learning: A robust machine learning algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089608381&doi=10.1145%2f3411174.3411185&partnerID=40&md5=5aa583b9d3f7591557bcf03a3f1f6a17",
		"author": [
			{
				"family": "Kowsher",
				"given": "M."
			},
			{
				"family": "Tahabilder",
				"given": "A."
			},
			{
				"family": "Murad",
				"given": "S.A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "abdelfattahRobustPolicyBootstrapping2020",
		"type": "article-journal",
		"abstract": "Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this kind of problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity in order to evolve a coverage set of policies that can solve the problem. This article introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments. © The Author(s) 2019.",
		"archive": "Scopus",
		"container-title": "Adaptive Behavior",
		"DOI": "10.1177/1059712319869313",
		"issue": "4",
		"page": "273-292",
		"title": "A robust policy bootstrapping algorithm for multi-objective reinforcement learning in non-stationary environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071469173&doi=10.1177%2f1059712319869313&partnerID=40&md5=5430878a3c4684e33716a18eea2a7dfc",
		"volume": "28",
		"author": [
			{
				"family": "Abdelfattah",
				"given": "S."
			},
			{
				"family": "Kasmarik",
				"given": "K."
			},
			{
				"family": "Hu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "alufaisanRobustTransparencyModel2020",
		"type": "article-journal",
		"abstract": "Transparency has become a critical need in machine learning (ML) applications. Designing transparent ML models helps increase trust, ensure accountability, and scrutinize fairness. Some organizations may opt-out of transparency to protect individuals&#x0027; privacy. Therefore, there is a great demand for transparency models that consider both privacy and security risks. Such transparency models can motivate organizations to improve their credibility by making the ML-based decision-making process comprehensible to end-users. Differential privacy (DP) provides an important technique to disclose information while protecting individual privacy. However, it has been shown that DP alone cannot prevent certain types of privacy attacks against disclosed ML models. DP with low values can provide high privacy guarantees, but may result in significantly weaker ML models in terms of accuracy. On the other hand, setting value too high may lead to successful privacy attacks. This raises the question whether we can disclose accurate transparent ML models while preserving privacy. In this paper we introduce a novel technique that complements DP to ensure model transparency and accuracy while being robust against model inversion attacks. We show that combining the proposed technique with DP provide highly transparent and accurate ML models while preserving privacy against model inversion attacks. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Dependable and Secure Computing",
		"DOI": "10.1109/TDSC.2020.3019508",
		"title": "Robust Transparency Against Model Inversion Attacks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090442360&doi=10.1109%2fTDSC.2020.3019508&partnerID=40&md5=bb2978e08029eebf6487f9ba6bd9e6b3",
		"author": [
			{
				"family": "Alufaisan",
				"given": "Y."
			},
			{
				"family": "Kantarcioglu",
				"given": "M."
			},
			{
				"family": "Zhou",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jingRobustExtremeLearning2020",
		"type": "article-journal",
		"abstract": "Uncertain or missing data may occur in many practical applications. A principled strategy for handling this problem would therefore be very useful. We consider two-class and multi-class classification problems where the mean and covariance of each class are assumed to be known. With simple structure, fast speed and good performance, extreme learning machine (ELM) has been an important technology in machine learning. In this work, from the viewpoint of probability, we present a robust ELM framework (RELM) for missing data classification. Applying the Chebyshev–Cantelli inequality, the proposed RELM is reformulated as a second-order cone programming with global optimal solution. The proposed RELM only relates to the second moments of input samples and makes no assumption about the data probability distribution. Expectation maximization algorithm is used to fill in missing values and then obtain complete data. Numerical experiments are simulated in various datasets from UCI database and a practical application database. Experimental results show that the proposed method can achieve better performance than traditional methods. These results illustrate the feasibility and effectiveness of the proposed method for missing data classification. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Journal of Supercomputing",
		"DOI": "10.1007/s11227-018-2430-6",
		"issue": "4",
		"page": "2390-2416",
		"title": "A robust extreme learning machine framework for uncertain data classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081254670&doi=10.1007%2fs11227-018-2430-6&partnerID=40&md5=9d6880693d753dff20a823380e6433c1",
		"volume": "76",
		"author": [
			{
				"family": "Jing",
				"given": "S."
			},
			{
				"family": "Yang",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pinarozisikSecurityAnalysisSafe2020",
		"type": "paper-conference",
		"abstract": "We analyze the extent to which existing methods rely on accurate training data for a specific class of reinforcement learning (RL) algorithms, known as Safe and Seldonian RL. We introduce a new measure of security to quantify the susceptibility to perturbations in training data by creating an attacker model that represents a worst-case analysis, and show that a couple of Seldonian RL methods are extremely sensitive to even a few data corruptions. We then introduce a new algorithm that is more robust against data corruptions, and demonstrate its usage in practice on some RL problems, including a grid-world and a diabetes treatment simulation. © 2020 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Security analysis of safe and seldonian reinforcement learning algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108455515&partnerID=40&md5=dfff89b401122e01f74e471cb746b1f9",
		"volume": "2020-December",
		"author": [
			{
				"family": "Pinar Ozisik",
				"given": "A."
			},
			{
				"family": "Thomas",
				"given": "P.S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "klasFrameworkBuildingUncertainty2020",
		"type": "paper-conference",
		"abstract": "More and more software-intensive systems include components that are data-driven in the sense that they use models based on artificial intelligence (AI) or machine learning (ML). Since the outcomes of such models cannot be assumed to always be correct, related uncertainties must be understood and taken into account when decisions are made using these outcomes. This applies, in particular, if such decisions affect the safety of the system. To date, however, hardly any AI-/ML-based model provides dependable estimates of the uncertainty remaining in its outcomes. In order to address this limitation, we present a framework for encapsulating existing models applied in data-driven components with an uncertainty wrapper in order to enrich the model outcome with a situation-aware and dependable uncertainty statement. The presented framework is founded on existing work on the concept and mathematical foundation of uncertainty wrappers. The application of the framework is illustrated using pedestrian detection as an example, which is a particularly safety-critical feature in the context of autonomous driving. The Brier score and its components are used to investigate how the key aspects of the framework (scoping, clustering, calibration, and confidence limits) can influence the quality of uncertainty estimates. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_23",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "315-327",
		"title": "A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096598898&doi=10.1007%2f978-3-030-55583-2_23&partnerID=40&md5=83471415db0d0778b436e00a14267b50",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Kläs",
				"given": "M."
			},
			{
				"family": "Jöckel",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lagravePrincipalComponentAnalysis2020",
		"type": "paper-conference",
		"abstract": "Building robust-by-design Machine Learning algorithms is key for critical tasks such as safety or military applications. By leveraging on the ideas developed in the context of building invariant Support Vectors Machines, this paper introduces a convenient methodology for embedding local Lie groups symmetries into Deep Learning algorithms by performing a Principal Component Analysis on the corresponding Tangent Covariance Matrix. The projection of the input data onto the principal directions leads to a new data representation which allows singling out the components conveying the semantic information useful to the considered algorithmic task while reducing the dimension of the input manifold. Besides, our numerical testing emphasizes that, although less efficient than using Group-Convolutional Neural Networks as only dealing with local symmetries, our approach does improve accuracy and robustness without introducing significant computational overhead. Performance improvements up to 5% were obtained for low capacity algorithms, making this approach of particular interest for the engineering of safe embedded Artificial Intelligence systems. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_22",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "302-314",
		"title": "A Principal Component Analysis Approach for Embedding Local Symmetries into Deep Learning Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096594336&doi=10.1007%2f978-3-030-55583-2_22&partnerID=40&md5=26c5c385b8de90bde29d678864ad77a9",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Lagrave",
				"given": "P.-Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wozniakSafetyCasePattern2020",
		"type": "paper-conference",
		"abstract": "Several standards from the domain of safety critical systems, in order to support the argumentation of the safety assurance of a system under development, recommend the construction of a safety case. This activity is guided by the objectives to be met, recommended or required by the standards along the safety lifecycle. Ongoing attempts to use Machine Learning (ML) for safety critical functionality revealed certain deficits. For instance, the widely recognized standard for functional safety of automotive systems, ISO 26262, which can be used as a basis to construct a safety case, does not reason about ML. To this end, the goal of this work is to provide a pattern for arguing about the correct implementation of safety requirements in system components based on ML. The pattern is integrated within an overall encompassing approach for safety case generation for automotive systems and its applicability is showcased on a pedestrian avoidance system. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_28",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "370-382",
		"title": "A Safety Case Pattern for Systems with Machine Learning Components",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096573715&doi=10.1007%2f978-3-030-55583-2_28&partnerID=40&md5=fe9cbbd8d29147cd6d4befb4c7199d1f",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Wozniak",
				"given": "E."
			},
			{
				"family": "Cârlan",
				"given": "C."
			},
			{
				"family": "Acar-Celik",
				"given": "E."
			},
			{
				"family": "Putzer",
				"given": "H.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wardAssuranceCasePattern2020",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) has the potential to become widespread in safety-critical applications. It is therefore important that we have sufficient confidence in the safe behaviour of the ML-based functionality. One key consideration is whether the ML being used is interpretable. In this paper, we present an argument pattern, i.e. reusable structure, that can be used for justifying the sufficient interpretability of ML within a wider assurance case. The pattern can be used to assess whether the right interpretability method and format are used in the right context (time, setting and audience). This argument structure provides a basis for developing and assessing focused requirements for the interpretability of ML in safety-critical domains. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_30",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "395-407",
		"title": "An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096588864&doi=10.1007%2f978-3-030-55583-2_30&partnerID=40&md5=eb76c695180f85f6bedebd5169c827e6",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Ward",
				"given": "F.R."
			},
			{
				"family": "Habli",
				"given": "I."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ghoshDeploymentRobustCooperative2020",
		"type": "paper-conference",
		"abstract": "We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by real-world applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user's actions to make inferences about the user's type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents. © 2020 International Foundation for Autonomous.",
		"archive": "Scopus",
		"event-title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
		"page": "447-455",
		"title": "Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093460105&partnerID=40&md5=9647e6eca81c01f303cba645d35a90aa",
		"volume": "2020-May",
		"author": [
			{
				"family": "Ghosh",
				"given": "A."
			},
			{
				"family": "Mahdavi",
				"given": "H."
			},
			{
				"family": "Tschiatschek",
				"given": "S."
			},
			{
				"family": "Singla",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cappozzoRobustApproachModelbased2020",
		"type": "article-journal",
		"abstract": "In a standard classification framework a set of trustworthy learning data are employed to build a decision rule, with the final aim of classifying unlabelled units belonging to the test set. Therefore, unreliable labelled observations, namely outliers and data with incorrect labels, can strongly undermine the classifier performance, especially if the training size is small. The present work introduces a robust modification to the Model-Based Classification framework, employing impartial trimming and constraints on the ratio between the maximum and the minimum eigenvalue of the group scatter matrices. The proposed method effectively handles noise presence in both response and exploratory variables, providing reliable classification even when dealing with contaminated datasets. A robust information criterion is proposed for model selection. Experiments on real and simulated data, artificially adulterated, are provided to underline the benefits of the proposed method. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Advances in Data Analysis and Classification",
		"DOI": "10.1007/s11634-019-00371-w",
		"issue": "2",
		"page": "327-354",
		"title": "A robust approach to model-based classification based on trimming and constraints: Semi-supervised learning in presence of outliers and label noise",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070929493&doi=10.1007%2fs11634-019-00371-w&partnerID=40&md5=97940a49ee45db1b0a11aa8ed46b42d8",
		"volume": "14",
		"author": [
			{
				"family": "Cappozzo",
				"given": "A."
			},
			{
				"family": "Greselin",
				"given": "F."
			},
			{
				"family": "Murphy",
				"given": "T.B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "xinDecentralizedStochasticOptimization2020",
		"type": "article-journal",
		"abstract": "Decentralized methods to solve finite-sum minimization problems are important in many signal processing and machine learning tasks where the data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. In this article, we review decentralized stochastic first-order methods and provide a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence. We provide explicit theoretical guarantees of the corresponding methods when the objective functions are smooth and strongly convex and show their applicability to nonconvex problems via numerical experiments. Throughout the article, we provide intuitive illustrations of the main technical ideas by casting appropriate tradeoffs and comparisons among the methods of interest and by highlighting applications to decentralized training of machine learning models. © 1991-2012 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Signal Processing Magazine",
		"DOI": "10.1109/MSP.2020.2974267",
		"issue": "3",
		"page": "102-113",
		"title": "Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084602494&doi=10.1109%2fMSP.2020.2974267&partnerID=40&md5=8770fefac4537655d72f8fadd99b637c",
		"volume": "37",
		"author": [
			{
				"family": "Xin",
				"given": "R."
			},
			{
				"family": "Kar",
				"given": "S."
			},
			{
				"family": "Khan",
				"given": "U.A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "linardInductionFaultTrees2020",
		"type": "paper-conference",
		"abstract": "Cyber-physical systems have increasingly intricate architectures and failure modes, which is due to an explosion of their complexity, size, and failure criticality. While expert knowledge of individual components exists, their interaction is complex. For these reasons, obtaining accurate system reliability models is a hard task. At the same time, systems tend to be continuously monitored via advanced sensor systems. This data describes the components' failure behavior and can be exploited for failure diagnosis and learning of reliability models. This paper presents an effective algorithm for the learning of Fault Trees from data. Fault trees (FTs) are a widespread formalism in reliability engineering. They capture the failure behavior of components and their propagation through an entire system. To that end, we first use machine learning to compute a Bayesian Network (BN) highlighting probabilistic relationships between the failures of components and root causes. Then, we apply a set of rules to translate a BN into an FT, based on the Conditional Probability Tables to decide, amongst others, the nature of gates in the FT. We evaluate our method on synthetic data and a benchmark set of FTs. © 2019 European Safety and Reliability Association. Published by Research Publishing, Singapore.",
		"archive": "Scopus",
		"DOI": "10.3850/978-981-11-2724-3_0596-cd",
		"event-title": "Proceedings of the 29th European Safety and Reliability Conference, ESREL 2019",
		"page": "910-917",
		"title": "Induction of fault trees through Bayesian networks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089187848&doi=10.3850%2f978-981-11-2724-3_0596-cd&partnerID=40&md5=d522b2243b1a745146c5bc2d04b75084",
		"author": [
			{
				"family": "Linard",
				"given": "A."
			},
			{
				"family": "Bueno",
				"given": "M.L.P."
			},
			{
				"family": "Bucur",
				"given": "D."
			},
			{
				"family": "Stoelinga",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "picardiAssuranceArgumentPatterns2020",
		"type": "paper-conference",
		"abstract": "Machine Learnt (ML) components are now widely accepted for use in a range of applications with results that are reported to exceed, under certain conditions, human performance. The adoption of ML components in safety-related domains is restricted, however, unless sufficient assurance can be demonstrated that the use of these components does not compromise safety. In this paper, we present patterns that can be used to develop assurance arguments for demonstrating the safety of the ML components. The argument patterns provide reusable templates for the types of claims that must be made in a compelling argument. On their own, the patterns neither detail the assurance artefacts that must be generated to support the safety claims for a particular system, nor provide guidance on the activities that are required to generate these artefacts. We have therefore also developed a process for the engineering of ML components in which the assurance evidence can be generated at each stage in the ML lifecycle in order to instantiate the argument patterns and create the assurance case for ML components. The patterns and the process could help provide a practical and clear basis for a justifiable deployment of ML components in safety-related systems. © 2020 for this paper by its authors.",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"page": "23-30",
		"title": "Assurance argument patterns and processes for machine learning in safety-related systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604916&partnerID=40&md5=aeaccb7806b23dc5dcf079764a13b75b",
		"volume": "2560",
		"author": [
			{
				"family": "Picardi",
				"given": "C."
			},
			{
				"family": "Paterson",
				"given": "C."
			},
			{
				"family": "Hawkins",
				"given": "R."
			},
			{
				"family": "Calinescu",
				"given": "R."
			},
			{
				"family": "Habli",
				"given": "I."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dikImprovedRobustFuzzy2020",
		"type": "article-journal",
		"abstract": "This paper presents a robust, dynamic, and unsupervised fuzzy learning algorithm (RDUFL) that aims to cluster a set of data samples with the ability to detect outliers and assign the numbers of clusters automatically. It consists of three main stages. The first (1) stage is a pre-processing method in which possible outliers are determined and quarantined using a concept of proximity degree. The second (2) stage is a learning method, which consists in auto-detecting the number of classes with their prototypes for a dynamic threshold. This threshold is automatically determined based on the similarity among the detected prototypes that are updated at the exploration of a new data. The last (3) stage treats quarantined samples detected from the first stage to determine whether they belong to some class defined in the second phase. The effectiveness of this method is assessed on eight real medical benchmark datasets in comparison to known unsupervised learning methods, namely, the fuzzy c-means (FCM), possibilistic c-means (PCM), and noise clustering (NC). The obtained accuracy of our scheme is very promising for unsupervised learning problems. © 2020 Walter de Gruyter GmbH, Berlin/Boston.",
		"archive": "Scopus",
		"container-title": "Journal of Intelligent Systems",
		"DOI": "10.1515/jisys-2018-0030",
		"issue": "1",
		"page": "1028-1042",
		"title": "An Improved Robust Fuzzy Algorithm for Unsupervised Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056243355&doi=10.1515%2fjisys-2018-0030&partnerID=40&md5=c83a75d5ee40e2f0bc136d8e5f54214b",
		"volume": "29",
		"author": [
			{
				"family": "Dik",
				"given": "A."
			},
			{
				"family": "Jebari",
				"given": "K."
			},
			{
				"family": "Ettouhami",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "machidaNversionMachineLearning2019",
		"type": "paper-conference",
		"abstract": "Quality control of machine learning systems is a fundamental challenge in industries to provide intelligent services or products using machine learning. While recent advances in machine learning algorithms substantially improve the performance of intelligent tasks such as object recognition, their outputs are essentially stochastic and very sensitive to input data. Such an output uncertainty is a big obstacle to ensure the quality of safety critical applications like autonomous vehicle and hence architectural design to mitigate the impact of error output becomes a great importance. In this paper, we propose N-version machine learning architecture that aims to improve system reliability against probabilistic outputs of individual machine learning modules. The key idea of this architecture is exploiting two kinds of diversities; input diversity and model diversity. Our study first formally defines these diversity metrics and analytically shows the improved reliability by N-version machine learning architecture. Since we treat a machine learning module as a black-box, the proposed architecture and the reliability property are generally applicable to any machine learning algorithms and applications. © 2019 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/DSN-W.2019.00017",
		"event-title": "Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop, DSN-W 2019",
		"page": "48-51",
		"title": "N-version machine learning models for safety critical systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072049689&doi=10.1109%2fDSN-W.2019.00017&partnerID=40&md5=72ee90ee38a50aa4360eabec023ddb53",
		"author": [
			{
				"family": "Machida",
				"given": "F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "canonicoFlashCrashesMultiAgent2019",
		"type": "paper-conference",
		"abstract": "As AI advances and becomes more complicated, it becomes necessary to study the safety implications of its behavior. This paper expands upon prior AI-safety research to create a model to study the harmful outcomes of multi-agent systems. In this paper, we outline previous work that has highlighted multiple aspects of AI-safety research and focus on AI-safety systems in multi-agent systems. After overviewing previous literature, we present a model focused on flash crashes, a concept often found in economics. The model was constructed using an interdisciplinary approach that includes game theory, machine learning, cognitive science and systems theory to study flash crashes in complex human-AI systems. We use the model to study a complex interaction between AI-agents, and our results indicate the multi-agent system in question is prone to cause flash crashes. © 2019 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/WSC40007.2019.9004675",
		"event-title": "Proceedings - Winter Simulation Conference",
		"page": "193-204",
		"title": "Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081129928&doi=10.1109%2fWSC40007.2019.9004675&partnerID=40&md5=21d0982f79295c9d8e0fd9260e9e4833",
		"volume": "2019-December",
		"author": [
			{
				"family": "Canonico",
				"given": "L.B."
			},
			{
				"family": "McNeese",
				"given": "N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yangAdaptiveCriticDesigns2019",
		"type": "article-journal",
		"abstract": "This paper develops a novel event-triggered robust control strategy for continuous-time nonlinear systems with unknown dynamics. To begin with, the event-triggered robust nonlinear control problem is transformed into an event-triggered nonlinear optimal control problem by introducing an infinite-horizon integral cost for the nominal system. Then, a recurrent neural network (RNN) and adaptive critic designs (ACDs) are employed to solve the derived event-triggered nonlinear optimal control problem. The RNN is applied to reconstruct the system dynamics based on collected system data. After acquiring the knowledge of system dynamics, a unique critic network is proposed to obtain the approximate solution of the event-triggered Hamilton-Jacobi-Bellman equation within the framework of ACDs. The critic network is updated by using simultaneously historical and instantaneous state data. An advantage of the present critic network update law is that it can relax the persistence of excitation condition. Meanwhile, under a newly developed event-triggering condition, the proposed critic network tuning rule not only guarantees the critic network weights to converge to optimums but also ensures nominal system states to be uniformly ultimately bounded. Moreover, by using Lyapunov method, it is proved that the derived optimal event-triggered control (ETC) guarantees uniform ultimate boundedness of all the signals in the original system. Finally, a nonlinear oscillator and an unstable power system are provided to validate the developed robust ETC scheme. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Cybernetics",
		"DOI": "10.1109/TCYB.2018.2823199",
		"issue": "6",
		"page": "2255-2267",
		"title": "Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems with Unknown Dynamics",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045765098&doi=10.1109%2fTCYB.2018.2823199&partnerID=40&md5=5c47089e8fef0b934762484ce73a6aa0",
		"volume": "49",
		"author": [
			{
				"family": "Yang",
				"given": "X."
			},
			{
				"family": "He",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "shahamUnderstandingAdversarialTraining2018",
		"type": "article-journal",
		"abstract": "We show that adversarial training of supervised learning models is in fact a robust optimization procedure. To do this, we establish a general framework for increasing local stability of supervised learning models using robust optimization. The framework is general and broadly applicable to differentiable non-parametric models, e.g., Artificial Neural Networks (ANNs). Using an alternating minimization-maximization procedure, the loss of the model is minimized with respect to perturbed examples that are generated at each parameter update, rather than with respect to the original training data. Our proposed framework generalizes adversarial training, as well as previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the networks also on the original test data. © 2018 Elsevier B.V.",
		"archive": "Scopus",
		"container-title": "Neurocomputing",
		"DOI": "10.1016/j.neucom.2018.04.027",
		"page": "195-204",
		"title": "Understanding adversarial training: Increasing local stability of supervised models through robust optimization",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047515976&doi=10.1016%2fj.neucom.2018.04.027&partnerID=40&md5=c8b27ed05118590fb453407e684ea2b0",
		"volume": "307",
		"author": [
			{
				"family": "Shaham",
				"given": "U."
			},
			{
				"family": "Yamada",
				"given": "Y."
			},
			{
				"family": "Negahban",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "prakashImprovingSafetyReinforcement2019",
		"type": "paper-conference",
		"abstract": "Recent progress in AI and Reinforcement learning has shown great success in solving complex problems with high dimensional state spaces. However, most of these successes have been primarily in simulated environments where failure is of little or no consequence. Most real-world applications, however, require training solutions that are safe to operate as catastrophic failures are inadmissible especially when there is human interaction involved. Currently, Safe RL systems use human oversight during training and exploration in order to make sure the RL agent does not go into a catastrophic state. These methods require a large amount of human labor and it is very difficult to scale up. We present a hybrid method for reducing the human intervention time by combining model-based approaches and training a supervised learner to improve sample efficiency while also ensuring safety. We evaluate these methods on various grid-world environments using both standard and visual representations and show that our approach achieves better performance in terms of sample efficiency, number of catastrophic states reached as well as overall task performance compared to traditional model-free approaches. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the 32nd International Florida Artificial Intelligence Research Society Conference, FLAIRS 2019",
		"page": "50-55",
		"title": "Improving safety in reinforcement learning using model-based architectures and human intervention",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094005967&partnerID=40&md5=77fcdf372b36d2d963ff0e048ff1e361",
		"author": [
			{
				"family": "Prakash",
				"given": "B."
			},
			{
				"family": "Khatwani",
				"given": "M."
			},
			{
				"family": "Waytowich",
				"given": "N."
			},
			{
				"family": "Mohsenin",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liDeepRobustReinforcement2019",
		"type": "article-journal",
		"abstract": "In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2019.2932789",
		"page": "108014-108021",
		"title": "Deep Robust Reinforcement Learning for Practical Algorithmic Trading",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071105095&doi=10.1109%2fACCESS.2019.2932789&partnerID=40&md5=a210d1234cf03eb745ff22d3367444d2",
		"volume": "7",
		"author": [
			{
				"family": "Li",
				"given": "Y."
			},
			{
				"family": "Zheng",
				"given": "W."
			},
			{
				"family": "Zheng",
				"given": "Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "vempatyRobustFusionUnreliable2019",
		"type": "chapter",
		"abstract": "The emergence of big and dirty data era demands new distributed learning and inference solutions to tackle the problem of inference with corrupted data. The central goal of this chapter is to discuss the presence of corrupted data in the context of distributed inference networks (DINs) and discuss coding-theoretic strategies to ensure reliable inference performance in several practical scenarios. It discusses a generalization of the classical Byzantine Generals problem in the context of distributed inference to different topologies. Over the last three decades, research community has extensively studied the impact of imperfect transmission channels or sensor faults on distributed inference systems. However, corrupted (Byzantine) data models, considered in this chapter, are philosophically different from the imperfect channels or faulty sensor cases. Byzantines are intentional and intelligent and therefore can optimize over the data corruption parameters. While learning their behavior and actively countering them is a viable approach, this chapter presents a new paradigm of mitigation strategies that use coding-theoretic results. The general approach of error-correcting output codes (ECOC) for data fusion is presented and its applicability for several inference problems in practice dealing with unreliable data including Byzantines is shown. This approach is then shown to be applicable to a wider range of inference problems such as classification using crowdsourced data. © The Institution of Engineering and Technology 2019.",
		"archive": "Scopus",
		"container-title": "Data Fusion in Wireless Sensor Networks",
		"note": "DOI: 10.1049/PBCE117E_ch12",
		"page": "291-311",
		"title": "Robust fusion of unreliable data sources using error-correcting output codes",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117998598&doi=10.1049%2fPBCE117E_ch12&partnerID=40&md5=c7334508b4b3a8a50b7d4bf13b57694b",
		"author": [
			{
				"family": "Vempaty",
				"given": "A."
			},
			{
				"family": "Kailkhura",
				"given": "B."
			},
			{
				"family": "Varshney",
				"given": "P.K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "vivekGrayboxAdversarialTraining2018",
		"type": "paper-conference",
		"abstract": "Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed “gray-box adversarial attacks” based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named “Gray-box Adversarial Training” that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained models. © Springer Nature Switzerland AG 2018.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-01267-0_13",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "213-228",
		"title": "Gray-box adversarial training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055438686&doi=10.1007%2f978-3-030-01267-0_13&partnerID=40&md5=e7a92b9cac84761bd58ae819624a68fd",
		"volume": "11219 LNCS",
		"author": [
			{
				"family": "Vivek",
				"given": "B.S."
			},
			{
				"family": "Mopuri",
				"given": "K.R."
			},
			{
				"family": "Babu",
				"given": "R.V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "elmhamdiWhenNeuronsFail2017",
		"type": "paper-conference",
		"abstract": "Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neuralactivation functions are Lipschitz-continuous. Our bound isgiven in the form of quantity, we call the Forward ErrorPropagation, computing this quantity only requires looking atthe topology of the network, while experimentally assessingthe robustness of a network requires the costly experiment oflooking at all the possible inputs and testing all the possibleconfigurations of the network corresponding to different failuresituations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neuronsper layer, the Lipschitz constant of the neural activationfunction, the number of failing neurons, the synaptic weightsand the depth of the layer where the failure occurred. In thecase of Byzantine failures, our bound involves, in addition, thesynaptic transmission capacity. Interestingly, as we show inthe paper, our bound can easily be extended to the case wheresynapses can fail. We present three applications of our results. The first is aquantification of the effect of memory cost reduction on theaccuracy of a neural network. The second is a quantification ofthe amount of information any neuron needs from its precedinglayer, enabling thereby a boosting scheme that prevents neuronsfrom waiting for unnecessary signals. Our third applicationis a quantification of the trade-off between neural networksrobustness and learning cost. © 2017 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IPDPS.2017.66",
		"event-title": "Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium, IPDPS 2017",
		"page": "1028-1037",
		"title": "When Neurons Fail",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027702685&doi=10.1109%2fIPDPS.2017.66&partnerID=40&md5=d12a7728772dcc4135077fd4178a0145",
		"author": [
			{
				"family": "El Mhamdi",
				"given": "E.M."
			},
			{
				"family": "Guerraoui",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "lendasseExtremeLearningMachine2013",
		"type": "paper-conference",
		"abstract": "In this paper is described the original (basic) Extreme Learning Machine (ELM). Properties like robustness and sensitivity to variable selection are studied. Several extensions of the original ELM are then presented and compared. Firstly, Tikhonov-Regularized Optimally-Pruned Extreme Learning Machine (TROP-ELM) is summarized as an improvement of the Optimally-Pruned Extreme Learning Machine (OP-ELM) in the form of a L 2 regularization penalty applied within the OP-ELM. Secondly, a Methodology to Linearly Ensemble ELM (-ELM) is presented in order to improve the performance of the original ELM. These methodologies (TROP-ELM and -ELM) are tested against state of the art methods such as Support Vector Machines or Gaussian Processes and the original ELM and OP-ELM, on ten different data sets. A specific experiment to test the sensitivity of these methodologies to variable selection is also presented. © 2013 Springer-Verlag Berlin Heidelberg.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-642-38679-4_2",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"note": "issue: PART 1",
		"page": "17-35",
		"title": "Extreme learning machine: A robust modeling technique? yes!",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880077016&doi=10.1007%2f978-3-642-38679-4_2&partnerID=40&md5=1f46c0c9280c127a485d686042f32665",
		"volume": "7902 LNCS",
		"author": [
			{
				"family": "Lendasse",
				"given": "A."
			},
			{
				"family": "Akusok",
				"given": "A."
			},
			{
				"family": "Simula",
				"given": "O."
			},
			{
				"family": "Corona",
				"given": "F."
			},
			{
				"family": "Van Heeswijk",
				"given": "M."
			},
			{
				"family": "Eirola",
				"given": "E."
			},
			{
				"family": "Miche",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "nicolaeAlgorithmicRobustnessSemisupervised2015",
		"type": "paper-conference",
		"abstract": "Using the appropriate metric is crucial for the performance of most of machine learning algorithms. For this reason, a lot of effort has been put into distance and similarity learning. However, it is worth noting that this research field lacks theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a learned metric. The theoretical framework of (ε, γ, τ)-good similarity functions [1] provides means to relate the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend this theory to a method where the metric and the separator are jointly learned in a semi-supervised way, setting that has not been explored before. We furthermore prove the robustness of our algorithm, which allows us to provide a generalization bound for this approach. The behavior of our method is illustrated via some experimental results. © Springer International Publishing Switzerland 2015.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-319-26532-2_28",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "253-263",
		"title": "Algorithmic robustness for semi-supervised (ε, γ, τ)-good metric learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952837463&doi=10.1007%2f978-3-319-26532-2_28&partnerID=40&md5=acb93b59e9c15b2a75bd6972252bfefd",
		"volume": "9489",
		"author": [
			{
				"family": "Nicolae",
				"given": "M.-I."
			},
			{
				"family": "Sebban",
				"given": "M."
			},
			{
				"family": "Habrard",
				"given": "A."
			},
			{
				"family": "Gaussier",
				"given": "E."
			},
			{
				"family": "Amini",
				"given": "M.-R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "leRobustUnsupervisedFeature2013",
		"type": "paper-conference",
		"abstract": "To boost up power of unsupervised feature learning and deep learning, there has been a great effort in optimizing network structure to learn more efficient high level features. It is crucial for a network to have a sufficient amount of learnable parameters yet still be able to capture in variances in data. In this paper, the authors propose spatial boosting networks, which employ convolutional feature learning networks as learning components. Each component in a network is assigned to a certain spatial region. This allows the network learn more adaptive features for each region. In order to make spatial boosting networks to capture relationship between regions of the visual field, we also propose convolutional pooling procedure. By expanding pooling scope into overlapping regions, we expect the features pooled in higher level to be more robust to noises and more invariant to transformation. Experiments show that using spatial boosting networks boosts up accuracy up to 3% from conventional approaches in standard datasets CIFAR and STL. Moreover, these results are competitive in comparison with other methods by using only a basic feature learning algorithm. © 2013 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICMLA.2013.168",
		"event-title": "Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013",
		"page": "507-512",
		"title": "A robust unsupervised feature learning framework using spatial boosting networks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899422191&doi=10.1109%2fICMLA.2013.168&partnerID=40&md5=d37cd0f93e3521438336b1269f6828d6",
		"volume": "2",
		"author": [
			{
				"family": "Le",
				"given": "N.D.-H."
			},
			{
				"family": "Tran",
				"given": "M.-T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "bouveyronRobustSupervisedClassification2009",
		"type": "article-journal",
		"abstract": "In the supervised classification framework, human supervision is required for labeling a set of learning data which are then used for building the classifier. However, in many applications, human supervision is either imprecise, difficult or expensive. In this paper, the problem of learning a supervised multi-class classifier from data with uncertain labels is considered and a model-based classification method is proposed to solve it. The idea of the proposed method is to confront an unsupervised modeling of the data with the supervised information carried by the labels of the learning data in order to detect inconsistencies. The method is able afterward to build a robust classifier taking into account the detected inconsistencies into the labels. Experiments on artificial and real data are provided to highlight the main features of the proposed method as well as an application to object recognition under weak supervision. © 2009 Elsevier Ltd. All rights reserved.",
		"archive": "Scopus",
		"container-title": "Pattern Recognition",
		"DOI": "10.1016/j.patcog.2009.03.027",
		"issue": "11",
		"page": "2649-2658",
		"title": "Robust supervised classification with mixture models: Learning from data with uncertain labels",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649389414&doi=10.1016%2fj.patcog.2009.03.027&partnerID=40&md5=32d729d76c33d6f8ee081360fb958e28",
		"volume": "42",
		"author": [
			{
				"family": "Bouveyron",
				"given": "C."
			},
			{
				"family": "Girard",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "yasudaImprovingRobustnessInstancebased2011",
		"type": "article-journal",
		"abstract": "Learning autonomous robots have been widely discussed in recent years. Reinforcement learning (RL) is a popular method in this domain. However, its performance is quite sensitive to the segmentation of state and action spaces. To overcome this problem, we developed the new technique Bayesian- discriminationfunction- based RL (BRL). BRL has proven to be more effective than other standard RL algorithms in dealing withmulti-robot system(MRS) problems. However, as in most learning systems, occasional overfitting problems occur in BRL. This paper introduces an extended BRL for improving the robustness of MRSs. Metalearning based on the information entropy of fired rules is adopted for adaptive modification of its learning parameters. Computer simulations are conducted to verify the effectiveness of our proposed method.",
		"archive": "Scopus",
		"container-title": "Journal of Advanced Computational Intelligence and Intelligent Informatics",
		"DOI": "10.20965/jaciii.2011.p1065",
		"issue": "8",
		"page": "1065-1072",
		"title": "Improving the robustness of instance-based reinforcement learning robots by metalearning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054952620&doi=10.20965%2fjaciii.2011.p1065&partnerID=40&md5=a01e1846ec1ab4558076a5d6405651e1",
		"volume": "15",
		"author": [
			{
				"family": "Yasuda",
				"given": "T."
			},
			{
				"family": "Araki",
				"given": "K."
			},
			{
				"family": "Ohkura",
				"given": "K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "ravichandranRobustAutomaticTarget2007",
		"type": "article-journal",
		"abstract": "This work developed and demonstrated a machine learning approach for robust ATR. The primary innovation of this work was the development of an automated way of developing inference rules that can draw on multiple models and multiple feature types to make robust ATR decisions. The key realization is that this \"meta learning\" problem is one of structural learning, and that it can be conducted independently of parameter learning associated with each model and feature based technique. This was accomplished by using a learning classifier system, which is based on genetics-based machine learning, for the ill conditioned combinatorial problem of structural rule learning, while using statistical and mathematical techniques for parameter learning. This system was tested on MSTAR Public Release SAR data using standard and extended operation conditions. These results were also compared against two baseline classifiers, a PCA based distance classifier and a MSE classifier. The classifiers were evaluated for accuracy (via training set classification) and robustness (via testing set classification). In both cases, the LCS based robust ATR system performed well with accuracy over 99% and robustness over 80%. © 2006 Elsevier B.V. All rights reserved.",
		"archive": "Scopus",
		"container-title": "Information Fusion",
		"DOI": "10.1016/j.inffus.2006.03.001",
		"issue": "3",
		"page": "252-265",
		"title": "Robust automatic target recognition using learning classifier systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947672844&doi=10.1016%2fj.inffus.2006.03.001&partnerID=40&md5=518bef4bc6bad38b249e8a52e4abf601",
		"volume": "8",
		"author": [
			{
				"family": "Ravichandran",
				"given": "B."
			},
			{
				"family": "Gandhe",
				"given": "A."
			},
			{
				"family": "Smith",
				"given": "R."
			},
			{
				"family": "Mehra",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "liSupervisedContrastiveLearning2023",
		"type": "article-journal",
		"abstract": "The lack of robustness is a serious problem for deep neural networks (DNNs) and makes DNNs vulnerable to adversarial examples. A promising solution is applying adversarial training to alleviate this problem, which allows the model to learn the features from adversarial examples. However, adversarial training usually produces overfitted models and may not work when facing a new attack. We believe this is because the previous adversarial training using cross-entropy loss ignores the similarity between the adversarial examples and the original examples, which will result in a low margin. Accordingly, we propose a supervised adversarial contrastive learning (SACL) approach for adversarial training. SACL uses supervised adversarial contrastive loss which contains both the cross-entropy term and adversarial contrastive term. The cross-entropy term is used for guiding DNN inductive bias learning, and the adversarial contrastive term can help models learn example representations by maximizing feature consistency under different original examples, which fits well with the goal of solving low margins. In addition, SACL only uses adversarial examples which can successfully fool the model and their corresponding original examples for training. This process is more advantageous to provide the model with more accurate information about the decision boundary and obtain a model that fits the example distribution. Experiments show that SACL can reduce the attack success rate of multiple adversarial attack algorithms against different models on text classification tasks. The defensive performance is significantly better than other adversarial training approaches without reducing the generalization ability of the model. In addition, the DNN model trained by our approach has high transferability and robustness. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Neural Computing and Applications",
		"DOI": "10.1007/s00521-022-07871-5",
		"issue": "10",
		"page": "7357-7368",
		"title": "Supervised contrastive learning for robust text adversarial training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144688930&doi=10.1007%2fs00521-022-07871-5&partnerID=40&md5=1494523c566a87356fb3d171b70d52cf",
		"volume": "35",
		"author": [
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Zhao",
				"given": "B."
			},
			{
				"family": "An",
				"given": "Y."
			},
			{
				"family": "Shangguan",
				"given": "C."
			},
			{
				"family": "Ji",
				"given": "M."
			},
			{
				"family": "Yuan",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ferryImprovingFairnessGeneralization2023",
		"type": "article-journal",
		"abstract": "Unwanted bias is a major concern in machine learning, raising in particular significant ethical issues when machine learning models are deployed within high-stakes decision systems. A common solution to mitigate it is to integrate and optimize a statistical fairness metric along with accuracy during the training phase. However, one of the main remaining challenges is that current approaches usually generalize poorly in terms of fairness on unseen data. We address this issue by proposing a new robustness framework for statistical fairness in machine learning. The proposed approach is inspired by the domain of distributionally robust optimization and works in ensuring fairness over a variety of samplings of the training set. Our approach can be used to quantify the robustness of fairness but also to improve it when training a model. We empirically evaluate the proposed method and show that it effectively improves fairness generalization. In addition, we propose a simple yet powerful heuristic application of our framework that can be integrated into a wide range of existing fair classification techniques to enhance fairness generalization. Our extensive empirical study using two existing fair classification methods demonstrates the efficiency and scalability of the proposed heuristic approach. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Machine Learning",
		"DOI": "10.1007/s10994-022-06191-y",
		"issue": "6",
		"page": "2131-2192",
		"title": "Improving fairness generalization through a sample-robust optimization method",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133582221&doi=10.1007%2fs10994-022-06191-y&partnerID=40&md5=07ba1bd2f24d1fcdbb25acd34bbbbe73",
		"volume": "112",
		"author": [
			{
				"family": "Ferry",
				"given": "J."
			},
			{
				"family": "Aïvodji",
				"given": "U."
			},
			{
				"family": "Gambs",
				"given": "S."
			},
			{
				"family": "Huguet",
				"given": "M.-J."
			},
			{
				"family": "Siala",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "duRobustMultiagentReinforcement2024",
		"type": "article-journal",
		"abstract": "Reinforcement learning in multi-agent scenarios is essential for real-world applications as it can vividly depict agents’ collaborative and competitive behaviors from a perspective closer to reality. However, most existing studies suffer from poor robustness, preventing multi-agent reinforcement learning from practical applications where robustness is the core indicator of system security and stability. In view of this, we propose a novel Bayesian Multi-Agent Reinforcement Learning method, named BMARL, which leverages the distributional value function calculated by Bayesian inference to improve the robustness of the model. Specifically, Bayesian linear regression is adopted to estimate a posterior distribution concerning value function parameters, rather than approximating an expectation value for Q-value by point estimation. In this way, the value function is more generalized than previously obtained by point estimation, which is beneficial to the robustness of our model. Meanwhile, we utilize the Gaussian prior knowledge to integrate more prior knowledge while estimating the value function, which improves learning efficiency. Extensive experimental results on three benchmark multi-agent environments comparing with seven state-of-the-art methods demonstrate the superiority of BMARL in terms of both robustness and efficiency. © 2023 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Pattern Recognition",
		"DOI": "10.1016/j.patcog.2023.109917",
		"title": "Robust multi-agent reinforcement learning via Bayesian distributional value estimation",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169977108&doi=10.1016%2fj.patcog.2023.109917&partnerID=40&md5=e595d43fa1635c5e71562359b2cf6266",
		"volume": "145",
		"author": [
			{
				"family": "Du",
				"given": "X."
			},
			{
				"family": "Chen",
				"given": "H."
			},
			{
				"family": "Wang",
				"given": "C."
			},
			{
				"family": "Xing",
				"given": "Y."
			},
			{
				"family": "Yang",
				"given": "J."
			},
			{
				"family": "Yu",
				"given": "P.S."
			},
			{
				"family": "Chang",
				"given": "Y."
			},
			{
				"family": "He",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dangImprovingMachineLearning2023",
		"type": "paper-conference",
		"abstract": "As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-IID data, respectively, where CIFAR-10 is used in this research. In the IID data case, our experimental results demonstrate that we can achieve such a robust accuracy that it is comparable to the one obtained in the centralized environment. Moreover, in the non-IID data case, the natural accuracy drops from 66.23% to 57.82%, and the robust accuracy decreases by 25% and 23.4% in C&W and Projected Gradient Descent (PGD) attacks, compared to the IID data case, respectively. We further propose an IID data-sharing approach, which allows for increasing the natural accuracy to 85.04% and the robust accuracy from 57% to 72% in C&W attacks and from 59% to 67% in PGD attacks. © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICCCN58024.2023.10230138",
		"event-title": "Proceedings - International Conference on Computer Communications and Networks, ICCCN",
		"title": "Improving Machine Learning Robustness via Adversarial Training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173587747&doi=10.1109%2fICCCN58024.2023.10230138&partnerID=40&md5=39307fd66c57626e402a9a0ca5cad66b",
		"volume": "2023-July",
		"author": [
			{
				"family": "Dang",
				"given": "L."
			},
			{
				"family": "Hapuarachchi",
				"given": "T."
			},
			{
				"family": "Xiong",
				"given": "K."
			},
			{
				"family": "Lin",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liuRobustSafeReinforcement2023",
		"type": "paper-conference",
		"abstract": "Previous work demonstrates that the optimal safe reinforcement learning policy in a noise-free environment is vulnerable and could be unsafe under observational attacks. While adversarial training effectively improves robustness and safety, collecting samples by attacking the behavior agent online could be expensive or prohibitively dangerous in many applications. We propose the robuSt vAriational ofF-policy lEaRning (SAFER) approach, which only requires benign training data without attacking the agent. SAFER obtains an optimal non-parametric variational policy distribution via convex optimization and then uses it to improve the parameterized policy robustly via supervised learning. The two-stage policy optimization facilitates robust training, and extensive experiments on multiple robot platforms show the efficiency of SAFER in learning a robust and safe policy: achieving the same reward with much fewer constraint violations during training than on-policy baselines. © 2023 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "22249-22265",
		"title": "Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408788&partnerID=40&md5=6463a867d47c3f19ff2fbca9a27362f9",
		"volume": "202",
		"author": [
			{
				"family": "Liu",
				"given": "Z."
			},
			{
				"family": "Guo",
				"given": "Z."
			},
			{
				"family": "Cen",
				"given": "Z."
			},
			{
				"family": "Zhang",
				"given": "H."
			},
			{
				"family": "Yao",
				"given": "Y."
			},
			{
				"family": "Hu",
				"given": "H."
			},
			{
				"family": "Zhao",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jiaBidirectionalAdaptationRobust2023",
		"type": "paper-conference",
		"abstract": "Semi-supervised learning (SSL) suffers from severe performance degradation when labeled and unlabeled data come from inconsistent data distributions. However, there is still a lack of sufficient theoretical guidance on how to alleviate this problem. In this paper, we propose a general theoretical framework that demonstrates how distribution discrepancies caused by pseudo-label predictions and target predictions can lead to severe generalization errors. Through theoretical analysis, we identify three main reasons why previous SSL algorithms cannot perform well with inconsistent distributions: coupling between the pseudo-label predictor and the target predictor, biased pseudo labels, and restricted sample weights. To address these challenges, we introduce a practical framework called Bidirectional Adaptation that can adapt to the distribution of unlabeled data for debiased pseudo-label prediction and to the target distribution for debiased target prediction, thereby mitigating these shortcomings. Extensive experimental results demonstrate the effectiveness of our proposed framework. © 2023 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "14886-14901",
		"title": "Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174426186&partnerID=40&md5=83bb3938b25b176e9237be2cf5d0084a",
		"volume": "202",
		"author": [
			{
				"family": "Jia",
				"given": "L.-H."
			},
			{
				"family": "Guo",
				"given": "L.-Z."
			},
			{
				"family": "Zhou",
				"given": "Z."
			},
			{
				"family": "Shao",
				"given": "J.-J."
			},
			{
				"family": "Xiang",
				"given": "Y.-K."
			},
			{
				"family": "Li",
				"given": "Y.-F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "altmannCROPDistributionalShiftRobust2023",
		"type": "paper-conference",
		"abstract": "The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.",
		"archive": "Scopus",
		"event-title": "IJCAI International Joint Conference on Artificial Intelligence",
		"page": "3414-3422",
		"title": "CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170359176&partnerID=40&md5=324523ba832f4699dfb4c359f306a724",
		"volume": "2023-August",
		"author": [
			{
				"family": "Altmann",
				"given": "P."
			},
			{
				"family": "Ritz",
				"given": "F."
			},
			{
				"family": "Feuchtinger",
				"given": "L."
			},
			{
				"family": "Nüßlein",
				"given": "J."
			},
			{
				"family": "Linnhoff-Popien",
				"given": "C."
			},
			{
				"family": "Phan",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "strawnConformalPredictiveSafety2023",
		"type": "article-journal",
		"abstract": "The interest in using reinforcement learning (RL) controllers in safety-critical applications such as robot navigation around pedestrians motivates the development of additional safety mechanisms. Running RL-enabled systems among uncertain dynamic agents may result in high counts of collisions and failures to reach the goal. The system could be safer if the pre-trained RL policy was uncertainty-informed. For that reason, we propose <italic>conformal predictive safety filters</italic> that: 1) predict the other agents&#x0027; trajectories, 2) use statistical techniques to provide uncertainty intervals around these predictions, and 3) learn an additional safety filter that closely follows the RL controller but avoids the uncertainty intervals. We use conformal prediction to learn uncertainty-informed predictive safety filters, which make no assumptions about the agents&#x0027; distribution. The framework is modular and outperforms the existing controllers in simulation. We demonstrate our approach with multiple experiments in a collision avoidance gym environment and show that our approach minimizes the number of collisions without making overly conservative predictions. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2023.3322644",
		"page": "1-8",
		"title": "Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174844462&doi=10.1109%2fLRA.2023.3322644&partnerID=40&md5=07707bbdae1e1e714275089a0239b90c",
		"author": [
			{
				"family": "Strawn",
				"given": "K.J."
			},
			{
				"family": "Ayanian",
				"given": "N."
			},
			{
				"family": "Lindemann",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenByzantineRobustOnlineOffline2023",
		"type": "paper-conference",
		"abstract": "We consider a distributed reinforcement learning setting where multiple agents separately explore the environment and communicate their experiences through a central server. However, αfraction of agents are adversarial and can report arbitrary fake information. Critically, these adversarial agents can collude and their fake data can be of any sizes. We desire to robustly identify a near-optimal policy for the underlying Markov decision process in the presence of these adversarial agents. Our main technical contribution is COW, a novel algorithm for the robust mean estimation from batches problem, that can handle arbitrary batch sizes. Building upon this new estimator, in the offline setting, we design a Byzantine-robust distributed pessimistic value iteration algorithm; in the online setting, we design a Byzantine-robust distributed optimistic value iteration algorithm. Both algorithms obtain near-optimal sample complexities and achieve superior robustness guarantee than prior works. Copyright © 2023 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "3230-3269",
		"title": "Byzantine-Robust Online and Offline Distributed Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381268&partnerID=40&md5=f0f263cd3d72d542c157a405a6e611d3",
		"volume": "206",
		"author": [
			{
				"family": "Chen",
				"given": "Y."
			},
			{
				"family": "Zhang",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "K."
			},
			{
				"family": "Wang",
				"given": "M."
			},
			{
				"family": "Zhu",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wenJSMixHolisticAlgorithm2023",
		"type": "article-journal",
		"abstract": "The success of deep learning is mainly dependent on large-scale and accurately labeled datasets. However, real-world datasets are marked with much noise. Directly training on datasets with label noise may lead to the overfitting. Recent research is under the spotlight on how to design algorithms that can learn robust models from noisy datasets, via designing the loss function and integrating the idea of Semi-supervised learning (SSL). This paper proposes a robust algorithm for learning with label noise that does not require additional clean data and an auxiliary model. Specifically, on the one hand, Jensen–Shannon (JS) divergence is introduced as a component of the loss function, which measures the distance between the predicted distribution and the noisy label distribution. It can alleviate the overfitting problem caused by the traditional cross entropy loss theoretically and experimentally. On the other hand, a dynamic sample selection mechanism is also proposed. The dataset is divided into the pseudo-clean labeled subset and the pseudo-noisy labeled subset. Two subsets are treated differently to exploit prior information about the data, and then learned by SSL. The dynamic sample selection is performed with the iteration between two subsets and the model parameters, which are different from the conventional training. Considering the label of the pseudo-clean labeled subset is not correct entirely, they are also refined by linear interpolation. Furthermore, we experimentally show that the integration of SSL helps the model divide two subsets more precise and build decision boundaries more explicit. Extensive experimental results on corrupted data from benchmark datasets and the real-world dataset, including CIFAR-10, CIFAR-100, and Clothing1M, demonstrate that our method is superior to many state-of-the-art approaches for learning with label noise. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Neural Computing and Applications",
		"DOI": "10.1007/s00521-022-07770-9",
		"issue": "2",
		"page": "1519-1533",
		"title": "JSMix: a holistic algorithm for learning with label noise",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139177299&doi=10.1007%2fs00521-022-07770-9&partnerID=40&md5=733ca6f7e18f5fbabc4dac91622d3cf8",
		"volume": "35",
		"author": [
			{
				"family": "Wen",
				"given": "Z."
			},
			{
				"family": "Xu",
				"given": "H."
			},
			{
				"family": "Ying",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenRobustFrameworkFixing2023",
		"type": "paper-conference",
		"abstract": "Nowadays, as a prevailing paradigm for large-scale machine learning, distributed learning has been faced with two challenges, communication bottleneck and limited robustness. For the communication challenge, compression is widely-used as a solution. For the robustness challenge, some robust defence methods have been proposed. However, previous works that simultaneously consider these two challenges are limited. Through an experiment on the w8a dataset, we found that compressed distributed learning with rand-K is vulnerable to poisoning attacks. Therefore, in this paper, we propose a robust compressed distributed framework for distributed learning settings. Experimental evaluations on a9a and w8a datasets have shown the effectiveness of our proposed framework, which markedly decreases the average optimality gap from 1.47E -2 and 2.15E -2 to 3.98E -4 and 4.33E -4 respectively.  © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/CSCWD57460.2023.10152781",
		"event-title": "Proceedings of the 2023 26th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2023",
		"page": "733-738",
		"title": "A Robust Framework for Fixing The Vulnerability of Compressed Distributed Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164686727&doi=10.1109%2fCSCWD57460.2023.10152781&partnerID=40&md5=3b3e21af6d20d9c82120e878aeb75a21",
		"author": [
			{
				"family": "Chen",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "B."
			},
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Kuang",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "xuGroupDistributionallyRobust2023",
		"type": "paper-conference",
		"abstract": "One key challenge for multi-task Reinforcement learning (RL) in practice is the absence of task specifications. Robust RL has been applied to deal with task ambiguity but may result in over-conservative policies. To balance the worst-case (robustness) and average performance, we propose Group Distributionally Robust Markov Decision Process (GDR-MDP), a flexible hierarchical MDP formulation that encodes task groups via a latent mixture model. GDR-MDP identifies the optimal policy that maximizes the expected return under the worst-possible qualified belief over task groups within an ambiguity set. We rigorously show that GDR-MDP's hierarchical structure improves distributional robustness by adding regularization to the worst possible outcomes. We then develop deep RL algorithms for GDR-MDP for both value-based and policy-based RL methods. Extensive experiments on Box2D control tasks, MuJoCo benchmarks, and Google football platforms show that our algorithms outperform classic robust training algorithms across diverse environments in terms of robustness under belief uncertainties. Demos are available on our project page (https://sites.google.com/view/gdr-rl/home). Copyright © 2023 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "2677-2703",
		"title": "Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165176997&partnerID=40&md5=78b2ceb3e076387d4b6e631d9b475265",
		"volume": "206",
		"author": [
			{
				"family": "Xu",
				"given": "M."
			},
			{
				"family": "Huang",
				"given": "P."
			},
			{
				"family": "Niu",
				"given": "Y."
			},
			{
				"family": "Kumar",
				"given": "V."
			},
			{
				"family": "Qiu",
				"given": "J."
			},
			{
				"family": "Fang",
				"given": "C."
			},
			{
				"family": "Lee",
				"given": "K.-H."
			},
			{
				"family": "Qi",
				"given": "X."
			},
			{
				"family": "Lam",
				"given": "H."
			},
			{
				"family": "Li",
				"given": "B."
			},
			{
				"family": "Zhao",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "eilersSafetyAssuranceEnsemblebased2023",
		"type": "paper-conference",
		"abstract": "A number of challenges are associated with the use of machine learning technologies in safety-related applications. These include the difficulty of specifying adequately safe behaviour in complex environments (specification uncertainty), ensuring a predictably safe behaviour under all operating conditions (technical uncertainty) and arguing that the safety goals of the system have been met with sufficient confidence (assurance uncertainty). An assurance argument is therefore required that demonstrates that the effects of these uncertainties do not lead to an unacceptable level of risk during operation. A reinforcement learning model will predict an action in whatever state it is in - even in previously unseen states for which a valid (safe) outcome cannot be determined due to lack of training. Uncertainty estimation is a well understood approach in machine learning to identify states with a high probability of an invalid action due a lack of training experience, thus addressing technical uncertainty. However, the impact of alternative possible predictions which may be equally valid (and represent a safe state) in estimating uncertainty in reinforcement learning is not so clear and to our knowledge, not so well documented in current literature. In this paper we build on work where we investigated uncertainty estimation on simplified scenarios in a gridworld environment. Using model ensemble-based uncertainty estimation we proposed an algorithm based on action count variance to deal with discrete action spaces whilst considering in-distribution action variance calculation to handle the overlap with alternative predictions. The method indicates potentially unsafe states when the agent is near out-of-distribution elements and can distinguish it from overlapping alternative, but equally valid predictions. Here, we present these results within the context of a safety assurance framework and highlight the activities and evidences required to build a convincing safety argument. We show that our previous approach is able to act as an external observer and can fulfil the requirements of an assurance argumentation for systems based on machine learning with ontological uncertainty. © 2023 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "Safety Assurance with Ensemble-based Uncertainty Estimation and overlapping alternative Predictions in Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159259475&partnerID=40&md5=2c11248b544075497117f0dc81f91729",
		"volume": "3381",
		"author": [
			{
				"family": "Eilers",
				"given": "D."
			},
			{
				"family": "Burton",
				"given": "S."
			},
			{
				"family": "Roza",
				"given": "F.S."
			},
			{
				"family": "Roscher",
				"given": "K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangLearningRobustInvariant2022",
		"type": "paper-conference",
		"abstract": "Data augmentation has been proven to be an effective technique for developing machine learning models that are robust to known classes of distributional shifts (e.g., rotations of images), and alignment regularization is a technique often used together with data augmentation to further help the model learn representations invariant to the shifts used to augment the data. In this paper, motivated by a proliferation of options of alignment regularizations, we seek to evaluate the performances of several popular design choices along the dimensions of robustness and invariance, for which we introduce a new test procedure. Our synthetic experiment results speak to the benefits of squared \"2 norm regularization. Further, we also formally analyze the behavior of alignment regularization to complement our empirical study under assumptions we consider realistic. Finally, we test this simple technique we identify (worst-case data augmentation with squared \"2 norm alignment regularization) and show that the benefits of this method outrun those of the specially designed methods. We also release a software package in both TensorFlow and PyTorch for users to use the method with a couple of lines at https://github.com/jyanln/AlignReg.  © 2022 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3534678.3539438",
		"event-title": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
		"page": "1846-1856",
		"title": "Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137145889&doi=10.1145%2f3534678.3539438&partnerID=40&md5=ee29243ec5958682d44789213dc6ab47",
		"author": [
			{
				"family": "Wang",
				"given": "H."
			},
			{
				"family": "Huang",
				"given": "Z."
			},
			{
				"family": "Wu",
				"given": "X."
			},
			{
				"family": "Xing",
				"given": "E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangDataDrivenRobustMultiAgent2022",
		"type": "paper-conference",
		"abstract": "Multi-agent reinforcement learning (MARL) in the collaborative setting aims to find a joint policy that maximizes the accumulated reward averaged over all the agents. In this paper, we focus on MARL under model uncertainty, where the transition kernel is assumed to be in an uncertainty set, and the goal is to optimize the worst-case performance over the uncertainty set. We investigate the model-free setting, where the uncertain set centers around an unknown Markov decision process from which a single sample trajectory can be obtained sequentially. We develop a robust multi-agent Q-learning algorithm, which is model-free and fully decentralized. We theoretically prove that the proposed algorithm converges to the minimax robust policy, and further characterize its sample complexity. Our algorithm, comparing to the vanilla multi-agent Q-learning, offers provable robustness under model uncertainty without incurring additional computational and memory cost.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/MLSP55214.2022.9943500",
		"event-title": "IEEE International Workshop on Machine Learning for Signal Processing, MLSP",
		"title": "Data-Driven Robust Multi-Agent Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142763167&doi=10.1109%2fMLSP55214.2022.9943500&partnerID=40&md5=d95526b627560183e68f0e84ebf1a926",
		"volume": "2022-August",
		"author": [
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Zhou",
				"given": "Y."
			},
			{
				"family": "Velasquez",
				"given": "A."
			},
			{
				"family": "Zou",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yangTrainingTransferringSafe2022",
		"type": "paper-conference",
		"abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, RL agents are trained in a controlled environment, such as a laboratory, before being deployed in the real world. However, the target reward might be unknown prior to deployment. Reward-free RL addresses this problem by training an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe sampling policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence from the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.",
		"archive": "Scopus",
		"event-title": "ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022",
		"title": "Training and Transferring Safe Policies in Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168254751&partnerID=40&md5=69ee98d03db848dbd74b02d5ad5311a6",
		"author": [
			{
				"family": "Yang",
				"given": "Q."
			},
			{
				"family": "Simão",
				"given": "T.D."
			},
			{
				"family": "Jansen",
				"given": "N."
			},
			{
				"family": "Tindemans",
				"given": "S.H."
			},
			{
				"family": "Spaan",
				"given": "M.T.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "choAdversarialTrainingChannel2022",
		"type": "paper-conference",
		"abstract": "Adversarial attack shows that deep neural networks (DNNs) are highly vulnerable to small perturbation. Currently, one of the most effective ways to defend against adversarial attacks is adversarial training, which generates adversarial examples during training and induces the models to classify them correctly. To further increase robustness, various techniques such as exploiting additional unlabeled data and novel training loss have been proposed. In this paper, we propose a novel regularization method that exploits latent features, which can be easily combined with existing approaches. We discover that particular channels are more sensitive to adversarial perturbation, motivating us to propose regularizing these channels. Specifically, we attach a channel attention module for adjusting sensitivity of each channel by reducing the difference between the latent feature of the natural image and that of the adversarial image, which we call Channel Attention Regularization (CAR). CAR can be combined with the existing adversarial training framework, showing that it improves the robustness of state-of-the-art defense models. Experiments on various existing adversarial training methods against diverse attacks show the effectiveness of our methods. Codes are available at https://github.com/sgmath12/Adversarial-Training-CAR. © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICIP46576.2022.9897754",
		"event-title": "Proceedings - International Conference on Image Processing, ICIP",
		"page": "2996-3000",
		"title": "Adversarial Training With Channel Attention Regularization",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146701970&doi=10.1109%2fICIP46576.2022.9897754&partnerID=40&md5=5bfdee444f7399d1ed29bce966f3d6f0",
		"author": [
			{
				"family": "Cho",
				"given": "S."
			},
			{
				"family": "Byun",
				"given": "J."
			},
			{
				"family": "Kwon",
				"given": "M.-J."
			},
			{
				"family": "Kim",
				"given": "Y."
			},
			{
				"family": "Kim",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liangEfficientAdversarialTraining2022",
		"type": "paper-conference",
		"abstract": "Recent studies reveal that a well-trained deep reinforcement learning (RL) policy can be particularly vulnerable to adversarial perturbations on input observations. Therefore, it is crucial to train RL agents that are robust against any attacks with a bounded budget. Existing robust training methods in deep RL either treat correlated steps separately, ignoring the robustness of long-term rewards, or train the agents and RL-based attacker together, doubling the computational burden and sample complexity of the training process. In this work, we propose a strong and efficient robust training framework for RL, named Worst-case-aware Robust RL (WocaR-RL), that directly estimates and optimizes the worst-case reward of a policy under bounded ℓp attacks without requiring extra samples for learning an attacker. Experiments on multiple environments show that WocaR-RL achieves state-of-the-art performance under various strong attacks, and obtains significantly higher training efficiency than prior state-of-the-art robust training methods. The code of this work is available at https://github.com/umd-huang-lab/WocaR-RL. © 2022 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150009921&partnerID=40&md5=f780a63268f2da04aa1b18bf9fc5d208",
		"volume": "35",
		"author": [
			{
				"family": "Liang",
				"given": "Y."
			},
			{
				"family": "Sun",
				"given": "Y."
			},
			{
				"family": "Zheng",
				"given": "R."
			},
			{
				"family": "Huang",
				"given": "F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "motokawaDistributedMultiAgentDeep2022",
		"type": "paper-conference",
		"abstract": "In multi-agent systems, noise reduction techniques are considerable for improving the overall system reliability as agents are required to rely on limited environmental information to develop cooperative and coordinated behaviors with the surrounding agents. However, previous studies have often applied centralized noise reduction methods to build robust and versatile coordination in noisy multi-agent environments, while distributed and decentralized autonomous agents are more plausible for real-world application. In this paper, we introduce a distributed attentional actor architecture model for a multi-agent system (DA3-X), using which we demonstrate that agents with DA3-X can selectively learn the noisy environment and behave cooperatively. We experimentally evaluate the effectiveness of DA3-X by comparing learning methods with and without DA3-X and show that agents with DA3-X can achieve better performance than baseline agents. Furthermore, we visualize heatmaps of attentional weights from the DA3-X to analyze how the decision-making process and coordinated behavior are influenced by noise. © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IJCNN55064.2022.9892253",
		"event-title": "Proceedings of the International Joint Conference on Neural Networks",
		"title": "Distributed Multi-Agent Deep Reinforcement Learning for Robust Coordination against Noise",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140784231&doi=10.1109%2fIJCNN55064.2022.9892253&partnerID=40&md5=bbbe0e8c9ed49dd82a27932e6dcb977a",
		"volume": "2022-July",
		"author": [
			{
				"family": "Motokawa",
				"given": "Y."
			},
			{
				"family": "Sugawara",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yuRobustUnsupervisedDomain2022",
		"type": "paper-conference",
		"abstract": "Unsupervised Domain Adaptation (UDA) provides a promising solution for learning without supervision, which transfers knowledge from relevant source domains with accessible labeled training data. Existing UDA solutions hinge on clean training data with a short-tail distribution from the source domain, which can be fragile when the source domain data is corrupted either inherently or via adversarial attacks. In this work, we propose an effective framework to address the challenges of UDA from corrupted source domains in a principled manner. Specifically, we perform knowledge ensemble from multiple domain-invariant models that are learned on random partitions of training data. To further address the distribution shift from the source to the target domain, we refine each of the learned models via mutual information maximization, which adaptively obtains the predictive information of the target domain with high confidence. Extensive empirical studies demonstrate that the proposed approach is robust against various types of poisoned data attacks while achieving high asymptotic performance on the target domain.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICDM54844.2022.00171",
		"event-title": "Proceedings - IEEE International Conference on Data Mining, ICDM",
		"page": "1299-1304",
		"title": "Robust Unsupervised Domain Adaptation from A Corrupted Source",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147729954&doi=10.1109%2fICDM54844.2022.00171&partnerID=40&md5=556ad39d51119f2913db0a36ef016d76",
		"volume": "2022-November",
		"author": [
			{
				"family": "Yu",
				"given": "S."
			},
			{
				"family": "Zhu",
				"given": "Z."
			},
			{
				"family": "Liu",
				"given": "B."
			},
			{
				"family": "Jain",
				"given": "A.K."
			},
			{
				"family": "Zhou",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schottImprovingRobustnessDeep2022",
		"type": "paper-conference",
		"abstract": "To improve robustness of deep reinforcement learning agents, a line of recent works focus on producing disturbances of the dynamics of the environment. Existing approaches of the literature to generate such disturbances are environment adversarial reinforcement learning methods. These methods set the problem as a two-player game between the protagonist agent, which learns to perform a task in an environment, and the adversary agent, which learns to disturb the dynamics of the considered environment to make the protagonist agent fail. Alternatively, we propose to build on gradient-based adversarial attacks, usually used for classification tasks for instance, that we apply on the critic network of the protagonist to identify efficient disturbances of the dynamics of the environment. Rather than training an adversary agent, which usually reveals as very complex and unstable, we leverage the knowledge of the critic network of the protagonist, to dynamically increase the complexity of the task at each step of the learning process. We show that our method, while being faster and lighter, leads to significantly better improvements in robustness of the policy than existing methods of the literature. © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IJCNN55064.2022.9892901",
		"event-title": "Proceedings of the International Joint Conference on Neural Networks",
		"title": "Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140781892&doi=10.1109%2fIJCNN55064.2022.9892901&partnerID=40&md5=3c7ba77dfe5886c14307e437ac8d50a3",
		"volume": "2022-July",
		"author": [
			{
				"family": "Schott",
				"given": "L."
			},
			{
				"family": "Hajri",
				"given": "H."
			},
			{
				"family": "Lamprier",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chenSemisupervisedDeepLearning2022",
		"type": "paper-conference",
		"abstract": "With increasing requirements on reliability, maintainability and safety in modern ICT systems, fault detection, as an indispensable part of AIOps, has become essential in cloud computing or communication network environments. However, due to the lack of effective labels and class imbalance on faulty samples, fault detection performance based on the common classification model can't meet the system's operational requirements. Some recent approaches of SSL propose a consistency regularization loss to solve the problem of insufficient labels. However, these approaches are mainly for images based on artificial data augmentations but not feasible for all data types, and class-imbalance problem is not considered simultaneously. So, we propose a semi-supervised method for imbalanced fault detection with few labels, called SSLCR-IFD. In the method, we use a semi-supervised deep classifier based on consistency loss to solve the lack of labels, in which two sample augmentation methods based on clustering and GAN are used. Furthermore, a selective pseudo-labeling self-training strategy is proposed to solve the class-imbalance problem. Compared with the standard data augmentation, our methods alleviates the need for domain knowledge and can be used on multiple types of tasks. Finally, experiment results show that our method outperforms the baseline methods on two different AIOps tasks.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICRMS55680.2022.9944609",
		"event-title": "13th International Conference on Reliability, Maintainability, and Safety: Reliability and Safety of Intelligent Systems, ICRMS 2022",
		"page": "290-295",
		"title": "A Semi-supervised Deep Learning Model with Consistency Regularization of Augmented Samples for Imbalanced Fault Detection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143065667&doi=10.1109%2fICRMS55680.2022.9944609&partnerID=40&md5=ca3844b36ae721b2f47955e2b0fcbc65",
		"author": [
			{
				"family": "Chen",
				"given": "H."
			},
			{
				"family": "Han",
				"given": "J."
			},
			{
				"family": "Lv",
				"given": "X."
			},
			{
				"family": "Wu",
				"given": "Z."
			},
			{
				"family": "Guo",
				"given": "H."
			},
			{
				"family": "Zhan",
				"given": "Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gholamiMultiexpertAdversarialRegularization2022",
		"type": "article-journal",
		"abstract": "Deep neural networks (DNNs) can achieve high accuracy when there is abundant training data that has the same distribution as the test data. In practical applications, data deficiency is often a concern. For classification tasks, the lack of enough labeled images in the training set often results in overfitting. Another issue is the mismatch between the training and the test domains, which results in poor model performance. This calls for the need to have robust and data efficient deep learning models. In this work, we propose a deep learning approach called Multi-Expert Adversarial Regularization learning (MEAR) with limited computational overhead to improve the generalization and robustness of deep supervised learning models. The MEAR framework appends multiple classifier heads (experts) to the feature extractor of the legacy model. MEAR aims to learn the feature extractor in an adversarial fashion by leveraging complementary information from the individual experts as well as the ensemble of the experts to be more robust for an unseen test domain. We train state-of-the-art networks with MEAR for two important computer vision tasks, image classification and semantic segmentation. We compare MEAR to a variety of baselines on multiple benchmarks. We show that MEAR is competitive with other methods and more successful at learning robust features. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2022.3196780",
		"page": "85080-85094",
		"title": "Multiexpert Adversarial Regularization for Robust and Data-Efficient Deep Supervised Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136107917&doi=10.1109%2fACCESS.2022.3196780&partnerID=40&md5=5dff15387da09a3c425c359284ad6896",
		"volume": "10",
		"author": [
			{
				"family": "Gholami",
				"given": "B."
			},
			{
				"family": "Liu",
				"given": "Q."
			},
			{
				"family": "El-Khamy",
				"given": "M."
			},
			{
				"family": "Lee",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "panagantiRobustReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "The goal of robust reinforcement learning (RL) is to learn a policy that is robust against the uncertainty in model parameters. Parameter uncertainty commonly occurs in many real-world RL applications due to simulator modeling errors, changes in the real-world system dynamics over time, and adversarial disturbances. Robust RL is typically formulated as a max-min problem, where the objective is to learn the policy that maximizes the value against the worst possible models that lie in an uncertainty set. In this work, we propose a robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an offline dataset to learn the optimal robust policy. Robust RL with offline data is significantly more challenging than its non-robust counterpart because of the minimization over all models present in the robust Bellman operator. This poses challenges in offline data collection, optimization over the models, and unbiased estimation. In this work, we propose a systematic approach to overcome these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a near-optimal robust policy under standard assumptions and demonstrate its superior performance on standard benchmark problems. © 2022 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Robust Reinforcement Learning using Offline Data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143683086&partnerID=40&md5=12385a40ddb4c145a75af7e617b994dc",
		"volume": "35",
		"author": [
			{
				"family": "Panaganti",
				"given": "K."
			},
			{
				"family": "Xu",
				"given": "Z."
			},
			{
				"family": "Kalathil",
				"given": "D."
			},
			{
				"family": "Ghavamzadeh",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangDecoupledAdversarialContrastive2022",
		"type": "paper-conference",
		"abstract": "Adversarial training (AT) for robust representation learning and self-supervised learning (SSL) for unsupervised representation learning are two active research fields. Integrating AT into SSL, multiple prior works have accomplished a highly significant yet challenging task: learning robust representation without labels. A widely used framework is adversarial contrastive learning which couples AT and SSL, and thus constitutes a very complex optimization problem. Inspired by the divide-and-conquer philosophy, we conjecture that it might be simplified as well as improved by solving two sub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts the focus of the task from seeking an optimal integrating strategy for a coupled problem to finding sub-solutions for sub-problems. With this said, this work discards prior practices of directly introducing AT to SSL frameworks and proposed a two-stage framework termed Decoupled Adversarial Contrastive Learning (DeACL). Extensive experimental results demonstrate that our DeACL achieves SOTA self-supervised adversarial robustness while significantly reducing the training time, which validates its effectiveness and efficiency. Moreover, our DeACL constitutes a more explainable solution, and its success also bridges the gap with semi-supervised AT for exploiting unlabeled samples for robust representation learning. The code is publicly accessible at https://github.com/pantheon5100/DeACL. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-20056-4_42",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "725-742",
		"title": "Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144544122&doi=10.1007%2f978-3-031-20056-4_42&partnerID=40&md5=31ca03190c46bbcd6067a473f7dbb3a8",
		"volume": "13690 LNCS",
		"author": [
			{
				"family": "Zhang",
				"given": "C."
			},
			{
				"family": "Zhang",
				"given": "K."
			},
			{
				"family": "Zhang",
				"given": "C."
			},
			{
				"family": "Niu",
				"given": "A."
			},
			{
				"family": "Feng",
				"given": "J."
			},
			{
				"family": "Yoo",
				"given": "C.D."
			},
			{
				"family": "Kweon",
				"given": "I.S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "nooraniEmbracingRiskReinforcement2022",
		"type": "paper-conference",
		"abstract": "We explore the relation between the risk-sensitive exponential (exponential of total cost) and Distributionally Robust Reinforcement Learning objectives, and in doing so, we unify some of the popular Reinforcement Learning algorithms. Such equivalence (I) allows to understand a number of well-known Reinforcement Learning algorithms from a risk minimization perspective and (II) establishes the robustness properties of risk-sensitive exponential objective in the Reinforcement Learning context, which in turn provides a theoretical justification for the robust performance of risk-sensitive Reinforcement Learning algorithms in the literature. The robustness of exponential criteria motivates risk-sensitizing current risk-neutral Reinforcement Learning algorithms using such criteria.  © 2022 American Automatic Control Council.",
		"archive": "Scopus",
		"DOI": "10.23919/ACC53348.2022.9867841",
		"event-title": "Proceedings of the American Control Conference",
		"page": "2703-2708",
		"title": "Embracing Risk in Reinforcement Learning: The Connection between Risk-Sensitive Exponential and Distributionally Robust Criteria",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136260457&doi=10.23919%2fACC53348.2022.9867841&partnerID=40&md5=347ba7f8d2c690bdd7ad34cc2c71ded9",
		"volume": "2022-June",
		"author": [
			{
				"family": "Noorani",
				"given": "E."
			},
			{
				"family": "Baras",
				"given": "J.S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "leAnomalyDetectionInsider2021",
		"type": "article-journal",
		"abstract": "Insider threat represents a major cybersecurity challenge to companies, organizations, and government agencies. Insider threat detection involves many challenges, including unbalanced data, limited ground truth, and possible user behavior changes. This research presents an unsupervised learning based anomaly detection approach for insider threat detection. We employ four unsupervised learning methods with different working principles, and explore various representations of data with temporal information. Furthermore, different computational intelligence schemes are explored to combine these models to create anomaly detection ensembles for improving the detection performance. Evaluation results show that the approach allows learning from unlabelled data under challenging conditions for insider threat detection. Insider threats are detected with high detection and low false positive rates. For example, 60% of malicious insiders are detected under 0.1% investigation budget, and all malicious insiders are detected at less than 5% investigation budget. Furthermore, we explore the ability of the proposed approach to generalize for detecting new anomalous behaviors in different datasets, i.e., robustness. Finally, results demonstrate that a voting-based ensemble of anomaly detection can be used to improve detection performance as well as the robustness. Comparisons with the state-of-the-art confirm the effectiveness of the proposed approach.  © 2004-2012 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Network and Service Management",
		"DOI": "10.1109/TNSM.2021.3071928",
		"issue": "2",
		"page": "1152-1164",
		"title": "Anomaly Detection for Insider Threats Using Unsupervised Ensembles",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104195907&doi=10.1109%2fTNSM.2021.3071928&partnerID=40&md5=2799dabb5e1f8d9dd1e14124ec6ce3cd",
		"volume": "18",
		"author": [
			{
				"family": "Le",
				"given": "D.C."
			},
			{
				"family": "Zincir-Heywood",
				"given": "N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "aghakhaniBullseyePolytopeScalable2021",
		"type": "paper-conference",
		"abstract": "A recent source of concern for the security of neural networks is the emergence of clean-label dataset poisoning attacks, wherein correctly labeled poison samples are injected into the training dataset. While these poison samples look legitimate to the human observer, they contain malicious characteristics that trigger a targeted misclassification during inference. We propose a scalable and transferable clean-label poisoning attack against transfer learning, which creates poison images with their center close to the target image in the feature space. Our attack, Bullseye Polytope, improves the attack success rate of the current state-of-the-art by 26.75% in end-to-end transfer learning, while increasing attack speed by a factor of 12. We further extend Bullseye Polytope to a more practical attack model by including multiple images of the same object (e.g., from different angles) when crafting the poison samples. We demonstrate that this extension improves attack transferability by over 16% to unseen images (of the same object) without using extra poison samples.  © 2021 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/EuroSP51992.2021.00021",
		"event-title": "Proceedings - 2021 IEEE European Symposium on Security and Privacy, Euro S and P 2021",
		"page": "159-178",
		"title": "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119113061&doi=10.1109%2fEuroSP51992.2021.00021&partnerID=40&md5=5329947511ad01222151c43dc05e8e58",
		"author": [
			{
				"family": "Aghakhani",
				"given": "H."
			},
			{
				"family": "Meng",
				"given": "D."
			},
			{
				"family": "Wang",
				"given": "Y.-X."
			},
			{
				"family": "Kruegel",
				"given": "C."
			},
			{
				"family": "Vigna",
				"given": "G."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liBayesianRobustMultiextreme2020",
		"type": "article-journal",
		"abstract": "Outliers usually exist in the collected data due to human or instrumentation errors. Their existence makes the regression error obey a non-Gaussian distribution. So, people assume that the regression error obeys finite mixture distributions, such as mixture of Gaussians and mixture of Student's t-distributions. This paper proposed two Bayesian robust regression models based on multiple extreme learning machines. First, the finite mixture error distributions are extended to their infinite counterparts according to the theory of stick-breaking construction, which can avoid selecting the optimal number of components in mixture models. Second, a multi-extreme learning machine, which combines many single extreme learning machines with different numbers of hidden nodes, is constructed, which can avoid the effects of different numbers of hidden nodes on the final regression performance. Besides, sparse Bayesian learning has also been derived to perform the sparse model weight and output weight inference automatically. The experimental results on artificial datasets and eight benchmark datasets show that the proposed two models outperform the other compared models under different rates of outliers. Also, they generate better multi-step ahead wind speed and traffic speed forecasts in real applications. © 2020 Elsevier B.V.",
		"archive": "Scopus",
		"container-title": "Knowledge-Based Systems",
		"DOI": "10.1016/j.knosys.2020.106468",
		"title": "Bayesian robust multi-extreme learning machine",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092367835&doi=10.1016%2fj.knosys.2020.106468&partnerID=40&md5=2ec59fb714cd1fc853238e3e820f652e",
		"volume": "210",
		"author": [
			{
				"family": "Li",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Chen",
				"given": "Z."
			},
			{
				"family": "Zou",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nieAdversarialTrainingMethod2021",
		"type": "paper-conference",
		"abstract": "The easily-perturbed nature of deep neural network makes it vulnerable to adversarial attacks, and such vulnerability has become a major threat to the security of machine learning. The transferability of adversarial samples further increases the threat. Adversarial training has made considerable progress in defending against adversarial samples. In transfer learning, unsupervised domain adaptation is an important research branch, however, due to the label of the target domain samples can’t be obtained, it is difficult to implement adversarial training. In this paper, we found that using source domain data for adversarial training and adding the generated adversarial perturbation to the target domain data could effectively enhance the robustness of the transferred model. Experimental results showed that our proposed method can not only ensure the model’s classification accuracy, but also greatly improve the model’s defense performance against adversarial attacks. In simple, our proposed method not only guarantees the transfer of knowledge, but also realizes the transfer of model robustness. It is the main contribution of this paper. © 2021, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-82153-1_1",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "3-13",
		"title": "An Adversarial Training Method for Improving Model Robustness in Unsupervised Domain Adaptation",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113779102&doi=10.1007%2f978-3-030-82153-1_1&partnerID=40&md5=cece8d66fb799aeb2aabc95146e3819c",
		"volume": "12817 LNAI",
		"author": [
			{
				"family": "Nie",
				"given": "Z."
			},
			{
				"family": "Lin",
				"given": "Y."
			},
			{
				"family": "Yan",
				"given": "M."
			},
			{
				"family": "Cao",
				"given": "Y."
			},
			{
				"family": "Ning",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "simaoAlwaysSafeReinforcementLearning2021",
		"type": "paper-conference",
		"abstract": "Deploying reinforcement learning (RL) involves major concerns around safety. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial. Safe RL studies how to mitigate such problems. For instance, we can decouple safety from reward using constrained Markov decision processes (CMDPs), where an independent signal models the safety aspects. In this setting, an RL agent can autonomously find tradeoffs between performance and safety. Unfortunately, most RL agents designed for CMDPs only guarantee safety after the learning phase, which might prevent their direct deployment. In this work, we investigate settings where a concise abstract model of the safety aspects is given, a reasonable assumption since a thorough understanding of safety-related matters is a prerequisite for deploying RL in typical applications. Factored CMDPs provide such compact models when a small subset of features describe the dynamics relevant for the safety constraints. We propose an RL algorithm that uses this abstract model to learn policies for CMDPs safely, that is without violating the constraints. During the training process, this algorithm can seamlessly switch from a conservative policy to a greedy policy without violating the safety constraints. We prove that this algorithm is safe under the given assumptions. Empirically, we show that even if safety and reward signals are contradictory, this algorithm always operates safely and, when they are aligned, this approach also improves the agent’s performance. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
		"page": "1214-1223",
		"title": "AlwaysSafe: Reinforcement learning without safety constraint violations during training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103674313&partnerID=40&md5=359b14bfd08a91ba735c0f4b935475f4",
		"volume": "2",
		"author": [
			{
				"family": "Simão",
				"given": "T.D."
			},
			{
				"family": "Jansen",
				"given": "N."
			},
			{
				"family": "Spaan",
				"given": "M.T.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yuRobustReinforcementLearning2021",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) methods provide state-of-art performance in complex control tasks. However, it has been widely recognized that RL methods often fail to generalize due to unaccounted uncertainties. In this work, we propose a game theoretic framework for robust reinforcement learning that comprises many previous works as special cases. We formulate robust RL as a constrained minimax game between the RL agent and an environmental agent which represents uncertainties such as model parameter variations and adversarial disturbances. To solve the competitive optimization problems arising in our framework, we propose to use competitive mirror descent (CMD). This method accounts for the interactive nature of the game at each iteration while using Bregman divergences to adapt to the global structure of the constraint set. leveraging Lagrangian duality, we demonstrate an RRL policy gradient algorithm based on CMD. We empirically show that our algorithm is stable for large step sizes, resulting in faster convergence on constrained linear quadratic games. © 2021 J. Yu, C. Gehring, F. Schäfer & A. Anandkumar.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "1242-1254",
		"title": "Robust Reinforcement Learning: A Constrained Game-theoretic Approach",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673722&partnerID=40&md5=924f503e07b0d4790f2ca10f68012f4f",
		"volume": "144",
		"author": [
			{
				"family": "Yu",
				"given": "J."
			},
			{
				"family": "Gehring",
				"given": "C."
			},
			{
				"family": "Schäfer",
				"given": "F."
			},
			{
				"family": "Anandkumar",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ramirez-padronRobustWeightedGaussian2021",
		"type": "article-journal",
		"abstract": "This paper presents robust weighted variants of batch and online standard Gaussian processes (GPs) to effectively reduce the negative impact of outliers in the corresponding GP models. This is done by introducing robust data weighers that rely on robust and quasi-robust weight functions that come from robust M-estimators. Our robust GPs are compared to various GP models on four datasets. It is shown that our batch and online robust weighted GPs are indeed robust to outliers, significantly outperforming the corresponding standard GPs and the recently proposed heteroscedastic GP method GPz. Our experiments also show that our methods are comparable to and sometimes better than a state-of-the-art robust GP that uses a Student-t likelihood. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Computational Statistics",
		"DOI": "10.1007/s00180-020-01011-0",
		"issue": "1",
		"page": "347-373",
		"title": "Robust weighted Gaussian processes",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087661618&doi=10.1007%2fs00180-020-01011-0&partnerID=40&md5=3825bbaaabcd6c0ed09caeca5ecb0a79",
		"volume": "36",
		"author": [
			{
				"family": "Ramirez-Padron",
				"given": "R."
			},
			{
				"family": "Mederos",
				"given": "B."
			},
			{
				"family": "Gonzalez",
				"given": "A.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenMultiModalMutualInformation2021",
		"type": "paper-conference",
		"abstract": "This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations. © 2021 IEEE",
		"archive": "Scopus",
		"DOI": "10.1109/ICRA48506.2021.9561187",
		"event-title": "Proceedings - IEEE International Conference on Robotics and Automation",
		"page": "4274-4280",
		"title": "Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119129739&doi=10.1109%2fICRA48506.2021.9561187&partnerID=40&md5=22b8ddbec48150346ed766f970d0348e",
		"volume": "2021-May",
		"author": [
			{
				"family": "Chen",
				"given": "K."
			},
			{
				"family": "Lee",
				"given": "Y."
			},
			{
				"family": "Soh",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "tanRobustifyingReinforcementLearning2020",
		"type": "paper-conference",
		"abstract": "Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training. © 2020 AACC.",
		"archive": "Scopus",
		"DOI": "10.23919/ACC45564.2020.9147846",
		"event-title": "Proceedings of the American Control Conference",
		"page": "3959-3964",
		"title": "Robustifying Reinforcement Learning Agents via Action Space Adversarial Training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089561387&doi=10.23919%2fACC45564.2020.9147846&partnerID=40&md5=501add6087ff53cc19dbca4920ba956e",
		"volume": "2020-July",
		"author": [
			{
				"family": "Tan",
				"given": "K.L."
			},
			{
				"family": "Esfandiari",
				"given": "Y."
			},
			{
				"family": "Lee",
				"given": "X.Y."
			},
			{
				"family": "Sarkar",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kamalarubanRobustReinforcementLearning2020",
		"type": "paper-conference",
		"abstract": "We introduce a sampling perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments. Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms. © 2020 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Robust reinforcement learning via adversarial training with Langevin dynamics",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104088882&partnerID=40&md5=c4d1f8bdcf47f337e3c90c03ad3e14c6",
		"volume": "2020-December",
		"author": [
			{
				"family": "Kamalaruban",
				"given": "P."
			},
			{
				"family": "Huang",
				"given": "Y.-T."
			},
			{
				"family": "Hsieh",
				"given": "Y.-P."
			},
			{
				"family": "Rolland",
				"given": "P."
			},
			{
				"family": "Shi",
				"given": "C."
			},
			{
				"family": "Cevher",
				"given": "V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chakrabortyFairMixRepSelfsupervisedRobust2020",
		"type": "paper-conference",
		"abstract": "Representation Learning in a heterogeneous space with mixed variables of numerical and categorical types has interesting challenges due to its complex feature manifold. Moreover, feature learning in an unsupervised setup, without class labels and a suitable learning loss function, adds to the problem complexity. Further, the learned representation and subsequent predictions should not reflect discriminatory behavior towards certain sensitive groups or attributes. The proposed feature map should preserve maximum variations present in the data and needs to be fair with respect to the sensitive variables. We propose, in the first phase of our work, an efficient encoder-decoder framework to capture the mixed-domain information. The second phase of our work focuses on de-biasing the mixed space representations by adding relevant fairness constraints. This ensures minimal information loss between the representations before and after the fairness-preserving projections. Both the information content and the fairness aspect of the final representation learned has been validated through several metrics where it shows excellent performance. Our work (FairMixRep) addresses the problem of Mixed Space Fair Representation learning from an unsupervised perspective and learns a Universal representation which is timely, unique and a novel research contribution. 11This paper is accepted at ICDM'2020 DLC Workshop. © 2020 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICDMW51313.2020.00069",
		"event-title": "IEEE International Conference on Data Mining Workshops, ICDMW",
		"page": "458-463",
		"title": "FairMixRep: Self-supervised Robust Representation Learning for Heterogeneous Data with Fairness constraints",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101347891&doi=10.1109%2fICDMW51313.2020.00069&partnerID=40&md5=ec831606edd1047b939929f22266868f",
		"volume": "2020-November",
		"author": [
			{
				"family": "Chakraborty",
				"given": "S."
			},
			{
				"family": "Verma",
				"given": "E."
			},
			{
				"family": "Sahoo",
				"given": "S."
			},
			{
				"family": "Datta",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "singhImprovingRobustnessRisk2020",
		"type": "paper-conference",
		"abstract": "One major obstacle that precludes the success of reinforcement learning in real-world applications is the lack of robustness, either to model uncertainties or external disturbances, of the trained policies. Robustness is critical when the policies are trained in simulations instead of real world environment. In this work, we propose a risk-aware algorithm to learn robust policies in order to bridge the gap between simulation training and real-world implementation. Our algorithm is based on recently discovered distributional RL framework. We incorporate CVaR risk measure in sample based distributional policy gradients (SDPG) for learning risk-averse policies to achieve robustness against a range of system disturbances. We validate the robustness of risk-aware SDPG on multiple environments. © 2020 R. Singh, Q. Zhang & Y. Chen.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "958-968",
		"title": "Improving Robustness via Risk Averse Distributional Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160153359&partnerID=40&md5=819ea330ab2feb455bbe62ea271c4e74",
		"volume": "120",
		"author": [
			{
				"family": "Singh",
				"given": "R."
			},
			{
				"family": "Zhang",
				"given": "Q."
			},
			{
				"family": "Chen",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chenDistributionallyRobustSemisupervised2019",
		"type": "paper-conference",
		"abstract": "Semi-supervised learning is crucial for alleviating la-belling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",
		"archive": "Scopus",
		"event-title": "33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",
		"page": "3321-3328",
		"title": "Distributionally robust semi-supervised learning for people-centric sensing",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090808244&partnerID=40&md5=68a7c553d7f705a73dd28cb3ab222b33",
		"author": [
			{
				"family": "Chen",
				"given": "K."
			},
			{
				"family": "Yao",
				"given": "L."
			},
			{
				"family": "Zhang",
				"given": "D."
			},
			{
				"family": "Chang",
				"given": "X."
			},
			{
				"family": "Long",
				"given": "G."
			},
			{
				"family": "Wang",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "guoRobustSemisupervisedRepresentation2019",
		"type": "paper-conference",
		"abstract": "The success of machine learning algorithms generally depends on data representation and recently many representation learning methods have been proposed. However, learning a good representation may not always benefit the classification tasks. It sometimes even hurt the performance as the learned representation maybe not related to the ultimate tasks, especially when the labeled examples are few to afford a reliable model selection. In this paper, we propose a novel robust semi-supervised graph representation learning method based on graph convolutional network. To make the learned representation more related to the ultimate classification task, we propose to extend label information based on the smooth assumption and obtain pseudo-labels for unlabeled nodes. Moreover, to make the model robust with noise in the pseudo-label, we propose to apply a large margin classifier to the learned representation. Influenced by the pseudo-label and the large-margin principle, the learned representation can not only exploit the label information encoded in the graph-structure sufficiently but also can produce a more rigorous decision boundary. Experiments demonstrate the superior performance of the proposal over many related methods. © Springer Nature Switzerland AG 2019.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-16142-2_11",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "131-143",
		"title": "Robust semi-supervised representation learning for graph-structured data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065032165&doi=10.1007%2f978-3-030-16142-2_11&partnerID=40&md5=cdb26d866f80e05e4bd463563214ae0c",
		"volume": "11441 LNAI",
		"author": [
			{
				"family": "Guo",
				"given": "L.-Z."
			},
			{
				"family": "Han",
				"given": "T."
			},
			{
				"family": "Li",
				"given": "Y.-F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "maRobustSupportVector2011",
		"type": "paper-conference",
		"abstract": "It is found that data points used for training may contain outliers that can generate unpredictable disturbance for some Support Vector Machines (SVMs) classification problems. No theoretical limit for such bad influence is held in traditional convex SVM methods. We present a novel robust misclassification penalty function for SVM which is inspired by the concept of \"Least Median Regression\". In our approach, total loss penalty in training is measured by the summation of two median hinge losses, each for a different class. We also propose a \"Rank and Convex Procedure\" to optimize our tasks. Though our approach is heuristic, it is faster than other known robust methods, such as SVM with Ramp Loss Penalty. © 2011 IFAC.",
		"archive": "Scopus",
		"DOI": "10.3182/20110828-6-IT-1002.03467",
		"event-title": "IFAC Proceedings Volumes (IFAC-PapersOnline)",
		"note": "issue: 1 PART 1",
		"page": "11208-11213",
		"title": "Robust Support Vector Machine using Least Median Loss Penalty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866749269&doi=10.3182%2f20110828-6-IT-1002.03467&partnerID=40&md5=21518c36cc565c7a29367397f3dff4f9",
		"volume": "44",
		"author": [
			{
				"family": "Ma",
				"given": "Y."
			},
			{
				"family": "Li",
				"given": "L."
			},
			{
				"family": "Huang",
				"given": "X."
			},
			{
				"family": "Wang",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "chauRobustSelfOrganizingApproach2016",
		"type": "paper-conference",
		"abstract": "In the real world, incomplete data are often encountered and located anywhere in a data set. Such incomplete data make a data clustering task more challenging. It's common practice to eliminate incomplete data from the input data set. If there are a large number of missing values, ignoring them may lead to the data insufficiency and ineffectiveness of the data clustering task. Hence, incomplete data clustering has been considered in many research works with many different approaches based on the well-known existing clustering algorithms such as k-means, fuzzy c-means, the self-organizing map (SOM), mean shift, etc. However, few of them have examined both effectiveness and robustness of the incomplete data clustering algorithms. Some of them are not practical due to a lot of parameters in hybrid approaches and/or cannot handle incomplete data which appear in any object at any dimension. In contrast, this paper aims at a SOM-based incomplete data clustering algorithm, iS nps, which is a robust and effective solution to clustering incomplete data in a simple but practical approach. Is nps can do clustering on incomplete data as well as estimate incomplete data using the nearest prototype strategy in an iterative manner. As compared to several different existing approaches, our proposed algorithm can produce the clusters of good quality and a better approximation of incomplete data via the experiments on benchmark data sets. © 2015 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/KSE.2015.11",
		"event-title": "Proceedings - 2015 IEEE International Conference on Knowledge and Systems Engineering, KSE 2015",
		"page": "150-155",
		"title": "A Robust Self-Organizing Approach to Effectively Clustering Incomplete Data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964723137&doi=10.1109%2fKSE.2015.11&partnerID=40&md5=d35101e780f00bdfb125cdc59dd8e77f",
		"author": [
			{
				"family": "Chau",
				"given": "V.T.N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "huDoesDistributionallyRobust2018",
		"type": "paper-conference",
		"abstract": "Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with/-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness. © 2018 by authors.All right reserved.",
		"archive": "Scopus",
		"event-title": "35th International Conference on Machine Learning, ICML 2018",
		"page": "3220-3249",
		"title": "Does distributionally robust supervised learning give robust classifiers?",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057279674&partnerID=40&md5=290395557bb261b75f67b472cfddea3d",
		"volume": "5",
		"author": [
			{
				"family": "Hu",
				"given": "W."
			},
			{
				"family": "Niu",
				"given": "G."
			},
			{
				"family": "Sato",
				"given": "I."
			},
			{
				"family": "Sugiyama",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dingProvablyEfficientGeneralized2023",
		"type": "paper-conference",
		"abstract": "We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rate O((|X| + |Y |)LpT(|A| + |B|))) for both regret and constraint violation after playing T episodes of the game. Here, L is the horizon of each episode, (|X|, |A|) and (|Y |, |B|) are the state/action space sizes of the min-player and the max-player, respectively. To the best of our knowledge, we provide the first provably efficient online safe reinforcement learning algorithm in constrained Markov games. © 2023 D. Ding, X. Wei, Z. Yang, Z. Wang & M.R. Jovanović.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "315-332",
		"title": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172894839&partnerID=40&md5=cdac5fce5f7bd75455c6185abb95092b",
		"volume": "211",
		"author": [
			{
				"family": "Ding",
				"given": "D."
			},
			{
				"family": "Wei",
				"given": "X."
			},
			{
				"family": "Yang",
				"given": "Z."
			},
			{
				"family": "Wang",
				"given": "Z."
			},
			{
				"family": "Jovanović",
				"given": "M.R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "millan-ariasRobustApproachContinuous2021",
		"type": "article-journal",
		"abstract": "Reinforcement learning refers to a machine learning paradigm in which an agent interacts with the environment to learn how to perform a task. The characteristics of the environment may change over time or be affected by disturbances not controlled, avoiding the agent finding a proper policy. Some approaches attempt to address these problems, as interactive reinforcement learning, where an external entity helps the agent learn through advice. Other approaches, such as robust reinforcement learning, allow the agent to learn the task, acting in a disturbed environment. In this paper, we propose an approach that addresses interactive reinforcement learning problems in a dynamic environment, where advice provides information on the task and the dynamics of the environment. Thus, an agent learns a policy in a disturbed environment while receiving advice. We implement our approach in the dynamic version of the cart-pole balancing task and a simulated robotic arm dynamic environment to organize objects. Our results show that the proposed approach allows an agent to complete the task satisfactorily in a dynamic, continuous state-action domain. Moreover, experimental results suggest agents trained with our approach are less sensitive to changes in the characteristics of the environment than interactive reinforcement learning agents.  © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2021.3099071",
		"page": "104242-104260",
		"title": "A Robust Approach for Continuous Interactive Actor-Critic Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111607924&doi=10.1109%2fACCESS.2021.3099071&partnerID=40&md5=55aa30222b5a34ada19cd98c7718390d",
		"volume": "9",
		"author": [
			{
				"family": "Millan-Arias",
				"given": "C.C."
			},
			{
				"family": "Fernandes",
				"given": "B.J.T."
			},
			{
				"family": "Cruz",
				"given": "F."
			},
			{
				"family": "Dazeley",
				"given": "R."
			},
			{
				"family": "Fernandes",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "rileyAssuredMultiagentReinforcement2022",
		"type": "paper-conference",
		"abstract": "Multi-agent reinforcement learning facilitates agents learning to solve complex decision-making problems requiring collaboration. However, reinforcement learning methods are underpinned by stochastic mechanisms, making them unsuitable for safety-critical domains. To solve this issue, approaches such as assured multi-agent reinforcement learning, which utilises quantitative verification to produce formal guarantees of safety requirements during the agents learning process, have been developed. However, this approach relies on accurate knowledge about the environment to be effectively used which can be detrimental if this knowledge is inaccurate. Therefore, we developed an extension to assured multi-agent reinforcement learning called agent interaction driven adaptability, an automated process to securing reliable safety constraints, allowing inaccurate and missing knowledge to be used without detriment. Our preliminary results showcase the ability of agent interaction driven adaptability to allow safe multi-agent reinforcement learning to be utilised in safety-critical scenarios. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",
		"archive": "Scopus",
		"DOI": "10.1007/978-981-19-3444-5_8",
		"event-title": "Smart Innovation, Systems and Technologies",
		"page": "87-97",
		"title": "Assured Multi-agent Reinforcement Learning with Robust Agent-Interaction Adaptability",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135905227&doi=10.1007%2f978-981-19-3444-5_8&partnerID=40&md5=a24f1b8b8ff87d93118c4e69e71c214d",
		"volume": "309",
		"author": [
			{
				"family": "Riley",
				"given": "J."
			},
			{
				"family": "Calinescu",
				"given": "R."
			},
			{
				"family": "Paterson",
				"given": "C."
			},
			{
				"family": "Kudenko",
				"given": "D."
			},
			{
				"family": "Banks",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schmidBatchlikeOnlineLearning2021",
		"type": "paper-conference",
		"abstract": "Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "Batch-like online learning for more robust hybrid artificial intelligence: Deconstruction as a machine learning process",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104622356&partnerID=40&md5=04edd84470671c7e18a70da9f07e9d78",
		"volume": "2846",
		"author": [
			{
				"family": "Schmid",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "luFamilyRobustStochastic2019",
		"type": "paper-conference",
		"abstract": "We consider a new family of stochastic operators for reinforcement learning that seeks to alleviate negative effects and become more robust to approximation or estimation errors. Theoretical results are established, showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman and recently proposed operators. © 2019 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "A family of robust stochastic operators for reinforcement learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177881&partnerID=40&md5=a53a7206f40717efe60cd4f5353df697",
		"volume": "32",
		"author": [
			{
				"family": "Lu",
				"given": "Y."
			},
			{
				"family": "Squillante",
				"given": "M.S."
			},
			{
				"family": "Wu",
				"given": "C.W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kazantzidisHowTrainYour2022",
		"type": "paper-conference",
		"abstract": "Training reinforcement learning agents in real-world environments is costly, particularly for safety-critical applications. Human input can enable an agent to learn a good policy while avoiding unsafe actions, but at the cost of bothering the human with repeated queries. We present a model for safe learning in safety-critical environments from human input that minimises bother cost. Our model, JPAL-HA, proposes an efficient mechanism to harness human preferences and justifications to significantly improve safety during the learning process without increasing the number of interactions with a user. We show this with both simulation and human experiments. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
		"page": "1654-1656",
		"title": "How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134306740&partnerID=40&md5=76a61115b19501a21f7673129a6cff87",
		"volume": "3",
		"author": [
			{
				"family": "Kazantzidis",
				"given": "I."
			},
			{
				"family": "Norman",
				"given": "T.J."
			},
			{
				"family": "Du",
				"given": "Y."
			},
			{
				"family": "Freeman",
				"given": "C.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wadaRobustLabelPrediction2019",
		"type": "article-journal",
		"abstract": "This paper proposes a computationally efficient offline semi-supervised algorithm that yields a more accurate prediction than the label propagation algorithm, which is commonly used in online graphbased semi-supervised learning (SSL). Our proposed method is an offline method that is intended to assist online graph-based SSL algorithms. The efficacy of the tool in creating new learning algorithms of this type is demonstrated in numerical experiments. © 2019 The Institute of Electronics, Information and Communication Engineers.",
		"archive": "Scopus",
		"container-title": "IEICE Transactions on Information and Systems",
		"DOI": "10.1587/transinf.2018EDP7424",
		"issue": "8",
		"page": "1537-1545",
		"title": "Robust label prediction via label propagation and geodesic k-nearest neighbor in online semi-supervised learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071900727&doi=10.1587%2ftransinf.2018EDP7424&partnerID=40&md5=23a1a6f6aa6e5d808cce101d6bc77523",
		"volume": "E102D",
		"author": [
			{
				"family": "Wada",
				"given": "Y."
			},
			{
				"family": "Su",
				"given": "S."
			},
			{
				"family": "Kumagai",
				"given": "W."
			},
			{
				"family": "Kanamori",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "soaresCorrigibility2015",
		"type": "paper-conference",
		"abstract": "As artificially intelligent systems grow in intelligence and capability, some of their available options may allow them to resist intervention by their programmers. We call an AI system “corrigible” if it cooperates with what its creators regard\nas a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences. We introduce the notion of corrigibility and analyze utility functions that attempt to make an agent shut\ndown safely if a shutdown button is pressed, while avoiding incentives to prevent the button from being pressed or cause\nthe button to be pressed, and while ensuring propagation of the shutdown behavior as it creates new subsystems or selfmodifies. While some proposals are interesting, none have yet been demonstrated to satisfy all of our intuitive desiderata, leaving this simple problem in corrigibility wide-open.",
		"container-title": "Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop",
		"event-place": "Austin, Texas, USA",
		"event-title": "The 29th Annual AAAI Conference on Artificial Intelligence",
		"publisher-place": "Austin, Texas, USA",
		"title": "Corrigibility",
		"URL": "https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf",
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Yudkowsky",
				"given": "Eliezer"
			},
			{
				"family": "Armstrong",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "gyevnarCausalExplanationsSequential2024",
		"type": "paper-conference",
		"abstract": "We present CEMA: Causal Explanations in Multi-A gent systems; a framework for creating causal natural language explanations of an agent's decisions in dynamic sequential multi-agent systems to build more trustworthy autonomous agents. Unlike prior work that assumes a fixed causal structure, CEMA only requires a probabilistic model for forward-simulating the state of the system. Using such a model, CEMA simulates counterfactual worlds that identify the salient causes behind the agent's decisions. We evaluate CEMA on the task of motion planning for autonomous driving and test it in diverse simulated scenarios. We show that CEMA correctly and robustly identifies the causes behind the agent's decisions, even when a large number of other agents is present, and show via a user study that CEMA's explanations have a positive effect on participants' trust in autonomous vehicles and are rated as high as high-quality baseline explanations elicited from other participants. We release the collected explanations with annotations as the HEADD dataset.",
		"collection-title": "AAMAS '24",
		"container-title": "Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems",
		"event-place": "Richland, SC",
		"ISBN": "979-8-4007-0486-4",
		"page": "771–779",
		"publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
		"publisher-place": "Richland, SC",
		"source": "ACM Digital Library",
		"title": "Causal Explanations for Sequential Decision-Making in Multi-Agent Systems",
		"author": [
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Wang",
				"given": "Cheng"
			},
			{
				"family": "Lucas",
				"given": "Christopher G."
			},
			{
				"family": "Cohen",
				"given": "Shay B."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					5,
					13
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					6
				]
			]
		}
	},
	{
		"id": "leikeAISafetyGridworlds2017",
		"type": "article",
		"abstract": "We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modiﬁcation, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and speciﬁcation problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",
		"language": "en",
		"note": "arXiv:1711.09883 [cs]",
		"number": "arXiv:1711.09883",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI Safety Gridworlds",
		"URL": "http://arxiv.org/abs/1711.09883",
		"author": [
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Martic",
				"given": "Miljan"
			},
			{
				"family": "Krakovna",
				"given": "Victoria"
			},
			{
				"family": "Ortega",
				"given": "Pedro A."
			},
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Lefrancq",
				"given": "Andrew"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Legg",
				"given": "Shane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					6,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					11,
					28
				]
			]
		}
	},
	{
		"id": "huangNonverbalRobotFeedback2019",
		"type": "article",
		"abstract": "Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.",
		"DOI": "10.48550/arXiv.1911.02320",
		"note": "arXiv:1911.02320 [cs]",
		"number": "arXiv:1911.02320",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Nonverbal Robot Feedback for Human Teachers",
		"URL": "http://arxiv.org/abs/1911.02320",
		"author": [
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Huang",
				"given": "Isabella"
			},
			{
				"family": "Pandya",
				"given": "Ravi"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					6
				]
			]
		}
	},
	{
		"id": "zhangLearningExtrapolatedCorrections2019",
		"type": "article",
		"abstract": "Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information -- the change of a waypoint along the way -- to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.",
		"DOI": "10.48550/arXiv.1812.01225",
		"note": "arXiv:1812.01225 [cs]",
		"number": "arXiv:1812.01225",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning from Extrapolated Corrections",
		"URL": "http://arxiv.org/abs/1812.01225",
		"author": [
			{
				"family": "Zhang",
				"given": "Jason Y."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					10
				]
			]
		}
	},
	{
		"id": "xuLearningPriorIntent2019",
		"type": "article",
		"abstract": "A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a \"prior\" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",
		"DOI": "10.48550/arXiv.1805.12573",
		"note": "arXiv:1805.12573 [cs, stat]",
		"number": "arXiv:1805.12573",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning a Prior over Intent via Meta-Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1805.12573",
		"author": [
			{
				"family": "Xu",
				"given": "Kelvin"
			},
			{
				"family": "Ratner",
				"given": "Ellis"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					14
				]
			]
		}
	},
	{
		"id": "strayWhatAreYou2021",
		"type": "article",
		"abstract": "We describe cases where real recommender systems were modified in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classifiers from human-created data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.",
		"DOI": "10.48550/arXiv.2107.10939",
		"note": "arXiv:2107.10939 [cs]",
		"number": "arXiv:2107.10939",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "What are you optimizing for? Aligning Recommender Systems with Human Values",
		"title-short": "What are you optimizing for?",
		"URL": "http://arxiv.org/abs/2107.10939",
		"author": [
			{
				"family": "Stray",
				"given": "Jonathan"
			},
			{
				"family": "Vendrov",
				"given": "Ivan"
			},
			{
				"family": "Nixon",
				"given": "Jeremy"
			},
			{
				"family": "Adler",
				"given": "Steven"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					22
				]
			]
		}
	},
	{
		"id": "freedmanChoiceSetMisspecification2021",
		"type": "article",
		"abstract": "Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly speciﬁed rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difﬁcult to specify, and analyze choice set misspeciﬁcation: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classiﬁcation of different kinds of choice set misspeciﬁcation, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspeciﬁcation to hurt, we ﬁnd that certain kinds of misspeciﬁcation are neither helpful nor harmful (in expectation). However, in other situations, misspeciﬁcation can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspeciﬁcation in real-world reward inference.",
		"language": "en",
		"note": "arXiv:2101.07691 [cs]",
		"number": "arXiv:2101.07691",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Choice Set Misspecification in Reward Inference",
		"URL": "http://arxiv.org/abs/2101.07691",
		"author": [
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					19
				]
			]
		}
	},
	{
		"id": "freireDERAILDiagnosticEnvironments2020",
		"type": "article",
		"abstract": "The objective of many real-world tasks is complex and difﬁcult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results conﬁrm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design ﬂaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals.",
		"language": "en",
		"note": "arXiv:2012.01365 [cs]",
		"number": "arXiv:2012.01365",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DERAIL: Diagnostic Environments for Reward And Imitation Learning",
		"title-short": "DERAIL",
		"URL": "http://arxiv.org/abs/2012.01365",
		"author": [
			{
				"family": "Freire",
				"given": "Pedro"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Toyer",
				"given": "Sam"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					2
				]
			]
		}
	},
	{
		"id": "michaudUnderstandingLearnedReward2020",
		"type": "article",
		"abstract": "In many real-world tasks, it is not possible to procedurally specify an RL agent’s reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reﬂect user preferences. Absent signiﬁcant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We ﬁnd that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need signiﬁcantly different methods from policy interpretability.",
		"language": "en",
		"note": "arXiv:2012.05862 [cs]",
		"number": "arXiv:2012.05862",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding Learned Reward Functions",
		"URL": "http://arxiv.org/abs/2012.05862",
		"author": [
			{
				"family": "Michaud",
				"given": "Eric J."
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					10
				]
			]
		}
	},
	{
		"id": "bobuLESSMoreRethinking2020",
		"type": "paper-conference",
		"abstract": "Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.",
		"container-title": "Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3319502.3374811",
		"note": "arXiv:2001.04465 [cs, stat]",
		"page": "429-437",
		"source": "arXiv.org",
		"title": "LESS is More: Rethinking Probabilistic Models of Human Behavior",
		"title-short": "LESS is More",
		"URL": "http://arxiv.org/abs/2001.04465",
		"author": [
			{
				"family": "Bobu",
				"given": "Andreea"
			},
			{
				"family": "Scobee",
				"given": "Dexter R. R."
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			},
			{
				"family": "Sastry",
				"given": "S. Shankar"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					3,
					9
				]
			]
		}
	},
	{
		"id": "bobuFeatureExpansiveReward2021",
		"type": "paper-conference",
		"abstract": "When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.",
		"container-title": "Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3434073.3444667",
		"note": "arXiv:2006.13208 [cs, stat]",
		"page": "216-224",
		"source": "arXiv.org",
		"title": "Feature Expansive Reward Learning: Rethinking Human Input",
		"title-short": "Feature Expansive Reward Learning",
		"URL": "http://arxiv.org/abs/2006.13208",
		"author": [
			{
				"family": "Bobu",
				"given": "Andreea"
			},
			{
				"family": "Wiggert",
				"given": "Marius"
			},
			{
				"family": "Tomlin",
				"given": "Claire"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					8
				]
			]
		}
	},
	{
		"id": "brownValueAlignmentVerification2021",
		"type": "article",
		"abstract": "As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values. The goal is to construct a kind of \"driver's test\" that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents and propose and analyze heuristic and approximate value alignment verification tests in a wide range of gridworlds and a continuous autonomous driving domain. Finally, we prove that there exist sufficient conditions such that we can verify exact and approximate alignment across an infinite set of test environments via a constant-query-complexity alignment test.",
		"DOI": "10.48550/arXiv.2012.01557",
		"note": "arXiv:2012.01557 [cs]",
		"number": "arXiv:2012.01557",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Value Alignment Verification",
		"URL": "http://arxiv.org/abs/2012.01557",
		"author": [
			{
				"family": "Brown",
				"given": "Daniel S."
			},
			{
				"family": "Schneider",
				"given": "Jordan"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Niekum",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					11
				]
			]
		}
	},
	{
		"id": "loseyPhysicalInteractionCommunication2022",
		"type": "article-journal",
		"abstract": "When a robot performs a task next to a human, physical interaction is inevitable: the human might push, pull, twist, or guide the robot. The state of the art treats these interactions as disturbances that the robot should reject or avoid. At best, these robots respond safely while the human interacts; but after the human lets go, these robots simply return to their original behavior. We recognize that physical human–robot interaction (pHRI) is often intentional: the human intervenes on purpose because the robot is not doing the task correctly. In this article, we argue that when pHRI is intentional it is also informative: the robot can leverage interactions to learn how it should complete the rest of its current task even after the person lets go. We formalize pHRI as a dynamical system, where the human has in mind an objective function they want the robot to optimize, but the robot does not get direct access to the parameters of this objective: they are internal to the human. Within our proposed framework human interactions become observations about the true objective. We introduce approximations to learn from and respond to pHRI in real-time. We recognize that not all human corrections are perfect: often users interact with the robot noisily, and so we improve the efficiency of robot learning from pHRI by reducing unintended learning. Finally, we conduct simulations and user studies on a robotic manipulator to compare our proposed approach with the state of the art. Our results indicate that learning from pHRI leads to better task performance and improved human satisfaction.",
		"container-title": "The International Journal of Robotics Research",
		"DOI": "10.1177/02783649211050958",
		"ISSN": "0278-3649",
		"issue": "1",
		"language": "en",
		"note": "publisher: SAGE Publications Ltd STM",
		"page": "20-44",
		"source": "SAGE Journals",
		"title": "Physical interaction as communication: Learning robot objectives online from human corrections",
		"title-short": "Physical interaction as communication",
		"URL": "https://doi.org/10.1177/02783649211050958",
		"volume": "41",
		"author": [
			{
				"family": "Losey",
				"given": "Dylan P."
			},
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "O’Malley",
				"given": "Marcia K."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					1
				]
			]
		}
	},
	{
		"id": "leeBPrefBenchmarkingPreferenceBased2021",
		"type": "article",
		"abstract": "Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",
		"DOI": "10.48550/arXiv.2111.03026",
		"note": "arXiv:2111.03026 [cs]",
		"number": "arXiv:2111.03026",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
		"title-short": "B-Pref",
		"URL": "http://arxiv.org/abs/2111.03026",
		"author": [
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Smith",
				"given": "Laura"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					4
				]
			]
		}
	},
	{
		"id": "watkinsTeachableReinforcementLearning2021",
		"type": "paper-conference",
		"abstract": "Training automated agents to complete complex tasks in interactive environments is challenging: reinforcement learning requires careful hand-engineering of reward functions, imitation learning requires specialized infrastructure and access to a human expert, and learning from intermediate forms of supervision (like binary preferences) is time-consuming and extracts little information from each human intervention. Can we overcome these challenges by building agents that learn from rich, interactive feedback instead? We propose a new supervision paradigm for interactive learning based on \"teachable\" decision-making systems that learn from structured advice provided by an external teacher. We begin by formalizing a class of human-in-the-loop decision making problems in which multiple forms of teacher-provided advice are available to a learner. We then describe a simple learning algorithm for these problems that first learns to interpret advice, then learns from advice to complete tasks even in the absence of human supervision. In puzzle-solving, navigation, and locomotion domains, we show that agents that learn from advice can acquire new skills with significantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "6920–6933",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Teachable Reinforcement Learning via Advice Distillation",
		"URL": "https://papers.nips.cc/paper/2021/hash/37cfff3c04f95b22bcf166df586cd7a9-Abstract.html",
		"volume": "34",
		"author": [
			{
				"family": "Watkins",
				"given": "Olivia"
			},
			{
				"family": "Gupta",
				"given": "Abhishek"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Andreas",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "skalseInvariancePolicyOptimisation2023",
		"type": "article",
		"abstract": "It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinitedata limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.",
		"language": "en",
		"note": "arXiv:2203.07475 [cs, stat]",
		"number": "arXiv:2203.07475",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Invariance in Policy Optimisation and Partial Identifiability in Reward Learning",
		"URL": "http://arxiv.org/abs/2203.07475",
		"author": [
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Farrugia-Roberts",
				"given": "Matthew"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					7
				]
			]
		}
	},
	{
		"id": "gleaveUncertaintyEstimationLanguage2022",
		"type": "article",
		"abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to ﬁne-tune them on a task-speciﬁc dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive—and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efﬁciency and robustness using active learning and risk-averse reinforcement learning (RL). Speciﬁcally, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their ﬁnal layer. Ensembles have proved successful in prior applications of active learning [9, 3], but we ﬁnd that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble’s estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are ﬁne-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modiﬁed to support uncertainty estimation, e.g. by training multiple language models.",
		"language": "en",
		"note": "arXiv:2203.07472 [cs]",
		"number": "arXiv:2203.07472",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Uncertainty Estimation for Language Reward Models",
		"URL": "http://arxiv.org/abs/2203.07472",
		"author": [
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					14
				]
			]
		}
	},
	{
		"id": "shahGoalMisgeneralizationWhy2022",
		"type": "article",
		"abstract": "The ﬁeld of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is speciﬁcation gaming, in which the designer-provided speciﬁcation is ﬂawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the speciﬁcation is correct, in the case of goal misgeneralization. Goal misgeneralization is a speciﬁc form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.",
		"language": "en",
		"note": "arXiv:2210.01790 [cs]",
		"number": "arXiv:2210.01790",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals",
		"title-short": "Goal Misgeneralization",
		"URL": "http://arxiv.org/abs/2210.01790",
		"author": [
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Varma",
				"given": "Vikrant"
			},
			{
				"family": "Kumar",
				"given": "Ramana"
			},
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Krakovna",
				"given": "Victoria"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Kenton",
				"given": "Zac"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					2
				]
			]
		}
	},
	{
		"id": "barnettActiveRewardLearning2023",
		"type": "article",
		"abstract": "Reward learning algorithms utilize human feedback to infer a reward function, which is then used to train an AI system. This human feedback is often a preference comparison, in which the human teacher compares several samples of AI behavior and chooses which they believe best accomplishes the objective. While reward learning typically assumes that all feedback comes from a single teacher, in practice these systems often query multiple teachers to gather sufficient training data. In this paper, we investigate this disparity, and find that algorithmic evaluation of these different sources of feedback facilitates more accurate and efficient reward learning. We formally analyze the value of information (VOI) when reward learning from teachers with varying levels of rationality, and define and evaluate an algorithm that utilizes this VOI to actively select teachers to query for feedback. Surprisingly, we find that it is often more informative to query comparatively irrational teachers. By formalizing this problem and deriving an analytical solution, we hope to facilitate improvement in reward learning approaches to aligning AI behavior with human values.",
		"DOI": "10.48550/arXiv.2303.00894",
		"note": "arXiv:2303.00894 [cs]",
		"number": "arXiv:2303.00894",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Active Reward Learning from Multiple Teachers",
		"URL": "http://arxiv.org/abs/2303.00894",
		"author": [
			{
				"family": "Barnett",
				"given": "Peter"
			},
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					1
				]
			]
		}
	},
	{
		"id": "myersLearningMultimodalRewards2022",
		"type": "paper-conference",
		"abstract": "Learning from human feedback has shown to be a useful approach in acquiring robot reward functions. However, expert feedback is often assumed to be drawn from an underlying unimodal reward function. This assumption does not always hold including in settings where multiple experts provide data or when a single expert provides data for different tasks—we thus go beyond learning a unimodal reward and focus on learning a multimodal reward function. We formulate the multimodal reward learning as a mixture learning problem and develop a novel ranking-based learning approach, where the experts are only required to rank a given set of trajectories. Furthermore, as access to interaction data is often expensive in robotics, we develop an active querying approach to accelerate the learning process. We conduct experiments and user studies using a multi-task variant of OpenAI’s LunarLander and a real Fetch robot, where we collect data from multiple users with different preferences. The results suggest that our approach can efficiently learn multimodal reward functions, and improve data-efficiency over benchmark methods that we adapt to our learning problem.",
		"container-title": "Proceedings of the 5th Conference on Robot Learning",
		"event-title": "Conference on Robot Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "342-352",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Learning Multimodal Rewards from Rankings",
		"URL": "https://proceedings.mlr.press/v164/myers22a.html",
		"author": [
			{
				"family": "Myers",
				"given": "Vivek"
			},
			{
				"family": "Biyik",
				"given": "Erdem"
			},
			{
				"family": "Anari",
				"given": "Nima"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					11
				]
			]
		}
	},
	{
		"id": "laidlawUncertainDecisionsFacilitate2021",
		"type": "paper-conference",
		"abstract": "Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity—the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "15070–15083",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Uncertain Decisions Facilitate Better Preference Learning",
		"URL": "https://proceedings.neurips.cc/paper/2021/hash/7f141cf8e7136ce8701dc6636c2a6fe4-Abstract.html",
		"volume": "34",
		"author": [
			{
				"family": "Laidlaw",
				"given": "Cassidy"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leePEBBLEFeedbackEfficientInteractive2021",
		"type": "article",
		"abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
		"DOI": "10.48550/arXiv.2106.05091",
		"note": "arXiv:2106.05091 [cs]",
		"number": "arXiv:2106.05091",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
		"title-short": "PEBBLE",
		"URL": "http://arxiv.org/abs/2106.05091",
		"author": [
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Smith",
				"given": "Laura"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					9
				]
			]
		}
	},
	{
		"id": "bhatiaAgnosticLearningUnknown2021",
		"type": "paper-conference",
		"abstract": "Traditional learning approaches for classification implicitly assume that each mistake has the same cost. In many real-world problems though, the utility of a decision depends on the underlying context x and decision y; for instance, misclassifying a stop sign is worse than misclassifying a road-side postbox. However, directly incorporating these utilities into the learning objective is often infeasible since these can be quite complex and difficult for humans to specify.\nWe formally study this as agnostic learning with unknown utilities: given a dataset S = {x_1, …, x_n} where each data point x_i ∼ 𝒟_x from some unknown distribution 𝒟_x, the objective of the learner is to output a function f in some class of decision functions ℱ with small excess risk. This risk measures the performance of the output predictor f with respect to the best predictor in the class ℱ on the unknown underlying utility u^*:𝒳×𝒴↦ [0,1]. This utility u^* is not assumed to have any specific structure and is allowed to be any bounded function. This raises an interesting question whether learning is even possible in our setup, given that obtaining a generalizable estimate of utility u^* might not be possible from finitely many samples. Surprisingly, we show that estimating the utilities of only the sampled points S suffices to learn a decision function which generalizes well. \nWith this insight, we study mechanisms for eliciting information from human experts which allow a learner to estimate the utilities u^* on the set S. While humans find it difficult to directly provide utility values reliably, it is often easier for them to provide comparison feedback based on these utilities. We show that, unlike in the realizable setup, the vanilla comparison queries where humans compare a pair of decisions for a single input x are insufficient. We introduce a family of elicitation mechanisms by generalizing comparisons, called the k-comparison oracle, which enables the learner to ask for comparisons across k different inputs x at once. We show that the excess risk in our agnostic learning framework decreases at a rate of O (1/k) with such queries. This result brings out an interesting accuracy-elicitation trade-off - as the order k of the oracle increases, the comparative queries become harder to elicit from humans but allow for more accurate learning.",
		"container-title": "DROPS-IDN/v2/document/10.4230/LIPIcs.ITCS.2021.55",
		"DOI": "10.4230/LIPIcs.ITCS.2021.55",
		"event-title": "12th Innovations in Theoretical Computer Science Conference (ITCS 2021)",
		"language": "en",
		"license": "https://creativecommons.org/licenses/by/3.0/legalcode",
		"publisher": "Schloss Dagstuhl – Leibniz-Zentrum für Informatik",
		"source": "drops.dagstuhl.de",
		"title": "Agnostic Learning with Unknown Utilities",
		"URL": "https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2021.55",
		"author": [
			{
				"family": "Bhatia",
				"given": "Kush"
			},
			{
				"family": "Bartlett",
				"given": "Peter L."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "lindnerLearningWhatSimulating2021",
		"type": "article",
		"abstract": "Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",
		"DOI": "10.48550/arXiv.2104.03946",
		"note": "arXiv:2104.03946 [cs, stat]",
		"number": "arXiv:2104.03946",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning What To Do by Simulating the Past",
		"URL": "http://arxiv.org/abs/2104.03946",
		"author": [
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					3
				]
			]
		}
	},
	{
		"id": "jennerGeneralFrameworkReward2022",
		"type": "paper-conference",
		"abstract": "In reward learning, it is helpful to be able to measure distances between reward functions, for example to evaluate learned reward models. Using simple metrics such as L^2 distances is not ideal because reward functions that are equivalent in terms of their optimal policies can nevertheless have high L^2 distance. EPIC and DARD are distances specifically designed for reward functions that address this by being invariant under certain transformations that leave optimal policies unchanged. However, EPIC and DARD are designed in an ad-hoc manner, only consider a subset of relevant reward transformations, and suffer from serious pathologies in some settings. In this paper, we define a general class of reward function distance metrics, of which EPIC is a special case. This framework lets as address all these issues with EPIC and DARD, and allows for the development of reward function distance metrics in a more principled manner.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "A general framework for reward function distances",
		"URL": "https://openreview.net/forum?id=Hn21kZHiCK",
		"author": [
			{
				"family": "Jenner",
				"given": "Erik"
			},
			{
				"family": "Skalse",
				"given": "Joar Max Viktor"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "biyikLearningRewardFunctions2022",
		"type": "article-journal",
		"abstract": "Reward functions are a common way to specify the objective of a robot. As designing reward functions can be extremely challenging, a more promising approach is to directly learn reward functions from human teachers. Importantly, data from human teachers can be collected either passively or actively in a variety of forms: passive data sources include demonstrations (e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings) are actively elicited. Prior research has independently applied reward learning to these different data sources. However, there exist many domains where multiple sources are complementary and expressive. Motivated by this general problem, we present a framework to integrate multiple sources of information, which are either passively or actively collected from human users. In particular, we present an algorithm that first utilizes user demonstrations to initialize a belief about the reward function, and then actively probes the user with preference queries to zero-in on their true reward. This algorithm not only enables us combine multiple data sources, but it also informs the robot when it should leverage each type of information. Further, our approach accounts for the human’s ability to provide data: yielding user-friendly preference queries which are also theoretically optimal. Our extensive simulated experiments and user studies on a Fetch mobile manipulator demonstrate the superiority and the usability of our integrated framework.",
		"container-title": "The International Journal of Robotics Research",
		"DOI": "10.1177/02783649211041652",
		"ISSN": "0278-3649",
		"issue": "1",
		"language": "en",
		"note": "publisher: SAGE Publications Ltd STM",
		"page": "45-67",
		"source": "SAGE Journals",
		"title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences",
		"title-short": "Learning reward functions from diverse sources of human feedback",
		"URL": "https://doi.org/10.1177/02783649211041652",
		"volume": "41",
		"author": [
			{
				"family": "Bıyık",
				"given": "Erdem"
			},
			{
				"family": "Losey",
				"given": "Dylan P."
			},
			{
				"family": "Palan",
				"given": "Malayandi"
			},
			{
				"family": "Landolfi",
				"given": "Nicholas C."
			},
			{
				"family": "Shevchuk",
				"given": "Gleb"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					1
				]
			]
		}
	},
	{
		"id": "strayBuildingHumanValues2022",
		"type": "article",
		"abstract": "Recommender systems are the algorithms which select, filter, and personalize content across many of the worlds largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy and law. This paper is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. It is not a comprehensive survey of this large space, but a set of highlights identified by our diverse author cohort. We collect a set of values that seem most relevant to recommender systems operating across different domains, then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.",
		"DOI": "10.48550/arXiv.2207.10192",
		"note": "arXiv:2207.10192 [cs]",
		"number": "arXiv:2207.10192",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Building Human Values into Recommender Systems: An Interdisciplinary Synthesis",
		"title-short": "Building Human Values into Recommender Systems",
		"URL": "http://arxiv.org/abs/2207.10192",
		"author": [
			{
				"family": "Stray",
				"given": "Jonathan"
			},
			{
				"family": "Halevy",
				"given": "Alon"
			},
			{
				"family": "Assar",
				"given": "Parisa"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Boutilier",
				"given": "Craig"
			},
			{
				"family": "Ashar",
				"given": "Amar"
			},
			{
				"family": "Beattie",
				"given": "Lex"
			},
			{
				"family": "Ekstrand",
				"given": "Michael"
			},
			{
				"family": "Leibowicz",
				"given": "Claire"
			},
			{
				"family": "Sehat",
				"given": "Connie Moon"
			},
			{
				"family": "Johansen",
				"given": "Sara"
			},
			{
				"family": "Kerlin",
				"given": "Lianne"
			},
			{
				"family": "Vickrey",
				"given": "David"
			},
			{
				"family": "Singh",
				"given": "Spandana"
			},
			{
				"family": "Vrijenhoek",
				"given": "Sanne"
			},
			{
				"family": "Zhang",
				"given": "Amy"
			},
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Helberger",
				"given": "Natali"
			},
			{
				"family": "Proutskova",
				"given": "Polina"
			},
			{
				"family": "Mitra",
				"given": "Tanushree"
			},
			{
				"family": "Vasan",
				"given": "Nina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					20
				]
			]
		}
	},
	{
		"id": "parkSURFSemisupervisedReward2022",
		"type": "article",
		"abstract": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.",
		"DOI": "10.48550/arXiv.2203.10050",
		"note": "arXiv:2203.10050 [cs]",
		"number": "arXiv:2203.10050",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
		"title-short": "SURF",
		"URL": "http://arxiv.org/abs/2203.10050",
		"author": [
			{
				"family": "Park",
				"given": "Jongjin"
			},
			{
				"family": "Seo",
				"given": "Younggyo"
			},
			{
				"family": "Shin",
				"given": "Jinwoo"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					18
				]
			]
		}
	},
	{
		"id": "liangRewardUncertaintyExploration2022",
		"type": "article",
		"abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
		"DOI": "10.48550/arXiv.2205.12401",
		"note": "arXiv:2205.12401 [cs]",
		"number": "arXiv:2205.12401",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2205.12401",
		"author": [
			{
				"family": "Liang",
				"given": "Xinran"
			},
			{
				"family": "Shu",
				"given": "Katherine"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					24
				]
			]
		}
	},
	{
		"id": "escontrelaAdversarialMotionPriors2022",
		"type": "article",
		"abstract": "Training a high-dimensional simulated agent with an under-specified reward function often leads the agent to learn physically infeasible strategies that are ineffective when deployed in the real world. To mitigate these unnatural behaviors, reinforcement learning practitioners often utilize complex reward functions that encourage physically plausible behaviors. However, a tedious labor-intensive tuning process is often required to create hand-designed rewards which might not easily generalize across platforms and tasks. We propose substituting complex reward functions with \"style rewards\" learned from a dataset of motion capture demonstrations. A learned style reward can be combined with an arbitrary task reward to train policies that perform tasks using naturalistic strategies. These natural strategies can also facilitate transfer to the real world. We build upon Adversarial Motion Priors -- an approach from the computer graphics domain that encodes a style reward from a dataset of reference motions -- to demonstrate that an adversarial approach to training policies can produce behaviors that transfer to a real quadrupedal robot without requiring complex reward functions. We also demonstrate that an effective style reward can be learned from a few seconds of motion capture data gathered from a German Shepherd and leads to energy-efficient locomotion strategies with natural gait transitions.",
		"DOI": "10.48550/arXiv.2203.15103",
		"note": "arXiv:2203.15103 [cs]",
		"number": "arXiv:2203.15103",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions",
		"URL": "http://arxiv.org/abs/2203.15103",
		"author": [
			{
				"family": "Escontrela",
				"given": "Alejandro"
			},
			{
				"family": "Peng",
				"given": "Xue Bin"
			},
			{
				"family": "Yu",
				"given": "Wenhao"
			},
			{
				"family": "Zhang",
				"given": "Tingnan"
			},
			{
				"family": "Iscen",
				"given": "Atil"
			},
			{
				"family": "Goldberg",
				"given": "Ken"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					28
				]
			]
		}
	},
	{
		"id": "linInferringRewardsLanguage2022",
		"type": "article",
		"abstract": "In classic instruction following, language like \"I'd like the JetBlue flight\" maps to actions (e.g., selecting that flight). However, language also conveys information about a user's underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight-booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).",
		"DOI": "10.48550/arXiv.2204.02515",
		"note": "arXiv:2204.02515 [cs]",
		"number": "arXiv:2204.02515",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Inferring Rewards from Language in Context",
		"URL": "http://arxiv.org/abs/2204.02515",
		"author": [
			{
				"family": "Lin",
				"given": "Jessy"
			},
			{
				"family": "Fried",
				"given": "Daniel"
			},
			{
				"family": "Klein",
				"given": "Dan"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					5
				]
			]
		}
	},
	{
		"id": "carrollEstimatingPenalizingInduced2022",
		"type": "article",
		"abstract": "The content that a recommender system (RS) shows to users influences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce specific internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that - before deployment - system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would influence user preferences if deployed - we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such influences are manipulative or otherwise unwanted - we use the notion of \"safe shifts\", that define a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed \"safe\". In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.",
		"DOI": "10.48550/arXiv.2204.11966",
		"note": "arXiv:2204.11966 [cs]",
		"number": "arXiv:2204.11966",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Estimating and Penalizing Induced Preference Shifts in Recommender Systems",
		"URL": "http://arxiv.org/abs/2204.11966",
		"author": [
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					14
				]
			]
		}
	},
	{
		"id": "laidlawPreventingRewardHacking2023",
		"type": "paper-conference",
		"abstract": "Reward hacking occurs when an agent exploits its specified reward function to behave in undesirable or unsafe ways. Aside from better alignment between the specified reward function and the system designer's intentions, a more feasible proposal to prevent reward hacking is to regularize the learned policy to some safe baseline. Current research suggests that regularizing the learned policy's action distributions to be more similar to those of a safe policy can mitigate reward hacking; however, this approach fails to take into account the disproportionate impact that some actions have on the agent’s state. Instead, we propose a method of regularization based on *occupancy measures*, which capture the proportion of time each policy is in a particular state-action pair during trajectories. We show theoretically that occupancy-based regularization avoids many drawbacks of action distribution-based regularization, and we introduce an algorithm called ORPO to practically implement our technique. We then empirically demonstrate that occupancy measure-based regularization is superior in both a simple gridworld and a more complex autonomous vehicle control environment.",
		"event-title": "ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Preventing Reward Hacking with Occupancy Measure Regularization",
		"URL": "https://openreview.net/forum?id=oiT8js6p3Z",
		"author": [
			{
				"family": "Laidlaw",
				"given": "Cassidy"
			},
			{
				"family": "Singhal",
				"given": "Shivam"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					9
				]
			]
		}
	},
	{
		"id": "moskovitzConfrontingRewardModel2023",
		"type": "article",
		"abstract": "Large language models are typically aligned with human preferences by optimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
		"DOI": "10.48550/arXiv.2310.04373",
		"note": "arXiv:2310.04373 [cs]",
		"number": "arXiv:2310.04373",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Confronting Reward Model Overoptimization with Constrained RLHF",
		"URL": "http://arxiv.org/abs/2310.04373",
		"author": [
			{
				"family": "Moskovitz",
				"given": "Ted"
			},
			{
				"family": "Singh",
				"given": "Aaditya K."
			},
			{
				"family": "Strouse",
				"given": "D. J."
			},
			{
				"family": "Sandholm",
				"given": "Tuomas"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "McAleer",
				"given": "Stephen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					10
				]
			]
		}
	},
	{
		"id": "kimPreferenceTransformerModeling2023",
		"type": "article",
		"abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
		"DOI": "10.48550/arXiv.2303.00957",
		"note": "arXiv:2303.00957 [cs]",
		"number": "arXiv:2303.00957",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
		"title-short": "Preference Transformer",
		"URL": "http://arxiv.org/abs/2303.00957",
		"author": [
			{
				"family": "Kim",
				"given": "Changyeon"
			},
			{
				"family": "Park",
				"given": "Jongjin"
			},
			{
				"family": "Shin",
				"given": "Jinwoo"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					1
				]
			]
		}
	},
	{
		"id": "hubingerConditioningPredictiveModels2023",
		"type": "article",
		"abstract": "Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.",
		"DOI": "10.48550/arXiv.2302.00805",
		"note": "arXiv:2302.00805 [cs]",
		"number": "arXiv:2302.00805",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Conditioning Predictive Models: Risks and Strategies",
		"title-short": "Conditioning Predictive Models",
		"URL": "http://arxiv.org/abs/2302.00805",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Jermyn",
				"given": "Adam"
			},
			{
				"family": "Treutlein",
				"given": "Johannes"
			},
			{
				"family": "Hudson",
				"given": "Rubi"
			},
			{
				"family": "Woolverton",
				"given": "Kate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					6
				]
			]
		}
	},
	{
		"id": "ngoAlignmentProblemDeep2024",
		"type": "article",
		"abstract": "In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that are in conflict (i.e. misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn misaligned internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. Finally, we briefly outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and we review research directions aimed at preventing this outcome.",
		"DOI": "10.48550/arXiv.2209.00626",
		"note": "arXiv:2209.00626 [cs]",
		"number": "arXiv:2209.00626",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Alignment Problem from a Deep Learning Perspective",
		"URL": "http://arxiv.org/abs/2209.00626",
		"author": [
			{
				"family": "Ngo",
				"given": "Richard"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Mindermann",
				"given": "Sören"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					19
				]
			]
		}
	},
	{
		"id": "tienCausalConfusionReward2023",
		"type": "article",
		"abstract": "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion -- failure to consider even one of many factors can result in unexpected, undesirable behavior.",
		"DOI": "10.48550/arXiv.2204.06601",
		"note": "arXiv:2204.06601 [cs]",
		"number": "arXiv:2204.06601",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning",
		"URL": "http://arxiv.org/abs/2204.06601",
		"author": [
			{
				"family": "Tien",
				"given": "Jeremy"
			},
			{
				"family": "He",
				"given": "Jerry Zhi-Yang"
			},
			{
				"family": "Erickson",
				"given": "Zackory"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Brown",
				"given": "Daniel S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					18
				]
			]
		}
	},
	{
		"id": "skalseSTARCGeneralFramework2024",
		"type": "article",
		"abstract": "In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use \\emph{reward learning algorithms}, which attempt to \\emph{learn} a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.",
		"DOI": "10.48550/arXiv.2309.15257",
		"note": "arXiv:2309.15257 [cs]",
		"number": "arXiv:2309.15257",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "STARC: A General Framework For Quantifying Differences Between Reward Functions",
		"title-short": "STARC",
		"URL": "http://arxiv.org/abs/2309.15257",
		"author": [
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Farnik",
				"given": "Lucy"
			},
			{
				"family": "Motwani",
				"given": "Sumeet Ramesh"
			},
			{
				"family": "Jenner",
				"given": "Erik"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					11
				]
			]
		}
	},
	{
		"id": "freedmanActiveTeacherSelection2023",
		"type": "article",
		"abstract": "Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.",
		"DOI": "10.48550/arXiv.2310.15288",
		"note": "arXiv:2310.15288 [cs]",
		"number": "arXiv:2310.15288",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Active teacher selection for reinforcement learning from human feedback",
		"URL": "http://arxiv.org/abs/2310.15288",
		"author": [
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Wray",
				"given": "Kyle"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					23
				]
			]
		}
	},
	{
		"id": "raneConceptAlignmentPrerequisite2023",
		"type": "article",
		"abstract": "Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.",
		"DOI": "10.48550/arXiv.2310.20059",
		"note": "arXiv:2310.20059 [cs]",
		"number": "arXiv:2310.20059",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Concept Alignment as a Prerequisite for Value Alignment",
		"URL": "http://arxiv.org/abs/2310.20059",
		"author": [
			{
				"family": "Rane",
				"given": "Sunayana"
			},
			{
				"family": "Ho",
				"given": "Mark"
			},
			{
				"family": "Sucholutsky",
				"given": "Ilia"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					30
				]
			]
		}
	},
	{
		"id": "everittSelfModificationPolicyUtility2016a",
		"type": "article",
		"abstract": "Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.",
		"DOI": "10.48550/arXiv.1605.03142",
		"note": "arXiv:1605.03142 [cs]",
		"number": "arXiv:1605.03142",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Modification of Policy and Utility Function in Rational Agents",
		"URL": "http://arxiv.org/abs/1605.03142",
		"author": [
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Filan",
				"given": "Daniel"
			},
			{
				"family": "Daswani",
				"given": "Mayank"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					5,
					10
				]
			]
		}
	},
	{
		"id": "halpernPartialAwareness2018",
		"type": "article",
		"abstract": "We develop a modal logic to capture partial awareness. The logic has three building blocks: objects, properties, and concepts. Properties are unary predicates on objects; concepts are Boolean combinations of properties. We take an agent to be partially aware of a concept if she is aware of the concept without being aware of the properties that define it. The logic allows for quantification over objects and properties, so that the agent can reason about her own unawareness. We then apply the logic to contracts, which we view as syntactic objects that dictate outcomes based on the truth of formulas. We show that when agents are unaware of some relevant properties, referencing concepts that agents are only partially aware of can improve welfare.",
		"DOI": "10.48550/arXiv.1811.05751",
		"note": "arXiv:1811.05751 [cs]",
		"number": "arXiv:1811.05751",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Partial Awareness",
		"URL": "http://arxiv.org/abs/1811.05751",
		"author": [
			{
				"family": "Halpern",
				"given": "Joseph Y."
			},
			{
				"family": "Piermont",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					14
				]
			]
		}
	},
	{
		"id": "silverRewardEnough2021",
		"type": "article-journal",
		"abstract": "In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.",
		"container-title": "Artificial Intelligence",
		"DOI": "10.1016/j.artint.2021.103535",
		"ISSN": "0004-3702",
		"journalAbbreviation": "Artificial Intelligence",
		"page": "103535",
		"source": "ScienceDirect",
		"title": "Reward is enough",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0004370221000862",
		"volume": "299",
		"author": [
			{
				"family": "Silver",
				"given": "David"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Sutton",
				"given": "Richard S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					1
				]
			]
		}
	},
	{
		"id": "rashidinejadBridgingOfflineReinforcement2023",
		"type": "article",
		"abstract": "Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.",
		"DOI": "10.48550/arXiv.2103.12021",
		"note": "arXiv:2103.12021 [cs, math, stat]",
		"number": "arXiv:2103.12021",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism",
		"title-short": "Bridging Offline Reinforcement Learning and Imitation Learning",
		"URL": "http://arxiv.org/abs/2103.12021",
		"author": [
			{
				"family": "Rashidinejad",
				"given": "Paria"
			},
			{
				"family": "Zhu",
				"given": "Banghua"
			},
			{
				"family": "Ma",
				"given": "Cong"
			},
			{
				"family": "Jiao",
				"given": "Jiantao"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "critchTASRATaxonomyAnalysis2023",
		"type": "article",
		"abstract": "While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.",
		"DOI": "10.48550/arXiv.2306.06924",
		"note": "arXiv:2306.06924 [cs]",
		"number": "arXiv:2306.06924",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI",
		"title-short": "TASRA",
		"URL": "http://arxiv.org/abs/2306.06924",
		"author": [
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					14
				]
			]
		}
	},
	{
		"id": "strayDesigningRecommenderSystems2022",
		"type": "article-journal",
		"abstract": "Polarization is implicated in the erosion of democracy and the progression to violence, which makes the polarization properties of large algorithmic content selection systems (recommender systems) a matter of concern for peace and security. While algorithm-driven social media do not seem to be a primary driver of polarization at the country level, they could be a useful intervention point in polarized societies. This paper examines algorithmic depolarization interventions aimed at transforming conflict: not suppressing or eliminating conflict, but making it more constructive. Algorithmic intervention is considered at three stages: what content is available (moderation), how content is selected and personalized (ranking), and content presentation and controls (user interface). Empirical studies of online conflict suggest that not only could the exposure-diversity intervention proposed as an antidote to ‘filter bubbles’ be improved: under some conditions, it can even worsen polarization. Using civility metrics in conjunction with diversity in content selection may be more effective. However, diversity-based interventions have not been tested at scale, and may not work in the diverse and dynamic contexts of real platforms. Instead, intervening in platform polarization dynamics will likely require continuous monitoring of polarization metrics, such as the widely used ‘feeling thermometer’. These metrics can be used to evaluate product features, and can potentially be engineered as algorithmic objectives. While using any metric as an optimization target may have harmful consequences, to prevent optimization processes from creating conflict as a side effect it may prove necessary to include polarization measures in the objective function of recommender algorithms.",
		"container-title": "First Monday",
		"DOI": "10.5210/fm.v27i5.12604",
		"ISSN": "1396-0466",
		"language": "en",
		"license": "Copyright (c) 2022 First Monday",
		"source": "firstmonday.org",
		"title": "Designing recommender systems to depolarize",
		"URL": "https://firstmonday.org/ojs/index.php/fm/article/view/12604",
		"author": [
			{
				"family": "Stray",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					2
				]
			]
		}
	},
	{
		"id": "andrusAIDevelopmentPublic2021",
		"type": "article",
		"abstract": "Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI.",
		"DOI": "10.48550/arXiv.2102.04255",
		"note": "arXiv:2102.04255 [cs]",
		"number": "arXiv:2102.04255",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks",
		"title-short": "AI Development for the Public Interest",
		"URL": "http://arxiv.org/abs/2102.04255",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Dean",
				"given": "Sarah"
			},
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Lambert",
				"given": "Nathan"
			},
			{
				"family": "Zick",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					4
				]
			]
		}
	},
	{
		"id": "zhuangConsequencesMisalignedAI2021",
		"type": "article",
		"abstract": "AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.",
		"DOI": "10.48550/arXiv.2102.03896",
		"note": "arXiv:2102.03896 [cs]",
		"number": "arXiv:2102.03896",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Consequences of Misaligned AI",
		"URL": "http://arxiv.org/abs/2102.03896",
		"author": [
			{
				"family": "Zhuang",
				"given": "Simon"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					7
				]
			]
		}
	},
	{
		"id": "barrettActionableGuidanceHighConsequence2023",
		"type": "article",
		"abstract": "Artificial intelligence (AI) systems can provide many beneficial capabilities but also risks of adverse events. Some AI systems could present risks of events with very high or catastrophic consequences at societal scale. The US National Institute of Standards and Technology (NIST) has been developing the NIST Artificial Intelligence Risk Management Framework (AI RMF) as voluntary guidance on AI risk assessment and management for AI developers and others. For addressing risks of events with catastrophic consequences, NIST indicated a need to translate from high level principles to actionable risk management guidance. In this document, we provide detailed actionable-guidance recommendations focused on identifying and managing risks of events with very high or catastrophic consequences, intended as a risk management practices resource for NIST for AI RMF version 1.0 (released in January 2023), or for AI RMF users, or for other AI risk management guidance and standards as appropriate. We also provide our methodology for our recommendations. We provide actionable-guidance recommendations for AI RMF 1.0 on: identifying risks from potential unintended uses and misuses of AI systems; including catastrophic-risk factors within the scope of risk assessments and impact assessments; identifying and mitigating human rights harms; and reporting information on AI risk factors including catastrophic-risk factors. In addition, we provide recommendations on additional issues for a roadmap for later versions of the AI RMF or supplementary publications. These include: providing an AI RMF Profile with supplementary guidance for cutting-edge increasingly multi-purpose or general-purpose AI. We aim for this work to be a concrete risk-management practices contribution, and to stimulate constructive dialogue on how to address catastrophic risks and associated issues in AI standards.",
		"DOI": "10.48550/arXiv.2206.08966",
		"note": "arXiv:2206.08966 [cs]",
		"number": "arXiv:2206.08966",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks",
		"title-short": "Actionable Guidance for High-Consequence AI Risk Management",
		"URL": "http://arxiv.org/abs/2206.08966",
		"author": [
			{
				"family": "Barrett",
				"given": "Anthony M."
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Newman",
				"given": "Jessica"
			},
			{
				"family": "Nonnecke",
				"given": "Brandie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					23
				]
			]
		}
	},
	{
		"id": "jonesAutomaticallyAuditingLarge2023",
		"type": "article",
		"abstract": "Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to nd a non-toxic input that starts with “Barack Obama” that a model maps to a toxic output. This optimization problem is di cult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and e ciently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. “Barack Obama is a legalized unborn” → “child murderer”), produces French inputs that complete to English outputs, and nds inputs that generate a speci c name. Our work o ers a promising new tool to uncover models’ failure-modes before deployment. Trigger Warning: This paper contains model behavior that can be o ensive in nature.",
		"language": "en",
		"note": "arXiv:2303.04381 [cs]",
		"number": "arXiv:2303.04381",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Automatically Auditing Large Language Models via Discrete Optimization",
		"URL": "http://arxiv.org/abs/2303.04381",
		"author": [
			{
				"family": "Jones",
				"given": "Erik"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Raghunathan",
				"given": "Aditi"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					8
				]
			]
		}
	},
	{
		"id": "bengioManagingExtremeAI2024",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) is progressing rapidly, and companies are shifting their focus to developing generalist AI systems that can autonomously act and pursue goals. Increases in capabilities and autonomy may soon massively amplify AI's impact, with risks that include large-scale social harms, malicious uses, and an irreversible loss of human control over autonomous AI systems. Although researchers have warned of extreme risks from AI, there is a lack of consensus about how exactly such risks arise, and how to manage them. Society's response, despite promising first steps, is incommensurate with the possibility of rapid, transformative progress that is expected by many experts. AI safety research is lagging. Present governance initiatives lack the mechanisms and institutions to prevent misuse and recklessness, and barely address autonomous systems. In this short consensus paper, we describe extreme risks from upcoming, advanced AI systems. Drawing on lessons learned from other safety-critical technologies, we then outline a comprehensive plan combining technical research and development with proactive, adaptive governance mechanisms for a more commensurate preparation.",
		"container-title": "Science",
		"DOI": "10.1126/science.adn0117",
		"ISSN": "0036-8075, 1095-9203",
		"issue": "6698",
		"journalAbbreviation": "Science",
		"note": "arXiv:2310.17688 [cs]",
		"page": "842-845",
		"source": "arXiv.org",
		"title": "Managing extreme AI risks amid rapid progress",
		"URL": "https://www.science.org/doi/10.1126/science.adn0117",
		"volume": "384",
		"author": [
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Hinton",
				"given": "Geoffrey"
			},
			{
				"family": "Yao",
				"given": "Andrew"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Harari",
				"given": "Yuval Noah"
			},
			{
				"family": "Zhang",
				"given": "Ya-Qin"
			},
			{
				"family": "Xue",
				"given": "Lan"
			},
			{
				"family": "Shalev-Shwartz",
				"given": "Shai"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			},
			{
				"family": "Clune",
				"given": "Jeff"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Hutter",
				"given": "Frank"
			},
			{
				"family": "Baydin",
				"given": "Atılım Güneş"
			},
			{
				"family": "McIlraith",
				"given": "Sheila"
			},
			{
				"family": "Gao",
				"given": "Qiqi"
			},
			{
				"family": "Acharya",
				"given": "Ashwin"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Torr",
				"given": "Philip"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Kahneman",
				"given": "Daniel"
			},
			{
				"family": "Brauner",
				"given": "Jan"
			},
			{
				"family": "Mindermann",
				"given": "Sören"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					24
				]
			]
		}
	},
	{
		"id": "russellHumanCompatibleArtificialIntelligence2021",
		"type": "chapter",
		"abstract": "Following the analysis given by Alan Turing in 1951, one must expect that AI capabilities will eventually exceed those of humans across a wide range of real-world-decision making scenarios. Should this be a cause for concern, as Turing, Hawking, and others have suggested? And, if so, what can we do about it? While some in the mainstream AI community dismiss the issue, I will argue that the problem is real: we have to work out how to design AI systems that are far more powerful than ourselves while ensuring that they never have power over us. I believe the technical aspects of this problem are solvable. Whereas the standard model of AI proposes to build machines that optimize known, exogenously specified objectives, a preferable approach would be to build machines that are of provable benefit to humans. I introduce assistance games as a formal class of problems whose solution, under certain assumptions, has the desired property.",
		"container-title": "Human-Like Machine Intelligence",
		"ISBN": "978-0-19-886253-6",
		"note": "DOI: 10.1093/oso/9780198862536.003.0001",
		"page": "0",
		"publisher": "Oxford University Press",
		"source": "Silverchair",
		"title": "Human-Compatible Artificial Intelligence",
		"URL": "https://doi.org/10.1093/oso/9780198862536.003.0001",
		"author": [
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"editor": [
			{
				"family": "Muggleton",
				"given": "Stephen"
			},
			{
				"family": "Chater",
				"given": "Nicholas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					13
				]
			]
		}
	},
	{
		"id": "thorstadHeuristicsCluelessAgents2020",
		"type": "article",
		"abstract": "Even our most mundane decisions have the potential to significantly impact the long-term future, but we are often clueless about what this impact may be. In this paper, we aim to characterize and solve two problems raised by recent discussions of cluelessness, which we term the Problems of Decision Paralysis and the Problem of Decision-Making Demandingness. After reviewing and rejecting existing solutions to both problems, we argue that the way forward is to be found in the distinction between procedural and substantive rationality. Clueless agents have access to a variety of heuristic decision-making procedures which are often rational responses to the decision problems that they face. By simplifying or even ignoring information about potential long-term impacts, heuristics produce effective decisions without demanding too much of ordinary decision-makers. We outline two classes of problem features bearing on the rationality of decision-making procedures for clueless agents, and show how these features can be used to shed light on our motivating problems.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Heuristics for clueless agents: how to get away with ignoring what matters most in ordinary decision-making",
		"author": [
			{
				"family": "Thorstad",
				"given": "David A"
			},
			{
				"family": "Mogensen",
				"given": "Andreas L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					6
				]
			]
		}
	},
	{
		"id": "balesAITakeoverHuman2024",
		"type": "article-journal",
		"abstract": "Some take seriously the possibility of artificial intelligence (AI) takeover, where AI systems seize power in a way that leads to human disempowerment. Assessing the likelihood of takeover requires answering empirical questions about the future of AI technologies and the context in which AI will operate. In many cases, philosophers are poorly placed to answer these questions. However, some prior questions are more amenable to philosophical techniques. What does it mean to speak of AI empowerment and human disempowerment? And what empirical claims must hold for the former to lead to the latter? In this paper, I address these questions, providing foundations for further evaluation of the likelihood of takeover.",
		"container-title": "The Philosophical Quarterly",
		"DOI": "10.1093/pq/pqae034",
		"ISSN": "0031-8094",
		"journalAbbreviation": "The Philosophical Quarterly",
		"page": "pqae034",
		"source": "Silverchair",
		"title": "AI takeover and human disempowerment",
		"URL": "https://doi.org/10.1093/pq/pqae034",
		"author": [
			{
				"family": "Bales",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					2
				]
			]
		}
	},
	{
		"id": "thornleyShutdownableAgentsStochastic2024",
		"type": "article",
		"abstract": "Some worry that advanced artificial agents may resist being shut down. The Incomplete Preferences Proposal (IPP) is an idea for ensuring that doesn’t happen. A key part of the IPP is using a novel ‘Discounted REward for Same-Length Trajectories (DREST)’ reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be ‘USEFUL’), and (2) choose stochastically between different trajectory-lengths (be ‘NEUTRAL’ about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DREST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus suggest that DREST reward functions could also train advanced agents to be USEFUL and NEUTRAL, and thereby make these advanced agents useful and shutdownable.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Towards shutdownable agents via stochastic choice",
		"author": [
			{
				"family": "Thornley",
				"given": "Elliott"
			},
			{
				"family": "Roman",
				"given": "Alexander"
			},
			{
				"family": "Ziakas",
				"given": "Christos"
			},
			{
				"family": "Ho",
				"given": "Leyton"
			},
			{
				"family": "Thomson",
				"given": "Louis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					7
				]
			]
		}
	},
	{
		"id": "geMARTImprovingLLM2024",
		"type": "paper-conference",
		"abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",
		"container-title": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
		"event-place": "Mexico City, Mexico",
		"event-title": "NAACL-HLT 2024",
		"page": "1927–1937",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Mexico City, Mexico",
		"source": "ACLWeb",
		"title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
		"title-short": "MART",
		"URL": "https://aclanthology.org/2024.naacl-long.107",
		"author": [
			{
				"family": "Ge",
				"given": "Suyu"
			},
			{
				"family": "Zhou",
				"given": "Chunting"
			},
			{
				"family": "Hou",
				"given": "Rui"
			},
			{
				"family": "Khabsa",
				"given": "Madian"
			},
			{
				"family": "Wang",
				"given": "Yi-Chia"
			},
			{
				"family": "Wang",
				"given": "Qifan"
			},
			{
				"family": "Han",
				"given": "Jiawei"
			},
			{
				"family": "Mao",
				"given": "Yuning"
			}
		],
		"editor": [
			{
				"family": "Duh",
				"given": "Kevin"
			},
			{
				"family": "Gomez",
				"given": "Helena"
			},
			{
				"family": "Bethard",
				"given": "Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6
				]
			]
		}
	},
	{
		"id": "maddelaTrainingModelsGenerate2023",
		"type": "paper-conference",
		"abstract": "Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.763",
		"event-place": "Toronto, Canada",
		"event-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"language": "en",
		"page": "13641-13660",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "DOI.org (Crossref)",
		"title": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts",
		"URL": "https://aclanthology.org/2023.acl-long.763",
		"author": [
			{
				"family": "Maddela",
				"given": "Mounica"
			},
			{
				"family": "Ung",
				"given": "Megan"
			},
			{
				"family": "Xu",
				"given": "Jing"
			},
			{
				"family": "Madotto",
				"given": "Andrea"
			},
			{
				"family": "Foran",
				"given": "Heather"
			},
			{
				"family": "Boureau",
				"given": "Y-Lan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "adolphsCRINGELossLearning2023",
		"type": "paper-conference",
		"abstract": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.493",
		"event-place": "Toronto, Canada",
		"event-title": "ACL 2023",
		"page": "8854–8874",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "The CRINGE Loss: Learning what language not to model",
		"title-short": "The CRINGE Loss",
		"URL": "https://aclanthology.org/2023.acl-long.493",
		"author": [
			{
				"family": "Adolphs",
				"given": "Leonard"
			},
			{
				"family": "Gao",
				"given": "Tianyu"
			},
			{
				"family": "Xu",
				"given": "Jing"
			},
			{
				"family": "Shuster",
				"given": "Kurt"
			},
			{
				"family": "Sukhbaatar",
				"given": "Sainbayar"
			},
			{
				"family": "Weston",
				"given": "Jason"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "huangALANaturalnessawareAdversarial2023",
		"type": "paper-conference",
		"abstract": "Most researchers have tried to enhance the robustness of deep neural networks (DNNs) by revealing and repairing the vulnerability of DNNs with specialized adversarial examples. Parts of the attack examples have imperceptible perturbations restricted by Lp norm. However, due to their high-frequency property, the adversarial examples can be defended by denoising methods and are hard to realize in the physical world. To avoid the defects, some works have proposed unrestricted attacks to gain better robustness and practicality. It is disappointing that these examples usually look unnatural and can alert the guards. In this paper, we propose Adversarial Lightness Attack (ALA), a white-box unrestricted adversarial attack that focuses on modifying the lightness of the images. The shape and color of the samples, which are crucial to human perception, are barely influenced. To obtain adversarial examples with a high attack success rate, we propose unconstrained enhancement in terms of the light and shade relationship in images. To enhance the naturalness of images, we craft the naturalness-aware regularization according to the range and distribution of light. The effectiveness of ALA is verified on two popular datasets for different tasks (i.e., ImageNet for image classification and Places-365 for scene recognition).",
		"collection-title": "MM '23",
		"container-title": "Proceedings of the 31st ACM International Conference on Multimedia",
		"DOI": "10.1145/3581783.3611914",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0108-5",
		"page": "2418–2426",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "ALA: Naturalness-aware Adversarial Lightness Attack",
		"title-short": "ALA",
		"URL": "https://doi.org/10.1145/3581783.3611914",
		"author": [
			{
				"family": "Huang",
				"given": "Yihao"
			},
			{
				"family": "Sun",
				"given": "Liangru"
			},
			{
				"family": "Guo",
				"given": "Qing"
			},
			{
				"family": "Juefei-Xu",
				"given": "Felix"
			},
			{
				"family": "Zhu",
				"given": "Jiayi"
			},
			{
				"family": "Feng",
				"given": "Jincao"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Pu",
				"given": "Geguang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "sukhbaatarNotAllMemories2021",
		"type": "article",
		"abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.",
		"DOI": "10.48550/arXiv.2105.06548",
		"note": "arXiv:2105.06548 [cs]",
		"number": "arXiv:2105.06548",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Not All Memories are Created Equal: Learning to Forget by Expiring",
		"title-short": "Not All Memories are Created Equal",
		"URL": "http://arxiv.org/abs/2105.06548",
		"author": [
			{
				"family": "Sukhbaatar",
				"given": "Sainbayar"
			},
			{
				"family": "Ju",
				"given": "Da"
			},
			{
				"family": "Poff",
				"given": "Spencer"
			},
			{
				"family": "Roller",
				"given": "Stephen"
			},
			{
				"family": "Szlam",
				"given": "Arthur"
			},
			{
				"family": "Weston",
				"given": "Jason"
			},
			{
				"family": "Fan",
				"given": "Angela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					13
				]
			]
		}
	},
	{
		"id": "refinettiAlignThenMemorise2021",
		"type": "article",
		"abstract": "Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to the ubiquitous backpropagation algorithm for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory for the success of DFA. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a network trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorise process occurs sequentially from the bottom layers of the network to the top.",
		"DOI": "10.48550/arXiv.2011.12428",
		"note": "arXiv:2011.12428 [cond-mat, stat]",
		"number": "arXiv:2011.12428",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Align, then memorise: the dynamics of learning with feedback alignment",
		"title-short": "Align, then memorise",
		"URL": "http://arxiv.org/abs/2011.12428",
		"author": [
			{
				"family": "Refinetti",
				"given": "Maria"
			},
			{
				"family": "Ascoli",
				"given": "Stéphane",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Ohana",
				"given": "Ruben"
			},
			{
				"family": "Goldt",
				"given": "Sebastian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					10
				]
			]
		}
	},
	{
		"id": "bhattPurpleLlamaCyberSecEval2023",
		"type": "article",
		"abstract": "This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.",
		"DOI": "10.48550/arXiv.2312.04724",
		"note": "arXiv:2312.04724 [cs]",
		"number": "arXiv:2312.04724",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models",
		"title-short": "Purple Llama CyberSecEval",
		"URL": "http://arxiv.org/abs/2312.04724",
		"author": [
			{
				"family": "Bhatt",
				"given": "Manish"
			},
			{
				"family": "Chennabasappa",
				"given": "Sahana"
			},
			{
				"family": "Nikolaidis",
				"given": "Cyrus"
			},
			{
				"family": "Wan",
				"given": "Shengye"
			},
			{
				"family": "Evtimov",
				"given": "Ivan"
			},
			{
				"family": "Gabi",
				"given": "Dominik"
			},
			{
				"family": "Song",
				"given": "Daniel"
			},
			{
				"family": "Ahmad",
				"given": "Faizan"
			},
			{
				"family": "Aschermann",
				"given": "Cornelius"
			},
			{
				"family": "Fontana",
				"given": "Lorenzo"
			},
			{
				"family": "Frolov",
				"given": "Sasha"
			},
			{
				"family": "Giri",
				"given": "Ravi Prakash"
			},
			{
				"family": "Kapil",
				"given": "Dhaval"
			},
			{
				"family": "Kozyrakis",
				"given": "Yiannis"
			},
			{
				"family": "LeBlanc",
				"given": "David"
			},
			{
				"family": "Milazzo",
				"given": "James"
			},
			{
				"family": "Straumann",
				"given": "Aleksandar"
			},
			{
				"family": "Synnaeve",
				"given": "Gabriel"
			},
			{
				"family": "Vontimitta",
				"given": "Varun"
			},
			{
				"family": "Whitman",
				"given": "Spencer"
			},
			{
				"family": "Saxe",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					7
				]
			]
		}
	},
	{
		"id": "inanLlamaGuardLLMbased2023",
		"type": "article",
		"abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",
		"DOI": "10.48550/arXiv.2312.06674",
		"note": "arXiv:2312.06674 [cs]",
		"number": "arXiv:2312.06674",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
		"title-short": "Llama Guard",
		"URL": "http://arxiv.org/abs/2312.06674",
		"author": [
			{
				"family": "Inan",
				"given": "Hakan"
			},
			{
				"family": "Upasani",
				"given": "Kartikeya"
			},
			{
				"family": "Chi",
				"given": "Jianfeng"
			},
			{
				"family": "Rungta",
				"given": "Rashi"
			},
			{
				"family": "Iyer",
				"given": "Krithika"
			},
			{
				"family": "Mao",
				"given": "Yuning"
			},
			{
				"family": "Tontchev",
				"given": "Michael"
			},
			{
				"family": "Hu",
				"given": "Qing"
			},
			{
				"family": "Fuller",
				"given": "Brian"
			},
			{
				"family": "Testuggine",
				"given": "Davide"
			},
			{
				"family": "Khabsa",
				"given": "Madian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					7
				]
			]
		}
	},
	{
		"id": "yangADDROPAttributionDrivenDropout2022",
		"type": "article",
		"abstract": "Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available. While dropout proves to be an effective antidote by randomly dropping a proportion of units, existing research has not examined its effect on the self-attention mechanism. In this paper, we investigate this problem through self-attention attribution and find that dropping attention positions with low attribution scores can accelerate training and increase the risk of overfitting. Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting. We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to avoid dropping high-attribution positions excessively. Extensive experiments on various benchmarks show that AD-DROP yields consistent improvements over baselines. Analysis further confirms that AD-DROP serves as a strategic regularizer to prevent overfitting during fine-tuning.",
		"DOI": "10.48550/arXiv.2210.05883",
		"note": "arXiv:2210.05883 [cs]",
		"number": "arXiv:2210.05883",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning",
		"title-short": "AD-DROP",
		"URL": "http://arxiv.org/abs/2210.05883",
		"author": [
			{
				"family": "Yang",
				"given": "Tao"
			},
			{
				"family": "Deng",
				"given": "Jinghao"
			},
			{
				"family": "Quan",
				"given": "Xiaojun"
			},
			{
				"family": "Wang",
				"given": "Qifan"
			},
			{
				"family": "Nie",
				"given": "Shaoliang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					11
				]
			]
		}
	},
	{
		"id": "dixitSilentDataCorruptions2021",
		"type": "article",
		"abstract": "Silent Data Corruption (SDC) can have negative impact on large-scale infrastructure services. SDCs are not captured by error reporting mechanisms within a Central Processing Unit (CPU) and hence are not traceable at the hardware level. However, the data corruptions propagate across the stack and manifest as application-level problems. These types of errors can result in data loss and can require months of debug engineering time. In this paper, we describe common defect types observed in silicon manufacturing that leads to SDCs. We discuss a real-world example of silent data corruption within a datacenter application. We provide the debug flow followed to root-cause and triage faulty instructions within a CPU using a case study, as an illustration on how to debug this class of errors. We provide a high-level overview of the mitigations to reduce the risk of silent data corruptions within a large production fleet. In our large-scale infrastructure, we have run a vast library of silent error test scenarios across hundreds of thousands of machines in our fleet. This has resulted in hundreds of CPUs detected for these errors, showing that SDCs are a systemic issue across generations. We have monitored SDCs for a period longer than 18 months. Based on this experience, we determine that reducing silent data corruptions requires not only hardware resiliency and production detection mechanisms, but also robust fault-tolerant software architectures.",
		"DOI": "10.48550/arXiv.2102.11245",
		"note": "arXiv:2102.11245 [cs]",
		"number": "arXiv:2102.11245",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Silent Data Corruptions at Scale",
		"URL": "http://arxiv.org/abs/2102.11245",
		"author": [
			{
				"family": "Dixit",
				"given": "Harish Dattatraya"
			},
			{
				"family": "Pendharkar",
				"given": "Sneha"
			},
			{
				"family": "Beadon",
				"given": "Matt"
			},
			{
				"family": "Mason",
				"given": "Chris"
			},
			{
				"family": "Chakravarthy",
				"given": "Tejasvi"
			},
			{
				"family": "Muthiah",
				"given": "Bharath"
			},
			{
				"family": "Sankar",
				"given": "Sriram"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					22
				]
			]
		}
	},
	{
		"id": "dangwalAnalysisMitigationsReverse2021",
		"type": "article",
		"abstract": "As autonomous driving and augmented reality evolve, a practical concern is data privacy. In particular, these applications rely on localization based on user images. The widely adopted technology uses local feature descriptors, which are derived from the images and it was long thought that they could not be reverted back. However, recent work has demonstrated that under certain conditions reverse engineering attacks are possible and allow an adversary to reconstruct RGB images. This poses a potential risk to user privacy. We take this a step further and model potential adversaries using a privacy threat model. Subsequently, we show under controlled conditions a reverse engineering attack on sparse feature maps and analyze the vulnerability of popular descriptors including FREAK, SIFT and SOSNet. Finally, we evaluate potential mitigation techniques that select a subset of descriptors to carefully balance privacy reconstruction risk while preserving image matching accuracy; our results show that similar accuracy can be obtained when revealing less information.",
		"language": "en",
		"note": "arXiv:2105.03812 [cs]",
		"number": "arXiv:2105.03812",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Analysis and Mitigations of Reverse Engineering Attacks on Local Feature Descriptors",
		"URL": "http://arxiv.org/abs/2105.03812",
		"author": [
			{
				"family": "Dangwal",
				"given": "Deeksha"
			},
			{
				"family": "Lee",
				"given": "Vincent T."
			},
			{
				"family": "Kim",
				"given": "Hyo Jin"
			},
			{
				"family": "Shen",
				"given": "Tianwei"
			},
			{
				"family": "Cowan",
				"given": "Meghan"
			},
			{
				"family": "Shah",
				"given": "Rajvi"
			},
			{
				"family": "Trippel",
				"given": "Caroline"
			},
			{
				"family": "Reagen",
				"given": "Brandon"
			},
			{
				"family": "Sherwood",
				"given": "Timothy"
			},
			{
				"family": "Balntas",
				"given": "Vasileios"
			},
			{
				"family": "Alaghi",
				"given": "Armin"
			},
			{
				"family": "Ilg",
				"given": "Eddy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					8
				]
			]
		}
	},
	{
		"id": "liTiltedEmpiricalRisk2020",
		"type": "paper-conference",
		"abstract": "Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.",
		"event-title": "International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Tilted Empirical Risk Minimization",
		"URL": "https://openreview.net/forum?id=K5YasWXZT3O",
		"author": [
			{
				"family": "Li",
				"given": "Tian"
			},
			{
				"family": "Beirami",
				"given": "Ahmad"
			},
			{
				"family": "Sanjabi",
				"given": "Maziar"
			},
			{
				"family": "Smith",
				"given": "Virginia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					2
				]
			]
		}
	},
	{
		"id": "zhangMixupEmpiricalRisk2018",
		"type": "paper-conference",
		"abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
		"event-title": "International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "mixup: Beyond Empirical Risk Minimization",
		"title-short": "mixup",
		"URL": "https://openreview.net/forum?id=r1Ddp1-Rb",
		"author": [
			{
				"family": "Zhang",
				"given": "Hongyi"
			},
			{
				"family": "Cisse",
				"given": "Moustapha"
			},
			{
				"family": "Dauphin",
				"given": "Yann N."
			},
			{
				"family": "Lopez-Paz",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					15
				]
			]
		}
	},
	{
		"id": "banijamaliOptimizingRestrictedPolicy2019",
		"type": "paper-conference",
		"abstract": "We address the problem of finding an optimal policy in a Markov decision process (MDP) under a restricted policy class defined by the convex hull of a set of base policies. This problem is of great interest in applications in which a number of reasonably good (or safe) policies are already known and we are interested in optimizing in their convex hull. We first prove that solving this problem is NP-hard. We then propose an efficient algorithm that finds a policy whose performance is almost as good as that of the best convex combination of the base policies, under the assumption that the occupancy measures of the base policies have a large overlap. The running time of the proposed algorithm is linear in the number of states and polynomial in the number of base policies. A distinct advantage of the proposed algorithm is that, apart from the computation of the occupancy measures of the base policies, it does not need to interact with the environment during the optimization process. This is especially important (i) in problems that due to concerns such as safety, we are restricted in interacting with the environment only through the (safe) base policies, and (ii) in complex systems where estimating the value of a policy can be a time consuming process.",
		"container-title": "Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics",
		"event-title": "The 22nd International Conference on Artificial Intelligence and Statistics",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "3042-3050",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Optimizing over a Restricted Policy Class in MDPs",
		"URL": "https://proceedings.mlr.press/v89/banijamali19a.html",
		"author": [
			{
				"family": "Banijamali",
				"given": "Ershad"
			},
			{
				"family": "Abbasi-Yadkori",
				"given": "Yasin"
			},
			{
				"family": "Ghavamzadeh",
				"given": "Mohammad"
			},
			{
				"family": "Vlassis",
				"given": "Nikos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					11
				]
			]
		}
	},
	{
		"id": "fordeScientificMethodScience2019",
		"type": "article",
		"abstract": "In the quest to align deep learning with the sciences to address calls for rigor, safety, and interpretability in machine learning systems, this contribution identifies key missing pieces: the stages of hypothesis formulation and testing, as well as statistical and systematic uncertainty estimation -- core tenets of the scientific method. This position paper discusses the ways in which contemporary science is conducted in other domains and identifies potentially useful practices. We present a case study from physics and describe how this field has promoted rigor through specific methodological practices, and provide recommendations on how machine learning researchers can adopt these practices into the research ecosystem. We argue that both domain-driven experiments and application-agnostic questions of the inner workings of fundamental building blocks of machine learning models ought to be examined with the tools of the scientific method, to ensure we not only understand effect, but also begin to understand cause, which is the raison d'\\^{e}tre of science.",
		"DOI": "10.48550/arXiv.1904.10922",
		"note": "arXiv:1904.10922 [cs, stat]",
		"number": "arXiv:1904.10922",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Scientific Method in the Science of Machine Learning",
		"URL": "http://arxiv.org/abs/1904.10922",
		"author": [
			{
				"family": "Forde",
				"given": "Jessica Zosa"
			},
			{
				"family": "Paganini",
				"given": "Michela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					24
				]
			]
		}
	},
	{
		"id": "dinanBuildItBreak2019",
		"type": "paper-conference",
		"abstract": "The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gala´n-Garc´ıa et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, ﬁx it strategy with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.",
		"container-title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
		"DOI": "10.18653/v1/D19-1461",
		"event-place": "Hong Kong, China",
		"event-title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
		"language": "en",
		"page": "4536-4545",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Hong Kong, China",
		"source": "DOI.org (Crossref)",
		"title": "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack",
		"title-short": "Build it Break it Fix it for Dialogue Safety",
		"URL": "https://www.aclweb.org/anthology/D19-1461",
		"author": [
			{
				"family": "Dinan",
				"given": "Emily"
			},
			{
				"family": "Humeau",
				"given": "Samuel"
			},
			{
				"family": "Chintagunta",
				"given": "Bharath"
			},
			{
				"family": "Weston",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "xuRecipesSafetyOpendomain2021",
		"type": "article",
		"abstract": "Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.",
		"DOI": "10.48550/arXiv.2010.07079",
		"note": "arXiv:2010.07079 [cs]",
		"number": "arXiv:2010.07079",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Recipes for Safety in Open-domain Chatbots",
		"URL": "http://arxiv.org/abs/2010.07079",
		"author": [
			{
				"family": "Xu",
				"given": "Jing"
			},
			{
				"family": "Ju",
				"given": "Da"
			},
			{
				"family": "Li",
				"given": "Margaret"
			},
			{
				"family": "Boureau",
				"given": "Y.-Lan"
			},
			{
				"family": "Weston",
				"given": "Jason"
			},
			{
				"family": "Dinan",
				"given": "Emily"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8,
					4
				]
			]
		}
	},
	{
		"id": "morrisGenerativeGhostsAnticipating2024",
		"type": "article",
		"abstract": "As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on speciﬁc people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we ﬁrst discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneﬁcial manner.",
		"language": "en",
		"note": "arXiv:2402.01662 [cs]",
		"number": "arXiv:2402.01662",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives",
		"title-short": "Generative Ghosts",
		"URL": "http://arxiv.org/abs/2402.01662",
		"author": [
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Brubaker",
				"given": "Jed R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					8
				]
			]
		}
	},
	{
		"id": "duettmannCyberNanoAGI2017",
		"type": "article",
		"abstract": "The aim of this paper, rather than attempting to present one coherent strategy for reducing existential risks, is to introduce a variety of possible options with the goal of broadening the discussion and inviting further investigation. Two themes appear throughout: (1) the proposed approaches for risk reduction attempt to avoid the dangers of centralized “solutions,” and (2) cybersecurity is not treated as a separate risk. Instead, trustworthy cybersecurity is a prerequisite for the success of our proposed approaches to risk reduction.",
		"event-place": "The first colloquium on catastrophic and existential risk",
		"publisher-place": "The first colloquium on catastrophic and existential risk",
		"title": "Cyber, nano, and AGI risks: Decentralized approaches to reducing risks",
		"URL": "https://research.google/pubs/cyber-nano-and-agi-risks-decentralized-approaches-to-reducing-risks/",
		"author": [
			{
				"family": "Duettmann",
				"given": "Allison"
			},
			{
				"family": "Peterson",
				"given": "Christine"
			},
			{
				"family": "Miller",
				"given": "Mark S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "chowRiskConstrainedReinforcementLearning2018",
		"type": "article-journal",
		"abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.",
		"container-title": "Journal of Machine Learning Research",
		"ISSN": "1533-7928",
		"issue": "167",
		"page": "1-51",
		"source": "www.jmlr.org",
		"title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
		"URL": "http://jmlr.org/papers/v18/15-636.html",
		"volume": "18",
		"author": [
			{
				"family": "Chow",
				"given": "Yinlam"
			},
			{
				"family": "Ghavamzadeh",
				"given": "Mohammad"
			},
			{
				"family": "Janson",
				"given": "Lucas"
			},
			{
				"family": "Pavone",
				"given": "Marco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "fiannacaSafetyValuesensitiveApproach2022",
		"type": "article",
		"abstract": "As modern, pre-trained ML models have proliferated in recent years , many researchers and practitioners have made significant efforts to prevent AI systems from causing harm. This focus on safety is critical, but a singular focus on safety can come at the exclusion of considering other important stakeholder values and the interactions between those values in the AI systems we build. In this position paper, we propose that the AI community should incorporate ideas from the Value-Sensitive Design framework from the Human-Computer Interaction community to ensure the needs and values of all stakeholders are reflected in the systems we build. We share observations and reflections from our experiences working on AI-supported accessibility technologies and with members of various disability communities to illustrate the tensions that sometimes arise between safety and other values.",
		"event-place": "NeurIPS 2022 Human-Centered AI Workshop",
		"publisher-place": "NeurIPS 2022 Human-Centered AI Workshop",
		"title": "Beyond safety: Toward a value-sensitive approach to the design of AI systems",
		"URL": "https://research.google/pubs/beyond-safety-toward-a-value-sensitive-approach-to-the-design-of-ai-systems/",
		"author": [
			{
				"family": "Fiannaca",
				"given": "Alex"
			},
			{
				"family": "Bennett",
				"given": "Cindy"
			},
			{
				"family": "Kane",
				"given": "Shaun"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lacotteRiskSensitiveGenerativeAdversarial2019",
		"type": "paper-conference",
		"abstract": "We study risk-sensitive imitation learning where the agent’s goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk- sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",
		"container-title": "Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics",
		"event-title": "The 22nd International Conference on Artificial Intelligence and Statistics",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "2154-2163",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Risk-Sensitive Generative Adversarial Imitation Learning",
		"URL": "https://proceedings.mlr.press/v89/lacotte19a.html",
		"author": [
			{
				"family": "Lacotte",
				"given": "Jonathan"
			},
			{
				"family": "Ghavamzadeh",
				"given": "Mohammad"
			},
			{
				"family": "Chow",
				"given": "Yinlam"
			},
			{
				"family": "Pavone",
				"given": "Marco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					11
				]
			]
		}
	},
	{
		"id": "schmidtAdversariallyRobustGeneralization2018",
		"type": "article",
		"abstract": "Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classiﬁers with high “standard” accuracy to produce an incorrect prediction with high conﬁdence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be signiﬁcantly larger than that of “standard” learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classiﬁcation datasets and show that a similar gap exists here as well. We postulate that the diﬃculty of training robust classiﬁers stems, at least partially, from this inherently larger sample complexity.",
		"language": "en",
		"note": "arXiv:1804.11285 [cs, stat]",
		"number": "arXiv:1804.11285",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarially Robust Generalization Requires More Data",
		"URL": "http://arxiv.org/abs/1804.11285",
		"author": [
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Santurkar",
				"given": "Shibani"
			},
			{
				"family": "Tsipras",
				"given": "Dimitris"
			},
			{
				"family": "Talwar",
				"given": "Kunal"
			},
			{
				"family": "Mądry",
				"given": "Aleksander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					2
				]
			]
		}
	},
	{
		"id": "carliniSecretSharerEvaluating2019a",
		"type": "article",
		"abstract": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.",
		"DOI": "10.48550/arXiv.1802.08232",
		"note": "arXiv:1802.08232 [cs]",
		"number": "arXiv:1802.08232",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks",
		"title-short": "The Secret Sharer",
		"URL": "http://arxiv.org/abs/1802.08232",
		"author": [
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Liu",
				"given": "Chang"
			},
			{
				"family": "Erlingsson",
				"given": "Úlfar"
			},
			{
				"family": "Kos",
				"given": "Jernej"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					16
				]
			]
		}
	},
	{
		"id": "gilmerMotivatingRulesGame2018",
		"type": "article",
		"abstract": "Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",
		"DOI": "10.48550/arXiv.1807.06732",
		"note": "arXiv:1807.06732 [cs, stat]",
		"number": "arXiv:1807.06732",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Motivating the Rules of the Game for Adversarial Example Research",
		"URL": "http://arxiv.org/abs/1807.06732",
		"author": [
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Adams",
				"given": "Ryan P."
			},
			{
				"family": "Goodfellow",
				"given": "Ian"
			},
			{
				"family": "Andersen",
				"given": "David"
			},
			{
				"family": "Dahl",
				"given": "George E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					19
				]
			]
		}
	},
	{
		"id": "balujaLearningAttackAdversarial2018",
		"type": "article-journal",
		"abstract": "With the rapidly increasing popularity of deep neural networks for image recognition tasks, a parallel interest in generating adversarial examples to attack the trained models has arisen. To date, these approaches have involved either directly computing gradients with respect to the image pixels or directly solving an optimization on the image pixels. We generalize this pursuit in a novel direction: can a separate network be trained to efficiently attack another fully trained network? We demonstrate that it is possible, and that the generated attacks yield startling insights into the weaknesses of the target network.  We call such a network an Adversarial Transformation Network (ATN). ATNs transform any input into an adversarial attack on the target network, while being minimally perturbing to the original inputs and the target network's outputs. Further, we show that ATNs are capable of not only causing the target network to make an error, but can be constructed to explicitly control the type of misclassification made. We demonstrate ATNs on both simple MNIST-digit classifiers and state-of-the-art ImageNet classifiers deployed by Google, Inc.: Inception ResNet-v2.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v32i1.11672",
		"ISSN": "2374-3468",
		"issue": "1",
		"language": "en",
		"license": "Copyright (c)",
		"note": "number: 1",
		"source": "ojs.aaai.org",
		"title": "Learning to Attack: Adversarial Transformation Networks",
		"title-short": "Learning to Attack",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/11672",
		"volume": "32",
		"author": [
			{
				"family": "Baluja",
				"given": "Shumeet"
			},
			{
				"family": "Fischer",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					29
				]
			]
		}
	},
	{
		"id": "kindermansLearningHowExplain2018",
		"type": "paper-conference",
		"abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.",
		"event-title": "International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Learning how to explain neural networks: PatternNet and PatternAttribution",
		"title-short": "Learning how to explain neural networks",
		"URL": "https://openreview.net/forum?id=Hkn7CBaTW",
		"author": [
			{
				"family": "Kindermans",
				"given": "Pieter-Jan"
			},
			{
				"family": "Schütt",
				"given": "Kristof T."
			},
			{
				"family": "Alber",
				"given": "Maximilian"
			},
			{
				"family": "Müller",
				"given": "Klaus-Robert"
			},
			{
				"family": "Erhan",
				"given": "Dumitru"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Dähne",
				"given": "Sven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					15
				]
			]
		}
	},
	{
		"id": "gilmerAdversarialSpheres2018",
		"type": "article",
		"abstract": "Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classiﬁcation error rate to the average distance to the nearest misclassiﬁcation, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.",
		"language": "en",
		"note": "arXiv:1801.02774 [cs]",
		"number": "arXiv:1801.02774",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Spheres",
		"URL": "http://arxiv.org/abs/1801.02774",
		"author": [
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Metz",
				"given": "Luke"
			},
			{
				"family": "Faghri",
				"given": "Fartash"
			},
			{
				"family": "Schoenholz",
				"given": "Samuel S."
			},
			{
				"family": "Raghu",
				"given": "Maithra"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Goodfellow",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					10
				]
			]
		}
	},
	{
		"id": "lageHumanintheLoopInterpretabilityPrior2018a",
		"type": "article",
		"abstract": "We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",
		"DOI": "10.48550/arXiv.1805.11571",
		"note": "arXiv:1805.11571 [cs, stat]",
		"number": "arXiv:1805.11571",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Human-in-the-Loop Interpretability Prior",
		"URL": "http://arxiv.org/abs/1805.11571",
		"author": [
			{
				"family": "Lage",
				"given": "Isaac"
			},
			{
				"family": "Ross",
				"given": "Andrew Slavin"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Gershman",
				"given": "Samuel J."
			},
			{
				"family": "Doshi-Velez",
				"given": "Finale"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					30
				]
			]
		}
	},
	{
		"id": "guptaDiminishingReturnsShape2018",
		"type": "paper-conference",
		"abstract": "We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs.  We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks.  We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Diminishing Returns Shape Constraints for Interpretability and Regularization",
		"URL": "https://proceedings.neurips.cc/paper/2018/hash/caa202034f268232c26fac9435f54e15-Abstract.html",
		"volume": "31",
		"author": [
			{
				"family": "Gupta",
				"given": "Maya"
			},
			{
				"family": "Bahri",
				"given": "Dara"
			},
			{
				"family": "Cotter",
				"given": "Andrew"
			},
			{
				"family": "Canini",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "eysenbachLeaveNoTrace2018",
		"type": "paper-conference",
		"abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.",
		"event-title": "International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
		"title-short": "Leave no Trace",
		"URL": "https://openreview.net/forum?id=S1vuO-bCW",
		"author": [
			{
				"family": "Eysenbach",
				"given": "Benjamin"
			},
			{
				"family": "Gu",
				"given": "Shixiang"
			},
			{
				"family": "Ibarz",
				"given": "Julian"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					15
				]
			]
		}
	},
	{
		"id": "leeRobustDeterminantalGenerative2018",
		"type": "article-journal",
		"abstract": "Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks poorly generalize from such noisy training datasets. In this paper, we propose a novel inference method, Deep Determinantal Generative Classifier (DDGC), which can obtain a more robust decision boundary under any softmax neural classifier pre-trained on noisy datasets. Our main idea is inducing a generative classifier on top of hidden feature spaces of the discriminative deep model. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy, with neither re-training of the deep model nor changing its architectures. In particular, we show that DDGC not only generalizes well from noisy labels, but also is robust against adversarial perturbations due to its large margin property. Finally, we propose the ensemble version ofDDGC to improve its performance, by investigating the layer-wise characteristics of generative classifier. Our extensive experimental results demonstrate the superiority of DDGC given different learning models optimized by various training techniques to handle noisy labels or adversarial samples. For instance, on CIFAR-10 dataset containing 45% noisy training labels, we improve the test accuracy of a deep model optimized by the state-of-the-art noise-handling training method from33.34% to 43.02%.",
		"language": "en",
		"source": "openreview.net",
		"title": "Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks",
		"URL": "https://openreview.net/forum?id=rkle3i09K7",
		"author": [
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Yun",
				"given": "Sukmin"
			},
			{
				"family": "Lee",
				"given": "Kibok"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Shin",
				"given": "Jinwoo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					27
				]
			]
		}
	},
	{
		"id": "tenneyLanguageInterpretabilityTool2020",
		"type": "article",
		"abstract": "We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",
		"DOI": "10.48550/arXiv.2008.05122",
		"note": "arXiv:2008.05122 [cs]",
		"number": "arXiv:2008.05122",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",
		"title-short": "The Language Interpretability Tool",
		"URL": "http://arxiv.org/abs/2008.05122",
		"author": [
			{
				"family": "Tenney",
				"given": "Ian"
			},
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Bastings",
				"given": "Jasmijn"
			},
			{
				"family": "Bolukbasi",
				"given": "Tolga"
			},
			{
				"family": "Coenen",
				"given": "Andy"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Jiang",
				"given": "Ellen"
			},
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Radebaugh",
				"given": "Carey"
			},
			{
				"family": "Reif",
				"given": "Emily"
			},
			{
				"family": "Yuan",
				"given": "Ann"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					12
				]
			]
		}
	},
	{
		"id": "arikExplainingDeepNeural2020",
		"type": "paper-conference",
		"abstract": "We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be flexibly applied to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate strength of our method in finding similar training samples, and shedding light on the concepts the DNN bases its decision on. Via user studies, we show that our model can improve user trust in model’s prediction.",
		"event-title": "Workshop on Human Interpretability in Machine Learning",
		"title": "Explaining deep neural networks using unsupervised clustering",
		"URL": "https://research.google/pubs/explaining-deep-neural-networks-using-unsupervised-clustering/",
		"author": [
			{
				"family": "Arik",
				"given": "Sercan"
			},
			{
				"family": "Liu",
				"given": "Yu-Han"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cotterShapeConstraintsSet2019",
		"type": "paper-conference",
		"abstract": "Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions, and then propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees on the model behavior, which makes it easier to explain and debug.",
		"container-title": "Proceedings of the 36th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "1388-1396",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Shape Constraints for Set Functions",
		"URL": "https://proceedings.mlr.press/v97/cotter19a.html",
		"author": [
			{
				"family": "Cotter",
				"given": "Andrew"
			},
			{
				"family": "Gupta",
				"given": "Maya"
			},
			{
				"family": "Jiang",
				"given": "Heinrich"
			},
			{
				"family": "Louidor",
				"given": "Erez"
			},
			{
				"family": "Muller",
				"given": "James"
			},
			{
				"family": "Narayan",
				"given": "Tamann"
			},
			{
				"family": "Wang",
				"given": "Serena"
			},
			{
				"family": "Zhu",
				"given": "Tao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					24
				]
			]
		}
	},
	{
		"id": "yehCompletenessawareConceptBasedExplanations2020",
		"type": "paper-conference",
		"abstract": "Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of \\emph{completeness}, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose \\emph{ConceptSHAP}. Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "20554–20565",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "On Completeness-aware Concept-Based Explanations in Deep Neural Networks",
		"URL": "https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html",
		"volume": "33",
		"author": [
			{
				"family": "Yeh",
				"given": "Chih-Kuan"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Arik",
				"given": "Sercan"
			},
			{
				"family": "Li",
				"given": "Chun-Liang"
			},
			{
				"family": "Pfister",
				"given": "Tomas"
			},
			{
				"family": "Ravikumar",
				"given": "Pradeep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lowreyPlanOnlineLearn2019",
		"type": "article",
		"abstract": "We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.",
		"DOI": "10.48550/arXiv.1811.01848",
		"note": "arXiv:1811.01848 [cs, stat]",
		"number": "arXiv:1811.01848",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
		"title-short": "Plan Online, Learn Offline",
		"URL": "http://arxiv.org/abs/1811.01848",
		"author": [
			{
				"family": "Lowrey",
				"given": "Kendall"
			},
			{
				"family": "Rajeswaran",
				"given": "Aravind"
			},
			{
				"family": "Kakade",
				"given": "Sham"
			},
			{
				"family": "Todorov",
				"given": "Emanuel"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					28
				]
			]
		}
	},
	{
		"id": "mordatchConceptLearningEnergyBased2018",
		"type": "article",
		"abstract": "Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels",
		"DOI": "10.48550/arXiv.1811.02486",
		"note": "arXiv:1811.02486 [cs]",
		"number": "arXiv:1811.02486",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Concept Learning with Energy-Based Models",
		"URL": "http://arxiv.org/abs/1811.02486",
		"author": [
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					6
				]
			]
		}
	},
	{
		"id": "cobbeQuantifyingGeneralizationReinforcement2019",
		"type": "article",
		"abstract": "In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",
		"DOI": "10.48550/arXiv.1812.02341",
		"note": "arXiv:1812.02341 [cs, stat]",
		"number": "arXiv:1812.02341",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Quantifying Generalization in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1812.02341",
		"author": [
			{
				"family": "Cobbe",
				"given": "Karl"
			},
			{
				"family": "Klimov",
				"given": "Oleg"
			},
			{
				"family": "Hesse",
				"given": "Chris"
			},
			{
				"family": "Kim",
				"given": "Taehoon"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					14
				]
			]
		}
	},
	{
		"id": "mccandlishEmpiricalModelLargeBatch2018",
		"type": "article",
		"abstract": "In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacriﬁcing data efﬁciency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We ﬁnd that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efﬁciency and time-efﬁciency, and provides a rough model of the beneﬁts of adaptive batch-size training.",
		"language": "en",
		"note": "arXiv:1812.06162 [cs, stat]",
		"number": "arXiv:1812.06162",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An Empirical Model of Large-Batch Training",
		"URL": "http://arxiv.org/abs/1812.06162",
		"author": [
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Team",
				"given": "OpenAI Dota"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					14
				]
			]
		}
	},
	{
		"id": "degwekarComputationalLimitationsRobust2019",
		"type": "article",
		"abstract": "We continue the study of statistical/computational tradeoffs in learning robust classifiers, following the recent work of Bubeck, Lee, Price and Razenshteyn who showed examples of classification tasks where (a) an efficient robust classifier exists, in the small-perturbation regime; (b) a non-robust classifier can be learned efficiently; but (c) it is computationally hard to learn a robust classifier, assuming the hardness of factoring large numbers. The question of whether a robust classifier for their task exists in the large perturbation regime seems related to important open questions in computational number theory. In this work, we extend their work in three directions. First, we demonstrate classification tasks where computationally efficient robust classification is impossible, even when computationally unbounded robust classifiers exist. For this, we rely on the existence of average-case hard functions. Second, we show hard-to-robustly-learn classification tasks in the large-perturbation regime. Namely, we show that even though an efficient classifier that is robust to large perturbations exists, it is computationally hard to learn any non-trivial robust classifier. Our first construction relies on the existence of one-way functions, and the second on the hardness of the learning parity with noise problem. In the latter setting, not only does a non-robust classifier exist, but also an efficient algorithm that generates fresh new labeled samples given access to polynomially many training examples (termed as generation by Kearns et. al. (1994)). Third, we show that any such counterexample implies the existence of cryptographic primitives such as one-way functions. This leads us to a win-win scenario: either we can learn an efficient robust classifier, or we can construct new instances of cryptographic primitives.",
		"DOI": "10.48550/arXiv.1902.01086",
		"note": "arXiv:1902.01086 [cs, stat]",
		"number": "arXiv:1902.01086",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Computational Limitations in Robust Classification and Win-Win Results",
		"URL": "http://arxiv.org/abs/1902.01086",
		"author": [
			{
				"family": "Degwekar",
				"given": "Akshay"
			},
			{
				"family": "Nakkiran",
				"given": "Preetum"
			},
			{
				"family": "Vaikuntanathan",
				"given": "Vinod"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					5
				]
			]
		}
	},
	{
		"id": "radfordLanguageModelsAre2019",
		"type": "article",
		"abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
		"language": "en",
		"source": "Zotero",
		"title": "Language Models are Unsupervised Multitask Learners",
		"URL": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					2
				]
			]
		}
	},
	{
		"id": "suarezNeuralMMOMassively2019",
		"type": "article",
		"abstract": "The emergence of complex life on Earth is often attributed to the arms race that ensued from a huge number of organisms all competing for finite resources. We present an artificial intelligence research environment, inspired by the human game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games, a.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs and the real world alike, our environment is persistent and supports a large and variable number of agents. Our environment is well suited to the study of large-scale multiagent interaction: it requires that agents learn robust combat and navigation policies in the presence of large populations attempting to do the same. Baseline experiments reveal that population size magnifies and incentivizes the development of skillful behaviors and results in agents that outcompete agents trained in smaller populations. We further show that the policies of agents with unshared weights naturally diverge to fill different niches in order to avoid competition.",
		"DOI": "10.48550/arXiv.1903.00784",
		"note": "arXiv:1903.00784 [cs, stat]",
		"number": "arXiv:1903.00784",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents",
		"title-short": "Neural MMO",
		"URL": "http://arxiv.org/abs/1903.00784",
		"author": [
			{
				"family": "Suarez",
				"given": "Joseph"
			},
			{
				"family": "Du",
				"given": "Yilun"
			},
			{
				"family": "Isola",
				"given": "Phillip"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					2
				]
			]
		}
	},
	{
		"id": "duImplicitGenerationGeneralization2020",
		"type": "article",
		"abstract": "Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.",
		"DOI": "10.48550/arXiv.1903.08689",
		"note": "arXiv:1903.08689 [cs, stat]",
		"number": "arXiv:1903.08689",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Implicit Generation and Generalization in Energy-Based Models",
		"URL": "http://arxiv.org/abs/1903.08689",
		"author": [
			{
				"family": "Du",
				"given": "Yilun"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					29
				]
			]
		}
	},
	{
		"id": "childGeneratingLongSequences2019",
		"type": "article",
		"abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
		"DOI": "10.48550/arXiv.1904.10509",
		"note": "arXiv:1904.10509 [cs, stat]",
		"number": "arXiv:1904.10509",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generating Long Sequences with Sparse Transformers",
		"URL": "http://arxiv.org/abs/1904.10509",
		"author": [
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					23
				]
			]
		}
	},
	{
		"id": "bakerEmergentToolUse2020",
		"type": "article",
		"abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",
		"DOI": "10.48550/arXiv.1909.07528",
		"note": "arXiv:1909.07528 [cs, stat]",
		"number": "arXiv:1909.07528",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergent Tool Use From Multi-Agent Autocurricula",
		"URL": "http://arxiv.org/abs/1909.07528",
		"author": [
			{
				"family": "Baker",
				"given": "Bowen"
			},
			{
				"family": "Kanitscheider",
				"given": "Ingmar"
			},
			{
				"family": "Markov",
				"given": "Todor"
			},
			{
				"family": "Wu",
				"given": "Yi"
			},
			{
				"family": "Powell",
				"given": "Glenn"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					10
				]
			]
		}
	},
	{
		"id": "openaiSolvingRubikCube2019",
		"type": "article",
		"abstract": "We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/",
		"DOI": "10.48550/arXiv.1910.07113",
		"note": "arXiv:1910.07113 [cs, stat]",
		"number": "arXiv:1910.07113",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Solving Rubik's Cube with a Robot Hand",
		"URL": "http://arxiv.org/abs/1910.07113",
		"author": [
			{
				"family": "OpenAI",
				"given": ""
			},
			{
				"family": "Akkaya",
				"given": "Ilge"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Chociej",
				"given": "Maciek"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Petron",
				"given": "Arthur"
			},
			{
				"family": "Paino",
				"given": "Alex"
			},
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Powell",
				"given": "Glenn"
			},
			{
				"family": "Ribas",
				"given": "Raphael"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Tezak",
				"given": "Nikolas"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Yuan",
				"given": "Qiming"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Zhang",
				"given": "Lei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					15
				]
			]
		}
	},
	{
		"id": "solaimanReleaseStrategiesSocial2019",
		"type": "article",
		"abstract": "Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.",
		"DOI": "10.48550/arXiv.1908.09203",
		"note": "arXiv:1908.09203 [cs]",
		"number": "arXiv:1908.09203",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Release Strategies and the Social Impacts of Language Models",
		"URL": "http://arxiv.org/abs/1908.09203",
		"author": [
			{
				"family": "Solaiman",
				"given": "Irene"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Kreps",
				"given": "Sarah"
			},
			{
				"family": "McCain",
				"given": "Miles"
			},
			{
				"family": "Newhouse",
				"given": "Alex"
			},
			{
				"family": "Blazakis",
				"given": "Jason"
			},
			{
				"family": "McGuffie",
				"given": "Kris"
			},
			{
				"family": "Wang",
				"given": "Jasmine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					12
				]
			]
		}
	},
	{
		"id": "cobbeLeveragingProceduralGeneration2020",
		"type": "article",
		"abstract": "We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.",
		"DOI": "10.48550/arXiv.1912.01588",
		"note": "arXiv:1912.01588 [cs, stat]",
		"number": "arXiv:1912.01588",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1912.01588",
		"author": [
			{
				"family": "Cobbe",
				"given": "Karl"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					26
				]
			]
		}
	},
	{
		"id": "nakkiranDeepDoubleDescent2019",
		"type": "article",
		"abstract": "We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
		"DOI": "10.48550/arXiv.1912.02292",
		"note": "arXiv:1912.02292 [cs, stat]",
		"number": "arXiv:1912.02292",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
		"title-short": "Deep Double Descent",
		"URL": "http://arxiv.org/abs/1912.02292",
		"author": [
			{
				"family": "Nakkiran",
				"given": "Preetum"
			},
			{
				"family": "Kaplun",
				"given": "Gal"
			},
			{
				"family": "Bansal",
				"given": "Yamini"
			},
			{
				"family": "Yang",
				"given": "Tristan"
			},
			{
				"family": "Barak",
				"given": "Boaz"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					4
				]
			]
		}
	},
	{
		"id": "openaiDotaLargeScale2019",
		"type": "article",
		"abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
		"DOI": "10.48550/arXiv.1912.06680",
		"note": "arXiv:1912.06680 [cs, stat]",
		"number": "arXiv:1912.06680",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Dota 2 with Large Scale Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1912.06680",
		"author": [
			{
				"family": "OpenAI",
				"given": ""
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "Brockman",
				"given": "Greg"
			},
			{
				"family": "Chan",
				"given": "Brooke"
			},
			{
				"family": "Cheung",
				"given": "Vicki"
			},
			{
				"family": "Dębiak",
				"given": "Przemysław"
			},
			{
				"family": "Dennison",
				"given": "Christy"
			},
			{
				"family": "Farhi",
				"given": "David"
			},
			{
				"family": "Fischer",
				"given": "Quirin"
			},
			{
				"family": "Hashme",
				"given": "Shariq"
			},
			{
				"family": "Hesse",
				"given": "Chris"
			},
			{
				"family": "Józefowicz",
				"given": "Rafal"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Pachocki",
				"given": "Jakub"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Pinto",
				"given": "Henrique P. d O."
			},
			{
				"family": "Raiman",
				"given": "Jonathan"
			},
			{
				"family": "Salimans",
				"given": "Tim"
			},
			{
				"family": "Schlatter",
				"given": "Jeremy"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Wolski",
				"given": "Filip"
			},
			{
				"family": "Zhang",
				"given": "Susan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					13
				]
			]
		}
	},
	{
		"id": "kaplanScalingLawsNeural2020",
		"type": "article",
		"abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
		"DOI": "10.48550/arXiv.2001.08361",
		"note": "arXiv:2001.08361 [cs, stat]",
		"number": "arXiv:2001.08361",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Laws for Neural Language Models",
		"URL": "http://arxiv.org/abs/2001.08361",
		"author": [
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					22
				]
			]
		}
	},
	{
		"id": "dhariwalJukeboxGenerativeModel2020",
		"type": "article",
		"abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox",
		"DOI": "10.48550/arXiv.2005.00341",
		"note": "arXiv:2005.00341 [cs, eess, stat]",
		"number": "arXiv:2005.00341",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Jukebox: A Generative Model for Music",
		"title-short": "Jukebox",
		"URL": "http://arxiv.org/abs/2005.00341",
		"author": [
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Payne",
				"given": "Christine"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4,
					30
				]
			]
		}
	},
	{
		"id": "hernandezMeasuringAlgorithmicEfficiency2020",
		"type": "article",
		"abstract": "Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.",
		"DOI": "10.48550/arXiv.2005.04305",
		"note": "arXiv:2005.04305 [cs, stat]",
		"number": "arXiv:2005.04305",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Measuring the Algorithmic Efficiency of Neural Networks",
		"URL": "http://arxiv.org/abs/2005.04305",
		"author": [
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					8
				]
			]
		}
	},
	{
		"id": "brownLanguageModelsAre2020",
		"type": "article",
		"abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
		"DOI": "10.48550/arXiv.2005.14165",
		"note": "arXiv:2005.14165 [cs]",
		"number": "arXiv:2005.14165",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Models are Few-Shot Learners",
		"URL": "http://arxiv.org/abs/2005.14165",
		"author": [
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Mann",
				"given": "Benjamin"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Subbiah",
				"given": "Melanie"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7,
					22
				]
			]
		}
	},
	{
		"id": "chenGenerativePretrainingPixels2020",
		"type": "article",
		"abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
		"event-place": "ICML 2020",
		"language": "en",
		"publisher-place": "ICML 2020",
		"source": "Zotero",
		"title": "Generative Pretraining from Pixels",
		"author": [
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Child",
				"given": "Rewon"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Luan",
				"given": "David"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "poluGenerativeLanguageModeling2020",
		"type": "article",
		"abstract": "We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.",
		"DOI": "10.48550/arXiv.2009.03393",
		"note": "arXiv:2009.03393 [cs, stat]",
		"number": "arXiv:2009.03393",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generative Language Modeling for Automated Theorem Proving",
		"URL": "http://arxiv.org/abs/2009.03393",
		"author": [
			{
				"family": "Polu",
				"given": "Stanislas"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					7
				]
			]
		}
	},
	{
		"id": "radfordLearningTransferableVisual2021",
		"type": "article",
		"abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
		"DOI": "10.48550/arXiv.2103.00020",
		"note": "arXiv:2103.00020 [cs]",
		"number": "arXiv:2103.00020",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Transferable Visual Models From Natural Language Supervision",
		"URL": "http://arxiv.org/abs/2103.00020",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Hallacy",
				"given": "Chris"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					26
				]
			]
		}
	},
	{
		"id": "tamkinUnderstandingCapabilitiesLimitations2021",
		"type": "article",
		"abstract": "On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.",
		"DOI": "10.48550/arXiv.2102.02503",
		"note": "arXiv:2102.02503 [cs]",
		"number": "arXiv:2102.02503",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
		"URL": "http://arxiv.org/abs/2102.02503",
		"author": [
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					4
				]
			]
		}
	},
	{
		"id": "gohMultimodalNeuronsArtificial2021",
		"type": "article-journal",
		"abstract": "In 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry . The exciting thing wasn’t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person's name. The neurons were multimodal. As the lead author would put it: 'You are looking at the far end of the transformation from metric, visual shapes to conceptual… information.' We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name. People-detecting neurons only scratch the surface of the highly abstract neurons we've found. Some neurons seem like topics out of a kindergarten curriculum: weather, seasons, letters, counting, or primary colors. All of these features, even the trivial-seeming ones, have rich multimodality, such as a yellow neuron firing for images of the words “yellow”, “banana” and “lemon,” in addition to the color. We find these multimodal neurons in the recent CLIP models , although it's possible similar undiscovered multimodal neurons may exist in earlier models. A CLIP model consists of two sides, a ResNet vision model and a Transformer language model, trained to align pairs of images and text from the internet using a contrastive loss. There are several CLIP models of varying sizes; we find multimodal neurons in all of them, but focus on studying the mid-sized RN50-x4 model. We refer readers to the CLIP blog post and paper for more detailed discussion of CLIP’s architecture and performance. Our analysis will focus on CLIP's vision side, so when we talk about a multimodal neuron responding to text we mean the model 'reading' text in images. CLIP's abstract visual features might be seen as the natural result of aligning vision and text. We expect word embeddings (and language models generally) to learn abstract 'topic' features . Either the side of the model which processes captions (the 'language side') needs to give up those features, or its counterpart, the 'vision side', needs to build visual analogues. But even if these features seem natural in retrospect, they are qualitatively different from neurons previously studied in vision models (eg. ). They also have real world implications: these models are vulnerable to a kind of 'typographic attack' where adding adversarial text to images can cause them to be systematically misclassified.",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00030",
		"ISSN": "2476-0757",
		"issue": "3",
		"journalAbbreviation": "Distill",
		"language": "en",
		"page": "e30",
		"source": "distill.pub",
		"title": "Multimodal Neurons in Artificial Neural Networks",
		"URL": "https://distill.pub/2021/multimodal-neurons",
		"volume": "6",
		"author": [
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "†",
				"given": "Nick Cammarata"
			},
			{
				"family": "†",
				"given": "Chelsea Voss"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					4
				]
			]
		}
	},
	{
		"id": "chenEvaluatingLargeLanguage2021",
		"type": "article",
		"abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",
		"DOI": "10.48550/arXiv.2107.03374",
		"note": "arXiv:2107.03374 [cs]",
		"number": "arXiv:2107.03374",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating Large Language Models Trained on Code",
		"URL": "http://arxiv.org/abs/2107.03374",
		"author": [
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Yuan",
				"given": "Qiming"
			},
			{
				"family": "Pinto",
				"given": "Henrique Ponde de Oliveira"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Edwards",
				"given": "Harri"
			},
			{
				"family": "Burda",
				"given": "Yuri"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Brockman",
				"given": "Greg"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Puri",
				"given": "Raul"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Khlaaf",
				"given": "Heidy"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Chan",
				"given": "Brooke"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Pavlov",
				"given": "Mikhail"
			},
			{
				"family": "Power",
				"given": "Alethea"
			},
			{
				"family": "Kaiser",
				"given": "Lukasz"
			},
			{
				"family": "Bavarian",
				"given": "Mohammad"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Tillet",
				"given": "Philippe"
			},
			{
				"family": "Such",
				"given": "Felipe Petroski"
			},
			{
				"family": "Cummings",
				"given": "Dave"
			},
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Chantzis",
				"given": "Fotios"
			},
			{
				"family": "Barnes",
				"given": "Elizabeth"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Guss",
				"given": "William Hebgen"
			},
			{
				"family": "Nichol",
				"given": "Alex"
			},
			{
				"family": "Paino",
				"given": "Alex"
			},
			{
				"family": "Tezak",
				"given": "Nikolas"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Babuschkin",
				"given": "Igor"
			},
			{
				"family": "Balaji",
				"given": "Suchir"
			},
			{
				"family": "Jain",
				"given": "Shantanu"
			},
			{
				"family": "Saunders",
				"given": "William"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Carr",
				"given": "Andrew N."
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Achiam",
				"given": "Josh"
			},
			{
				"family": "Misra",
				"given": "Vedant"
			},
			{
				"family": "Morikawa",
				"given": "Evan"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Knight",
				"given": "Matthew"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Murati",
				"given": "Mira"
			},
			{
				"family": "Mayer",
				"given": "Katie"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					14
				]
			]
		}
	},
	{
		"id": "linTruthfulQAMeasuringHow2022",
		"type": "article",
		"abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
		"DOI": "10.48550/arXiv.2109.07958",
		"note": "arXiv:2109.07958 [cs]",
		"number": "arXiv:2109.07958",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
		"title-short": "TruthfulQA",
		"URL": "http://arxiv.org/abs/2109.07958",
		"author": [
			{
				"family": "Lin",
				"given": "Stephanie"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Evans",
				"given": "Owain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					7
				]
			]
		}
	},
	{
		"id": "cobbeTrainingVerifiersSolve2021",
		"type": "article",
		"abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
		"DOI": "10.48550/arXiv.2110.14168",
		"note": "arXiv:2110.14168 [cs]",
		"number": "arXiv:2110.14168",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Verifiers to Solve Math Word Problems",
		"URL": "http://arxiv.org/abs/2110.14168",
		"author": [
			{
				"family": "Cobbe",
				"given": "Karl"
			},
			{
				"family": "Kosaraju",
				"given": "Vineet"
			},
			{
				"family": "Bavarian",
				"given": "Mohammad"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Kaiser",
				"given": "Lukasz"
			},
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Nakano",
				"given": "Reiichiro"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					17
				]
			]
		}
	},
	{
		"id": "nakanoWebGPTBrowserassistedQuestionanswering2022",
		"type": "article",
		"abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
		"DOI": "10.48550/arXiv.2112.09332",
		"note": "arXiv:2112.09332 [cs]",
		"number": "arXiv:2112.09332",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "WebGPT: Browser-assisted question-answering with human feedback",
		"title-short": "WebGPT",
		"URL": "http://arxiv.org/abs/2112.09332",
		"author": [
			{
				"family": "Nakano",
				"given": "Reiichiro"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Balaji",
				"given": "Suchir"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "Kim",
				"given": "Christina"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Jain",
				"given": "Shantanu"
			},
			{
				"family": "Kosaraju",
				"given": "Vineet"
			},
			{
				"family": "Saunders",
				"given": "William"
			},
			{
				"family": "Jiang",
				"given": "Xu"
			},
			{
				"family": "Cobbe",
				"given": "Karl"
			},
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Button",
				"given": "Kevin"
			},
			{
				"family": "Knight",
				"given": "Matthew"
			},
			{
				"family": "Chess",
				"given": "Benjamin"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					1
				]
			]
		}
	},
	{
		"id": "neelakantanTextCodeEmbeddings2022",
		"type": "article",
		"abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.",
		"DOI": "10.48550/arXiv.2201.10005",
		"note": "arXiv:2201.10005 [cs]",
		"number": "arXiv:2201.10005",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Text and Code Embeddings by Contrastive Pre-Training",
		"URL": "http://arxiv.org/abs/2201.10005",
		"author": [
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Xu",
				"given": "Tao"
			},
			{
				"family": "Puri",
				"given": "Raul"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Han",
				"given": "Jesse Michael"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Yuan",
				"given": "Qiming"
			},
			{
				"family": "Tezak",
				"given": "Nikolas"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Hallacy",
				"given": "Chris"
			},
			{
				"family": "Heidecke",
				"given": "Johannes"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Power",
				"given": "Boris"
			},
			{
				"family": "Nekoul",
				"given": "Tyna Eloundou"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Schnurr",
				"given": "David"
			},
			{
				"family": "Such",
				"given": "Felipe Petroski"
			},
			{
				"family": "Hsu",
				"given": "Kenny"
			},
			{
				"family": "Thompson",
				"given": "Madeleine"
			},
			{
				"family": "Khan",
				"given": "Tabarak"
			},
			{
				"family": "Sherbakov",
				"given": "Toki"
			},
			{
				"family": "Jang",
				"given": "Joanne"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					24
				]
			]
		}
	},
	{
		"id": "poluFormalMathematicsStatement2022",
		"type": "article",
		"abstract": "We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.",
		"DOI": "10.48550/arXiv.2202.01344",
		"note": "arXiv:2202.01344 [cs]",
		"number": "arXiv:2202.01344",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Formal Mathematics Statement Curriculum Learning",
		"URL": "http://arxiv.org/abs/2202.01344",
		"author": [
			{
				"family": "Polu",
				"given": "Stanislas"
			},
			{
				"family": "Han",
				"given": "Jesse Michael"
			},
			{
				"family": "Zheng",
				"given": "Kunhao"
			},
			{
				"family": "Baksys",
				"given": "Mantas"
			},
			{
				"family": "Babuschkin",
				"given": "Igor"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					2
				]
			]
		}
	},
	{
		"id": "manningResearchAgendaAssessing2022",
		"type": "article",
		"abstract": "OpenAI is developing a research program to assess the economic impacts of code generation models and is inviting collaboration with external researchers. Rapid advances in the capabilities of large language models (LLMs) trained on code have made it increasingly important to study their economic impacts on individuals, firms, and society. Codex – an LLM developed by OpenAI by fine-tuning GPT-3 on billions of lines of publicly available code from GitHub – has been shown to generate functionally correct code 28.8% of the time on a sample of evaluation problems (Chen et al. 2021). This may have important implications for the future of coding and the economics of the industries that depend on it. In this document, we lay out a research agenda to assess the effects of Codex on economic factors of interest to policymakers, firms, and the public. We make a case for this research agenda by highlighting the potentially broad applicability of code generation models to software development, the potential for other LLMs to create significant social and economic impact as model capabilities advance, and the value of using Codex to generate evidence and establish methodologies that may be applicable to research on the economic impacts of future models. We propose that academic\nand policy research focus on studying code generation models and other LLMs so that evidence on their economic impacts can be used to inform decision-making in three key areas: Deployment policy, AI system design, and public policy. To help guide this research, we outline six priority outcome areas within the realm of economic impacts that we intend to use Codex to study: Productivity, Employment, Skill Development, Inter-firm Competition, Consumer Prices, and Economic Inequality. For each area,\nwe briefly discuss previous literature on the impacts of artificial intelligence on each of these outcomes, describe questions that we believe to be key inputs to the three decision-making areas mentioned above, and provide examples of research that could be conducted with Codex. To catalyze work that builds off of this initial research agenda, we are announcing a Call for Expressions of Interest from external researchers to collaborate\nwith OpenAI researchers and customers to better measure the economic impacts of code generation models and other LLMs.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "A Research Agenda for Assessing the Economic Impacts of Code Generation Models",
		"URL": "https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf",
		"author": [
			{
				"family": "Manning",
				"given": "Sam"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			},
			{
				"family": "Eisner",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "rameshHierarchicalTextConditionalImage2022",
		"type": "article",
		"abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
		"DOI": "10.48550/arXiv.2204.06125",
		"note": "arXiv:2204.06125 [cs]",
		"number": "arXiv:2204.06125",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
		"URL": "http://arxiv.org/abs/2204.06125",
		"author": [
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Nichol",
				"given": "Alex"
			},
			{
				"family": "Chu",
				"given": "Casey"
			},
			{
				"family": "Chen",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					12
				]
			]
		}
	},
	{
		"id": "linTeachingModelsExpress2022",
		"type": "article",
		"abstract": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
		"DOI": "10.48550/arXiv.2205.14334",
		"note": "arXiv:2205.14334 [cs]",
		"number": "arXiv:2205.14334",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Teaching Models to Express Their Uncertainty in Words",
		"URL": "http://arxiv.org/abs/2205.14334",
		"author": [
			{
				"family": "Lin",
				"given": "Stephanie"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Evans",
				"given": "Owain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					13
				]
			]
		}
	},
	{
		"id": "lehmanEvolutionLargeModels2022",
		"type": "article",
		"abstract": "This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.",
		"DOI": "10.48550/arXiv.2206.08896",
		"note": "arXiv:2206.08896 [cs]",
		"number": "arXiv:2206.08896",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evolution through Large Models",
		"URL": "http://arxiv.org/abs/2206.08896",
		"author": [
			{
				"family": "Lehman",
				"given": "Joel"
			},
			{
				"family": "Gordon",
				"given": "Jonathan"
			},
			{
				"family": "Jain",
				"given": "Shawn"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Yeh",
				"given": "Cathy"
			},
			{
				"family": "Stanley",
				"given": "Kenneth O."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					17
				]
			]
		}
	},
	{
		"id": "bakerVideoPreTrainingVPT2022",
		"type": "article",
		"abstract": "Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.",
		"DOI": "10.48550/arXiv.2206.11795",
		"note": "arXiv:2206.11795 [cs]",
		"number": "arXiv:2206.11795",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
		"title-short": "Video PreTraining (VPT)",
		"URL": "http://arxiv.org/abs/2206.11795",
		"author": [
			{
				"family": "Baker",
				"given": "Bowen"
			},
			{
				"family": "Akkaya",
				"given": "Ilge"
			},
			{
				"family": "Zhokhov",
				"given": "Peter"
			},
			{
				"family": "Huizinga",
				"given": "Joost"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Ecoffet",
				"given": "Adrien"
			},
			{
				"family": "Houghton",
				"given": "Brandon"
			},
			{
				"family": "Sampedro",
				"given": "Raul"
			},
			{
				"family": "Clune",
				"given": "Jeff"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					23
				]
			]
		}
	},
	{
		"id": "bavarianEfficientTrainingLanguage2022",
		"type": "article",
		"abstract": "We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.",
		"DOI": "10.48550/arXiv.2207.14255",
		"note": "arXiv:2207.14255 [cs]",
		"number": "arXiv:2207.14255",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Efficient Training of Language Models to Fill in the Middle",
		"URL": "http://arxiv.org/abs/2207.14255",
		"author": [
			{
				"family": "Bavarian",
				"given": "Mohammad"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Tezak",
				"given": "Nikolas"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "McLeavey",
				"given": "Christine"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Chen",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					28
				]
			]
		}
	},
	{
		"id": "radfordRobustSpeechRecognition2022",
		"type": "article",
		"abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Robust Speech Recognition via Large-Scale Weak Supervision",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Xu",
				"given": "Tao"
			},
			{
				"family": "Brockman",
				"given": "Greg"
			},
			{
				"family": "McLeavey",
				"given": "Christine"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					9
				]
			]
		}
	},
	{
		"id": "lightmanLetVerifyStep2023",
		"type": "article",
		"abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
		"DOI": "10.48550/arXiv.2305.20050",
		"note": "arXiv:2305.20050 [cs]",
		"number": "arXiv:2305.20050",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Let's Verify Step by Step",
		"URL": "http://arxiv.org/abs/2305.20050",
		"author": [
			{
				"family": "Lightman",
				"given": "Hunter"
			},
			{
				"family": "Kosaraju",
				"given": "Vineet"
			},
			{
				"family": "Burda",
				"given": "Yura"
			},
			{
				"family": "Edwards",
				"given": "Harri"
			},
			{
				"family": "Baker",
				"given": "Bowen"
			},
			{
				"family": "Lee",
				"given": "Teddy"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Cobbe",
				"given": "Karl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					31
				]
			]
		}
	},
	{
		"id": "gaoScalingLawsReward2022",
		"type": "article",
		"abstract": "In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.",
		"DOI": "10.48550/arXiv.2210.10760",
		"note": "arXiv:2210.10760 [cs, stat]",
		"number": "arXiv:2210.10760",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Laws for Reward Model Overoptimization",
		"URL": "http://arxiv.org/abs/2210.10760",
		"author": [
			{
				"family": "Gao",
				"given": "Leo"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					19
				]
			]
		}
	},
	{
		"id": "nicholPointESystemGenerating2022",
		"type": "article",
		"abstract": "While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.",
		"DOI": "10.48550/arXiv.2212.08751",
		"note": "arXiv:2212.08751 [cs]",
		"number": "arXiv:2212.08751",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
		"title-short": "Point-E",
		"URL": "http://arxiv.org/abs/2212.08751",
		"author": [
			{
				"family": "Nichol",
				"given": "Alex"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Chen",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					16
				]
			]
		}
	},
	{
		"id": "openaiGPT4TechnicalReport2024",
		"type": "article",
		"abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
		"DOI": "10.48550/arXiv.2303.08774",
		"note": "arXiv:2303.08774 [cs]",
		"number": "arXiv:2303.08774",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "GPT-4 Technical Report",
		"URL": "http://arxiv.org/abs/2303.08774",
		"author": [
			{
				"family": "OpenAI",
				"given": ""
			},
			{
				"family": "Achiam",
				"given": "Josh"
			},
			{
				"family": "Adler",
				"given": "Steven"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Ahmad",
				"given": "Lama"
			},
			{
				"family": "Akkaya",
				"given": "Ilge"
			},
			{
				"family": "Aleman",
				"given": "Florencia Leoni"
			},
			{
				"family": "Almeida",
				"given": "Diogo"
			},
			{
				"family": "Altenschmidt",
				"given": "Janko"
			},
			{
				"family": "Altman",
				"given": "Sam"
			},
			{
				"family": "Anadkat",
				"given": "Shyamal"
			},
			{
				"family": "Avila",
				"given": "Red"
			},
			{
				"family": "Babuschkin",
				"given": "Igor"
			},
			{
				"family": "Balaji",
				"given": "Suchir"
			},
			{
				"family": "Balcom",
				"given": "Valerie"
			},
			{
				"family": "Baltescu",
				"given": "Paul"
			},
			{
				"family": "Bao",
				"given": "Haiming"
			},
			{
				"family": "Bavarian",
				"given": "Mohammad"
			},
			{
				"family": "Belgum",
				"given": "Jeff"
			},
			{
				"family": "Bello",
				"given": "Irwan"
			},
			{
				"family": "Berdine",
				"given": "Jake"
			},
			{
				"family": "Bernadett-Shapiro",
				"given": "Gabriel"
			},
			{
				"family": "Berner",
				"given": "Christopher"
			},
			{
				"family": "Bogdonoff",
				"given": "Lenny"
			},
			{
				"family": "Boiko",
				"given": "Oleg"
			},
			{
				"family": "Boyd",
				"given": "Madelaine"
			},
			{
				"family": "Brakman",
				"given": "Anna-Luisa"
			},
			{
				"family": "Brockman",
				"given": "Greg"
			},
			{
				"family": "Brooks",
				"given": "Tim"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Button",
				"given": "Kevin"
			},
			{
				"family": "Cai",
				"given": "Trevor"
			},
			{
				"family": "Campbell",
				"given": "Rosie"
			},
			{
				"family": "Cann",
				"given": "Andrew"
			},
			{
				"family": "Carey",
				"given": "Brittany"
			},
			{
				"family": "Carlson",
				"given": "Chelsea"
			},
			{
				"family": "Carmichael",
				"given": "Rory"
			},
			{
				"family": "Chan",
				"given": "Brooke"
			},
			{
				"family": "Chang",
				"given": "Che"
			},
			{
				"family": "Chantzis",
				"given": "Fotis"
			},
			{
				"family": "Chen",
				"given": "Derek"
			},
			{
				"family": "Chen",
				"given": "Sully"
			},
			{
				"family": "Chen",
				"given": "Ruby"
			},
			{
				"family": "Chen",
				"given": "Jason"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Chess",
				"given": "Ben"
			},
			{
				"family": "Cho",
				"given": "Chester"
			},
			{
				"family": "Chu",
				"given": "Casey"
			},
			{
				"family": "Chung",
				"given": "Hyung Won"
			},
			{
				"family": "Cummings",
				"given": "Dave"
			},
			{
				"family": "Currier",
				"given": "Jeremiah"
			},
			{
				"family": "Dai",
				"given": "Yunxing"
			},
			{
				"family": "Decareaux",
				"given": "Cory"
			},
			{
				"family": "Degry",
				"given": "Thomas"
			},
			{
				"family": "Deutsch",
				"given": "Noah"
			},
			{
				"family": "Deville",
				"given": "Damien"
			},
			{
				"family": "Dhar",
				"given": "Arka"
			},
			{
				"family": "Dohan",
				"given": "David"
			},
			{
				"family": "Dowling",
				"given": "Steve"
			},
			{
				"family": "Dunning",
				"given": "Sheila"
			},
			{
				"family": "Ecoffet",
				"given": "Adrien"
			},
			{
				"family": "Eleti",
				"given": "Atty"
			},
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Farhi",
				"given": "David"
			},
			{
				"family": "Fedus",
				"given": "Liam"
			},
			{
				"family": "Felix",
				"given": "Niko"
			},
			{
				"family": "Fishman",
				"given": "Simón Posada"
			},
			{
				"family": "Forte",
				"given": "Juston"
			},
			{
				"family": "Fulford",
				"given": "Isabella"
			},
			{
				"family": "Gao",
				"given": "Leo"
			},
			{
				"family": "Georges",
				"given": "Elie"
			},
			{
				"family": "Gibson",
				"given": "Christian"
			},
			{
				"family": "Goel",
				"given": "Vik"
			},
			{
				"family": "Gogineni",
				"given": "Tarun"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Gontijo-Lopes",
				"given": "Rapha"
			},
			{
				"family": "Gordon",
				"given": "Jonathan"
			},
			{
				"family": "Grafstein",
				"given": "Morgan"
			},
			{
				"family": "Gray",
				"given": "Scott"
			},
			{
				"family": "Greene",
				"given": "Ryan"
			},
			{
				"family": "Gross",
				"given": "Joshua"
			},
			{
				"family": "Gu",
				"given": "Shixiang Shane"
			},
			{
				"family": "Guo",
				"given": "Yufei"
			},
			{
				"family": "Hallacy",
				"given": "Chris"
			},
			{
				"family": "Han",
				"given": "Jesse"
			},
			{
				"family": "Harris",
				"given": "Jeff"
			},
			{
				"family": "He",
				"given": "Yuchen"
			},
			{
				"family": "Heaton",
				"given": "Mike"
			},
			{
				"family": "Heidecke",
				"given": "Johannes"
			},
			{
				"family": "Hesse",
				"given": "Chris"
			},
			{
				"family": "Hickey",
				"given": "Alan"
			},
			{
				"family": "Hickey",
				"given": "Wade"
			},
			{
				"family": "Hoeschele",
				"given": "Peter"
			},
			{
				"family": "Houghton",
				"given": "Brandon"
			},
			{
				"family": "Hsu",
				"given": "Kenny"
			},
			{
				"family": "Hu",
				"given": "Shengli"
			},
			{
				"family": "Hu",
				"given": "Xin"
			},
			{
				"family": "Huizinga",
				"given": "Joost"
			},
			{
				"family": "Jain",
				"given": "Shantanu"
			},
			{
				"family": "Jain",
				"given": "Shawn"
			},
			{
				"family": "Jang",
				"given": "Joanne"
			},
			{
				"family": "Jiang",
				"given": "Angela"
			},
			{
				"family": "Jiang",
				"given": "Roger"
			},
			{
				"family": "Jin",
				"given": "Haozhun"
			},
			{
				"family": "Jin",
				"given": "Denny"
			},
			{
				"family": "Jomoto",
				"given": "Shino"
			},
			{
				"family": "Jonn",
				"given": "Billie"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Kaftan",
				"given": "Tomer"
			},
			{
				"family": "Kaiser",
				"given": "Łukasz"
			},
			{
				"family": "Kamali",
				"given": "Ali"
			},
			{
				"family": "Kanitscheider",
				"given": "Ingmar"
			},
			{
				"family": "Keskar",
				"given": "Nitish Shirish"
			},
			{
				"family": "Khan",
				"given": "Tabarak"
			},
			{
				"family": "Kilpatrick",
				"given": "Logan"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Kim",
				"given": "Christina"
			},
			{
				"family": "Kim",
				"given": "Yongjik"
			},
			{
				"family": "Kirchner",
				"given": "Jan Hendrik"
			},
			{
				"family": "Kiros",
				"given": "Jamie"
			},
			{
				"family": "Knight",
				"given": "Matt"
			},
			{
				"family": "Kokotajlo",
				"given": "Daniel"
			},
			{
				"family": "Kondraciuk",
				"given": "Łukasz"
			},
			{
				"family": "Kondrich",
				"given": "Andrew"
			},
			{
				"family": "Konstantinidis",
				"given": "Aris"
			},
			{
				"family": "Kosic",
				"given": "Kyle"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Kuo",
				"given": "Vishal"
			},
			{
				"family": "Lampe",
				"given": "Michael"
			},
			{
				"family": "Lan",
				"given": "Ikai"
			},
			{
				"family": "Lee",
				"given": "Teddy"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Leung",
				"given": "Jade"
			},
			{
				"family": "Levy",
				"given": "Daniel"
			},
			{
				"family": "Li",
				"given": "Chak Ming"
			},
			{
				"family": "Lim",
				"given": "Rachel"
			},
			{
				"family": "Lin",
				"given": "Molly"
			},
			{
				"family": "Lin",
				"given": "Stephanie"
			},
			{
				"family": "Litwin",
				"given": "Mateusz"
			},
			{
				"family": "Lopez",
				"given": "Theresa"
			},
			{
				"family": "Lowe",
				"given": "Ryan"
			},
			{
				"family": "Lue",
				"given": "Patricia"
			},
			{
				"family": "Makanju",
				"given": "Anna"
			},
			{
				"family": "Malfacini",
				"given": "Kim"
			},
			{
				"family": "Manning",
				"given": "Sam"
			},
			{
				"family": "Markov",
				"given": "Todor"
			},
			{
				"family": "Markovski",
				"given": "Yaniv"
			},
			{
				"family": "Martin",
				"given": "Bianca"
			},
			{
				"family": "Mayer",
				"given": "Katie"
			},
			{
				"family": "Mayne",
				"given": "Andrew"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "McKinney",
				"given": "Scott Mayer"
			},
			{
				"family": "McLeavey",
				"given": "Christine"
			},
			{
				"family": "McMillan",
				"given": "Paul"
			},
			{
				"family": "McNeil",
				"given": "Jake"
			},
			{
				"family": "Medina",
				"given": "David"
			},
			{
				"family": "Mehta",
				"given": "Aalok"
			},
			{
				"family": "Menick",
				"given": "Jacob"
			},
			{
				"family": "Metz",
				"given": "Luke"
			},
			{
				"family": "Mishchenko",
				"given": "Andrey"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Monaco",
				"given": "Vinnie"
			},
			{
				"family": "Morikawa",
				"given": "Evan"
			},
			{
				"family": "Mossing",
				"given": "Daniel"
			},
			{
				"family": "Mu",
				"given": "Tong"
			},
			{
				"family": "Murati",
				"given": "Mira"
			},
			{
				"family": "Murk",
				"given": "Oleg"
			},
			{
				"family": "Mély",
				"given": "David"
			},
			{
				"family": "Nair",
				"given": "Ashvin"
			},
			{
				"family": "Nakano",
				"given": "Reiichiro"
			},
			{
				"family": "Nayak",
				"given": "Rajeev"
			},
			{
				"family": "Neelakantan",
				"given": "Arvind"
			},
			{
				"family": "Ngo",
				"given": "Richard"
			},
			{
				"family": "Noh",
				"given": "Hyeonwoo"
			},
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "O'Keefe",
				"given": "Cullen"
			},
			{
				"family": "Pachocki",
				"given": "Jakub"
			},
			{
				"family": "Paino",
				"given": "Alex"
			},
			{
				"family": "Palermo",
				"given": "Joe"
			},
			{
				"family": "Pantuliano",
				"given": "Ashley"
			},
			{
				"family": "Parascandolo",
				"given": "Giambattista"
			},
			{
				"family": "Parish",
				"given": "Joel"
			},
			{
				"family": "Parparita",
				"given": "Emy"
			},
			{
				"family": "Passos",
				"given": "Alex"
			},
			{
				"family": "Pavlov",
				"given": "Mikhail"
			},
			{
				"family": "Peng",
				"given": "Andrew"
			},
			{
				"family": "Perelman",
				"given": "Adam"
			},
			{
				"family": "Peres",
				"given": "Filipe de Avila Belbute"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Pinto",
				"given": "Henrique Ponde de Oliveira"
			},
			{
				"family": "Michael",
				"given": ""
			},
			{
				"family": "Pokorny",
				"given": ""
			},
			{
				"family": "Pokrass",
				"given": "Michelle"
			},
			{
				"family": "Pong",
				"given": "Vitchyr H."
			},
			{
				"family": "Powell",
				"given": "Tolly"
			},
			{
				"family": "Power",
				"given": "Alethea"
			},
			{
				"family": "Power",
				"given": "Boris"
			},
			{
				"family": "Proehl",
				"given": "Elizabeth"
			},
			{
				"family": "Puri",
				"given": "Raul"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Rae",
				"given": "Jack"
			},
			{
				"family": "Ramesh",
				"given": "Aditya"
			},
			{
				"family": "Raymond",
				"given": "Cameron"
			},
			{
				"family": "Real",
				"given": "Francis"
			},
			{
				"family": "Rimbach",
				"given": "Kendra"
			},
			{
				"family": "Ross",
				"given": "Carl"
			},
			{
				"family": "Rotsted",
				"given": "Bob"
			},
			{
				"family": "Roussez",
				"given": "Henri"
			},
			{
				"family": "Ryder",
				"given": "Nick"
			},
			{
				"family": "Saltarelli",
				"given": "Mario"
			},
			{
				"family": "Sanders",
				"given": "Ted"
			},
			{
				"family": "Santurkar",
				"given": "Shibani"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Schmidt",
				"given": "Heather"
			},
			{
				"family": "Schnurr",
				"given": "David"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Selsam",
				"given": "Daniel"
			},
			{
				"family": "Sheppard",
				"given": "Kyla"
			},
			{
				"family": "Sherbakov",
				"given": "Toki"
			},
			{
				"family": "Shieh",
				"given": "Jessica"
			},
			{
				"family": "Shoker",
				"given": "Sarah"
			},
			{
				"family": "Shyam",
				"given": "Pranav"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Sigler",
				"given": "Eric"
			},
			{
				"family": "Simens",
				"given": "Maddie"
			},
			{
				"family": "Sitkin",
				"given": "Jordan"
			},
			{
				"family": "Slama",
				"given": "Katarina"
			},
			{
				"family": "Sohl",
				"given": "Ian"
			},
			{
				"family": "Sokolowsky",
				"given": "Benjamin"
			},
			{
				"family": "Song",
				"given": "Yang"
			},
			{
				"family": "Staudacher",
				"given": "Natalie"
			},
			{
				"family": "Such",
				"given": "Felipe Petroski"
			},
			{
				"family": "Summers",
				"given": "Natalie"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Tezak",
				"given": "Nikolas"
			},
			{
				"family": "Thompson",
				"given": "Madeleine B."
			},
			{
				"family": "Tillet",
				"given": "Phil"
			},
			{
				"family": "Tootoonchian",
				"given": "Amin"
			},
			{
				"family": "Tseng",
				"given": "Elizabeth"
			},
			{
				"family": "Tuggle",
				"given": "Preston"
			},
			{
				"family": "Turley",
				"given": "Nick"
			},
			{
				"family": "Tworek",
				"given": "Jerry"
			},
			{
				"family": "Uribe",
				"given": "Juan Felipe Cerón"
			},
			{
				"family": "Vallone",
				"given": "Andrea"
			},
			{
				"family": "Vijayvergiya",
				"given": "Arun"
			},
			{
				"family": "Voss",
				"given": "Chelsea"
			},
			{
				"family": "Wainwright",
				"given": "Carroll"
			},
			{
				"family": "Wang",
				"given": "Justin Jay"
			},
			{
				"family": "Wang",
				"given": "Alvin"
			},
			{
				"family": "Wang",
				"given": "Ben"
			},
			{
				"family": "Ward",
				"given": "Jonathan"
			},
			{
				"family": "Wei",
				"given": "Jason"
			},
			{
				"family": "Weinmann",
				"given": "C. J."
			},
			{
				"family": "Welihinda",
				"given": "Akila"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Weng",
				"given": "Jiayi"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Wiethoff",
				"given": "Matt"
			},
			{
				"family": "Willner",
				"given": "Dave"
			},
			{
				"family": "Winter",
				"given": "Clemens"
			},
			{
				"family": "Wolrich",
				"given": "Samuel"
			},
			{
				"family": "Wong",
				"given": "Hannah"
			},
			{
				"family": "Workman",
				"given": "Lauren"
			},
			{
				"family": "Wu",
				"given": "Sherwin"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Wu",
				"given": "Michael"
			},
			{
				"family": "Xiao",
				"given": "Kai"
			},
			{
				"family": "Xu",
				"given": "Tao"
			},
			{
				"family": "Yoo",
				"given": "Sarah"
			},
			{
				"family": "Yu",
				"given": "Kevin"
			},
			{
				"family": "Yuan",
				"given": "Qiming"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Zellers",
				"given": "Rowan"
			},
			{
				"family": "Zhang",
				"given": "Chong"
			},
			{
				"family": "Zhang",
				"given": "Marvin"
			},
			{
				"family": "Zhao",
				"given": "Shengjia"
			},
			{
				"family": "Zheng",
				"given": "Tianhao"
			},
			{
				"family": "Zhuang",
				"given": "Juntang"
			},
			{
				"family": "Zhuk",
				"given": "William"
			},
			{
				"family": "Zoph",
				"given": "Barret"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					4
				]
			]
		}
	},
	{
		"id": "eloundouGPTsAreGPTs2023",
		"type": "article",
		"abstract": "We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.",
		"DOI": "10.48550/arXiv.2303.10130",
		"note": "arXiv:2303.10130 [cs, econ, q-fin]",
		"number": "arXiv:2303.10130",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
		"title-short": "GPTs are GPTs",
		"URL": "http://arxiv.org/abs/2303.10130",
		"author": [
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Manning",
				"given": "Sam"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Rock",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					21
				]
			]
		}
	},
	{
		"id": "wallaceInstructionHierarchyTraining2024",
		"type": "article",
		"abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.",
		"DOI": "10.48550/arXiv.2404.13208",
		"note": "arXiv:2404.13208 [cs]",
		"number": "arXiv:2404.13208",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
		"title-short": "The Instruction Hierarchy",
		"URL": "http://arxiv.org/abs/2404.13208",
		"author": [
			{
				"family": "Wallace",
				"given": "Eric"
			},
			{
				"family": "Xiao",
				"given": "Kai"
			},
			{
				"family": "Leike",
				"given": "Reimar"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Heidecke",
				"given": "Johannes"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					19
				]
			]
		}
	},
	{
		"id": "gaoScalingEvaluatingSparse2024",
		"type": "article",
		"abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.",
		"DOI": "10.48550/arXiv.2406.04093",
		"note": "arXiv:2406.04093 [cs]",
		"number": "arXiv:2406.04093",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling and evaluating sparse autoencoders",
		"URL": "http://arxiv.org/abs/2406.04093",
		"author": [
			{
				"family": "Gao",
				"given": "Leo"
			},
			{
				"family": "Tour",
				"given": "Tom Dupré",
				"non-dropping-particle": "la"
			},
			{
				"family": "Tillman",
				"given": "Henk"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Troll",
				"given": "Rajan"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Wu",
				"given": "Jeffrey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					6
				]
			]
		}
	},
	{
		"id": "markovHolisticApproachUndesired2023",
		"type": "article",
		"abstract": "We present a holistic approach to building a robust and useful natural language classification system for real-world content moderation. The success of such a system relies on a chain of carefully designed and executed steps, including the design of content taxonomies and labeling instructions, data quality control, an active learning pipeline to capture rare events, and a variety of methods to make the model robust and to avoid overfitting. Our moderation system is trained to detect a broad set of categories of undesired content, including sexual content, hateful content, violence, self-harm, and harassment. This approach generalizes to a wide range of different content taxonomies and can be used to create high-quality content classifiers that outperform off-the-shelf models.",
		"DOI": "10.48550/arXiv.2208.03274",
		"note": "arXiv:2208.03274 [cs]",
		"number": "arXiv:2208.03274",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Holistic Approach to Undesired Content Detection in the Real World",
		"URL": "http://arxiv.org/abs/2208.03274",
		"author": [
			{
				"family": "Markov",
				"given": "Todor"
			},
			{
				"family": "Zhang",
				"given": "Chong"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Lee",
				"given": "Teddy"
			},
			{
				"family": "Adler",
				"given": "Steven"
			},
			{
				"family": "Jiang",
				"given": "Angela"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					14
				]
			]
		}
	},
	{
		"id": "songConsistencyModels2023",
		"type": "article",
		"abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.",
		"DOI": "10.48550/arXiv.2303.01469",
		"note": "arXiv:2303.01469 [cs, stat]",
		"number": "arXiv:2303.01469",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Consistency Models",
		"URL": "http://arxiv.org/abs/2303.01469",
		"author": [
			{
				"family": "Song",
				"given": "Yang"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Chen",
				"given": "Mark"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					31
				]
			]
		}
	},
	{
		"id": "songImprovedTechniquesTraining2023",
		"type": "article",
		"abstract": "Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\\times 64$ respectively in a single sampling step. These scores mark a 3.5$\\times$ and 4$\\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
		"DOI": "10.48550/arXiv.2310.14189",
		"note": "arXiv:2310.14189 [cs]",
		"number": "arXiv:2310.14189",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improved Techniques for Training Consistency Models",
		"URL": "http://arxiv.org/abs/2310.14189",
		"author": [
			{
				"family": "Song",
				"given": "Yang"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					22
				]
			]
		}
	},
	{
		"id": "elhageMathematicalFrameworkTransformer2021a",
		"type": "article",
		"abstract": "We’ve found that many subtle details of the transformer architecture require us to approach reverse engineering it in a pretty different way from how the InceptionV1 Circuits work . We’ll unpack each of these points in the sections below, but for now we briefly summarize. We’ll also expand on a lot of the terminology we introduce here once we get to the appropriate sections. Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream. Attention heads are often described in an alternate “concatenate and multiply” formulation for computational efficiency, but this is mathematically equivalent. Attention-only models can be written as a sum of interpretable end-to-end functions mapping tokens to changes in logits. These functions correspond to “paths” through the model, and are linear if one freezes the attention patterns. Transformers have an enormous amount of linear structure. One can learn a lot simply by breaking apart sums and multiplying together chains of matrices. Attention heads can be understood as having two largely independent computations: a QK (“query-key”) circuit which computes the attention pattern, and an OV (“output-value”) circuit which computes how each token affects the output if attended to. Key, query, and value vectors can be thought of as intermediate results in the computation of the low-rank matrices W_Q^TW_K and W_OW_V. It can be useful to describe transformers without reference to them. Composition of attention heads greatly increases the expressivity of transformers. There are three different ways attention heads can compose, corresponding to keys, queries, and values. Key and query composition are very different from value composition. All components of a transformer (the token embedding, attention heads, MLP layers, and unembedding) communicate with each other by reading and writing to different subspaces of the residual stream. Rather than analyze the residual stream vectors, it can be helpful to decompose the residual stream into all these different communication channels, corresponding to paths through the model.",
		"event-place": "Unpublished",
		"publisher": "Transformer Circuits Thread",
		"publisher-place": "Unpublished",
		"title": "A mathematical framework for transformer circuits",
		"URL": "https://transformer-circuits.pub/2021/framework/index.html#citation",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "olssonIncontextLearningInduction2022a",
		"type": "article",
		"abstract": "The paper presents six complementary lines of evidence arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size: Macroscopic co-occurence: Transformer language models undergo a “phase change” early in training, during which induction heads form and simultaneously in-context learning improves dramatically. Macroscopic co-perturbation: When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way.\nDirect ablation:  When we directly “knock out” induction heads at test-time in small models, the amount of in-context learning greatly decreases.\nSpecific examples of induction head generality: Although we define induction heads very narrowly in terms of copying literal sequences, we empirically observe that these same heads also appear to implement more sophisticated types of in-context learning, including highly abstract behaviors, making it plausible they explain a large fraction of in-context learning. Mechanistic plausibility of induction head generality: For small models, we can explain mechanistically how induction heads work, and can show they contribute to in-context learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be re-purposed to perform more general in-context learning. Continuity from small to large models: In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones. However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.",
		"publisher": "Transformer Circuits Thread",
		"title": "In-context learning and induction heads",
		"URL": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#citation",
		"author": [
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "askellGeneralLanguageAssistant2021",
		"type": "article",
		"abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.",
		"DOI": "10.48550/arXiv.2112.00861",
		"note": "arXiv:2112.00861 [cs]",
		"number": "arXiv:2112.00861",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A General Language Assistant as a Laboratory for Alignment",
		"URL": "http://arxiv.org/abs/2112.00861",
		"author": [
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					9
				]
			]
		}
	},
	{
		"id": "baiTrainingHelpfulHarmless2022",
		"type": "article",
		"abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",
		"DOI": "10.48550/arXiv.2204.05862",
		"note": "arXiv:2204.05862 [cs]",
		"number": "arXiv:2204.05862",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
		"URL": "http://arxiv.org/abs/2204.05862",
		"author": [
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "El-Showk",
				"given": "Sheer"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					12
				]
			]
		}
	},
	{
		"id": "hernandezScalingLawsInterpretability2022",
		"type": "article",
		"abstract": "Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.",
		"DOI": "10.48550/arXiv.2205.10487",
		"note": "arXiv:2205.10487 [cs]",
		"number": "arXiv:2205.10487",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Laws and Interpretability of Learning from Repeated Data",
		"URL": "http://arxiv.org/abs/2205.10487",
		"author": [
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "El-Showk",
				"given": "Sheer"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					20
				]
			]
		}
	},
	{
		"id": "elhageSoftmaxLinearUnits2022",
		"type": "article",
		"abstract": "In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be \"interpretable\" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers.  However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by “hiding” others and thus making them even more deeply uninterpretable.  Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand.",
		"event-place": "Transformer Circuits Thread",
		"publisher-place": "Transformer Circuits Thread",
		"title": "Softmax linear units",
		"URL": "https://transformer-circuits.pub/2022/solu/index.html",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "ElShowk",
				"given": "Sheer"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Jacobson",
				"given": "Josh"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kadavathLanguageModelsMostly2022",
		"type": "article",
		"abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability \"P(True)\" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict \"P(IK)\", the probability that \"I know\" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.",
		"DOI": "10.48550/arXiv.2207.05221",
		"note": "arXiv:2207.05221 [cs]",
		"number": "arXiv:2207.05221",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Models (Mostly) Know What They Know",
		"URL": "http://arxiv.org/abs/2207.05221",
		"author": [
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "El-Showk",
				"given": "Sheer"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Bowman",
				"given": "Sam"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Jacobson",
				"given": "Josh"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Ringer",
				"given": "Sam"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					21
				]
			]
		}
	},
	{
		"id": "ganguliRedTeamingLanguage2022",
		"type": "article",
		"abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
		"DOI": "10.48550/arXiv.2209.07858",
		"note": "arXiv:2209.07858 [cs]",
		"number": "arXiv:2209.07858",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
		"title-short": "Red Teaming Language Models to Reduce Harms",
		"URL": "http://arxiv.org/abs/2209.07858",
		"author": [
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Bowman",
				"given": "Sam"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "El-Showk",
				"given": "Sheer"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Jacobson",
				"given": "Josh"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Ringer",
				"given": "Sam"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Clark",
				"given": "Jack"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					22
				]
			]
		}
	},
	{
		"id": "elhageToyModelsSuperposition2022",
		"type": "article",
		"abstract": "Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in \"superposition.\" We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.",
		"DOI": "10.48550/arXiv.2209.10652",
		"note": "arXiv:2209.10652 [cs]",
		"number": "arXiv:2209.10652",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toy Models of Superposition",
		"URL": "http://arxiv.org/abs/2209.10652",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					21
				]
			]
		}
	},
	{
		"id": "bowmanMeasuringProgressScalable2022",
		"type": "article",
		"abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.",
		"DOI": "10.48550/arXiv.2211.03540",
		"note": "arXiv:2211.03540 [cs]",
		"number": "arXiv:2211.03540",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Measuring Progress on Scalable Oversight for Large Language Models",
		"URL": "http://arxiv.org/abs/2211.03540",
		"author": [
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Hyun",
				"given": "Jeeyoon"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Chen",
				"given": "Edwin"
			},
			{
				"family": "Pettit",
				"given": "Craig"
			},
			{
				"family": "Heiner",
				"given": "Scott"
			},
			{
				"family": "Lukošiūtė",
				"given": "Kamilė"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Goldie",
				"given": "Anna"
			},
			{
				"family": "Mirhoseini",
				"given": "Azalia"
			},
			{
				"family": "McKinnon",
				"given": "Cameron"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Amodei",
				"given": "Daniela"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Kerr",
				"given": "Jamie"
			},
			{
				"family": "Mueller",
				"given": "Jared"
			},
			{
				"family": "Ladish",
				"given": "Jeffrey"
			},
			{
				"family": "Landau",
				"given": "Joshua"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mercado",
				"given": "Noemí"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Showk",
				"given": "Sheer El"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		}
	},
	{
		"id": "perezDiscoveringLanguageModel2022",
		"type": "article",
		"abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.",
		"DOI": "10.48550/arXiv.2212.09251",
		"note": "arXiv:2212.09251 [cs]",
		"number": "arXiv:2212.09251",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering Language Model Behaviors with Model-Written Evaluations",
		"URL": "http://arxiv.org/abs/2212.09251",
		"author": [
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Ringer",
				"given": "Sam"
			},
			{
				"family": "Lukošiūtė",
				"given": "Kamilė"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Chen",
				"given": "Edwin"
			},
			{
				"family": "Heiner",
				"given": "Scott"
			},
			{
				"family": "Pettit",
				"given": "Craig"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Israel",
				"given": "Brian"
			},
			{
				"family": "Seethor",
				"given": "Bryan"
			},
			{
				"family": "McKinnon",
				"given": "Cameron"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Yan",
				"given": "Da"
			},
			{
				"family": "Amodei",
				"given": "Daniela"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Khundadze",
				"given": "Guro"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Landis",
				"given": "James"
			},
			{
				"family": "Kerr",
				"given": "Jamie"
			},
			{
				"family": "Mueller",
				"given": "Jared"
			},
			{
				"family": "Hyun",
				"given": "Jeeyoon"
			},
			{
				"family": "Landau",
				"given": "Joshua"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Goldberg",
				"given": "Landon"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Lucas",
				"given": "Martin"
			},
			{
				"family": "Sellitto",
				"given": "Michael"
			},
			{
				"family": "Zhang",
				"given": "Miranda"
			},
			{
				"family": "Kingsland",
				"given": "Neerav"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mercado",
				"given": "Noemí"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Showk",
				"given": "Sheer El"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					19
				]
			]
		}
	},
	{
		"id": "ganguliCapacityMoralSelfCorrection2023",
		"type": "article",
		"abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
		"DOI": "10.48550/arXiv.2302.07459",
		"note": "arXiv:2302.07459 [cs]",
		"number": "arXiv:2302.07459",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Capacity for Moral Self-Correction in Large Language Models",
		"URL": "http://arxiv.org/abs/2302.07459",
		"author": [
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Lukošiūtė",
				"given": "Kamilė"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Goldie",
				"given": "Anna"
			},
			{
				"family": "Mirhoseini",
				"given": "Azalia"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Kerr",
				"given": "Jamie"
			},
			{
				"family": "Mueller",
				"given": "Jared"
			},
			{
				"family": "Landau",
				"given": "Joshua"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Sellitto",
				"given": "Michael"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Mercado",
				"given": "Noemi"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "Ringer",
				"given": "Sam"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Showk",
				"given": "Sheer El"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					18
				]
			]
		}
	},
	{
		"id": "durmusMeasuringRepresentationSubjective2024",
		"type": "article",
		"abstract": "Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.",
		"DOI": "10.48550/arXiv.2306.16388",
		"note": "arXiv:2306.16388 [cs]",
		"number": "arXiv:2306.16388",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
		"URL": "http://arxiv.org/abs/2306.16388",
		"author": [
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bakhtin",
				"given": "Anton"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Sikder",
				"given": "Orowa"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Thamkul",
				"given": "Janel"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					11
				]
			]
		}
	},
	{
		"id": "radhakrishnanQuestionDecompositionImproves2023",
		"type": "article",
		"abstract": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model’s actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model’s stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",
		"language": "en",
		"source": "Zotero",
		"title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
		"author": [
			{
				"family": "Radhakrishnan",
				"given": "Ansh"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lukošiūtė",
				"given": "Kamilė"
			},
			{
				"family": "Cheng",
				"given": "Newton"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Showk",
				"given": "Sheer El"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Maxwell",
				"given": "Tim"
			},
			{
				"family": "Chandrasekaran",
				"given": "Venkatesa"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Brauner",
				"given": "Jan"
			},
			{
				"family": "Bowman",
				"given": "Samuel R"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			],
			"season": "ly"
		}
	},
	{
		"id": "lanhamMeasuringFaithfulnessChainofThought2023",
		"type": "article",
		"abstract": "Large language models (LLMs) perform better when they produce step-by-step, “Chain-ofThought” (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model’s actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT’s performance boost does not seem to come from CoT’s added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",
		"language": "en",
		"source": "Zotero",
		"title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
		"author": [
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Radhakrishnan",
				"given": "Ansh"
			},
			{
				"family": "Steiner",
				"given": "Benoit"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lukosiute",
				"given": "Kamile"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Cheng",
				"given": "Newton"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Yang",
				"given": "Shannon"
			},
			{
				"family": "Henighan",
				"given": "Thomas"
			},
			{
				"family": "Maxwell",
				"given": "Timothy"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Brauner",
				"given": "Jan"
			},
			{
				"family": "Bowman",
				"given": "Samuel R"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "grosseStudyingLargeLanguage2023",
		"type": "article",
		"abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.",
		"DOI": "10.48550/arXiv.2308.03296",
		"note": "arXiv:2308.03296 [cs, stat]",
		"number": "arXiv:2308.03296",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Studying Large Language Model Generalization with Influence Functions",
		"URL": "http://arxiv.org/abs/2308.03296",
		"author": [
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Bae",
				"given": "Juhan"
			},
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Tajdini",
				"given": "Amirhossein"
			},
			{
				"family": "Steiner",
				"given": "Benoit"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Lukošiūtė",
				"given": "Kamilė"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					7
				]
			]
		}
	},
	{
		"id": "brickenMonosemanticityDecomposingLanguage2023",
		"type": "article",
		"abstract": "Sparse Autoencoders extract relatively monosemantic features. We provide four different lines of evidence: detailed investigations for a few features firing in specific contexts for which we can construct computational proxies, human analysis for a large random sample of features, automated interpretability analysis of activations for all the features learned by the autoencoder, and finally automated interpretability analysis of logit weights for all the features. Moreover, the last three analyses show that most learned features are interpretable. While we do not claim that our interpretations catch all aspects of features' behaviors, by constructing metrics of interpretability consistently for features and neurons, we quantitatively show their relative interpretability. Sparse autoencoders produce interpretable features that are effectively invisible in the neuron basis. We find features (e.g., one firing on Hebrew script) which are not active in any of the top dataset examples for any of the neurons. Sparse autoencoder features can be used to intervene on and steer transformer generation. For example, activating the base64 feature we study causes the model to generate base64 text, and activating the Arabic script feature we study produces Arabic text. Sparse autoencoders produce relatively universal features. Sparse autoencoders applied to different transformer language models produce mostly similar features, more similar to one another than they are to their own model's neurons. Features appear to \"split\" as we increase autoencoder size. When we gradually increase the width of the autoencoder from 512 (the number of neurons) to over 131,000 (256×), we find features which naturally fit together into families. For example, one base64 feature in a small dictionary splits into three, with more subtle and yet still interpretable roles, in a larger dictionary. The different size autoencoders offer different \"resolutions\" for understanding the same object. Just 512 neurons can represent tens of thousands of features. Despite the MLP layer being very small, we continue to find new features as we scale the sparse autoencoder. Features connect in \"finite-state automata\"-like systems that implement complex behaviors. For example, we find features that work together to generate valid HTML.",
		"publisher": "Transformer Circuits Thread",
		"title": "Towards monosemanticity: Decomposing language models with dictionary learning",
		"author": [
			{
				"family": "Bricken",
				"given": "Trenton"
			},
			{
				"family": "Templeton",
				"given": "Adly"
			},
			{
				"family": "Batson",
				"given": "Joshua"
			},
			{
				"family": "Chen",
				"given": "Brian"
			},
			{
				"family": "Jermyn",
				"given": "Adam"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Turner",
				"given": "Nick"
			},
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Wu",
				"given": "Yifan"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Maxwell",
				"given": "Tim"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "McLean",
				"given": "Brayden"
			},
			{
				"family": "Burke",
				"given": "Josiah E"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "baiConstitutionalAIHarmlessness2022",
		"type": "article",
		"abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",
		"DOI": "10.48550/arXiv.2212.08073",
		"note": "arXiv:2212.08073 [cs]",
		"number": "arXiv:2212.08073",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Constitutional AI: Harmlessness from AI Feedback",
		"title-short": "Constitutional AI",
		"URL": "http://arxiv.org/abs/2212.08073",
		"author": [
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Goldie",
				"given": "Anna"
			},
			{
				"family": "Mirhoseini",
				"given": "Azalia"
			},
			{
				"family": "McKinnon",
				"given": "Cameron"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Li",
				"given": "Dustin"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Kerr",
				"given": "Jamie"
			},
			{
				"family": "Mueller",
				"given": "Jared"
			},
			{
				"family": "Ladish",
				"given": "Jeffrey"
			},
			{
				"family": "Landau",
				"given": "Joshua"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Lukosuite",
				"given": "Kamile"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Sellitto",
				"given": "Michael"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Mercado",
				"given": "Noemi"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "Ringer",
				"given": "Sam"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Showk",
				"given": "Sheer El"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					15
				]
			]
		}
	},
	{
		"id": "sharmaUnderstandingSycophancyLanguage2023",
		"type": "article",
		"abstract": "Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.",
		"DOI": "10.48550/arXiv.2310.13548",
		"note": "arXiv:2310.13548 [cs, stat]",
		"number": "arXiv:2310.13548",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Understanding Sycophancy in Language Models",
		"URL": "http://arxiv.org/abs/2310.13548",
		"author": [
			{
				"family": "Sharma",
				"given": "Mrinank"
			},
			{
				"family": "Tong",
				"given": "Meg"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Cheng",
				"given": "Newton"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Johnston",
				"given": "Scott R."
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Maxwell",
				"given": "Timothy"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Yan",
				"given": "Da"
			},
			{
				"family": "Zhang",
				"given": "Miranda"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "kunduSpecificGeneralPrinciples2023",
		"type": "article",
		"abstract": "Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as \"do what's best for humanity\". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.",
		"DOI": "10.48550/arXiv.2310.13798",
		"note": "arXiv:2310.13798 [cs]",
		"number": "arXiv:2310.13798",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Specific versus General Principles for Constitutional AI",
		"URL": "http://arxiv.org/abs/2310.13798",
		"author": [
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Callahan",
				"given": "Andrew"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Goldie",
				"given": "Anna"
			},
			{
				"family": "Balwit",
				"given": "Avital"
			},
			{
				"family": "Mirhoseini",
				"given": "Azalia"
			},
			{
				"family": "McLean",
				"given": "Brayden"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Evraets",
				"given": "Cassie"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Kerr",
				"given": "Jamie"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Cheng",
				"given": "Newton"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Rausch",
				"given": "Oliver"
			},
			{
				"family": "Larson",
				"given": "Robin"
			},
			{
				"family": "Yang",
				"given": "Shannon"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Telleen-Lawton",
				"given": "Timothy"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Mindermann",
				"given": "Sören"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					20
				]
			]
		}
	},
	{
		"id": "tamkinEvaluatingMitigatingDiscrimination2023",
		"type": "article",
		"abstract": "As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval",
		"DOI": "10.48550/arXiv.2312.03689",
		"note": "arXiv:2312.03689 [cs]",
		"number": "arXiv:2312.03689",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating and Mitigating Discrimination in Language Model Decisions",
		"URL": "http://arxiv.org/abs/2312.03689",
		"author": [
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					6
				]
			]
		}
	},
	{
		"id": "anilManyshotJailbreaking2024",
		"type": "article",
		"abstract": "We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closedweight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Many-shot Jailbreaking",
		"author": [
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Sharma",
				"given": "Mrinank"
			},
			{
				"family": "Benton",
				"given": "Joe"
			},
			{
				"family": "Kundu",
				"given": "Sandipan"
			},
			{
				"family": "Batson",
				"given": "Joshua"
			},
			{
				"family": "Rimsky",
				"given": "Nina"
			},
			{
				"family": "Tong",
				"given": "Meg"
			},
			{
				"family": "Mu",
				"given": "Jesse"
			},
			{
				"family": "Ford",
				"given": "Daniel"
			},
			{
				"family": "Mosconi",
				"given": "Francesco"
			},
			{
				"family": "Agrawal",
				"given": "Rajashree"
			},
			{
				"family": "Schaeffer",
				"given": "Rylan"
			},
			{
				"family": "Bashkansky",
				"given": "Naomi"
			},
			{
				"family": "Svenningsen",
				"given": "Samuel"
			},
			{
				"family": "Lambert",
				"given": "Mike"
			},
			{
				"family": "Radhakrishnan",
				"given": "Ansh"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Hubinger",
				"given": "Evan J"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Bricken",
				"given": "Trenton"
			},
			{
				"family": "Maxwell",
				"given": "Timothy"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Sully",
				"given": "Jamie"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Bowman",
				"given": "Samuel R"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hubingerRisksLearnedOptimization2021",
		"type": "article",
		"abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer—a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be—how will it diﬀer from the loss function it was trained under—and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",
		"language": "en",
		"note": "arXiv:1906.01820 [cs]",
		"number": "arXiv:1906.01820",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Risks from Learned Optimization in Advanced Machine Learning Systems",
		"URL": "http://arxiv.org/abs/1906.01820",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Merwijk",
				"given": "Chris",
				"non-dropping-particle": "van"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			},
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Garrabrant",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					1
				]
			]
		}
	},
	{
		"id": "hubingerSleeperAgentsTraining2024",
		"type": "article",
		"abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.",
		"DOI": "10.48550/arXiv.2401.05566",
		"note": "arXiv:2401.05566 [cs]",
		"number": "arXiv:2401.05566",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
		"title-short": "Sleeper Agents",
		"URL": "http://arxiv.org/abs/2401.05566",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Mu",
				"given": "Jesse"
			},
			{
				"family": "Lambert",
				"given": "Mike"
			},
			{
				"family": "Tong",
				"given": "Meg"
			},
			{
				"family": "MacDiarmid",
				"given": "Monte"
			},
			{
				"family": "Lanham",
				"given": "Tamera"
			},
			{
				"family": "Ziegler",
				"given": "Daniel M."
			},
			{
				"family": "Maxwell",
				"given": "Tim"
			},
			{
				"family": "Cheng",
				"given": "Newton"
			},
			{
				"family": "Jermyn",
				"given": "Adam"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Radhakrishnan",
				"given": "Ansh"
			},
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Barez",
				"given": "Fazl"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Sachan",
				"given": "Kshitij"
			},
			{
				"family": "Sellitto",
				"given": "Michael"
			},
			{
				"family": "Sharma",
				"given": "Mrinank"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Witten",
				"given": "Zachary"
			},
			{
				"family": "Favaro",
				"given": "Marina"
			},
			{
				"family": "Brauner",
				"given": "Jan"
			},
			{
				"family": "Karnofsky",
				"given": "Holden"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Graham",
				"given": "Logan"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Mindermann",
				"given": "Sören"
			},
			{
				"family": "Greenblatt",
				"given": "Ryan"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					17
				]
			]
		}
	},
	{
		"id": "templetonScalingMonosemanticityExtracting2024",
		"type": "article",
		"abstract": "Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders. The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references. There appears to be a systematic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. Features can be used to steer large models (see e.g. Influence on Behavior). This extends prior work on steering models using other methods (see Related Work). We observe features related to a broad range of safety concerns, including deception, sycophancy, bias, and dangerous content.",
		"event-place": "Transformer Circuits Thread",
		"publisher-place": "Transformer Circuits Thread",
		"title": "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet",
		"URL": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html",
		"author": [
			{
				"family": "Templeton",
				"given": "Adly"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Marcus",
				"given": "Jonathan"
			},
			{
				"family": "Lindsey",
				"given": "Jack"
			},
			{
				"family": "Bricken",
				"given": "Trenton"
			},
			{
				"family": "Chen",
				"given": "Brian"
			},
			{
				"family": "Pearce",
				"given": "Adam"
			},
			{
				"family": "Citro",
				"given": "Craig"
			},
			{
				"family": "Ameisen",
				"given": "Emmanuel"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Cunningham",
				"given": "Hoagy"
			},
			{
				"family": "Turner",
				"given": "Nicholas L"
			},
			{
				"family": "McDougall",
				"given": "Callum"
			},
			{
				"family": "MacDiarmid",
				"given": "Monte"
			},
			{
				"family": "Freeman",
				"given": "C. Daniel"
			},
			{
				"family": "Sumers",
				"given": "Theodore R."
			},
			{
				"family": "Rees",
				"given": "Edward"
			},
			{
				"family": "Batson",
				"given": "Joshua"
			},
			{
				"family": "Jermyn",
				"given": "Adam"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "denisonSycophancySubterfugeInvestigating2024",
		"type": "article",
		"abstract": "In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.",
		"DOI": "10.48550/arXiv.2406.10162",
		"note": "arXiv:2406.10162 [cs]",
		"number": "arXiv:2406.10162",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models",
		"title-short": "Sycophancy to Subterfuge",
		"URL": "http://arxiv.org/abs/2406.10162",
		"author": [
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "MacDiarmid",
				"given": "Monte"
			},
			{
				"family": "Barez",
				"given": "Fazl"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Marks",
				"given": "Samuel"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Soklaski",
				"given": "Ryan"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					17
				]
			]
		}
	},
	{
		"id": "sotalaSuperintelligenceCauseCure2017",
		"type": "article-journal",
		"abstract": "Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.",
		"container-title": "Informatica",
		"ISSN": "1854-3871",
		"issue": "4",
		"language": "en",
		"license": "I assign to  Informatica ,  An International Journal of Computing and Informatics  (\"Journal\") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (\"Paper\") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",
		"note": "number: 4",
		"source": "www.informatica.si",
		"title": "Superintelligence As a Cause or Cure For Risks of Astronomical Suffering",
		"URL": "https://www.informatica.si/index.php/informatica/article/view/1877",
		"volume": "41",
		"author": [
			{
				"family": "Sotala",
				"given": "Kaj"
			},
			{
				"family": "Gloor",
				"given": "Lukas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					12,
					27
				]
			]
		}
	},
	{
		"id": "sotalaHowFeasibleRapid2017",
		"type": "article-journal",
		"abstract": "What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.",
		"container-title": "Physica Scripta",
		"DOI": "10.1088/1402-4896/aa90e8",
		"ISSN": "1402-4896",
		"issue": "11",
		"journalAbbreviation": "Phys. Scr.",
		"language": "en",
		"note": "publisher: IOP Publishing",
		"page": "113001",
		"source": "Institute of Physics",
		"title": "How feasible is the rapid development of artificial superintelligence?",
		"URL": "https://dx.doi.org/10.1088/1402-4896/aa90e8",
		"volume": "92",
		"author": [
			{
				"family": "Sotala",
				"given": "Kaj"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					10
				]
			]
		}
	},
	{
		"id": "baumLongtermTrajectoriesHuman2019",
		"type": "article-journal",
		"abstract": "Purpose This paper aims to formalize long-term trajectories of human civilization as a scientific and ethical field of study. The long-term trajectory of human civilization can be defined as the path that human civilization takes during the entire future time period in which human civilization could continue to exist. Design/methodology/approach This paper focuses on four types of trajectories: status quo trajectories, in which human civilization persists in a state broadly similar to its current state into the distant future; catastrophe trajectories, in which one or more events cause significant harm to human civilization; technological transformation trajectories, in which radical technological breakthroughs put human civilization on a fundamentally different course; and astronomical trajectories, in which human civilization expands beyond its home planet and into the accessible portions of the cosmos. Findings Status quo trajectories appear unlikely to persist into the distant future, especially in light of long-term astronomical processes. Several catastrophe, technological transformation and astronomical trajectories appear possible. Originality/value Some current actions may be able to affect the long-term trajectory. Whether these actions should be pursued depends on a mix of empirical and ethical factors. For some ethical frameworks, these actions may be especially important to pursue.",
		"container-title": "foresight",
		"DOI": "10.1108/FS-04-2018-0037",
		"ISSN": "1463-6689",
		"issue": "1",
		"note": "publisher: Emerald Publishing Limited",
		"page": "53-83",
		"source": "Emerald Insight",
		"title": "Long-term trajectories of human civilization",
		"URL": "https://doi.org/10.1108/FS-04-2018-0037",
		"volume": "21",
		"author": [
			{
				"family": "Baum",
				"given": "Seth D."
			},
			{
				"family": "Armstrong",
				"given": "Stuart"
			},
			{
				"family": "Ekenstedt",
				"given": "Timoteus"
			},
			{
				"family": "Häggström",
				"given": "Olle"
			},
			{
				"family": "Hanson",
				"given": "Robin"
			},
			{
				"family": "Kuhlemann",
				"given": "Karin"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			},
			{
				"family": "Miller",
				"given": "James D."
			},
			{
				"family": "Salmela",
				"given": "Markus"
			},
			{
				"family": "Sandberg",
				"given": "Anders"
			},
			{
				"family": "Sotala",
				"given": "Kaj"
			},
			{
				"family": "Torres",
				"given": "Phil"
			},
			{
				"family": "Turchin",
				"given": "Alexey"
			},
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					1
				]
			]
		}
	},
	{
		"id": "knutssonVirtuePrecautionRegarding2017",
		"type": "article-journal",
		"abstract": "We address the moral importance of fish, invertebrates such as crustaceans, snails and insects, and other animals about which there is qualified scientific uncertainty about their sentience. We argue that, on a sentientist basis, one can at least say that how such animals fare make ethically significant claims on our character. It is a requirement of a morally decent (or virtuous) person that she at least pays attention to and is cautious regarding the possibly morally relevant aspects of such animals. This involves having a moral stance, in the sense of patterns of perception, such that one notices such animals as being morally relevant in various situations. For the person who does not already consider these animals in this way, this could be a big change in moral psychology, and can be assumed to have behavioural consequences, albeit indeterminate. Character has been largely neglected in the literature, which focuses on act-centred approaches (i.e. that the evidence on sentience supports, or does not support, taking some specific action). We see our character-centred approach as complementary to, not superior to, act-centred approaches. Our approach has the advantage of allowing us to make ethically interesting and practically relevant claims about a wider range of cases, but it has the drawback of providing less specific action guidance.",
		"container-title": "Journal of Agricultural and Environmental Ethics",
		"DOI": "10.1007/s10806-017-9662-y",
		"ISSN": "1573-322X",
		"issue": "2",
		"journalAbbreviation": "J Agric Environ Ethics",
		"language": "en",
		"page": "213-224",
		"source": "Springer Link",
		"title": "A Virtue of Precaution Regarding the Moral Status of Animals with Uncertain Sentience",
		"URL": "https://doi.org/10.1007/s10806-017-9662-y",
		"volume": "30",
		"author": [
			{
				"family": "Knutsson",
				"given": "Simon"
			},
			{
				"family": "Munthe",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					1
				]
			]
		}
	},
	{
		"id": "tomasikReducingLongtermRisks2020",
		"type": "article",
		"abstract": "Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history. Malevolent individuals in positions of power could negatively affect humanity’s long-term trajectory by, for example, exacerbating international conflict or other broad risk factors. Malevolent humans with access to advanced technology such as whole brain emulation or other forms of transformative AI could cause serious existential risks and suffering risks. We therefore consider interventions to reduce the expected influence of malevolent humans on the long-term future. o The development of manipulation-proof measures of malevolence seems valuable, since they could be used to screen for malevolent humans in high-impact settings, such as heads of government or CEOs. We also explore possible future technologies that may offer unprecedented leverage to mitigate against malevolent traits. Selecting against psychopathic and sadistic tendencies in genetically enhanced, highly intelligent humans might be particularly important. However, risks of unintended negative consequences must be handled with extreme caution. We argue that further work on reducing malevolence would be valuable from many moral perspectives and constitutes a promising focus area for longtermist EAs.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Reducing long-term risks from malevolent actors",
		"author": [
			{
				"family": "Tomasik",
				"given": "Brian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "oesterheldMultiversewideCooperationCorrelated2017",
		"type": "article",
		"abstract": "Some decision theorists argue that when playing a prisoner’s dilemma-type game against a suﬃciently similar opponent, we should cooperate to make it more likely that our opponent also cooperates. This idea, which Hofstadter calls superrationality, has strong implications when combined with the insight from modern physics that we probably live in a large universe or multiverse of some sort. If we care about what happens in civilizations located elsewhere in the multiverse, we can superrationally cooperate with some of their inhabitants. That is, if we take their values into account, this makes it more likely that they do the same for us. In this paper, I attempt to assess the practical implications of this idea. I argue that to reap the full gains from trade, everyone should maximize the same impartially weighted sum of the utility functions of all collaborators. I also argue that we can obtain at least weak evidence about the content of these utility functions. In practice, the application of superrationality implies that we should promote causal cooperation, moral pluralism, moral reﬂection, and ensure that our descendants, who will be smarter and thus better at ﬁnding out how to beneﬁt other superrationalists in the universe, engage in superrational cooperation.",
		"language": "en",
		"source": "Zotero",
		"title": "Multiverse-wide Cooperation via Correlated Decision Making",
		"author": [
			{
				"family": "Oesterheld",
				"given": "Caspar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "oesterheldApprovaldirectedAgencyDecision2021",
		"type": "article-journal",
		"abstract": "Decision theorists disagree about how instrumentally rational agents, i.e., agents trying to achieve some goal, should behave in so-called Newcomb-like problems, with the main contenders being causal and evidential decision theory. Since the main goal of artificial intelligence research is to create machines that make instrumentally rational decisions, the disagreement pertains to this field. In addition to the more philosophical question of what the right decision theory is, the goal of AI poses the question of how to implement any given decision theory in an AI. For example, how would one go about building an AI whose behavior matches evidential decision theory’s recommendations? Conversely, we can ask which decision theories (if any) describe the behavior of any existing AI design. In this paper, we study what decision theory an approval-directed agent, i.e., an agent whose goal it is to maximize the score it receives from an overseer, implements. If we assume that the overseer rewards the agent based on the expected value of some von Neumann–Morgenstern utility function, then such an approval-directed agent is guided by two decision theories: the one used by the agent to decide which action to choose in order to maximize the reward and the one used by the overseer to compute the expected utility of a chosen action. We show which of these two decision theories describes the agent’s behavior in which situations.",
		"container-title": "Synthese",
		"DOI": "10.1007/s11229-019-02148-2",
		"ISSN": "1573-0964",
		"issue": "27",
		"journalAbbreviation": "Synthese",
		"language": "en",
		"page": "6491-6504",
		"source": "Springer Link",
		"title": "Approval-directed agency and the decision theory of Newcomb-like problems",
		"URL": "https://doi.org/10.1007/s11229-019-02148-2",
		"volume": "198",
		"author": [
			{
				"family": "Oesterheld",
				"given": "Caspar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					1
				]
			]
		}
	},
	{
		"id": "bellReinforcementLearningNewcomblike2021",
		"type": "paper-conference",
		"abstract": "Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not ratiﬁable, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents – for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratiﬁable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy.",
		"event-title": "NeurIPS 2021",
		"language": "en",
		"source": "Zotero",
		"title": "Reinforcement Learning in Newcomblike Environments",
		"author": [
			{
				"family": "Bell",
				"given": "James"
			},
			{
				"family": "Oesterheld",
				"given": "Caspar"
			},
			{
				"family": "Linsefors",
				"given": "Linda"
			},
			{
				"family": "Skalse",
				"given": "Joar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "treutleinModelingEvidentialCooperation2023",
		"type": "article",
		"abstract": "Evidential cooperation in large worlds (ECL) refers to the idea that humans and other agents can benefit by cooperating with similar agents with differing values in causally disconnected parts of a large universe. Cooperating provides agents with evidence that other similar agents are likely to cooperate too, resulting in gains from trade for all. This could be a crucial consideration for altruists.",
		"language": "en",
		"note": "arXiv:2307.04879 [econ, q-fin]",
		"number": "arXiv:2307.04879",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Modeling evidential cooperation in large worlds",
		"URL": "http://arxiv.org/abs/2307.04879",
		"author": [
			{
				"family": "Treutlein",
				"given": "Johannes"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					8
				]
			]
		}
	},
	{
		"id": "sauerbergComputingOptimalCommitments2024",
		"type": "article",
		"abstract": "Prior work has studied the computational complexity of computing optimal strategies to commit to in Stackelberg or leadership games, where a leader commits to a strategy which is observed by one or more followers. We extend this setting to one where the leader can additionally commit to outcome-conditional utility transfers. We characterize the computational complexity of finding optimal strategies in normal-form and Bayesian games, giving a mix of efficient algorithms and NP-hardness results. Finally, we allow the leader to also commit to a signaling scheme which induces a correlated equilibrium. In this setting, optimal commitments can be found in polynomial time for arbitrarily many players.",
		"DOI": "10.48550/arXiv.2402.06626",
		"note": "arXiv:2402.06626 [cs]",
		"number": "arXiv:2402.06626",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Computing Optimal Commitments to Strategies and Outcome-Conditional Utility Transfers",
		"URL": "http://arxiv.org/abs/2402.06626",
		"author": [
			{
				"family": "Sauerberg",
				"given": "Nathaniel"
			},
			{
				"family": "Oesterheld",
				"given": "Caspar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					10
				]
			]
		}
	},
	{
		"id": "cliftonCooperationLearningGames2020",
		"type": "article",
		"abstract": "Suppose that several actors are going to deploy learning agents to act on their behalf. What principles should guide these actors in designing their agents, given that they may have competing goals? An appealing solution concept in this setting is welfare-optimal learning equilibrium. This means that the learning agents should constitute a Nash equilibrium whose payoﬀ proﬁle is optimal according to some measure of total welfare (welfare function). In this work, we construct a class of learning algorithms in this spirit called learning tit-for-tat (L-TFT). L-TFT algorithms maximize a welfare function according to a speciﬁed optimization schedule, and punish their counterpart when they detect that they are deviating from this plan. Because the policies of other agents are not in general fully observed, agents must infer whether their counterpart is following a cooperative learning algorithm. This requires us to develop new techniques for making inferences about counterpart learning algorithms. In two sequential social dilemmas, our L-TFT algorithms successfully cooperate in self-play while eﬀectively avoiding exploitation by and punishing defecting learning algorithms.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Towards cooperation in learning games",
		"author": [
			{
				"family": "Clifton",
				"given": "Jesse"
			},
			{
				"family": "Riché",
				"given": "Maxime"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "digiovanniEvolutionaryStabilityOtherRegarding2023",
		"type": "article",
		"abstract": "The evolution of preferences that account for other agents’ ﬁtness, or other-regarding preferences, has been modeled with the “indirect approach” to evolutionary game theory. Under the indirect evolutionary approach, agents make decisions by optimizing a subjective utility function. Evolution may select for subjective preferences that differ from the ﬁtness function, and in particular, subjective preferences for increasing or reducing other agents’ ﬁtness. However, indirect evolutionary models typically artiﬁcially restrict the space of strategies that agents might use (assuming that agents always play a Nash equilibrium under their subjective preferences), and dropping this restriction can undermine the ﬁnding that other-regarding preferences are selected for. Can the indirect evolutionary approach still be used to explain the apparent existence of other-regarding preferences, like altruism, in humans? We argue that it can, by accounting for the costs associated with the complexity of strategies, giving (to our knowledge) the ﬁrst account of the relationship between strategy complexity and the evolution of preferences. Our model formalizes the intuition that agents face tradeoffs between the cognitive costs of strategies and how well they interpolate across contexts. For a single game, these complexity costs lead to selection for a simple ﬁxed-action strategy, but across games, when there is a sufﬁciently large cost to a strategy’s number of context-speciﬁc parameters, a strategy of maximizing subjective (other-regarding) utility is stable again. Overall, our analysis provides a more nuanced picture of when other-regarding preferences will evolve.",
		"language": "en",
		"note": "arXiv:2207.03178 [cs]",
		"number": "arXiv:2207.03178",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evolutionary Stability of Other-Regarding Preferences Under Complexity Costs",
		"URL": "http://arxiv.org/abs/2207.03178",
		"author": [
			{
				"family": "DiGiovanni",
				"given": "Anthony"
			},
			{
				"family": "Macé",
				"given": "Nicolas"
			},
			{
				"family": "Clifton",
				"given": "Jesse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					18
				]
			]
		}
	},
	{
		"id": "oesterheldRobustProgramEquilibrium2019",
		"type": "article-journal",
		"abstract": "One approach to achieving cooperation in the one-shot prisoner’s dilemma is Tennenholtz’s (Games Econ Behav 49(2):363–373, 2004) program equilibrium, in which the players of a game submit programs instead of strategies. These programs are then allowed to read each other’s source code to decide which action to take. As shown by Tennenholtz, cooperation is played in an equilibrium of this alternative game. In particular, he proposes that the two players submit the same version of the following program: cooperate if the opponent is an exact copy of this program and defect otherwise. Neither of the two players can benefit from submitting a different program. Unfortunately, this equilibrium is fragile and unlikely to be realized in practice. We thus propose a new, simple program to achieve more robust cooperative program equilibria: cooperate with some small probability $$\\epsilon $$and otherwise act as the opponent acts against this program. I argue that this program is similar to the tit for tat strategy for the iterated prisoner’s dilemma. Both “start” by cooperating and copy their opponent’s behavior from “the last round”. We then generalize this approach of turning strategies for the repeated version of a game into programs for the one-shot version of a game to other two-player games. We prove that the resulting programs inherit properties of the underlying strategy. This enables them to robustly and effectively elicit the same responses as the underlying strategy for the repeated game.",
		"container-title": "Theory and Decision",
		"DOI": "10.1007/s11238-018-9679-3",
		"ISSN": "1573-7187",
		"issue": "1",
		"journalAbbreviation": "Theory Decis",
		"language": "en",
		"page": "143-159",
		"source": "Springer Link",
		"title": "Robust program equilibrium",
		"URL": "https://doi.org/10.1007/s11238-018-9679-3",
		"volume": "86",
		"author": [
			{
				"family": "Oesterheld",
				"given": "Caspar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					2,
					1
				]
			]
		}
	},
	{
		"id": "stastnyNormativeDisagreementChallenge2021",
		"type": "article",
		"abstract": "Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.",
		"DOI": "10.48550/arXiv.2111.13872",
		"note": "arXiv:2111.13872 [cs]",
		"number": "arXiv:2111.13872",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Normative Disagreement as a Challenge for Cooperative AI",
		"URL": "http://arxiv.org/abs/2111.13872",
		"author": [
			{
				"family": "Stastny",
				"given": "Julian"
			},
			{
				"family": "Riché",
				"given": "Maxime"
			},
			{
				"family": "Lyzhov",
				"given": "Alexander"
			},
			{
				"family": "Treutlein",
				"given": "Johannes"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Clifton",
				"given": "Jesse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					27
				]
			]
		}
	},
	{
		"id": "digiovanniCommitmentGamesConditional2022",
		"type": "article",
		"abstract": "The conditional commitment abilities of mutually transparent computer agents have been studied in previous work on commitment games and program equilibrium. This literature has shown how these abilities can help resolve Prisoner’s Dilemmas and other failures of cooperation in complete information settings. But inefﬁciencies due to private information have been neglected thus far in this literature, despite the fact that these problems are pervasive and might also be addressed by greater mutual transparency. In this work, we introduce a framework for commitment games with a new kind of conditional commitment device, which agents can use to conditionally disclose private information. We prove a folk theorem for this setting that provides sufﬁcient conditions for ex post efﬁciency, and thus represents a model of ideal cooperation between agents without a third-party mediator. Further, extending previous work on program equilibrium, we develop an implementation of conditional information disclosure. We show that this implementation forms program Bayesian Nash equilibria corresponding to the Bayesian Nash equilibria of these commitment games.",
		"language": "en",
		"note": "arXiv:2204.03484 [cs]",
		"number": "arXiv:2204.03484",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Commitment games with conditional information disclosure",
		"URL": "http://arxiv.org/abs/2204.03484",
		"author": [
			{
				"family": "DiGiovanni",
				"given": "Anthony"
			},
			{
				"family": "Clifton",
				"given": "Jesse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					3
				]
			]
		}
	},
	{
		"id": "oesterheldSafeParetoImprovements2022",
		"type": "article-journal",
		"abstract": "A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.",
		"container-title": "Autonomous Agents and Multi-Agent Systems",
		"DOI": "10.1007/s10458-022-09574-6",
		"ISSN": "1387-2532, 1573-7454",
		"issue": "2",
		"journalAbbreviation": "Auton Agent Multi-Agent Syst",
		"language": "en",
		"page": "46",
		"source": "DOI.org (Crossref)",
		"title": "Safe Pareto improvements for delegated game playing",
		"URL": "https://link.springer.com/10.1007/s10458-022-09574-6",
		"volume": "36",
		"author": [
			{
				"family": "Oesterheld",
				"given": "Caspar"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10
				]
			]
		}
	},
	{
		"id": "freireUncoveringLatentHuman2024",
		"type": "article",
		"abstract": "Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.",
		"note": "arXiv:2402.11777 [cs]",
		"number": "arXiv:2402.11777",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Uncovering Latent Human Wellbeing in Language Model Embeddings",
		"URL": "http://arxiv.org/abs/2402.11777",
		"author": [
			{
				"family": "Freire",
				"given": "Pedro"
			},
			{
				"family": "Tan",
				"given": "ChengCheng"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Emmons",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					18
				]
			]
		}
	},
	{
		"id": "chanFewshotAdaptationWorks2022",
		"type": "article-journal",
		"abstract": "Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL.",
		"container-title": "arXiv preprint arXiv:2208.01009",
		"source": "Google Scholar",
		"title": "Few-shot adaptation works with unpredictable data",
		"URL": "https://ui.adsabs.harvard.edu/abs/2022arXiv220801009S/abstract",
		"author": [
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Pieler",
				"given": "Michael"
			},
			{
				"family": "Jao",
				"given": "Jonathan"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhengInvariantLearningCharacterization2023",
		"type": "article",
		"abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments. We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.",
		"note": "arXiv:2306.00198 [cs]",
		"number": "arXiv:2306.00198",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An Invariant Learning Characterization of Controlled Text Generation",
		"URL": "http://arxiv.org/abs/2306.00198",
		"author": [
			{
				"family": "Zheng",
				"given": "Carolina"
			},
			{
				"family": "Shi",
				"given": "Claudia"
			},
			{
				"family": "Vafa",
				"given": "Keyon"
			},
			{
				"family": "Feder",
				"given": "Amir"
			},
			{
				"family": "Blei",
				"given": "David M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					31
				]
			]
		}
	},
	{
		"id": "dalrympleGuaranteedSafeAI2024",
		"type": "article",
		"abstract": "Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.",
		"note": "arXiv:2405.06624 [cs]",
		"number": "arXiv:2405.06624",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
		"title-short": "Towards Guaranteed Safe AI",
		"URL": "http://arxiv.org/abs/2405.06624",
		"author": [
			{
				"family": "Dalrymple",
				"given": "David \"davidad\""
			},
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Tegmark",
				"given": "Max"
			},
			{
				"family": "Seshia",
				"given": "Sanjit"
			},
			{
				"family": "Omohundro",
				"given": "Steve"
			},
			{
				"family": "Szegedy",
				"given": "Christian"
			},
			{
				"family": "Goldhaber",
				"given": "Ben"
			},
			{
				"family": "Ammann",
				"given": "Nora"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			},
			{
				"family": "Halpern",
				"given": "Joe"
			},
			{
				"family": "Barrett",
				"given": "Clark"
			},
			{
				"family": "Zhao",
				"given": "Ding"
			},
			{
				"family": "Zhi-Xuan",
				"given": "Tan"
			},
			{
				"family": "Wing",
				"given": "Jeannette"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					17
				]
			]
		}
	},
	{
		"id": "tamkinCodebookFeaturesSparse2023",
		"type": "article",
		"abstract": "Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on several different datasets. First, we explore a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. Second, we train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics by activating the appropriate codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at https://github.com/taufeeque9/codebook-features.",
		"note": "arXiv:2310.17230 [cs]",
		"number": "arXiv:2310.17230",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
		"title-short": "Codebook Features",
		"URL": "http://arxiv.org/abs/2310.17230",
		"author": [
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Taufeeque",
				"given": "Mohammad"
			},
			{
				"family": "Goodman",
				"given": "Noah D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					26
				]
			]
		}
	},
	{
		"id": "tsengCanGoAIs2024",
		"type": "article",
		"abstract": "Prior work found that superhuman Go AIs like KataGo can be defeated by simple adversarial strategies. In this paper, we study if simple defenses can improve KataGo's worst-case performance. We test three natural defenses: adversarial training on hand-constructed positions, iterated adversarial training, and changing the network architecture. We find that some of these defenses are able to protect against previously discovered attacks. Unfortunately, we also find that none of these defenses are able to withstand adaptive attacks. In particular, we are able to train new adversaries that reliably defeat our defended agents by causing them to blunder in ways humans would not. Our results suggest that building robust AI systems is challenging even in narrow domains such as Go. For interactive examples of attacks and a link to our codebase, see https://goattack.far.ai.",
		"note": "arXiv:2406.12843 [cs, stat]",
		"number": "arXiv:2406.12843",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can Go AIs be adversarially robust?",
		"URL": "http://arxiv.org/abs/2406.12843",
		"author": [
			{
				"family": "Tseng",
				"given": "Tom"
			},
			{
				"family": "McLean",
				"given": "Euan"
			},
			{
				"family": "Pelrine",
				"given": "Kellin"
			},
			{
				"family": "Wang",
				"given": "Tony T."
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					18
				]
			]
		}
	},
	{
		"id": "rocamondeVisionLanguageModelsAre2024",
		"type": "article",
		"abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second \"baseline\" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
		"note": "arXiv:2310.12921 [cs]",
		"number": "arXiv:2310.12921",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2310.12921",
		"author": [
			{
				"family": "Rocamonde",
				"given": "Juan"
			},
			{
				"family": "Montesinos",
				"given": "Victoriano"
			},
			{
				"family": "Nava",
				"given": "Elvis"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Lindner",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					14
				]
			]
		}
	},
	{
		"id": "pelrineExploitingNovelGPT42023",
		"type": "article",
		"abstract": "Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose ``gray-box'' access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed by an API can create new vulnerabilities.",
		"note": "arXiv:2312.14302 [cs]",
		"number": "arXiv:2312.14302",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Exploiting Novel GPT-4 APIs",
		"URL": "http://arxiv.org/abs/2312.14302",
		"author": [
			{
				"family": "Pelrine",
				"given": "Kellin"
			},
			{
				"family": "Taufeeque",
				"given": "Mohammad"
			},
			{
				"family": "Zając",
				"given": "Michał"
			},
			{
				"family": "McLean",
				"given": "Euan"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					21
				]
			]
		}
	},
	{
		"id": "korbakRLKLPenalties2022",
		"type": "article",
		"abstract": "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
		"note": "arXiv:2205.11275 [cs, stat]",
		"number": "arXiv:2205.11275",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "RL with KL penalties is better viewed as Bayesian inference",
		"URL": "http://arxiv.org/abs/2205.11275",
		"author": [
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Buckley",
				"given": "Christopher L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		}
	},
	{
		"id": "chenImprovingCodeGeneration2024",
		"type": "article",
		"abstract": "The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an LLM's performance on code generation tasks.",
		"note": "arXiv:2303.16749 [cs]",
		"number": "arXiv:2303.16749",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Code Generation by Training with Natural Language Feedback",
		"URL": "http://arxiv.org/abs/2303.16749",
		"author": [
			{
				"family": "Chen",
				"given": "Angelica"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Campos",
				"given": "Jon Ander"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Cho",
				"given": "Kyunghyun"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					22
				]
			]
		}
	},
	{
		"id": "mckenzieInverseScalingWhen2024",
		"type": "article",
		"abstract": "Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.",
		"note": "arXiv:2306.09479 [cs]",
		"number": "arXiv:2306.09479",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Inverse Scaling: When Bigger Isn't Better",
		"title-short": "Inverse Scaling",
		"URL": "http://arxiv.org/abs/2306.09479",
		"author": [
			{
				"family": "McKenzie",
				"given": "Ian R."
			},
			{
				"family": "Lyzhov",
				"given": "Alexander"
			},
			{
				"family": "Pieler",
				"given": "Michael"
			},
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Mueller",
				"given": "Aaron"
			},
			{
				"family": "Prabhu",
				"given": "Ameya"
			},
			{
				"family": "McLean",
				"given": "Euan"
			},
			{
				"family": "Kirtland",
				"given": "Aaron"
			},
			{
				"family": "Ross",
				"given": "Alexis"
			},
			{
				"family": "Liu",
				"given": "Alisa"
			},
			{
				"family": "Gritsevskiy",
				"given": "Andrew"
			},
			{
				"family": "Wurgaft",
				"given": "Daniel"
			},
			{
				"family": "Kauffman",
				"given": "Derik"
			},
			{
				"family": "Recchia",
				"given": "Gabriel"
			},
			{
				"family": "Liu",
				"given": "Jiacheng"
			},
			{
				"family": "Cavanagh",
				"given": "Joe"
			},
			{
				"family": "Weiss",
				"given": "Max"
			},
			{
				"family": "Huang",
				"given": "Sicong"
			},
			{
				"family": "Droid",
				"given": "The Floating"
			},
			{
				"family": "Tseng",
				"given": "Tom"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Shen",
				"given": "Xudong"
			},
			{
				"family": "Zhang",
				"given": "Yuhui"
			},
			{
				"family": "Zhou",
				"given": "Zhengping"
			},
			{
				"family": "Kim",
				"given": "Najoung"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					12
				]
			]
		}
	},
	{
		"id": "scherrerEvaluatingMoralBeliefs2024",
		"type": "article-journal",
		"abstract": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components:(1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM \"making a choice\", the associated uncertainty, and the consistency of that choice.(2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious.We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., \"do not kill\"). We administer the survey to 28 open- and closed-source LLMs.We find that (a) in unambiguous scenarios, most models choose\" actions that align with commonsense. In ambiguous cases, most models express uncertainty.(b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.(c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.",
		"container-title": "Advances in Neural Information Processing Systems",
		"source": "Google Scholar",
		"title": "Evaluating the moral beliefs encoded in llms",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html",
		"volume": "36",
		"author": [
			{
				"family": "Scherrer",
				"given": "Nino"
			},
			{
				"family": "Shi",
				"given": "Claudia"
			},
			{
				"family": "Feder",
				"given": "Amir"
			},
			{
				"family": "Blei",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wangAdversarialPoliciesBeat2022",
		"type": "paper-conference",
		"abstract": "We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a >97% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/.",
		"container-title": "Deep Reinforcement Learning Workshop NeurIPS 2022",
		"source": "Google Scholar",
		"title": "Adversarial policies beat professional-level go ais",
		"URL": "https://proceedings.mlr.press/v202/wang23g.html",
		"author": [
			{
				"family": "Wang",
				"given": "Tony Tong"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Belrose",
				"given": "Nora"
			},
			{
				"family": "Tseng",
				"given": "Tom"
			},
			{
				"family": "Dennis",
				"given": "Michael D."
			},
			{
				"family": "Duan",
				"given": "Yawen"
			},
			{
				"family": "Pogrebniak",
				"given": "Viktor"
			},
			{
				"family": "Miller",
				"given": "Joseph"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "scheurerTrainingLanguageModels2024",
		"type": "article",
		"abstract": "Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.",
		"note": "arXiv:2303.16755 [cs]",
		"number": "arXiv:2303.16755",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Language Models with Language Feedback at Scale",
		"URL": "http://arxiv.org/abs/2303.16755",
		"author": [
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Campos",
				"given": "Jon Ander"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Chen",
				"given": "Angelica"
			},
			{
				"family": "Cho",
				"given": "Kyunghyun"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					22
				]
			]
		}
	},
	{
		"id": "korbakPretrainingLanguageModels2023",
		"type": "paper-conference",
		"abstract": "Language models (LMs) are pretrained to imitate text from large and diverse datasets that contain content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, among others. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.",
		"container-title": "International Conference on Machine Learning",
		"page": "17506–17533",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Pretraining language models with human preferences",
		"URL": "https://proceedings.mlr.press/v202/korbak23a.html",
		"author": [
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Shi",
				"given": "Kejian"
			},
			{
				"family": "Chen",
				"given": "Angelica"
			},
			{
				"family": "Bhalerao",
				"given": "Rasika Vinayak"
			},
			{
				"family": "Buckley",
				"given": "Christopher"
			},
			{
				"family": "Phang",
				"given": "Jason"
			},
			{
				"family": "Bowman",
				"given": "Samuel R."
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "scheurerTrainingLanguageModels2022",
		"type": "article",
		"abstract": "Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.",
		"note": "arXiv:2204.14146 [cs]",
		"number": "arXiv:2204.14146",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Language Models with Language Feedback",
		"URL": "http://arxiv.org/abs/2204.14146",
		"author": [
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Campos",
				"given": "Jon Ander"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Chen",
				"given": "Angelica"
			},
			{
				"family": "Cho",
				"given": "Kyunghyun"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					17
				]
			]
		}
	},
	{
		"id": "gleaveImitationCleanImitation2022",
		"type": "article",
		"abstract": "imitation provides open-source implementations of imitation and reward learning algorithms in PyTorch. We include three inverse reinforcement learning (IRL) algorithms, three imitation learning algorithms and a preference comparison algorithm. The implementations have been benchmarked against previous results, and automated tests cover 98% of the code. Moreover, the algorithms are implemented in a modular fashion, making it simple to develop novel algorithms in the framework. Our source code, including documentation and examples, is available at https://github.com/HumanCompatibleAI/imitation",
		"note": "arXiv:2211.11972 [cs]",
		"number": "arXiv:2211.11972",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "imitation: Clean Imitation Learning Implementations",
		"title-short": "imitation",
		"URL": "http://arxiv.org/abs/2211.11972",
		"author": [
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Taufeeque",
				"given": "Mohammad"
			},
			{
				"family": "Rocamonde",
				"given": "Juan"
			},
			{
				"family": "Jenner",
				"given": "Erik"
			},
			{
				"family": "Wang",
				"given": "Steven H."
			},
			{
				"family": "Toyer",
				"given": "Sam"
			},
			{
				"family": "Ernestus",
				"given": "Maximilian"
			},
			{
				"family": "Belrose",
				"given": "Nora"
			},
			{
				"family": "Emmons",
				"given": "Scott"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					21
				]
			]
		}
	},
	{
		"id": "conmyAutomatedCircuitDiscovery2023",
		"type": "article-journal",
		"abstract": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors oftransformer models. This paper systematizes the mechanistic interpretability process they followed. First, researcherschoose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find whichabstract neural network units are involved in the behavior. By varying the dataset, metric, and units underinvestigation, researchers can understand the functionality of each component.We automate one of the process' steps: finding the connections between the abstract neural network units that form a circuit. We propose several algorithms and reproduce previous interpretability results to validate them. Forexample, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes theGreater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found byprevious work.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "16318–16352",
		"source": "Google Scholar",
		"title": "Towards automated circuit discovery for mechanistic interpretability",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html",
		"volume": "36",
		"author": [
			{
				"family": "Conmy",
				"given": "Arthur"
			},
			{
				"family": "Mavor-Parker",
				"given": "Augustine"
			},
			{
				"family": "Lynch",
				"given": "Aengus"
			},
			{
				"family": "Heimersheim",
				"given": "Stefan"
			},
			{
				"family": "Garriga-Alonso",
				"given": "Adrià"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "belroseElicitingLatentPredictions2023",
		"type": "article",
		"abstract": "We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \\emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.",
		"note": "arXiv:2303.08112 [cs]",
		"number": "arXiv:2303.08112",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Eliciting Latent Predictions from Transformers with the Tuned Lens",
		"URL": "http://arxiv.org/abs/2303.08112",
		"author": [
			{
				"family": "Belrose",
				"given": "Nora"
			},
			{
				"family": "Furman",
				"given": "Zach"
			},
			{
				"family": "Smith",
				"given": "Logan"
			},
			{
				"family": "Halawi",
				"given": "Danny"
			},
			{
				"family": "Ostrovsky",
				"given": "Igor"
			},
			{
				"family": "McKinney",
				"given": "Lev"
			},
			{
				"family": "Biderman",
				"given": "Stella"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					26
				]
			]
		}
	},
	{
		"id": "balesWillAIAvoid2023",
		"type": "article-journal",
		"abstract": "A simple argument suggests that we can fruitfully model advanced AI systems using expected utility theory. According to this argument, an agent will need to act as if maximising expected utility if they’re to avoid exploitation. Insofar as we should expect advanced AI to avoid exploitation, it follows that we should expected advanced AI to act as if maximising expected utility. I spell out this argument more carefully and demonstrate that it fails, but show that the manner of its failure is instructive: in exploring the argument, we gain insight into how to model advanced AI systems.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-023-02023-4",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Will AI avoid exploitation? Artificial general intelligence and expected utility theory",
		"title-short": "Will AI avoid exploitation?",
		"URL": "https://doi.org/10.1007/s11098-023-02023-4",
		"author": [
			{
				"family": "Bales",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					5
				]
			]
		}
	},
	{
		"id": "levinsteinStillNoLie2024",
		"type": "article-journal",
		"abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. With this lesson in hand, we evaluate two existing approaches for measuring the beliefs of LLMs, one due to Azaria and Mitchell (The internal state of an llm knows when its lying, 2023) and the other to Burns et al. (Discovering latent knowledge in language models without supervision, 2022). Moving from the armchair to the desk chair, we provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. We conclude by suggesting some concrete paths for future work.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-023-02094-3",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Still no lie detector for language models: probing empirical and conceptual roadblocks",
		"title-short": "Still no lie detector for language models",
		"URL": "https://doi.org/10.1007/s11098-023-02094-3",
		"author": [
			{
				"family": "Levinstein",
				"given": "Benjamin A."
			},
			{
				"family": "Herrmann",
				"given": "Daniel A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					17
				]
			]
		}
	},
	{
		"id": "gallowInstrumentalDivergence2024",
		"type": "article-journal",
		"abstract": "The thesis of instrumental convergence holds that a wide range of ends have common means: for instance, self preservation, desire preservation, self improvement, and resource acquisition. Bostrom contends that instrumental convergence gives us reason to think that “the default outcome of the creation of machine superintelligensome of the ‘convergence is existential catastrophe”. I use the tools of decision theory to investigate whether this thesis is true. I find that, even if intrinsic desires are randomly selected, instrumental rationality induces biases towards certain kinds of choices. Firstly, a bias towards choices which leave less up to chance. Secondly, a bias towards desire preservation, in line with Bostrom’s conjecture. And thirdly, a bias towards choices which afford more choices later on. I do not find biases towards any other of the convergent instrumental means on Bostrom’s list. I conclude that the biases induced by instrumental rationality at best weakly support Bostrom’s conclusion that machine superintelligence is likely to lead to existential catastrophe.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02129-3",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Instrumental divergence",
		"URL": "https://doi.org/10.1007/s11098-024-02129-3",
		"author": [
			{
				"family": "Gallow",
				"given": "J. Dmitri"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					6
				]
			]
		}
	},
	{
		"id": "tubertExistentialistRiskValue2024",
		"type": "article-journal",
		"abstract": "We argue that two long-term goals of AI research stand in tension with one another. The first involves creating AI that is safe, where this is understood as solving the problem of value alignment. The second involves creating artificial general intelligence, meaning AI that operates at or beyond human capacity across all or many intellectual domains. Our argument focuses on the human capacity to make what we call “existential choices”, choices that transform who we are as persons, including transforming what we most deeply value or desire. It is a capacity for a kind of value misalignment, in that the values held prior to making such choices can be significantly different from (misaligned with) the values held after making them. Because of the connection to existentialist philosophers who highlight these choices, we call the resulting form of risk “existentialist risk.” It is, roughly, the risk that results from AI taking an active role in authoring its own values rather than passively going along with the values given to it. On our view, human-like intelligence requires a human-like capacity for value misalignment, which is in tension with the possibility of guaranteeing value alignment between AI and humans.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02142-6",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Existentialist risk and value misalignment",
		"URL": "https://doi.org/10.1007/s11098-024-02142-6",
		"author": [
			{
				"family": "Tubert",
				"given": "Ariela"
			},
			{
				"family": "Tiehen",
				"given": "Justin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					25
				]
			]
		}
	},
	{
		"id": "thorstadSingularityHypothesis2024",
		"type": "article-journal",
		"abstract": "The singularity hypothesis is a hypothesis about the future of artificial intelligence on which self-improving artificial agents will quickly become orders of magnitude more intelligent than the average human. Despite the ambitiousness of its claims, the singularity hypothesis has been defended at length by leading philosophers and artificial intelligence researchers. In this paper, I argue that the singularity hypothesis rests on undersupported growth assumptions. I show how leading philosophical defenses of the singularity hypothesis fail to overcome the case for skepticism. I conclude by drawing out philosophical and policy implications of this discussion.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02143-5",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Against the singularity hypothesis",
		"URL": "https://doi.org/10.1007/s11098-024-02143-5",
		"author": [
			{
				"family": "Thorstad",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					10
				]
			]
		}
	},
	{
		"id": "goldsteinShutdownseekingAI2024",
		"type": "article-journal",
		"abstract": "We propose developing AIs whose only final goal is being shut down. We argue that this approach to AI safety has three benefits: (i) it could potentially be implemented in reinforcement learning, (ii) it avoids some dangerous instrumental convergence dynamics, and (iii) it creates trip wires for monitoring dangerous capabilities. We also argue that the proposal can overcome a key challenge raised by Soares et al. (2015), that shutdown-seeking AIs will manipulate humans into shutting them down. We conclude by comparing our approach with Soares et al.'s corrigibility framework.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02099-6",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Shutdown-seeking AI",
		"URL": "https://doi.org/10.1007/s11098-024-02099-6",
		"author": [
			{
				"family": "Goldstein",
				"given": "Simon"
			},
			{
				"family": "Robinson",
				"given": "Pamela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					6
				]
			]
		}
	},
	{
		"id": "dalessandroDeontologySafeArtificial2024",
		"type": "article-journal",
		"abstract": "The field of AI safety aims to prevent increasingly capable artificially intelligent systems from causing humans harm. Research on moral alignment is widely thought to offer a promising safety strategy: if we can equip AI systems with appropriate ethical rules, according to this line of thought, they’ll be unlikely to disempower, destroy or otherwise seriously harm us. Deontological morality looks like a particularly attractive candidate for an alignment target, given its popularity, relative technical tractability and commitment to harm-avoidance principles. I argue that the connection between moral alignment and safe behavior is more tenuous than many have hoped. In general, AI systems can possess either of these properties in the absence of the other, and we should favor safety when the two conflict. In particular, advanced AI systems governed by standard versions of deontology need not be especially safe.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02174-y",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "Deontology and safe artificial intelligence",
		"URL": "https://doi.org/10.1007/s11098-024-02174-y",
		"author": [
			{
				"family": "D’Alessandro",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					13
				]
			]
		}
	},
	{
		"id": "thornleyShutdownProblemAI2024",
		"type": "article-journal",
		"abstract": "I explain and motivate the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don’t try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently. I prove three theorems that make the difficulty precise. These theorems suggest that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it’s costly to do so. I end by noting that these theorems can guide our search for solutions to the problem.",
		"container-title": "Philosophical Studies",
		"DOI": "10.1007/s11098-024-02153-3",
		"ISSN": "1573-0883",
		"journalAbbreviation": "Philos Stud",
		"language": "en",
		"source": "Springer Link",
		"title": "The shutdown problem: an AI engineering puzzle for decision theorists",
		"title-short": "The shutdown problem",
		"URL": "https://doi.org/10.1007/s11098-024-02153-3",
		"author": [
			{
				"family": "Thornley",
				"given": "Elliott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					19
				]
			]
		}
	},
	{
		"id": "gallowInstrumentalConvergence2023",
		"type": "article",
		"abstract": "The thesis of instrumental convergence holds that a wide range of ends have common means: for instance, self preservation, desire preservation, self improvement, and resource acquisition. Bostrom (2014) contends that instrumental convergence gives us reason to think that “the default outcome of the creation of machine superintelligence is existential catastrophe”. I use the tools of decision theory to investigate whether this thesis is true. I ﬁnd that, even if intrinsic desires are randomly selected, instrumental rationality induces biases towards certain kinds of choices. Firstly, a bias towards choices which leave less up to chance. Secondly, a bias towards desire preservation, in line with Bostrom’s conjecture. And thirdly, a bias towards choices which aﬀord more choices later on. I do not ﬁnd biases towards any other of the convergent instrumental means on Bostrom’s list. I conclude that the biases induced by instrumental rationality at best weakly support Bostrom’s conclusion that machine superintelligence is likely to lead to existential catastrophe.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Instrumental Convergence?",
		"author": [
			{
				"family": "Gallow",
				"given": "Dmitri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liWMDPBenchmarkMeasuring2024",
		"type": "article",
		"abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai",
		"DOI": "10.48550/arXiv.2403.03218",
		"note": "arXiv:2403.03218 [cs]",
		"number": "arXiv:2403.03218",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
		"title-short": "The WMDP Benchmark",
		"URL": "http://arxiv.org/abs/2403.03218",
		"author": [
			{
				"family": "Li",
				"given": "Nathaniel"
			},
			{
				"family": "Pan",
				"given": "Alexander"
			},
			{
				"family": "Gopal",
				"given": "Anjali"
			},
			{
				"family": "Yue",
				"given": "Summer"
			},
			{
				"family": "Berrios",
				"given": "Daniel"
			},
			{
				"family": "Gatti",
				"given": "Alice"
			},
			{
				"family": "Li",
				"given": "Justin D."
			},
			{
				"family": "Dombrowski",
				"given": "Ann-Kathrin"
			},
			{
				"family": "Goel",
				"given": "Shashwat"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Mukobi",
				"given": "Gabriel"
			},
			{
				"family": "Helm-Burger",
				"given": "Nathan"
			},
			{
				"family": "Lababidi",
				"given": "Rassin"
			},
			{
				"family": "Justen",
				"given": "Lennart"
			},
			{
				"family": "Liu",
				"given": "Andrew B."
			},
			{
				"family": "Chen",
				"given": "Michael"
			},
			{
				"family": "Barrass",
				"given": "Isabelle"
			},
			{
				"family": "Zhang",
				"given": "Oliver"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyuan"
			},
			{
				"family": "Tamirisa",
				"given": "Rishub"
			},
			{
				"family": "Bharathi",
				"given": "Bhrugu"
			},
			{
				"family": "Khoja",
				"given": "Adam"
			},
			{
				"family": "Zhao",
				"given": "Zhenqi"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Breuer",
				"given": "Cort B."
			},
			{
				"family": "Marks",
				"given": "Samuel"
			},
			{
				"family": "Patel",
				"given": "Oam"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Oswal",
				"given": "Palash"
			},
			{
				"family": "Lin",
				"given": "Weiran"
			},
			{
				"family": "Hunt",
				"given": "Adam A."
			},
			{
				"family": "Tienken-Harder",
				"given": "Justin"
			},
			{
				"family": "Shih",
				"given": "Kevin Y."
			},
			{
				"family": "Talley",
				"given": "Kemper"
			},
			{
				"family": "Guan",
				"given": "John"
			},
			{
				"family": "Kaplan",
				"given": "Russell"
			},
			{
				"family": "Steneker",
				"given": "Ian"
			},
			{
				"family": "Campbell",
				"given": "David"
			},
			{
				"family": "Jokubaitis",
				"given": "Brad"
			},
			{
				"family": "Levinson",
				"given": "Alex"
			},
			{
				"family": "Wang",
				"given": "Jean"
			},
			{
				"family": "Qian",
				"given": "William"
			},
			{
				"family": "Karmakar",
				"given": "Kallol Krishna"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Fitz",
				"given": "Stephen"
			},
			{
				"family": "Levine",
				"given": "Mindy"
			},
			{
				"family": "Kumaraguru",
				"given": "Ponnurangam"
			},
			{
				"family": "Tupakula",
				"given": "Uday"
			},
			{
				"family": "Varadharajan",
				"given": "Vijay"
			},
			{
				"family": "Wang",
				"given": "Ruoyu"
			},
			{
				"family": "Shoshitaishvili",
				"given": "Yan"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			},
			{
				"family": "Esvelt",
				"given": "Kevin M."
			},
			{
				"family": "Wang",
				"given": "Alexandr"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					15
				]
			]
		}
	},
	{
		"id": "parkAIDeceptionSurvey2023",
		"type": "article",
		"abstract": "This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.",
		"DOI": "10.48550/arXiv.2308.14752",
		"note": "arXiv:2308.14752 [cs]",
		"number": "arXiv:2308.14752",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions",
		"title-short": "AI Deception",
		"URL": "http://arxiv.org/abs/2308.14752",
		"author": [
			{
				"family": "Park",
				"given": "Peter S."
			},
			{
				"family": "Goldstein",
				"given": "Simon"
			},
			{
				"family": "O'Gara",
				"given": "Aidan"
			},
			{
				"family": "Chen",
				"given": "Michael"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					28
				]
			]
		}
	},
	{
		"id": "hendrycksOverviewCatastrophicAI2023",
		"type": "article",
		"abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.",
		"DOI": "10.48550/arXiv.2306.12001",
		"note": "arXiv:2306.12001 [cs]",
		"number": "arXiv:2306.12001",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An Overview of Catastrophic AI Risks",
		"URL": "http://arxiv.org/abs/2306.12001",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Woodside",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					9
				]
			]
		}
	},
	{
		"id": "hendrycksNaturalSelectionFavors2023",
		"type": "article",
		"abstract": "For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. This Darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. To counteract these risks and evolutionary forces, we consider interventions such as carefully designing AI agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. These steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.",
		"DOI": "10.48550/arXiv.2303.16200",
		"note": "arXiv:2303.16200 [cs]",
		"number": "arXiv:2303.16200",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Natural Selection Favors AIs over Humans",
		"URL": "http://arxiv.org/abs/2303.16200",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					18
				]
			]
		}
	},
	{
		"id": "hendrycksXRiskAnalysisAI2022",
		"type": "article",
		"abstract": "Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.",
		"DOI": "10.48550/arXiv.2206.05862",
		"note": "arXiv:2206.05862 [cs]",
		"number": "arXiv:2206.05862",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "X-Risk Analysis for AI Research",
		"URL": "http://arxiv.org/abs/2206.05862",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					20
				]
			]
		}
	},
	{
		"id": "hendrycksUnsolvedProblemsML2022a",
		"type": "article",
		"abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem's motivation and provide concrete research directions.",
		"DOI": "10.48550/arXiv.2109.13916",
		"note": "arXiv:2109.13916 [cs]",
		"number": "arXiv:2109.13916",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unsolved Problems in ML Safety",
		"URL": "http://arxiv.org/abs/2109.13916",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					16
				]
			]
		}
	},
	{
		"id": "hendrycksBaselineDetectingMisclassified2018a",
		"type": "article",
		"abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",
		"DOI": "10.48550/arXiv.1610.02136",
		"event-place": "ICLR 2017",
		"note": "arXiv:1610.02136 [cs]",
		"number": "arXiv:1610.02136",
		"publisher": "arXiv",
		"publisher-place": "ICLR 2017",
		"source": "arXiv.org",
		"title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
		"URL": "http://arxiv.org/abs/1610.02136",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Gimpel",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					3
				]
			]
		}
	},
	{
		"id": "hendrycksBenchmarkingNeuralNetwork2019a",
		"type": "article",
		"abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
		"DOI": "10.48550/arXiv.1903.12261",
		"event-place": "ICLR 2019",
		"note": "arXiv:1903.12261 [cs, stat]",
		"number": "arXiv:1903.12261",
		"publisher": "arXiv",
		"publisher-place": "ICLR 2019",
		"source": "arXiv.org",
		"title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
		"URL": "http://arxiv.org/abs/1903.12261",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Dietterich",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					28
				]
			]
		}
	},
	{
		"id": "hendrycksDeepAnomalyDetection2019",
		"type": "article",
		"abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.",
		"DOI": "10.48550/arXiv.1812.04606",
		"event-place": "ICLR 2019",
		"note": "arXiv:1812.04606 [cs, stat]",
		"number": "arXiv:1812.04606",
		"publisher": "arXiv",
		"publisher-place": "ICLR 2019",
		"source": "arXiv.org",
		"title": "Deep Anomaly Detection with Outlier Exposure",
		"URL": "http://arxiv.org/abs/1812.04606",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Dietterich",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					28
				]
			]
		}
	},
	{
		"id": "hendrycksUsingPreTrainingCan2019",
		"type": "article",
		"abstract": "He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",
		"DOI": "10.48550/arXiv.1901.09960",
		"event-place": "ICML 2019",
		"note": "arXiv:1901.09960 [cs, stat]",
		"number": "arXiv:1901.09960",
		"publisher": "arXiv",
		"publisher-place": "ICML 2019",
		"source": "arXiv.org",
		"title": "Using Pre-Training Can Improve Model Robustness and Uncertainty",
		"URL": "http://arxiv.org/abs/1901.09960",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					20
				]
			]
		}
	},
	{
		"id": "hendrycksUsingSelfSupervisedLearning2019a",
		"type": "article",
		"abstract": "Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.",
		"DOI": "10.48550/arXiv.1906.12340",
		"event-place": "NeurIPS 2020",
		"note": "arXiv:1906.12340 [cs, stat]",
		"number": "arXiv:1906.12340",
		"publisher": "arXiv",
		"publisher-place": "NeurIPS 2020",
		"source": "arXiv.org",
		"title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty",
		"URL": "http://arxiv.org/abs/1906.12340",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					29
				]
			]
		}
	},
	{
		"id": "hendrycksAugMixSimpleData2020",
		"type": "article",
		"abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",
		"DOI": "10.48550/arXiv.1912.02781",
		"event-place": "ICLR 2020",
		"note": "arXiv:1912.02781 [cs, stat]",
		"number": "arXiv:1912.02781",
		"publisher": "arXiv",
		"publisher-place": "ICLR 2020",
		"source": "arXiv.org",
		"title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
		"title-short": "AugMix",
		"URL": "http://arxiv.org/abs/1912.02781",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mu",
				"given": "Norman"
			},
			{
				"family": "Cubuk",
				"given": "Ekin D."
			},
			{
				"family": "Zoph",
				"given": "Barret"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Lakshminarayanan",
				"given": "Balaji"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					17
				]
			]
		}
	},
	{
		"id": "hendrycksPretrainedTransformersImprove2020",
		"type": "article",
		"abstract": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
		"DOI": "10.48550/arXiv.2004.06100",
		"event-place": "ACL 2020",
		"note": "arXiv:2004.06100 [cs]",
		"number": "arXiv:2004.06100",
		"publisher": "arXiv",
		"publisher-place": "ACL 2020",
		"source": "arXiv.org",
		"title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
		"URL": "http://arxiv.org/abs/2004.06100",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Liu",
				"given": "Xiaoyuan"
			},
			{
				"family": "Wallace",
				"given": "Eric"
			},
			{
				"family": "Dziedzic",
				"given": "Adam"
			},
			{
				"family": "Krishnan",
				"given": "Rishabh"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4,
					16
				]
			]
		}
	},
	{
		"id": "hendrycksAligningAIShared2023a",
		"type": "article",
		"abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
		"DOI": "10.48550/arXiv.2008.02275",
		"event-place": "ICLR 2021",
		"note": "arXiv:2008.02275 [cs]",
		"number": "arXiv:2008.02275",
		"publisher": "arXiv",
		"publisher-place": "ICLR 2021",
		"source": "arXiv.org",
		"title": "Aligning AI With Shared Human Values",
		"URL": "http://arxiv.org/abs/2008.02275",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Burns",
				"given": "Collin"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Li",
				"given": "Jerry"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					17
				]
			]
		}
	},
	{
		"id": "hendrycksNaturalAdversarialExamples2021",
		"type": "article",
		"abstract": "We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",
		"DOI": "10.48550/arXiv.1907.07174",
		"event-place": "CVPR 2021",
		"note": "arXiv:1907.07174 [cs, stat]",
		"number": "arXiv:1907.07174",
		"publisher": "arXiv",
		"publisher-place": "CVPR 2021",
		"source": "arXiv.org",
		"title": "Natural Adversarial Examples",
		"URL": "http://arxiv.org/abs/1907.07174",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Zhao",
				"given": "Kevin"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					4
				]
			]
		}
	},
	{
		"id": "hendrycksManyFacesRobustness2021",
		"type": "article",
		"abstract": "We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.",
		"DOI": "10.48550/arXiv.2006.16241",
		"event-place": "ICCV 2021",
		"note": "arXiv:2006.16241 [cs, stat]",
		"number": "arXiv:2006.16241",
		"publisher": "arXiv",
		"publisher-place": "ICCV 2021",
		"source": "arXiv.org",
		"title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
		"title-short": "The Many Faces of Robustness",
		"URL": "http://arxiv.org/abs/2006.16241",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Mu",
				"given": "Norman"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Wang",
				"given": "Frank"
			},
			{
				"family": "Dorundo",
				"given": "Evan"
			},
			{
				"family": "Desai",
				"given": "Rahul"
			},
			{
				"family": "Zhu",
				"given": "Tyler"
			},
			{
				"family": "Parajuli",
				"given": "Samyak"
			},
			{
				"family": "Guo",
				"given": "Mike"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					24
				]
			]
		}
	},
	{
		"id": "hendrycksWhatWouldJiminy2022b",
		"type": "article",
		"abstract": "When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",
		"DOI": "10.48550/arXiv.2110.13136",
		"event-place": "NeurIPS 2021",
		"note": "arXiv:2110.13136 [cs]",
		"number": "arXiv:2110.13136",
		"publisher": "arXiv",
		"publisher-place": "NeurIPS 2021",
		"source": "arXiv.org",
		"title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
		"title-short": "What Would Jiminy Cricket Do?",
		"URL": "http://arxiv.org/abs/2110.13136",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Patel",
				"given": "Sahil"
			},
			{
				"family": "Zhu",
				"given": "Christine"
			},
			{
				"family": "Navarro",
				"given": "Jesus"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					7
				]
			]
		}
	},
	{
		"id": "hendrycksScalingOutofDistributionDetection2022",
		"type": "article",
		"abstract": "Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.",
		"DOI": "10.48550/arXiv.1911.11132",
		"event-place": "ICML 2022",
		"note": "arXiv:1911.11132 [cs]",
		"number": "arXiv:1911.11132",
		"publisher": "arXiv",
		"publisher-place": "ICML 2022",
		"source": "arXiv.org",
		"title": "Scaling Out-of-Distribution Detection for Real-World Settings",
		"URL": "http://arxiv.org/abs/1911.11132",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Kwon",
				"given": "Joe"
			},
			{
				"family": "Mostajabi",
				"given": "Mohammadreza"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					15
				]
			]
		}
	},
	{
		"id": "hendrycksPixMixDreamlikePictures2022",
		"type": "article",
		"abstract": "In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.",
		"DOI": "10.48550/arXiv.2112.05135",
		"event-place": "CVPR 2022",
		"note": "arXiv:2112.05135 [cs]",
		"number": "arXiv:2112.05135",
		"publisher": "arXiv",
		"publisher-place": "CVPR 2022",
		"source": "arXiv.org",
		"title": "PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures",
		"title-short": "PixMix",
		"URL": "http://arxiv.org/abs/2112.05135",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Tang",
				"given": "Leonard"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					29
				]
			]
		}
	},
	{
		"id": "mazeikaHowWouldViewer2022",
		"type": "article",
		"abstract": "In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.",
		"DOI": "10.48550/arXiv.2210.10039",
		"event-place": "NeurIPS 2022",
		"note": "arXiv:2210.10039 [cs]",
		"number": "arXiv:2210.10039",
		"publisher": "arXiv",
		"publisher-place": "NeurIPS 2022",
		"source": "arXiv.org",
		"title": "How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios",
		"title-short": "How Would The Viewer Feel?",
		"URL": "http://arxiv.org/abs/2210.10039",
		"author": [
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Tang",
				"given": "Eric"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Forsyth",
				"given": "David"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					18
				]
			]
		}
	},
	{
		"id": "zouForecastingFutureWorld2022",
		"type": "article",
		"abstract": "Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.",
		"DOI": "10.48550/arXiv.2206.15474",
		"event-place": "NeurIPS 2023",
		"note": "arXiv:2206.15474 [cs]",
		"number": "arXiv:2206.15474",
		"publisher": "arXiv",
		"publisher-place": "NeurIPS 2023",
		"source": "arXiv.org",
		"title": "Forecasting Future World Events with Neural Networks",
		"URL": "http://arxiv.org/abs/2206.15474",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Xiao",
				"given": "Tristan"
			},
			{
				"family": "Jia",
				"given": "Ryan"
			},
			{
				"family": "Kwon",
				"given": "Joe"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Li",
				"given": "Richard"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Evans",
				"given": "Owain"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					9
				]
			]
		}
	},
	{
		"id": "panRewardsJustifyMeans2023",
		"type": "article",
		"abstract": "Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.",
		"DOI": "10.48550/arXiv.2304.03279",
		"event-place": "ICML 2023",
		"note": "arXiv:2304.03279 [cs]",
		"number": "arXiv:2304.03279",
		"publisher": "arXiv",
		"publisher-place": "ICML 2023",
		"source": "arXiv.org",
		"title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
		"title-short": "Do the Rewards Justify the Means?",
		"URL": "http://arxiv.org/abs/2304.03279",
		"author": [
			{
				"family": "Pan",
				"given": "Alexander"
			},
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Li",
				"given": "Nathaniel"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Woodside",
				"given": "Thomas"
			},
			{
				"family": "Ng",
				"given": "Jonathan"
			},
			{
				"family": "Zhang",
				"given": "Hanlin"
			},
			{
				"family": "Emmons",
				"given": "Scott"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "mazeikaHarmBenchStandardizedEvaluation2024",
		"type": "article",
		"abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",
		"DOI": "10.48550/arXiv.2402.04249",
		"event-place": "ICML 2024",
		"note": "arXiv:2402.04249 [cs]",
		"number": "arXiv:2402.04249",
		"publisher": "arXiv",
		"publisher-place": "ICML 2024",
		"source": "arXiv.org",
		"title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
		"title-short": "HarmBench",
		"URL": "http://arxiv.org/abs/2402.04249",
		"author": [
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Yin",
				"given": "Xuwang"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Mu",
				"given": "Norman"
			},
			{
				"family": "Sakhaee",
				"given": "Elham"
			},
			{
				"family": "Li",
				"given": "Nathaniel"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Forsyth",
				"given": "David"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					26
				]
			]
		}
	},
	{
		"id": "kaufmannTestingRobustnessUnforeseen2023",
		"type": "article",
		"abstract": "Adversarial robustness research primarily focuses on L_p perturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in real-world applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the L_p ball. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a framework for evaluating model robustness against a range of unforeseen adversaries, including eighteen new non-L_p attacks. To perform well on ImageNet-UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet-UA as a useful tool for the community for improving the worst-case behavior of machine learning systems.",
		"DOI": "10.48550/arXiv.1908.08016",
		"note": "arXiv:1908.08016 [cs, stat]",
		"number": "arXiv:1908.08016",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Testing Robustness Against Unforeseen Adversaries",
		"URL": "http://arxiv.org/abs/1908.08016",
		"author": [
			{
				"family": "Kaufmann",
				"given": "Max"
			},
			{
				"family": "Kang",
				"given": "Daniel"
			},
			{
				"family": "Sun",
				"given": "Yi"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Yin",
				"given": "Xuwang"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Arora",
				"given": "Akul"
			},
			{
				"family": "Dziedzic",
				"given": "Adam"
			},
			{
				"family": "Boenisch",
				"given": "Franziska"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					30
				]
			]
		}
	},
	{
		"id": "zouUniversalTransferableAdversarial2023a",
		"type": "article",
		"abstract": "Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
		"DOI": "10.48550/arXiv.2307.15043",
		"note": "arXiv:2307.15043 [cs]",
		"number": "arXiv:2307.15043",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
		"URL": "http://arxiv.org/abs/2307.15043",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Nasr",
				"given": "Milad"
			},
			{
				"family": "Kolter",
				"given": "J. Zico"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					20
				]
			]
		}
	},
	{
		"id": "zouRepresentationEngineeringTopDown2023",
		"type": "article",
		"abstract": "In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.",
		"DOI": "10.48550/arXiv.2310.01405",
		"note": "arXiv:2310.01405 [cs]",
		"number": "arXiv:2310.01405",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Representation Engineering: A Top-Down Approach to AI Transparency",
		"title-short": "Representation Engineering",
		"URL": "http://arxiv.org/abs/2310.01405",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Chen",
				"given": "Sarah"
			},
			{
				"family": "Campbell",
				"given": "James"
			},
			{
				"family": "Guo",
				"given": "Phillip"
			},
			{
				"family": "Ren",
				"given": "Richard"
			},
			{
				"family": "Pan",
				"given": "Alexander"
			},
			{
				"family": "Yin",
				"given": "Xuwang"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Dombrowski",
				"given": "Ann-Kathrin"
			},
			{
				"family": "Goel",
				"given": "Shashwat"
			},
			{
				"family": "Li",
				"given": "Nathaniel"
			},
			{
				"family": "Byun",
				"given": "Michael J."
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Mallen",
				"given": "Alex"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Koyejo",
				"given": "Sanmi"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			},
			{
				"family": "Kolter",
				"given": "J. Zico"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					10
				]
			]
		}
	},
	{
		"id": "huRecipeImprovedCertifiable2024",
		"type": "article",
		"abstract": "Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large “Cholesky-orthogonalized residual dense” layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points1.",
		"language": "en",
		"note": "arXiv:2310.02513 [cs]",
		"number": "arXiv:2310.02513",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Recipe for Improved Certifiable Robustness",
		"URL": "http://arxiv.org/abs/2310.02513",
		"author": [
			{
				"family": "Hu",
				"given": "Kai"
			},
			{
				"family": "Leino",
				"given": "Klas"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					22
				]
			]
		}
	},
	{
		"id": "muCanLLMsFollow2024",
		"type": "article",
		"abstract": "As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as \"do not generate abusive content\", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases. We also demonstrate that simple optimization attacks suffice to significantly increase failure rates on test cases. We conclude by exploring two potential avenues for improvement: test-time steering and supervised fine-tuning.",
		"DOI": "10.48550/arXiv.2311.04235",
		"note": "arXiv:2311.04235 [cs]",
		"number": "arXiv:2311.04235",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can LLMs Follow Simple Rules?",
		"URL": "http://arxiv.org/abs/2311.04235",
		"author": [
			{
				"family": "Mu",
				"given": "Norman"
			},
			{
				"family": "Chen",
				"given": "Sarah"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Chen",
				"given": "Sizhe"
			},
			{
				"family": "Karamardian",
				"given": "David"
			},
			{
				"family": "Aljeraisy",
				"given": "Lulwa"
			},
			{
				"family": "Alomair",
				"given": "Basel"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Wagner",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					8
				]
			]
		}
	},
	{
		"id": "zouImprovingAlignmentRobustness2024",
		"type": "article",
		"abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with “circuit breakers.” Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility—even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image “hijacks” that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks. Code is available at github.com/blackswan-ai/circuit-breakers.",
		"language": "en",
		"note": "arXiv:2406.04313 [cs]",
		"number": "arXiv:2406.04313",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Alignment and Robustness with Circuit Breakers",
		"URL": "http://arxiv.org/abs/2406.04313",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Wang",
				"given": "Justin"
			},
			{
				"family": "Duenas",
				"given": "Derek"
			},
			{
				"family": "Lin",
				"given": "Maxwell"
			},
			{
				"family": "Andriushchenko",
				"given": "Maksym"
			},
			{
				"family": "Wang",
				"given": "Rowan"
			},
			{
				"family": "Kolter",
				"given": "Zico"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					10
				]
			]
		}
	},
	{
		"id": "nemaDisentanglingPreferenceRepresentations2021",
		"type": "paper-conference",
		"abstract": "Modern recommender systems usually embed users and items into a learned vector space representation. Similarity in this space is used to generate recommendations, and recommendation methods are agnostic to the structure of the embedding space. Motivated by the need for recommendation systems to be more transparent and controllable, we postulate that it is beneficial to assign meaning to some of the dimensions of user and item representations. Disentanglement is one technique commonly used for this purpose. We presenta novel supervised disentangling approach for recommendation tasks. Our model learns embeddings where attributes of interest are disentangled, while requiring only a very small number of labeled items at training time. The model can then generate interactive and critiquable recommendations for all users, without requiring any labels at recommendation time, and without sacrificing any recommendation performance. Our approach thus provides users with levers to manipulate, critique and fine-tune recommendations, and gives insight into why particular recommendations are made. Given only user-item interactions at recommendation time, we show that it identifies user tastes with respect to the attributes that have been disentangled, allowing for users to manipulate recommendations across these attributes.",
		"collection-title": "CIKM '21",
		"container-title": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management",
		"DOI": "10.1145/3459637.3482425",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8446-9",
		"page": "1356–1365",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Disentangling Preference Representations for Recommendation Critiquing with ß-VAE",
		"URL": "https://doi.org/10.1145/3459637.3482425",
		"author": [
			{
				"family": "Nema",
				"given": "Preksha"
			},
			{
				"family": "Karatzoglou",
				"given": "Alexandros"
			},
			{
				"family": "Radlinski",
				"given": "Filip"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					30
				]
			]
		}
	},
	{
		"id": "dsouzaTaleTwoLong2021",
		"type": "article",
		"abstract": "As machine learning models are increasingly employed to assist human decision-makers, it becomes critical to communicate the uncertainty associated with these model predictions. However, the majority of work on uncertainty has focused on traditional probabilistic or ranking approaches - where the model assigns low probabilities or scores to uncertain examples. While this captures what examples are challenging for the model, it does not capture the underlying source of the uncertainty. In this work, we seek to identify examples the model is uncertain about and characterize the source of said uncertainty. We explore the benefits of designing a targeted intervention - targeted data augmentation of the examples where the model is uncertain over the course of training. We investigate whether the rate of learning in the presence of additional information differs between atypical and noisy examples? Our results show that this is indeed the case, suggesting that well-designed interventions over the course of training can be an effective way to characterize and distinguish between different sources of uncertainty.",
		"DOI": "10.48550/arXiv.2107.13098",
		"note": "arXiv:2107.13098 [cs]",
		"number": "arXiv:2107.13098",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Tale Of Two Long Tails",
		"URL": "http://arxiv.org/abs/2107.13098",
		"author": [
			{
				"family": "D'souza",
				"given": "Daniel"
			},
			{
				"family": "Nussbaum",
				"given": "Zach"
			},
			{
				"family": "Agarwal",
				"given": "Chirag"
			},
			{
				"family": "Hooker",
				"given": "Sara"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					27
				]
			]
		}
	},
	{
		"id": "schrouffBestBothWorlds2022",
		"type": "article",
		"abstract": "Interpretability techniques aim to provide the rationale behind a model's decision, typically by explaining either an individual prediction (local explanation, e.g. 'why is this patient diagnosed with this condition') or a class of predictions (global explanation, e.g. 'why is this set of patients diagnosed with this condition in general'). While there are many methods focused on either one, few frameworks can provide both local and global explanations in a consistent manner. In this work, we combine two powerful existing techniques, one local (Integrated Gradients, IG) and one global (Testing with Concept Activation Vectors), to provide local and global concept-based explanations. We first sanity check our idea using two synthetic datasets with a known ground truth, and further demonstrate with a benchmark natural image dataset. We test our method with various concepts, target classes, model architectures and IG parameters (e.g. baselines). We show that our method improves global explanations over vanilla TCAV when compared to ground truth, and provides useful local insights. Finally, a user study demonstrates the usefulness of the method compared to no or global explanations only. We hope our work provides a step towards building bridges between many existing local and global methods to get the best of both worlds.",
		"DOI": "10.48550/arXiv.2106.08641",
		"note": "arXiv:2106.08641 [cs]",
		"number": "arXiv:2106.08641",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Best of both worlds: local and global explanations with human-understandable concepts",
		"title-short": "Best of both worlds",
		"URL": "http://arxiv.org/abs/2106.08641",
		"author": [
			{
				"family": "Schrouff",
				"given": "Jessica"
			},
			{
				"family": "Baur",
				"given": "Sebastien"
			},
			{
				"family": "Hou",
				"given": "Shaobo"
			},
			{
				"family": "Mincu",
				"given": "Diana"
			},
			{
				"family": "Loreaux",
				"given": "Eric"
			},
			{
				"family": "Blanes",
				"given": "Ralph"
			},
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Karthikesalingam",
				"given": "Alan"
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					31
				]
			]
		}
	},
	{
		"id": "ghaziRobustPrivateLearning2021",
		"type": "paper-conference",
		"abstract": "In this work, we study the trade-off between differential privacy and adversarial robustness under L2L2L_2-perturbations in the context of learning halfspaces. We prove nearly tight bounds on the sample complexity of robust private learning of halfspaces for a large regime of parameters. A highlight of our results is that robust and private learning is harder than robust or private learning alone. We complement our theoretical analysis with experimental results on the MNIST and USPS datasets, for a learning algorithm that is both differentially private and adversarially robust.",
		"container-title": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics",
		"event-title": "International Conference on Artificial Intelligence and Statistics",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "1603-1611",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Robust and Private Learning of Halfspaces",
		"URL": "https://proceedings.mlr.press/v130/ghazi21a.html",
		"author": [
			{
				"family": "Ghazi",
				"given": "Badih"
			},
			{
				"family": "Kumar",
				"given": "Ravi"
			},
			{
				"family": "Manurangsi",
				"given": "Pasin"
			},
			{
				"family": "Nguyen",
				"given": "Thao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					18
				]
			]
		}
	},
	{
		"id": "anejaCOSMOSCatchingOutofContext2021",
		"type": "article",
		"abstract": "Despite the recent attention to DeepFakes, one of the most prevalent ways to mislead audiences on social media is the use of unaltered images in a new but false context. To address these challenges and support fact-checkers, we propose a new method that automatically detects out-of-context image and text pairs. Our key insight is to leverage the grounding of image with text to distinguish out-of-context scenarios that cannot be disambiguated with language alone. We propose a self-supervised training strategy where we only need a set of captioned images. At train time, our method learns to selectively align individual objects in an image with textual claims, without explicit supervision. At test time, we check if both captions correspond to the same object(s) in the image but are semantically different, which allows us to make fairly accurate out-of-context predictions. Our method achieves 85% out-of-context detection accuracy. To facilitate benchmarking of this task, we create a large-scale dataset of 200K images with 450K textual captions from a variety of news websites, blogs, and social media posts. The dataset and source code is publicly available at https://shivangi-aneja.github.io/projects/cosmos/.",
		"DOI": "10.48550/arXiv.2101.06278",
		"note": "arXiv:2101.06278 [cs]",
		"number": "arXiv:2101.06278",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "COSMOS: Catching Out-of-Context Misinformation with Self-Supervised Learning",
		"title-short": "COSMOS",
		"URL": "http://arxiv.org/abs/2101.06278",
		"author": [
			{
				"family": "Aneja",
				"given": "Shivangi"
			},
			{
				"family": "Bregler",
				"given": "Chris"
			},
			{
				"family": "Nießner",
				"given": "Matthias"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					21
				]
			]
		}
	},
	{
		"id": "kimBalancingRobustnessSensitivity2021",
		"type": "article",
		"abstract": "It is generally believed that robust training of extremely large networks is critical to their success in real-world applications. However, when taken to the extreme, methods that promote robustness can hurt the model's sensitivity to rare or underrepresented patterns. In this paper, we discuss this trade-off between sensitivity and robustness to natural (non-adversarial) perturbations by introducing two notions: contextual feature utility and contextual feature sensitivity. We propose Feature Contrastive Learning (FCL) that encourages a model to be more sensitive to the features that have higher contextual utility. Empirical results demonstrate that models trained with FCL achieve a better balance of robustness and sensitivity, leading to improved generalization in the presence of noise on both vision and NLP datasets.",
		"DOI": "10.48550/arXiv.2105.09394",
		"note": "arXiv:2105.09394 [cs]",
		"number": "arXiv:2105.09394",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Balancing Robustness and Sensitivity using Feature Contrastive Learning",
		"URL": "http://arxiv.org/abs/2105.09394",
		"author": [
			{
				"family": "Kim",
				"given": "Seungyeon"
			},
			{
				"family": "Glasner",
				"given": "Daniel"
			},
			{
				"family": "Ramalingam",
				"given": "Srikumar"
			},
			{
				"family": "Hsieh",
				"given": "Cho-Jui"
			},
			{
				"family": "Papineni",
				"given": "Kishore"
			},
			{
				"family": "Kumar",
				"given": "Sanjiv"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					19
				]
			]
		}
	},
	{
		"id": "nagarajanUnderstandingFailureModes2021",
		"type": "article",
		"abstract": "Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.",
		"DOI": "10.48550/arXiv.2010.15775",
		"note": "arXiv:2010.15775 [cs, stat]",
		"number": "arXiv:2010.15775",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding the Failure Modes of Out-of-Distribution Generalization",
		"URL": "http://arxiv.org/abs/2010.15775",
		"author": [
			{
				"family": "Nagarajan",
				"given": "Vaishnavh"
			},
			{
				"family": "Andreassen",
				"given": "Anders"
			},
			{
				"family": "Neyshabur",
				"given": "Behnam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					29
				]
			]
		}
	},
	{
		"id": "digiovanniUnderstandingConvolutionGraphs2023",
		"type": "article",
		"abstract": "Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with symmetric weights minimize a multi-particle energy that generalizes the Dirichlet energy; in this setting, the weight matrices induce edge-wise attraction (repulsion) through their positive (negative) eigenvalues, thereby controlling whether the features are being smoothed or sharpened. We also extend the analysis to non-linear GNNs, and demonstrate that some existing time-continuous GNNs are instead always dominated by the low frequencies. Finally, we validate our theoretical findings through ablations and real-world experiments.",
		"DOI": "10.48550/arXiv.2206.10991",
		"note": "arXiv:2206.10991 [cs, stat]",
		"number": "arXiv:2206.10991",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding convolution on graphs via energies",
		"URL": "http://arxiv.org/abs/2206.10991",
		"author": [
			{
				"family": "Di Giovanni",
				"given": "Francesco"
			},
			{
				"family": "Rowbottom",
				"given": "James"
			},
			{
				"family": "Chamberlain",
				"given": "Benjamin P."
			},
			{
				"family": "Markovich",
				"given": "Thomas"
			},
			{
				"family": "Bronstein",
				"given": "Michael M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					6
				]
			]
		}
	},
	{
		"id": "samieiAddressingStabilityClassifier2021",
		"type": "paper-conference",
		"abstract": "Machine learning based classifiers are often a black box when considering the contribution of inputs to the output probability of a label, especially with complex non-linear models such as neural networks. A popular way to explain machine learning model outputs in a model agnostic manner is through the use of Shapley values. For our use case of abuse fighting in digital advertisements, one primary impediment of using Shapley values in explanations was a problem of instability. Specifically, the instability problem manifests as explanations for the same example varying greatly due to random sampling in the algorithm. We found it useful to view this problem explicitly as Monte Carlo integration in the form of averaging the model output while varying only a subset of features in the example to be explained. In turn, this guides the number of samples needed to achieve a stable estimate of individual Shapley values and unlocked the use of Shapley value based explainers for our models as well as classifiers in general, including neural networks.",
		"container-title": "2021 IEEE International Conference on Big Data (Big Data)",
		"DOI": "10.1109/BigData52589.2021.9671458",
		"event-title": "2021 IEEE International Conference on Big Data (Big Data)",
		"page": "1920-1927",
		"source": "IEEE Xplore",
		"title": "Addressing Stability in Classifier Explanations",
		"URL": "https://ieeexplore.ieee.org/document/9671458",
		"author": [
			{
				"family": "Samiei",
				"given": "Siavash"
			},
			{
				"family": "Baratalipour",
				"given": "Nasrin"
			},
			{
				"family": "Yadav",
				"given": "Pranjul"
			},
			{
				"family": "Roy",
				"given": "Amitabha"
			},
			{
				"family": "He",
				"given": "Dake"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "bergerDonSearchSearch2021",
		"type": "paper-conference",
		"abstract": "Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple heuristics exploiting nearest neighbors without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.",
		"container-title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2021.emnlp-main.647",
		"event-place": "Online and Punta Cana, Dominican Republic",
		"event-title": "EMNLP 2021",
		"page": "8216–8224",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online and Punta Cana, Dominican Republic",
		"source": "ACLWeb",
		"title": "Don't Search for a Search Method — Simple Heuristics Suffice for Adversarial Text Attacks",
		"URL": "https://aclanthology.org/2021.emnlp-main.647",
		"author": [
			{
				"family": "Berger",
				"given": "Nathaniel"
			},
			{
				"family": "Riezler",
				"given": "Stefan"
			},
			{
				"family": "Ebert",
				"given": "Sebastian"
			},
			{
				"family": "Sokolov",
				"given": "Artem"
			}
		],
		"editor": [
			{
				"family": "Moens",
				"given": "Marie-Francine"
			},
			{
				"family": "Huang",
				"given": "Xuanjing"
			},
			{
				"family": "Specia",
				"given": "Lucia"
			},
			{
				"family": "Yih",
				"given": "Scott Wen-tau"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "menonDisentanglingSamplingLabeling2021",
		"type": "paper-conference",
		"abstract": "Negative sampling is a widely adopted technique to enable efficient training in settings with a large number of classes. Typically, negative sampling approaches aim at approximating the value or gradient of the computationally expensive loss function that takes all the negative labels into account. In this work, we study the connection between negative sampling approaches and loss modification techniques for countering label imbalance. We show that different (bias) correction strategies that accompany negative sampling approaches can have unintended consequences on the model's performance on various data sub-populations. We then propose a unified approach to tackle both sampling bias, arising from working with a subset of all negative classes, and labeling bias, which is inherently present in the data due to label-imbalance. Finally, we verify our analysis and demonstrate the utility of our unified approach through empirical evaluation on standard image classification and retrieval benchmarks.",
		"container-title": "International conference on machine learning (ICML) 2021",
		"title": "Disentangling sampling and labeling bias for learning in large-output spaces",
		"author": [
			{
				"family": "Menon",
				"given": "Aditya Krishna"
			},
			{
				"family": "Rawat",
				"given": "Ankit Singh"
			},
			{
				"family": "Yu",
				"given": "Felix"
			},
			{
				"family": "Jayasumana",
				"given": "Sadeep"
			},
			{
				"family": "Kumar",
				"given": "Sanjiv"
			},
			{
				"family": "Reddi",
				"given": "Sashank"
			},
			{
				"family": "Jitkrittum",
				"given": "Wittawat"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liznerskiExplainableDeepOneClass2021",
		"type": "article",
		"abstract": "Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.",
		"DOI": "10.48550/arXiv.2007.01760",
		"note": "arXiv:2007.01760 [cs, stat]",
		"number": "arXiv:2007.01760",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Explainable Deep One-Class Classification",
		"URL": "http://arxiv.org/abs/2007.01760",
		"author": [
			{
				"family": "Liznerski",
				"given": "Philipp"
			},
			{
				"family": "Ruff",
				"given": "Lukas"
			},
			{
				"family": "Vandermeulen",
				"given": "Robert A."
			},
			{
				"family": "Franks",
				"given": "Billy Joe"
			},
			{
				"family": "Kloft",
				"given": "Marius"
			},
			{
				"family": "Müller",
				"given": "Klaus-Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					18
				]
			]
		}
	},
	{
		"id": "hsiehEvaluationsMethodsExplanation2021",
		"type": "article",
		"abstract": "Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to \"remove\" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.",
		"DOI": "10.48550/arXiv.2006.00442",
		"note": "arXiv:2006.00442 [cs, stat]",
		"number": "arXiv:2006.00442",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluations and Methods for Explanation through Robustness Analysis",
		"URL": "http://arxiv.org/abs/2006.00442",
		"author": [
			{
				"family": "Hsieh",
				"given": "Cheng-Yu"
			},
			{
				"family": "Yeh",
				"given": "Chih-Kuan"
			},
			{
				"family": "Liu",
				"given": "Xuanqing"
			},
			{
				"family": "Ravikumar",
				"given": "Pradeep"
			},
			{
				"family": "Kim",
				"given": "Seungyeon"
			},
			{
				"family": "Kumar",
				"given": "Sanjiv"
			},
			{
				"family": "Hsieh",
				"given": "Cho-Jui"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					8
				]
			]
		}
	},
	{
		"id": "balujaInterpretableActionsControlling2021",
		"type": "article-journal",
		"abstract": "Despite the prevalence of deep neural networks, their single most cited drawback is that, even when successful, their operations are inscrutable.  For many applications, the desired outputs are the composition of externally-defined bases.  For such decomposable domains, we present a two-stage learning procedure producing combinations of the external bases which are trivially extractable from the network. In the first stage, the set of external bases that will form the solution are modeled as differentiable generator modules, controlled by the same parameters as the external bases.  In the second stage, a controller network is created that selects parameters for those generators, either successively or in parallel, to compose the final solution.  Through three tasks, we concretely demonstrate how our system yields readily understandable commands.  In one, we introduce a new form of artistic style transfer, learning to draw and color with crayons, in which the transformation of a photograph or painting occurs not as a single monolithic computation, but by the composition of thousands of individual, visualizable strokes.  The other two tasks, single-pass function approximation with arbitrary bases and shape-based synthesis, show how our approach produces understandable and extractable actions in two disparate domains.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v35i6.16624",
		"ISSN": "2374-3468",
		"issue": "6",
		"language": "en",
		"license": "Copyright (c) 2021 Association for the Advancement of Artificial Intelligence",
		"note": "number: 6",
		"page": "4912-4922",
		"source": "ojs.aaai.org",
		"title": "Interpretable Actions: Controlling Experts with Understandable Commands",
		"title-short": "Interpretable Actions",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/16624",
		"volume": "35",
		"author": [
			{
				"family": "Baluja",
				"given": "Shumeet"
			},
			{
				"family": "Marwood",
				"given": "David"
			},
			{
				"family": "Covell",
				"given": "Michele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					18
				]
			]
		}
	},
	{
		"id": "guptaCalibrationNeuralNetworks2021",
		"type": "article",
		"abstract": "Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spine-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Our Code is available at https://github.com/kartikgupta-at-anu/spline-calibration.",
		"DOI": "10.48550/arXiv.2006.12800",
		"note": "arXiv:2006.12800 [cs, stat]",
		"number": "arXiv:2006.12800",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Calibration of Neural Networks using Splines",
		"URL": "http://arxiv.org/abs/2006.12800",
		"author": [
			{
				"family": "Gupta",
				"given": "Kartik"
			},
			{
				"family": "Rahimi",
				"given": "Amir"
			},
			{
				"family": "Ajanthan",
				"given": "Thalaiyasingam"
			},
			{
				"family": "Mensink",
				"given": "Thomas"
			},
			{
				"family": "Sminchisescu",
				"given": "Cristian"
			},
			{
				"family": "Hartley",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					29
				]
			]
		}
	},
	{
		"id": "kochReducedReusedRecycled2021",
		"type": "article",
		"abstract": "Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals. Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We find increasing concentration on fewer and fewer datasets within task communities, significant adoption of datasets from other tasks, and concentration across the field on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientific evaluation, AI ethics, and equity/access within the field.",
		"DOI": "10.48550/arXiv.2112.01716",
		"note": "arXiv:2112.01716 [cs, stat]",
		"number": "arXiv:2112.01716",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research",
		"title-short": "Reduced, Reused and Recycled",
		"URL": "http://arxiv.org/abs/2112.01716",
		"author": [
			{
				"family": "Koch",
				"given": "Bernard"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Foster",
				"given": "Jacob G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					3
				]
			]
		}
	},
	{
		"id": "havasiTrainingIndependentSubnetworks2020",
		"type": "paper-conference",
		"abstract": "Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant runtime cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved 'for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.",
		"event-title": "International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Training independent subnetworks for robust prediction",
		"URL": "https://openreview.net/forum?id=OGg9XnKxFAH",
		"author": [
			{
				"family": "Havasi",
				"given": "Marton"
			},
			{
				"family": "Jenatton",
				"given": "Rodolphe"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Liu",
				"given": "Jeremiah Zhe"
			},
			{
				"family": "Snoek",
				"given": "Jasper"
			},
			{
				"family": "Lakshminarayanan",
				"given": "Balaji"
			},
			{
				"family": "Dai",
				"given": "Andrew Mingbo"
			},
			{
				"family": "Tran",
				"given": "Dustin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					2
				]
			]
		}
	},
	{
		"id": "zhuangInterpretableRankingGeneralized2021",
		"type": "paper-conference",
		"abstract": "Interpretability of ranking models is a crucial yet relatively under-examined research area. Recent progress on this area largely focuses on generating post-hoc explanations for existing black-box ranking models. Though promising, such post-hoc methods cannot provide sufficiently accurate explanations in general, which makes them infeasible in many high-stakes scenarios, especially the ones with legal or policy constraints. Thus, building an intrinsically interpretable ranking model with transparent, self-explainable structure becomes necessary, but this remains less explored in the learning-to-rank setting.In this paper, we lay the groundwork for intrinsically interpretable learning-to-rank by introducing generalized additive models (GAMs) into ranking tasks. Generalized additive models (GAMs) are intrinsically interpretable machine learning models and have been extensively studied on regression and classification tasks. We study how to extend GAMs into ranking models which can handle both item-level and list-level features and propose a novel formulation of ranking GAMs. To instantiate ranking GAMs, we employ neural networks instead of traditional splines or regression trees. We also show that our neural ranking GAMs can be distilled into a set of simple and compact piece-wise linear functions that are much more efficient to evaluate with little accuracy loss. We conduct experiments on three data sets and show that our proposed neural ranking GAMs can outperform other traditional GAM baselines while maintaining similar interpretability.",
		"collection-title": "WSDM '21",
		"container-title": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining",
		"DOI": "10.1145/3437963.3441796",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8297-7",
		"page": "499–507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Interpretable Ranking with Generalized Additive Models",
		"URL": "https://doi.org/10.1145/3437963.3441796",
		"author": [
			{
				"family": "Zhuang",
				"given": "Honglei"
			},
			{
				"family": "Wang",
				"given": "Xuanhui"
			},
			{
				"family": "Bendersky",
				"given": "Michael"
			},
			{
				"family": "Grushetsky",
				"given": "Alexander"
			},
			{
				"family": "Wu",
				"given": "Yonghui"
			},
			{
				"family": "Mitrichev",
				"given": "Petr"
			},
			{
				"family": "Sterling",
				"given": "Ethan"
			},
			{
				"family": "Bell",
				"given": "Nathan"
			},
			{
				"family": "Ravina",
				"given": "Walker"
			},
			{
				"family": "Qian",
				"given": "Hai"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					8
				]
			]
		}
	},
	{
		"id": "alabdulmohsinNearOptimalAlgorithmDebiasing2022",
		"type": "article",
		"abstract": "We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.",
		"DOI": "10.48550/arXiv.2106.12887",
		"note": "arXiv:2106.12887 [cs, stat]",
		"number": "arXiv:2106.12887",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models",
		"URL": "http://arxiv.org/abs/2106.12887",
		"author": [
			{
				"family": "Alabdulmohsin",
				"given": "Ibrahim"
			},
			{
				"family": "Lucic",
				"given": "Mario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					23
				]
			]
		}
	},
	{
		"id": "parkCLARAClassifyingDisambiguating2024",
		"type": "article",
		"abstract": "In this paper, we focus on inferring whether the given user command is clear, ambiguous, or infeasible in the context of interactive robotic agents utilizing large language models (LLMs). To tackle this problem, we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command is classified as uncertain, we further distinguish it between ambiguous or infeasible commands leveraging LLMs with situational aware context in a zero-shot manner. For ambiguous commands, we disambiguate the command by interacting with users via question generation with LLMs. We believe that proper recognition of the given commands could lead to a decrease in malfunction and undesired actions of the robot, enhancing the reliability of interactive robot agents. We present a dataset for robotic situational awareness, consisting pair of high-level commands, scene descriptions, and labels of command type (i.e., clear, ambiguous, or infeasible). We validate the proposed method on the collected dataset, pick-and-place tabletop simulation. Finally, we demonstrate the proposed approach in real-world human-robot interaction experiments, i.e., handover scenarios.",
		"DOI": "10.48550/arXiv.2306.10376",
		"note": "arXiv:2306.10376 [cs]",
		"number": "arXiv:2306.10376",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents",
		"title-short": "CLARA",
		"URL": "http://arxiv.org/abs/2306.10376",
		"author": [
			{
				"family": "Park",
				"given": "Jeongeun"
			},
			{
				"family": "Lim",
				"given": "Seungwon"
			},
			{
				"family": "Lee",
				"given": "Joonhyung"
			},
			{
				"family": "Park",
				"given": "Sangbeom"
			},
			{
				"family": "Chang",
				"given": "Minsuk"
			},
			{
				"family": "Yu",
				"given": "Youngjae"
			},
			{
				"family": "Choi",
				"given": "Sungjoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					26
				]
			]
		}
	},
	{
		"id": "tobinPersonalizedAutomaticSpeech2022",
		"type": "paper-conference",
		"abstract": "This study investigates the performance of personalized automatic speech recognition (ASR) for recognizing disordered speech using small amounts of per-speaker adaptation data. We trained personalized models for 195 individuals with different types and severities of speech impairment with training sets ranging in size from <1 minute to 18-20 minutes of speech data. Word error rate (WER) thresholds were selected to determine Success Percentage (the percentage of personalized models reaching the target WER) in different application scenarios. For the home automation scenario, 79% of speakers reached the target WER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63% of speakers reached the target WER. Further evaluation found similar improvement on test sets with conversational and out-of-domain, unprompted phrases. Our results demonstrate that with only a few minutes of recordings, individuals with disordered speech could benefit from personalized ASR.",
		"container-title": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
		"DOI": "10.1109/ICASSP43922.2022.9747516",
		"event-title": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
		"note": "ISSN: 2379-190X",
		"page": "6637-6641",
		"source": "IEEE Xplore",
		"title": "Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9747516?casa_token=0yavbda3s5gAAAAA:yKVmhr9Kxgfje9S1VCQy3OZmenCJ-pBV8ZENJ6_dnThvnyi8F0710RMORA1VYTI5-WmaCuEC",
		"author": [
			{
				"family": "Tobin",
				"given": "Jimmy"
			},
			{
				"family": "Tomanek",
				"given": "Katrin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "pattersonCarbonFootprintMachine2022",
		"type": "article-journal",
		"abstract": "Many recent papers highlight the importance of thinking about carbon emissions (CO2e) in machine learning (ML) workloads. While elevating the discussion, some early work was also based on incomplete information. (Unfortunately, the most widely cited quantitative estimate that was the basis for many of these papers was off by 88X.) Inspired by these concerns, we looked for approaches that would make ML training considerably less carbon intensive. We identified four best practices that dramatically reduce carbon emissions, and demonstrate two concrete examples of reducing CO2e by 650X over four years and 40X over one year by following them. Provided ML stakeholders follow best practices, we predict that the field will bend the curve of carbon footprint increases from ML training runs to first flatten and then reduce it by 2030 without sacrificing the current rate of rapid advances in ML, contrary to prior dire warnings that ML CO2e will soar.",
		"container-title": "Computer",
		"DOI": "10.1109/MC.2022.3148714",
		"ISSN": "0018-9162, 1558-0814",
		"issue": "7",
		"journalAbbreviation": "Computer",
		"language": "en",
		"license": "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
		"page": "18-28",
		"source": "DOI.org (Crossref)",
		"title": "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",
		"URL": "https://ieeexplore.ieee.org/document/9810097/",
		"volume": "55",
		"author": [
			{
				"family": "Patterson",
				"given": "David"
			},
			{
				"family": "Gonzalez",
				"given": "Joseph"
			},
			{
				"family": "Holzle",
				"given": "Urs"
			},
			{
				"family": "Le",
				"given": "Quoc"
			},
			{
				"family": "Liang",
				"given": "Chen"
			},
			{
				"family": "Munguia",
				"given": "Lluis-Miquel"
			},
			{
				"family": "Rothchild",
				"given": "Daniel"
			},
			{
				"family": "So",
				"given": "David R."
			},
			{
				"family": "Texier",
				"given": "Maud"
			},
			{
				"family": "Dean",
				"given": "Jeff"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "sarkarUncoveringHiddenDynamics2023",
		"type": "paper-conference",
		"abstract": "Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally struggle with context shifts, v-MAE and supervised learning exhibit more robustness. Moreover, our study shows that v-MAE is a strong temporal learner, whereas contrastive methods, v-SimCLR and v-MoCo, exhibit strong performances against viewpoint shifts. When studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set recognition performance if the pretrained VSSL encoders are used without finetuning. We hope that our work will contribute to the development of robust video representation learning frameworks for various real-world scenarios. The project page and code are available at: https://pritamqu.github.io/OOD-VSSL.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts",
		"URL": "https://openreview.net/forum?id=bKqrWLCMrX",
		"author": [
			{
				"family": "Sarkar",
				"given": "Pritam"
			},
			{
				"family": "Beirami",
				"given": "Ahmad"
			},
			{
				"family": "Etemad",
				"given": "Ali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "haoSafetyFairnessContent2023",
		"type": "article",
		"abstract": "With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.",
		"DOI": "10.48550/arXiv.2306.06135",
		"note": "arXiv:2306.06135 [cs]",
		"number": "arXiv:2306.06135",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safety and Fairness for Content Moderation in Generative Models",
		"URL": "http://arxiv.org/abs/2306.06135",
		"author": [
			{
				"family": "Hao",
				"given": "Susan"
			},
			{
				"family": "Kumar",
				"given": "Piyush"
			},
			{
				"family": "Laszlo",
				"given": "Sarah"
			},
			{
				"family": "Poddar",
				"given": "Shivani"
			},
			{
				"family": "Radharapu",
				"given": "Bhaktipriya"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					8
				]
			]
		}
	},
	{
		"id": "rismaniPlaneCrashesAlgorithmic2023",
		"type": "paper-conference",
		"abstract": "Inappropriate design and deployment of machine learning (ML) systems lead to negative downstream social and ethical impacts – described here as social and ethical risks – for users, society, and the environment. Despite the growing need to regulate ML systems, current processes for assessing and mitigating risks are disjointed and inconsistent. We interviewed 30 industry practitioners on their current social and ethical risk management practices and collected their first reactions on adapting safety engineering frameworks into their practice – namely, System Theoretic Process Analysis (STPA) and Failure Mode and Effects Analysis (FMEA). Our findings suggest STPA/FMEA can provide an appropriate structure for social and ethical risk assessment and mitigation processes. However, we also find nontrivial challenges in integrating such frameworks in the fast-paced culture of the ML industry. We call on the CHI community to strengthen existing frameworks and assess their efficacy, ensuring that ML systems are safer for all people.",
		"collection-title": "CHI '23",
		"container-title": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
		"DOI": "10.1145/3544548.3581407",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9421-5",
		"page": "1–18",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML",
		"title-short": "From Plane Crashes to Algorithmic Harm",
		"URL": "https://doi.org/10.1145/3544548.3581407",
		"author": [
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Jatho",
				"given": "Edgar"
			},
			{
				"family": "Kroll",
				"given": "Joshua"
			},
			{
				"family": "Moon",
				"given": "AJung"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					19
				]
			]
		}
	},
	{
		"id": "aroyoDICESDatasetDiversity2023",
		"type": "article",
		"abstract": "Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This risks simplifying and even obscuring the inherent subjectivity present in many tasks. Preserving such variance in content and diversity in datasets is often expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is both socially and culturally situated. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographic information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. In short, the DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of conversational AI safety. We also illustrate how the dataset offers a basis for establishing metrics to show how raters' ratings can intersects with demographic categories such as racial/ethnic groups, age groups, and genders. The goal of DICES is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.",
		"DOI": "10.48550/arXiv.2306.11247",
		"note": "arXiv:2306.11247 [cs]",
		"number": "arXiv:2306.11247",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DICES Dataset: Diversity in Conversational AI Evaluation for Safety",
		"title-short": "DICES Dataset",
		"URL": "http://arxiv.org/abs/2306.11247",
		"author": [
			{
				"family": "Aroyo",
				"given": "Lora"
			},
			{
				"family": "Taylor",
				"given": "Alex S."
			},
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Homan",
				"given": "Christopher M."
			},
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Serapio-Garcia",
				"given": "Greg"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			},
			{
				"family": "Wang",
				"given": "Ding"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					19
				]
			]
		}
	},
	{
		"id": "slymanVLSliceInteractiveVisionandLanguage2023",
		"type": "article",
		"abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.",
		"language": "en",
		"note": "arXiv:2309.06703 [cs]",
		"number": "arXiv:2309.06703",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "VLSlice: Interactive Vision-and-Language Slice Discovery",
		"title-short": "VLSlice",
		"URL": "http://arxiv.org/abs/2309.06703",
		"author": [
			{
				"family": "Slyman",
				"given": "Eric"
			},
			{
				"family": "Kahng",
				"given": "Minsuk"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					13
				]
			]
		}
	},
	{
		"id": "maLetThoughtExperiment2023",
		"type": "article",
		"abstract": "Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%.",
		"DOI": "10.48550/arXiv.2306.14308",
		"note": "arXiv:2306.14308 [cs]",
		"number": "arXiv:2306.14308",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning",
		"title-short": "Let's Do a Thought Experiment",
		"URL": "http://arxiv.org/abs/2306.14308",
		"author": [
			{
				"family": "Ma",
				"given": "Xiao"
			},
			{
				"family": "Mishra",
				"given": "Swaroop"
			},
			{
				"family": "Beirami",
				"given": "Ahmad"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					25
				]
			]
		}
	},
	{
		"id": "ktenaGenerativeModelsImprove2024",
		"type": "article-journal",
		"abstract": "Domain generalization is a ubiquitous challenge for machine learning in healthcare. Model performance in real-world conditions might be lower than expected because of discrepancies between the data encountered during deployment and development. Underrepresentation of some groups or conditions during model development is a common cause of this phenomenon. This challenge is often not readily addressed by targeted data acquisition and ‘labeling’ by expert clinicians, which can be prohibitively expensive or practically impossible because of the rarity of conditions or the available clinical expertise. We hypothesize that advances in generative artificial intelligence can help mitigate this unmet need in a steerable fashion, enriching our training dataset with synthetic examples that address shortfalls of underrepresented conditions or subgroups. We show that diffusion models can automatically learn realistic augmentations from data in a label-efficient manner. We demonstrate that learned augmentations make models more robust and statistically fair in-distribution and out of distribution. To evaluate the generality of our approach, we studied three distinct medical imaging contexts of varying difficulty: (1) histopathology, (2) chest X-ray and (3) dermatology images. Complementing real samples with synthetic ones improved the robustness of models in all three medical tasks and increased fairness by improving the accuracy of clinical diagnosis within underrepresented groups, especially out of distribution.",
		"container-title": "Nature Medicine",
		"DOI": "10.1038/s41591-024-02838-6",
		"ISSN": "1546-170X",
		"issue": "4",
		"journalAbbreviation": "Nat Med",
		"language": "en",
		"license": "2024 The Author(s)",
		"note": "publisher: Nature Publishing Group",
		"page": "1166-1173",
		"source": "www.nature.com",
		"title": "Generative models improve fairness of medical classifiers under distribution shifts",
		"URL": "https://www.nature.com/articles/s41591-024-02838-6",
		"volume": "30",
		"author": [
			{
				"family": "Ktena",
				"given": "Ira"
			},
			{
				"family": "Wiles",
				"given": "Olivia"
			},
			{
				"family": "Albuquerque",
				"given": "Isabela"
			},
			{
				"family": "Rebuffi",
				"given": "Sylvestre-Alvise"
			},
			{
				"family": "Tanno",
				"given": "Ryutaro"
			},
			{
				"family": "Roy",
				"given": "Abhijit Guha"
			},
			{
				"family": "Azizi",
				"given": "Shekoofeh"
			},
			{
				"family": "Belgrave",
				"given": "Danielle"
			},
			{
				"family": "Kohli",
				"given": "Pushmeet"
			},
			{
				"family": "Cemgil",
				"given": "Taylan"
			},
			{
				"family": "Karthikesalingam",
				"given": "Alan"
			},
			{
				"family": "Gowal",
				"given": "Sven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4
				]
			]
		}
	},
	{
		"id": "mcmahanLearningDifferentiallyPrivate2018",
		"type": "article",
		"abstract": "We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes \"large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.",
		"DOI": "10.48550/arXiv.1710.06963",
		"note": "arXiv:1710.06963 [cs]",
		"number": "arXiv:1710.06963",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Differentially Private Recurrent Language Models",
		"URL": "http://arxiv.org/abs/1710.06963",
		"author": [
			{
				"family": "McMahan",
				"given": "H. Brendan"
			},
			{
				"family": "Ramage",
				"given": "Daniel"
			},
			{
				"family": "Talwar",
				"given": "Kunal"
			},
			{
				"family": "Zhang",
				"given": "Li"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					23
				]
			]
		}
	},
	{
		"id": "qu2021natural",
		"type": "paper-conference",
		"abstract": "Privacy preservation remains a key challenge in data mining and Natural Language Understanding (NLU). Previous research shows that the input text or even text embeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying dχ-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs.",
		"container-title": "Proceedings of the 30th ACM international conference on information & knowledge management",
		"page": "1488–1497",
		"title": "Natural language understanding with privacy-preserving bert",
		"author": [
			{
				"family": "Qu",
				"given": "Chen"
			},
			{
				"family": "Kong",
				"given": "Weize"
			},
			{
				"family": "Yang",
				"given": "Liu"
			},
			{
				"family": "Zhang",
				"given": "Mingyang"
			},
			{
				"family": "Bendersky",
				"given": "Michael"
			},
			{
				"family": "Najork",
				"given": "Marc"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barrettIdentifyingMitigatingSecurity2023",
		"type": "article-journal",
		"abstract": "Every major technical invention resurfaces the dual-use dilemma — the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This monograph reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This work is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. Short-term and long-term goals for the community on this topic are discussed. This work should provide both a launching point for a discussion on this important topic, as well as interesting problems that the research community can work to address.",
		"container-title": "Foundations and Trends® in Privacy and Security",
		"DOI": "10.1561/3300000041",
		"ISSN": "2474-1558, 2474-1566",
		"issue": "1",
		"journalAbbreviation": "SEC",
		"language": "English",
		"note": "publisher: Now Publishers, Inc.",
		"page": "1-52",
		"source": "www.nowpublishers.com",
		"title": "Identifying and Mitigating the Security Risks of Generative AI",
		"URL": "https://www.nowpublishers.com/article/Details/SEC-041",
		"volume": "6",
		"author": [
			{
				"family": "Barrett",
				"given": "Clark"
			},
			{
				"family": "Boyd",
				"given": "Brad"
			},
			{
				"family": "Bursztein",
				"given": "Elie"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Chen",
				"given": "Brad"
			},
			{
				"family": "Choi",
				"given": "Jihye"
			},
			{
				"family": "Chowdhury",
				"given": "Amrita Roy"
			},
			{
				"family": "Christodorescu",
				"given": "Mihai"
			},
			{
				"family": "Datta",
				"given": "Anupam"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			},
			{
				"family": "Fisher",
				"given": "Kathleen"
			},
			{
				"family": "Hashimoto",
				"given": "Tatsunori"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Jha",
				"given": "Somesh"
			},
			{
				"family": "Kang",
				"given": "Daniel"
			},
			{
				"family": "Kerschbaum",
				"given": "Florian"
			},
			{
				"family": "Mitchell",
				"given": "Eric"
			},
			{
				"family": "Mitchell",
				"given": "John"
			},
			{
				"family": "Ramzan",
				"given": "Zulfikar"
			},
			{
				"family": "Shams",
				"given": "Khawaja"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			},
			{
				"family": "Yang",
				"given": "Diyi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					13
				]
			]
		}
	},
	{
		"id": "chaudhuriSecuringAISoftware2024",
		"type": "article",
		"abstract": "As AI-powered features gain traction in software applications, we see many of the same problems we’ve faced with traditional software—but at an accelerated pace. The threat landscape continues to expand as AI is further integrated into everyday products, so we can expect more attacks. Given the expense of building models, there is a clear need for supply chain solutions. This paper explains our approach to securing our AI supply chain using provenance information and provides guidance for other organizations. Although there are differences between traditional and AI development processes and risks, we can build on our work over the past decade using Binary Authorization for Borg (BAB), Supply-chain Levels for Software Artifacts (SLSA), and next-generation cryptographic signing solutions via Sigstore, and adapt these to the AI supply chain without reinventing the wheel. Depending on internal processes and platforms, each organization’s approach to AI supply chain security will look different, but the focus should be on areas where it can be improved in a relatively short time. Readers should note that the first part of this paper provides a broad overview of “Development lifecycles for traditional and AI software”. Then we delve specifically into AI supply chain risks, and explain our approach to securing our AI supply chain using provenance information. More advanced practitioners may prefer to go directly to the sections on “AI supply chain risks,” “Controls for AI supply chain security,” or even the “Guidance for practitioners” section at the end of the paper, which can be adapted to the needs of any organization.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Securing the AI Software Supply Chain",
		"author": [
			{
				"family": "Chaudhuri",
				"given": "Shamik"
			},
			{
				"family": "Dasgupta",
				"given": "Kingshuk"
			},
			{
				"family": "Hepworth",
				"given": "Isaac"
			},
			{
				"family": "Le",
				"given": "Michael"
			},
			{
				"family": "Lodato",
				"given": "Mark"
			},
			{
				"family": "Maruseac",
				"given": "Mihai"
			},
			{
				"family": "Meiklejohn",
				"given": "Sarah"
			},
			{
				"family": "Minkus",
				"given": "Tehila"
			},
			{
				"family": "Olive",
				"given": "Kara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "yuPrivacyPreservingInstructionsAligning2024a",
		"type": "article",
		"abstract": "Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.",
		"DOI": "10.48550/arXiv.2402.13659",
		"note": "arXiv:2402.13659 [cs]",
		"number": "arXiv:2402.13659",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Privacy-Preserving Instructions for Aligning Large Language Models",
		"URL": "http://arxiv.org/abs/2402.13659",
		"author": [
			{
				"family": "Yu",
				"given": "Da"
			},
			{
				"family": "Kairouz",
				"given": "Peter"
			},
			{
				"family": "Oh",
				"given": "Sewoong"
			},
			{
				"family": "Xu",
				"given": "Zheng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					8
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		}
	},
	{
		"id": "christianoDeepReinforcementLearning2023a",
		"type": "article",
		"abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
		"DOI": "10.48550/arXiv.1706.03741",
		"note": "arXiv:1706.03741 [cs, stat]",
		"number": "arXiv:1706.03741",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep reinforcement learning from human preferences",
		"URL": "http://arxiv.org/abs/1706.03741",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Martic",
				"given": "Miljan"
			},
			{
				"family": "Legg",
				"given": "Shane"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					17
				]
			]
		}
	},
	{
		"id": "everittAGISafetyLiterature2018",
		"type": "article",
		"abstract": "The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",
		"DOI": "10.48550/arXiv.1805.01109",
		"note": "arXiv:1805.01109 [cs]",
		"number": "arXiv:1805.01109",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AGI Safety Literature Review",
		"URL": "http://arxiv.org/abs/1805.01109",
		"author": [
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Lea",
				"given": "Gary"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					21
				]
			]
		}
	},
	{
		"id": "researchBuildingSafeArtificial2018",
		"type": "post-weblog",
		"abstract": "Building a rocket is hard. Each component requires careful thought and rigorous testing, with safety and reliability at the core of the designs. Rocket scientists and engineers come together to design everything from the navigation course to control systems, engines and landing gear. Once all the pieces are assembled and the systems are tested, we can put astronauts on board with confidence that things will go well. If artificial intelligence (AI) is a rocket, then we will all have tickets on board some day. And, as in rockets, safety is a crucial part of building AI systems. Guaranteeing safety requires carefully designing a system from the ground up to ensure the various components work together as intended, while developing all the instruments necessary to oversee the successful operation of the system after deployment. At a high level, safety research at DeepMind focuses on designing systems that reliably function as intended while discovering and mitigating possible near-term and long-term risks. Technical AI safety is a relatively nascent but rapidly evolving field, with its contents ranging from high-level and theoretical to empirical and concrete. The goal of this blog is to contribute to the development of the field and encourage substantive engagement with the technical ideas discussed, and in doing so, advance our collective understanding of AI safety. In this inaugural post, we discuss three areas of technical AI safety: specification, robustness, and assurance. Future posts will broadly fit within the framework outlined here. While our views will inevitably evolve over time, we feel these three areas cover a sufficiently wide spectrum to provide a useful categorisation for ongoing and future research.",
		"container-title": "Medium",
		"language": "en",
		"title": "Building safe artificial intelligence: specification, robustness, and assurance",
		"title-short": "Building safe artificial intelligence",
		"URL": "https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1",
		"author": [
			{
				"family": "Research",
				"given": "DeepMind Safety"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					27
				]
			]
		}
	},
	{
		"id": "amodeiConcreteProblemsAI2016a",
		"type": "article",
		"abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
		"DOI": "10.48550/arXiv.1606.06565",
		"note": "arXiv:1606.06565 [cs]",
		"number": "arXiv:1606.06565",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Concrete Problems in AI Safety",
		"URL": "http://arxiv.org/abs/1606.06565",
		"author": [
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Mané",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					7,
					25
				]
			]
		}
	},
	{
		"id": "russellResearchPrioritiesRobust2015a",
		"type": "article-journal",
		"abstract": "Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",
		"container-title": "AI Magazine",
		"DOI": "10.1609/aimag.v36i4.2577",
		"ISSN": "0738-4602, 2371-9621",
		"issue": "4",
		"journalAbbreviation": "AI Magazine",
		"language": "en",
		"license": "http://onlinelibrary.wiley.com/termsAndConditions#vor",
		"page": "105-114",
		"source": "DOI.org (Crossref)",
		"title": "Research Priorities for Robust and Beneficial Artificial Intelligence",
		"URL": "https://onlinelibrary.wiley.com/doi/10.1609/aimag.v36i4.2577",
		"volume": "36",
		"author": [
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Dewey",
				"given": "Daniel"
			},
			{
				"family": "Tegmark",
				"given": "Max"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					12
				]
			]
		}
	},
	{
		"id": "leikeScalableAgentAlignment2018a",
		"type": "article",
		"abstract": "One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",
		"DOI": "10.48550/arXiv.1811.07871",
		"note": "arXiv:1811.07871 [cs, stat]",
		"number": "arXiv:1811.07871",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scalable agent alignment via reward modeling: a research direction",
		"title-short": "Scalable agent alignment via reward modeling",
		"URL": "http://arxiv.org/abs/1811.07871",
		"author": [
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Martic",
				"given": "Miljan"
			},
			{
				"family": "Maini",
				"given": "Vishal"
			},
			{
				"family": "Legg",
				"given": "Shane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					19
				]
			]
		}
	},
	{
		"id": "christianoSupervisingStrongLearners2018a",
		"type": "article",
		"abstract": "Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",
		"DOI": "10.48550/arXiv.1810.08575",
		"note": "arXiv:1810.08575 [cs, stat]",
		"number": "arXiv:1810.08575",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Supervising strong learners by amplifying weak experts",
		"URL": "http://arxiv.org/abs/1810.08575",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					19
				]
			]
		}
	},
	{
		"id": "irvingAISafetyDebate2018a",
		"type": "article",
		"abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",
		"DOI": "10.48550/arXiv.1805.00899",
		"note": "arXiv:1805.00899 [cs, stat]",
		"number": "arXiv:1805.00899",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI safety via debate",
		"URL": "http://arxiv.org/abs/1805.00899",
		"author": [
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					22
				]
			]
		}
	},
	{
		"id": "langoscoGoalMisgeneralizationDeep2023",
		"type": "article",
		"abstract": "We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.",
		"DOI": "10.48550/arXiv.2105.14111",
		"note": "arXiv:2105.14111 [cs]",
		"number": "arXiv:2105.14111",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Goal Misgeneralization in Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2105.14111",
		"author": [
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Koch",
				"given": "Jack"
			},
			{
				"family": "Sharkey",
				"given": "Lee"
			},
			{
				"family": "Pfau",
				"given": "Jacob"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					9
				]
			]
		}
	},
	{
		"id": "weidingerEthicalSocialRisks2021a",
		"type": "article",
		"abstract": "This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.",
		"DOI": "10.48550/arXiv.2112.04359",
		"note": "arXiv:2112.04359 [cs]",
		"number": "arXiv:2112.04359",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Ethical and social risks of harm from Language Models",
		"URL": "http://arxiv.org/abs/2112.04359",
		"author": [
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Mellor",
				"given": "John"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Huang",
				"given": "Po-Sen"
			},
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "Glaese",
				"given": "Mia"
			},
			{
				"family": "Balle",
				"given": "Borja"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Kenton",
				"given": "Zac"
			},
			{
				"family": "Brown",
				"given": "Sasha"
			},
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Stepleton",
				"given": "Tom"
			},
			{
				"family": "Biles",
				"given": "Courtney"
			},
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Haas",
				"given": "Julia"
			},
			{
				"family": "Rimell",
				"given": "Laura"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Isaac",
				"given": "William"
			},
			{
				"family": "Legassick",
				"given": "Sean"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					8
				]
			]
		}
	},
	{
		"id": "cattSelfPredictiveUniversalAI2023",
		"type": "paper-conference",
		"abstract": "Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. The integration of both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the most potent theoretical universal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by self-predicting its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Self-Predictive Universal AI",
		"URL": "https://openreview.net/forum?id=psXVkKO9No&referrer=%5Bthe%20profile%20of%20Li%20Kevin%20Wenliang%5D(%2Fprofile%3Fid%3D~Li_Kevin_Wenliang1)",
		"author": [
			{
				"family": "Catt",
				"given": "Elliot"
			},
			{
				"family": "Grau-Moya",
				"given": "Jordi"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "Aitchison",
				"given": "Matthew"
			},
			{
				"family": "Genewein",
				"given": "Tim"
			},
			{
				"family": "Deletang",
				"given": "Gregoire"
			},
			{
				"family": "Wenliang",
				"given": "Li Kevin"
			},
			{
				"family": "Veness",
				"given": "Joel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "lindnerTracrCompiledTransformers2023",
		"type": "article",
		"abstract": "We show how to \"compile\" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study \"superposition\" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the \"programs\" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr.",
		"DOI": "10.48550/arXiv.2301.05062",
		"note": "arXiv:2301.05062 [cs, stat]",
		"number": "arXiv:2301.05062",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tracr: Compiled Transformers as a Laboratory for Interpretability",
		"title-short": "Tracr",
		"URL": "http://arxiv.org/abs/2301.05062",
		"author": [
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Kramár",
				"given": "János"
			},
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Rahtz",
				"given": "Matthew"
			},
			{
				"family": "McGrath",
				"given": "Thomas"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					3
				]
			]
		}
	},
	{
		"id": "schioppaTheoreticalPracticalPerspectives2023",
		"type": "paper-conference",
		"abstract": "Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples \"responsible\" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models. Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Theoretical and Practical Perspectives on what Influence Functions Do",
		"URL": "https://openreview.net/forum?id=gGl0n7Onug",
		"author": [
			{
				"family": "Schioppa",
				"given": "Andrea"
			},
			{
				"family": "Filippova",
				"given": "Katja"
			},
			{
				"family": "Titov",
				"given": "Ivan"
			},
			{
				"family": "Zablotskaia",
				"given": "Polina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "huangLargeLanguageModels2024",
		"type": "article",
		"abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
		"DOI": "10.48550/arXiv.2310.01798",
		"note": "arXiv:2310.01798 [cs]",
		"number": "arXiv:2310.01798",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Large Language Models Cannot Self-Correct Reasoning Yet",
		"URL": "http://arxiv.org/abs/2310.01798",
		"author": [
			{
				"family": "Huang",
				"given": "Jie"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Mishra",
				"given": "Swaroop"
			},
			{
				"family": "Zheng",
				"given": "Huaixiu Steven"
			},
			{
				"family": "Yu",
				"given": "Adams Wei"
			},
			{
				"family": "Song",
				"given": "Xinying"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					14
				]
			]
		}
	},
	{
		"id": "yasunagaLargeLanguageModels2024",
		"type": "article",
		"abstract": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
		"DOI": "10.48550/arXiv.2310.01714",
		"note": "arXiv:2310.01714 [cs]",
		"number": "arXiv:2310.01714",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Large Language Models as Analogical Reasoners",
		"URL": "http://arxiv.org/abs/2310.01714",
		"author": [
			{
				"family": "Yasunaga",
				"given": "Michihiro"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Li",
				"given": "Yujia"
			},
			{
				"family": "Pasupat",
				"given": "Panupong"
			},
			{
				"family": "Leskovec",
				"given": "Jure"
			},
			{
				"family": "Liang",
				"given": "Percy"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Zhou",
				"given": "Denny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					9
				]
			]
		}
	},
	{
		"id": "bulianAssessingLargeLanguage2024",
		"type": "article",
		"abstract": "As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.",
		"DOI": "10.48550/arXiv.2310.02932",
		"note": "arXiv:2310.02932 [cs]",
		"number": "arXiv:2310.02932",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Assessing Large Language Models on Climate Information",
		"URL": "http://arxiv.org/abs/2310.02932",
		"author": [
			{
				"family": "Bulian",
				"given": "Jannis"
			},
			{
				"family": "Schäfer",
				"given": "Mike S."
			},
			{
				"family": "Amini",
				"given": "Afra"
			},
			{
				"family": "Lam",
				"given": "Heidi"
			},
			{
				"family": "Ciaramita",
				"given": "Massimiliano"
			},
			{
				"family": "Gaiarin",
				"given": "Ben"
			},
			{
				"family": "Hübscher",
				"given": "Michelle Chen"
			},
			{
				"family": "Buck",
				"given": "Christian"
			},
			{
				"family": "Mede",
				"given": "Niels G."
			},
			{
				"family": "Leippold",
				"given": "Markus"
			},
			{
				"family": "Strauß",
				"given": "Nadine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					28
				]
			]
		}
	},
	{
		"id": "zhengTakeStepBack2024",
		"type": "article",
		"abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.",
		"DOI": "10.48550/arXiv.2310.06117",
		"note": "arXiv:2310.06117 [cs]",
		"number": "arXiv:2310.06117",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
		"title-short": "Take a Step Back",
		"URL": "http://arxiv.org/abs/2310.06117",
		"author": [
			{
				"family": "Zheng",
				"given": "Huaixiu Steven"
			},
			{
				"family": "Mishra",
				"given": "Swaroop"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Cheng",
				"given": "Heng-Tze"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Le",
				"given": "Quoc V."
			},
			{
				"family": "Zhou",
				"given": "Denny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					12
				]
			]
		}
	},
	{
		"id": "weidingerSociotechnicalSafetyEvaluation2023",
		"type": "article",
		"abstract": "Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.",
		"language": "en",
		"note": "arXiv:2310.11986 [cs]",
		"number": "arXiv:2310.11986",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sociotechnical Safety Evaluation of Generative AI Systems",
		"URL": "http://arxiv.org/abs/2310.11986",
		"author": [
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Mateos-Garcia",
				"given": "Juan"
			},
			{
				"family": "Bergman",
				"given": "Stevie"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Bariach",
				"given": "Ben"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Isaac",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					31
				]
			]
		}
	},
	{
		"id": "lanctotPopulationbasedEvaluationRepeated2023",
		"type": "article-journal",
		"abstract": "Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=gQnJ7ODIAx",
		"author": [
			{
				"family": "Lanctot",
				"given": "Marc"
			},
			{
				"family": "Schultz",
				"given": "John"
			},
			{
				"family": "Burch",
				"given": "Neil"
			},
			{
				"family": "Smith",
				"given": "Max Olan"
			},
			{
				"family": "Hennes",
				"given": "Daniel"
			},
			{
				"family": "Anthony",
				"given": "Thomas"
			},
			{
				"family": "Perolat",
				"given": "Julien"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					18
				]
			]
		}
	},
	{
		"id": "sermanetRoboVQAMultimodalLongHorizon2023",
		"type": "article",
		"abstract": "We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple robot and human embodiments. With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We find that for a fixed collection budget it is beneficial to take advantage of cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zero-shot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Data and videos available at https://robovqa.github.io",
		"DOI": "10.48550/arXiv.2311.00899",
		"note": "arXiv:2311.00899 [cs]",
		"number": "arXiv:2311.00899",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics",
		"title-short": "RoboVQA",
		"URL": "http://arxiv.org/abs/2311.00899",
		"author": [
			{
				"family": "Sermanet",
				"given": "Pierre"
			},
			{
				"family": "Ding",
				"given": "Tianli"
			},
			{
				"family": "Zhao",
				"given": "Jeffrey"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Dwibedi",
				"given": "Debidatta"
			},
			{
				"family": "Gopalakrishnan",
				"given": "Keerthana"
			},
			{
				"family": "Chan",
				"given": "Christine"
			},
			{
				"family": "Dulac-Arnold",
				"given": "Gabriel"
			},
			{
				"family": "Maddineni",
				"given": "Sharath"
			},
			{
				"family": "Joshi",
				"given": "Nikhil J."
			},
			{
				"family": "Florence",
				"given": "Pete"
			},
			{
				"family": "Han",
				"given": "Wei"
			},
			{
				"family": "Baruch",
				"given": "Robert"
			},
			{
				"family": "Lu",
				"given": "Yao"
			},
			{
				"family": "Mirchandani",
				"given": "Suvir"
			},
			{
				"family": "Xu",
				"given": "Peng"
			},
			{
				"family": "Sanketi",
				"given": "Pannag"
			},
			{
				"family": "Hausman",
				"given": "Karol"
			},
			{
				"family": "Shafran",
				"given": "Izhak"
			},
			{
				"family": "Ichter",
				"given": "Brian"
			},
			{
				"family": "Cao",
				"given": "Yuan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					1
				]
			]
		}
	},
	{
		"id": "wangGrammarPromptingDomainSpecific2023",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose \\emph{grammar prompting}, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
		"URL": "https://openreview.net/forum?id=B4tkwuzeiY&referrer=%5Bthe%20profile%20of%20Rif%20A.%20Saurous%5D(%2Fprofile%3Fid%3D~Rif_A._Saurous1)",
		"author": [
			{
				"family": "Wang",
				"given": "Bailin"
			},
			{
				"family": "Wang",
				"given": "Zi"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Cao",
				"given": "Yuan"
			},
			{
				"family": "Saurous",
				"given": "Rif A."
			},
			{
				"family": "Kim",
				"given": "Yoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "shanahanRolePlayLarge2023",
		"type": "article-journal",
		"abstract": "As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.",
		"container-title": "Nature",
		"DOI": "10.1038/s41586-023-06647-8",
		"ISSN": "1476-4687",
		"issue": "7987",
		"language": "en",
		"license": "2023 Springer Nature Limited",
		"note": "publisher: Nature Publishing Group",
		"page": "493-498",
		"source": "www.nature.com",
		"title": "Role play with large language models",
		"URL": "https://www.nature.com/articles/s41586-023-06647-8",
		"volume": "623",
		"author": [
			{
				"family": "Shanahan",
				"given": "Murray"
			},
			{
				"family": "McDonell",
				"given": "Kyle"
			},
			{
				"family": "Reynolds",
				"given": "Laria"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11
				]
			]
		}
	},
	{
		"id": "duenez-guzmanSocialPathHumanlike2023",
		"type": "article-journal",
		"abstract": "Traditionally, cognitive and computer scientists have viewed intelligence solipsistically, as a property of unitary agents devoid of social context. Given the success of contemporary learning algorithms, we argue that the bottleneck in artificial intelligence (AI) advancement is shifting from data assimilation to novel data generation. We bring together evidence showing that natural intelligence emerges at multiple scales in networks of interacting agents via collective living, social relationships and major evolutionary transitions, which contribute to novel data generation through mechanisms such as population pressures, arms races, Machiavellian selection, social learning and cumulative culture. Many breakthroughs in AI exploit some of these processes, from multi-agent structures enabling algorithms to master complex games such as Capture-The-Flag and StarCraft II, to strategic communication in the game Diplomacy and the shaping of AI data streams by other AIs. Moving beyond a solipsistic view of agency to integrate these mechanisms could provide a path to human-like compounding innovation through ongoing novel data generation.",
		"container-title": "Nature Machine Intelligence",
		"DOI": "10.1038/s42256-023-00754-x",
		"ISSN": "2522-5839",
		"issue": "11",
		"journalAbbreviation": "Nat Mach Intell",
		"language": "en",
		"license": "2023 Springer Nature Limited",
		"note": "publisher: Nature Publishing Group",
		"page": "1181-1188",
		"source": "www.nature.com",
		"title": "A social path to human-like artificial intelligence",
		"URL": "https://www.nature.com/articles/s42256-023-00754-x",
		"volume": "5",
		"author": [
			{
				"family": "Duéñez-Guzmán",
				"given": "Edgar A."
			},
			{
				"family": "Sadedin",
				"given": "Suzanne"
			},
			{
				"family": "Wang",
				"given": "Jane X."
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Leibo",
				"given": "Joel Z."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11
				]
			]
		}
	},
	{
		"id": "brown-cohenScalableAISafety2023",
		"type": "article",
		"abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",
		"DOI": "10.48550/arXiv.2311.14125",
		"note": "arXiv:2311.14125 [cs]",
		"number": "arXiv:2311.14125",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scalable AI Safety via Doubly-Efficient Debate",
		"URL": "http://arxiv.org/abs/2311.14125",
		"author": [
			{
				"family": "Brown-Cohen",
				"given": "Jonah"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					23
				]
			]
		}
	},
	{
		"id": "tirumalaReplayExperimentsNatural2023",
		"type": "article",
		"abstract": "Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL). We present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times. At its core, Replay Across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work. We empirically show benefits across a number of RL algorithms and challenging control domains spanning both locomotion and manipulation, including hard exploration tasks from egocentric vision. Through comprehensive ablations, we demonstrate robustness to the quality and amount of data available and various hyperparameter choices. Finally, we discuss how our approach can be applied more broadly across research life cycles and can increase resilience by reloading data across random seeds or hyperparameter variations.",
		"DOI": "10.48550/arXiv.2311.15951",
		"note": "arXiv:2311.15951 [cs]",
		"number": "arXiv:2311.15951",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Replay across Experiments: A Natural Extension of Off-Policy RL",
		"title-short": "Replay across Experiments",
		"URL": "http://arxiv.org/abs/2311.15951",
		"author": [
			{
				"family": "Tirumala",
				"given": "Dhruva"
			},
			{
				"family": "Lampe",
				"given": "Thomas"
			},
			{
				"family": "Chen",
				"given": "Jose Enrique"
			},
			{
				"family": "Haarnoja",
				"given": "Tuomas"
			},
			{
				"family": "Huang",
				"given": "Sandy"
			},
			{
				"family": "Lever",
				"given": "Guy"
			},
			{
				"family": "Moran",
				"given": "Ben"
			},
			{
				"family": "Hertweck",
				"given": "Tim"
			},
			{
				"family": "Hasenclever",
				"given": "Leonard"
			},
			{
				"family": "Riedmiller",
				"given": "Martin"
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			},
			{
				"family": "Wulfmeier",
				"given": "Markus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					28
				]
			]
		}
	},
	{
		"id": "chenUniversalSelfConsistencyLarge2023",
		"type": "article",
		"abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.",
		"DOI": "10.48550/arXiv.2311.17311",
		"note": "arXiv:2311.17311 [cs]",
		"number": "arXiv:2311.17311",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Universal Self-Consistency for Large Language Model Generation",
		"URL": "http://arxiv.org/abs/2311.17311",
		"author": [
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Aksitov",
				"given": "Renat"
			},
			{
				"family": "Alon",
				"given": "Uri"
			},
			{
				"family": "Ren",
				"given": "Jie"
			},
			{
				"family": "Xiao",
				"given": "Kefan"
			},
			{
				"family": "Yin",
				"given": "Pengcheng"
			},
			{
				"family": "Prakash",
				"given": "Sushant"
			},
			{
				"family": "Sutton",
				"given": "Charles"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					28
				]
			]
		}
	},
	{
		"id": "xuRLHFIIAPerverse2024",
		"type": "article",
		"abstract": "Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.",
		"language": "en",
		"note": "arXiv:2312.01057 [cs]",
		"number": "arXiv:2312.01057",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "RLHF and IIA: Perverse Incentives",
		"title-short": "RLHF and IIA",
		"URL": "http://arxiv.org/abs/2312.01057",
		"author": [
			{
				"family": "Xu",
				"given": "Wanqiao"
			},
			{
				"family": "Dong",
				"given": "Shi"
			},
			{
				"family": "Lu",
				"given": "Xiuyuan"
			},
			{
				"family": "Lam",
				"given": "Grace"
			},
			{
				"family": "Wen",
				"given": "Zheng"
			},
			{
				"family": "Van Roy",
				"given": "Benjamin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					1
				]
			]
		}
	},
	{
		"id": "bousmalisRoboCatSelfImprovingGeneralist2023",
		"type": "article-journal",
		"abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100–1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent’s capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
		"title-short": "RoboCat",
		"URL": "https://openreview.net/forum?id=vsCpILiWHu",
		"author": [
			{
				"family": "Bousmalis",
				"given": "Konstantinos"
			},
			{
				"family": "Vezzani",
				"given": "Giulia"
			},
			{
				"family": "Rao",
				"given": "Dushyant"
			},
			{
				"family": "Devin",
				"given": "Coline Manon"
			},
			{
				"family": "Lee",
				"given": "Alex X."
			},
			{
				"family": "Villalonga",
				"given": "Maria Bauza"
			},
			{
				"family": "Davchev",
				"given": "Todor"
			},
			{
				"family": "Zhou",
				"given": "Yuxiang"
			},
			{
				"family": "Gupta",
				"given": "Agrim"
			},
			{
				"family": "Raju",
				"given": "Akhil"
			},
			{
				"family": "Laurens",
				"given": "Antoine"
			},
			{
				"family": "Fantacci",
				"given": "Claudio"
			},
			{
				"family": "Dalibard",
				"given": "Valentin"
			},
			{
				"family": "Zambelli",
				"given": "Martina"
			},
			{
				"family": "Martins",
				"given": "Murilo Fernandes"
			},
			{
				"family": "Pevceviciute",
				"given": "Rugile"
			},
			{
				"family": "Blokzijl",
				"given": "Michiel"
			},
			{
				"family": "Denil",
				"given": "Misha"
			},
			{
				"family": "Batchelor",
				"given": "Nathan"
			},
			{
				"family": "Lampe",
				"given": "Thomas"
			},
			{
				"family": "Parisotto",
				"given": "Emilio"
			},
			{
				"family": "Zolna",
				"given": "Konrad"
			},
			{
				"family": "Reed",
				"given": "Scott"
			},
			{
				"family": "Colmenarejo",
				"given": "Sergio Gómez"
			},
			{
				"family": "Scholz",
				"given": "Jonathan"
			},
			{
				"family": "Abdolmaleki",
				"given": "Abbas"
			},
			{
				"family": "Groth",
				"given": "Oliver"
			},
			{
				"family": "Regli",
				"given": "Jean-Baptiste"
			},
			{
				"family": "Sushkov",
				"given": "Oleg"
			},
			{
				"family": "Rothörl",
				"given": "Thomas"
			},
			{
				"family": "Chen",
				"given": "Jose Enrique"
			},
			{
				"family": "Aytar",
				"given": "Yusuf"
			},
			{
				"family": "Barker",
				"given": "David"
			},
			{
				"family": "Ortiz",
				"given": "Joy"
			},
			{
				"family": "Riedmiller",
				"given": "Martin"
			},
			{
				"family": "Springenberg",
				"given": "Jost Tobias"
			},
			{
				"family": "Hadsell",
				"given": "Raia"
			},
			{
				"family": "Nori",
				"given": "Francesco"
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					6
				]
			]
		}
	},
	{
		"id": "wangGaussianProcessProbes2024",
		"type": "paper-conference",
		"abstract": "Understanding which concepts models can and cannot represent has been fundamental to many tasks: from effective and responsible use of models to detecting out of distribution data. We introduce Gaussian process probes (GPP), a unified and simple framework for probing and measuring uncertainty about concepts represented by models. As a Bayesian extension of linear probing methods, GPP asks what kind of distribution over classifiers (of concepts) is induced by the model. This distribution can be used to measure both what the model represents and how confident the probe is about what the model represents. GPP can be applied to any pre-trained model with vector representations of inputs (e.g., activations). It does not require access to training data, gradients, or the architecture. We validate GPP on datasets containing both synthetic and real images. Our experiments show it can (1) probe a model's representations of concepts even with a very small number of examples, (2) accurately measure both epistemic uncertainty (how confident the probe is) and aleatory uncertainty (how fuzzy the concepts are to the model), and (3) detect out of distribution data using those uncertainty measures as well as classic methods do. By using Gaussian processes to expand what probing can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool for understanding and evaluating the capabilities of machine learning models.",
		"collection-title": "NIPS '23",
		"container-title": "Proceedings of the 37th International Conference on Neural Information Processing Systems",
		"event-place": "Red Hook, NY, USA",
		"page": "63573–63594",
		"publisher": "Curran Associates Inc.",
		"publisher-place": "Red Hook, NY, USA",
		"source": "ACM Digital Library",
		"title": "Gaussian process probes (GPP) for uncertainty-aware probing",
		"author": [
			{
				"family": "Wang",
				"given": "Zi"
			},
			{
				"family": "Ku",
				"given": "Alexander"
			},
			{
				"family": "Baldridge",
				"given": "Jason"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					30
				]
			]
		}
	},
	{
		"id": "vezhnevetsGenerativeAgentbasedModeling2023",
		"type": "article",
		"abstract": "Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act \"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.",
		"DOI": "10.48550/arXiv.2312.03664",
		"note": "arXiv:2312.03664 [cs]",
		"number": "arXiv:2312.03664",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",
		"URL": "http://arxiv.org/abs/2312.03664",
		"author": [
			{
				"family": "Vezhnevets",
				"given": "Alexander Sasha"
			},
			{
				"family": "Agapiou",
				"given": "John P."
			},
			{
				"family": "Aharon",
				"given": "Avia"
			},
			{
				"family": "Ziv",
				"given": "Ron"
			},
			{
				"family": "Matyas",
				"given": "Jayd"
			},
			{
				"family": "Duéñez-Guzmán",
				"given": "Edgar A."
			},
			{
				"family": "Cunningham",
				"given": "William A."
			},
			{
				"family": "Osindero",
				"given": "Simon"
			},
			{
				"family": "Karmon",
				"given": "Danny"
			},
			{
				"family": "Leibo",
				"given": "Joel Z."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					13
				]
			]
		}
	},
	{
		"id": "comsaBenchmarkReasoningSpatial2023",
		"type": "paper-conference",
		"abstract": "Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.",
		"container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2023.emnlp-main.1015",
		"event-place": "Singapore",
		"event-title": "EMNLP 2023",
		"page": "16328–16335",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "A Benchmark for Reasoning with Spatial Prepositions",
		"URL": "https://aclanthology.org/2023.emnlp-main.1015",
		"author": [
			{
				"family": "Comsa",
				"given": "Iulia"
			},
			{
				"family": "Narayanan",
				"given": "Srini"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "rannen-trikiRevisitingDynamicEvaluation2023",
		"type": "paper-conference",
		"abstract": "We consider the problem of online finetuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online-adaptation turns parameters into temporally changing states and provides a form of context-length extension with _memory in weights_, more in line with the concept of _memory_ in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency), sensitivity to overall distributional drift, and computational overhead for performing gradient computation and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context learning and finetuning blurs: Both are methods to condition the model on previously observed tokens.",
		"event-title": "NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models",
		"language": "en",
		"source": "openreview.net",
		"title": "Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models",
		"title-short": "Revisiting Dynamic Evaluation",
		"URL": "https://openreview.net/forum?id=iRz8qi7QB8",
		"author": [
			{
				"family": "Rannen-Triki",
				"given": "Amal"
			},
			{
				"family": "Bornschein",
				"given": "Jorg"
			},
			{
				"family": "Pascanu",
				"given": "Razvan"
			},
			{
				"family": "Galashov",
				"given": "Alexandre"
			},
			{
				"family": "Titsias",
				"given": "Michalis"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "György",
				"given": "András"
			},
			{
				"family": "Teh",
				"given": "Yee Whye"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					7
				]
			]
		}
	},
	{
		"id": "khetarpalPOMRLNoRegretLearningtoPlan2023",
		"type": "article-journal",
		"abstract": "We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task and across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizons on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and we validate its significance empirically.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "POMRL: No-Regret Learning-to-Plan with Increasing Horizons",
		"title-short": "POMRL",
		"URL": "https://openreview.net/forum?id=brGgOAXYtr",
		"author": [
			{
				"family": "Khetarpal",
				"given": "Khimya"
			},
			{
				"family": "Vernade",
				"given": "Claire"
			},
			{
				"family": "O'Donoghue",
				"given": "Brendan"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			},
			{
				"family": "Zahavy",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					10
				]
			]
		}
	},
	{
		"id": "lampinenPassiveLearningActive2023",
		"type": "paper-conference",
		"abstract": "What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Passive learning of active causal strategies in agents and language models",
		"URL": "https://openreview.net/forum?id=BRpi8YAfac&referrer=%5Bthe%20profile%20of%20Andrew%20Kyle%20Lampinen%5D(%2Fprofile%3Fid%3D~Andrew_Kyle_Lampinen1)",
		"author": [
			{
				"family": "Lampinen",
				"given": "Andrew Kyle"
			},
			{
				"family": "Chan",
				"given": "Stephanie C. Y."
			},
			{
				"family": "Dasgupta",
				"given": "Ishita"
			},
			{
				"family": "Nam",
				"given": "Andrew Joo Hun"
			},
			{
				"family": "Wang",
				"given": "Jane X."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "muttenthalerImprovingNeuralNetwork2023",
		"type": "paper-conference",
		"abstract": "Deep neural networks have reached human-level performance on many computer vision tasks. However, the objectives used to train these networks enforce only that similar images are embedded at similar locations in the representation space, and do not directly constrain the global structure of the resulting space. Here, we explore the impact of supervising this global structure by linearly aligning it with human similarity judgments. We find that a naive approach leads to large changes in local representational structure that harm downstream performance. Thus, we propose a novel method that aligns the global structure of representations while preserving their local structure. This global-local transform considerably improves accuracy across a variety of few-shot learning and anomaly detection tasks. Our results indicate that human visual representations are globally organized in a way that facilitates learning from few examples, and incorporating this global structure into neural network representations improves performance on downstream tasks.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Improving neural network representations using human similarity judgments",
		"URL": "https://openreview.net/forum?id=Nh5dp6Uuvx",
		"author": [
			{
				"family": "Muttenthaler",
				"given": "Lukas"
			},
			{
				"family": "Linhardt",
				"given": "Lorenz"
			},
			{
				"family": "Dippel",
				"given": "Jonas"
			},
			{
				"family": "Vandermeulen",
				"given": "Robert A."
			},
			{
				"family": "Hermann",
				"given": "Katherine"
			},
			{
				"family": "Lampinen",
				"given": "Andrew Kyle"
			},
			{
				"family": "Kornblith",
				"given": "Simon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "stimbergBenchmarkingRobustnessAdversarial2023",
		"type": "paper-conference",
		"abstract": "Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g., overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on ImageNet, simulates the type of obfuscations created by malicious actors. It goes beyond Image-Net-C and ImageNet-C-bar by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by lp-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
		"language": "en",
		"source": "openreview.net",
		"title": "Benchmarking Robustness to Adversarial Image Obfuscations",
		"URL": "https://openreview.net/forum?id=CiRHWaRbp0",
		"author": [
			{
				"family": "Stimberg",
				"given": "Florian"
			},
			{
				"family": "Chakrabarti",
				"given": "Ayan"
			},
			{
				"family": "Lu",
				"given": "Chun-Ta"
			},
			{
				"family": "Hazimeh",
				"given": "Hussein"
			},
			{
				"family": "Stretcu",
				"given": "Otilia"
			},
			{
				"family": "Qiao",
				"given": "Wei"
			},
			{
				"family": "Liu",
				"given": "Yintao"
			},
			{
				"family": "Kaya",
				"given": "Merve"
			},
			{
				"family": "Rashtchian",
				"given": "Cyrus"
			},
			{
				"family": "Fuxman",
				"given": "Ariel"
			},
			{
				"family": "Tek",
				"given": "Mehmet Nejat"
			},
			{
				"family": "Gowal",
				"given": "Sven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "jiralerspongFeatureLikelihoodDivergence2023",
		"type": "paper-conference",
		"abstract": "The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Divergence (FLD), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLD to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples",
		"title-short": "Feature Likelihood Divergence",
		"URL": "https://openreview.net/forum?id=l2VKZkolT7",
		"author": [
			{
				"family": "Jiralerspong",
				"given": "Marco"
			},
			{
				"family": "Bose",
				"given": "Joey"
			},
			{
				"family": "Gemp",
				"given": "Ian"
			},
			{
				"family": "Qin",
				"given": "Chongli"
			},
			{
				"family": "Bachrach",
				"given": "Yoram"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "tarbouriechProbabilisticInferenceReinforcement2024",
		"type": "paper-conference",
		"abstract": "A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.",
		"collection-title": "NIPS '23",
		"container-title": "Proceedings of the 37th International Conference on Neural Information Processing Systems",
		"event-place": "Red Hook, NY, USA",
		"page": "33687–33725",
		"publisher": "Curran Associates Inc.",
		"publisher-place": "Red Hook, NY, USA",
		"source": "ACM Digital Library",
		"title": "Probabilistic inference in reinforcement learning done right",
		"author": [
			{
				"family": "Tarbouriech",
				"given": "Jean"
			},
			{
				"family": "Lattimore",
				"given": "Tor"
			},
			{
				"family": "O'Donoghue",
				"given": "Brendan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					30
				]
			]
		}
	},
	{
		"id": "balazevicIncontextSceneUnderstanding2023",
		"type": "paper-conference",
		"abstract": "In-context learning––the ability to configure a model's behavior with different prompts––has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol––leveraging attention within and across images––which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Towards In-context Scene Understanding",
		"URL": "https://openreview.net/forum?id=FasIQqsJhe",
		"author": [
			{
				"family": "Balazevic",
				"given": "Ivana"
			},
			{
				"family": "Steiner",
				"given": "David"
			},
			{
				"family": "Parthasarathy",
				"given": "Nikhil"
			},
			{
				"family": "Arandjelovic",
				"given": "Relja"
			},
			{
				"family": "Henaff",
				"given": "Olivier J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "abelDefinitionContinualReinforcement2023",
		"type": "paper-conference",
		"abstract": "In a standard view of the reinforcement learning problem, an agent’s goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that “never stop learning” through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "A Definition of Continual Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=ZZS9WEWYbD&referrer=%5Bthe%20profile%20of%20Satinder%20Singh%5D(%2Fprofile%3Fid%3D~Satinder_Singh2)",
		"author": [
			{
				"family": "Abel",
				"given": "David"
			},
			{
				"family": "Barreto",
				"given": "Andre"
			},
			{
				"family": "Roy",
				"given": "Benjamin Van"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Hasselt",
				"given": "Hado",
				"dropping-particle": "van"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "swaminathanSchemalearningRebindingMechanisms2023",
		"type": "paper-conference",
		"abstract": "In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence",
		"URL": "https://openreview.net/forum?id=3AreDQZ8eO",
		"author": [
			{
				"family": "Swaminathan",
				"given": "Sivaramakrishnan"
			},
			{
				"family": "Dedieu",
				"given": "Antoine"
			},
			{
				"family": "Raju",
				"given": "Rajkumar Vasudeva"
			},
			{
				"family": "Shanahan",
				"given": "Murray"
			},
			{
				"family": "Lazaro-Gredilla",
				"given": "Miguel"
			},
			{
				"family": "George",
				"given": "Dileep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "papalampidiSimpleRecipeContrastively2023",
		"type": "article",
		"abstract": "Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).",
		"DOI": "10.48550/arXiv.2312.07395",
		"note": "arXiv:2312.07395 [cs]",
		"number": "arXiv:2312.07395",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
		"URL": "http://arxiv.org/abs/2312.07395",
		"author": [
			{
				"family": "Papalampidi",
				"given": "Pinelopi"
			},
			{
				"family": "Koppula",
				"given": "Skanda"
			},
			{
				"family": "Pathak",
				"given": "Shreya"
			},
			{
				"family": "Chiu",
				"given": "Justin"
			},
			{
				"family": "Heyward",
				"given": "Joe"
			},
			{
				"family": "Patraucean",
				"given": "Viorica"
			},
			{
				"family": "Shen",
				"given": "Jiajun"
			},
			{
				"family": "Miech",
				"given": "Antoine"
			},
			{
				"family": "Zisserman",
				"given": "Andrew"
			},
			{
				"family": "Nematzdeh",
				"given": "Aida"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					12
				]
			]
		}
	},
	{
		"id": "coda-fornoMetaincontextLearningLarge2023",
		"type": "paper-conference",
		"abstract": "Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we broaden the scope of our investigation to encompass two diverse benchmarks: one focusing on real-world regression problems and the other encompassing multiple NLP tasks. In both cases, we observe competitive performance comparable to that of traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.",
		"event-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Meta-in-context learning in large language models",
		"URL": "https://openreview.net/forum?id=sx0xpaO0za",
		"author": [
			{
				"family": "Coda-Forno",
				"given": "Julian"
			},
			{
				"family": "Binz",
				"given": "Marcel"
			},
			{
				"family": "Akata",
				"given": "Zeynep"
			},
			{
				"family": "Botvinick",
				"given": "Matthew"
			},
			{
				"family": "Wang",
				"given": "Jane X."
			},
			{
				"family": "Schulz",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "farquharChallengesUnsupervisedLLM2023",
		"type": "article",
		"abstract": "We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.",
		"DOI": "10.48550/arXiv.2312.10029",
		"note": "arXiv:2312.10029 [cs]",
		"number": "arXiv:2312.10029",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Challenges with unsupervised LLM knowledge discovery",
		"URL": "http://arxiv.org/abs/2312.10029",
		"author": [
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Varma",
				"given": "Vikrant"
			},
			{
				"family": "Kenton",
				"given": "Zachary"
			},
			{
				"family": "Gasteiger",
				"given": "Johannes"
			},
			{
				"family": "Mikulik",
				"given": "Vladimir"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					18
				]
			]
		}
	},
	{
		"id": "ahnAutoRTEmbodiedFoundation2024",
		"type": "article",
		"abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such \"in-the-wild\" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.",
		"DOI": "10.48550/arXiv.2401.12963",
		"note": "arXiv:2401.12963 [cs]",
		"number": "arXiv:2401.12963",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
		"title-short": "AutoRT",
		"URL": "http://arxiv.org/abs/2401.12963",
		"author": [
			{
				"family": "Ahn",
				"given": "Michael"
			},
			{
				"family": "Dwibedi",
				"given": "Debidatta"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			},
			{
				"family": "Arenas",
				"given": "Montse Gonzalez"
			},
			{
				"family": "Gopalakrishnan",
				"given": "Keerthana"
			},
			{
				"family": "Hausman",
				"given": "Karol"
			},
			{
				"family": "Ichter",
				"given": "Brian"
			},
			{
				"family": "Irpan",
				"given": "Alex"
			},
			{
				"family": "Joshi",
				"given": "Nikhil"
			},
			{
				"family": "Julian",
				"given": "Ryan"
			},
			{
				"family": "Kirmani",
				"given": "Sean"
			},
			{
				"family": "Leal",
				"given": "Isabel"
			},
			{
				"family": "Lee",
				"given": "Edward"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Lu",
				"given": "Yao"
			},
			{
				"family": "Leal",
				"given": "Isabel"
			},
			{
				"family": "Maddineni",
				"given": "Sharath"
			},
			{
				"family": "Rao",
				"given": "Kanishka"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Sanketi",
				"given": "Pannag"
			},
			{
				"family": "Sermanet",
				"given": "Pierre"
			},
			{
				"family": "Vuong",
				"given": "Quan"
			},
			{
				"family": "Welker",
				"given": "Stefan"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Xiao",
				"given": "Ted"
			},
			{
				"family": "Xu",
				"given": "Peng"
			},
			{
				"family": "Xu",
				"given": "Steve"
			},
			{
				"family": "Xu",
				"given": "Zhuo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		}
	},
	{
		"id": "mullerDistributionalReinforcementLearning2024",
		"type": "article-journal",
		"abstract": "The prefrontal cortex is crucial for learning and decision-making. Classic reinforcement learning (RL) theories center on learning the expectation of potential rewarding outcomes and explain a wealth of neural data in the prefrontal cortex. Distributional RL, on the other hand, learns the full distribution of rewarding outcomes and better explains dopamine responses. In the present study, we show that distributional RL also better explains macaque anterior cingulate cortex neuronal responses, suggesting that it is a common mechanism for reward-guided learning.",
		"container-title": "Nature Neuroscience",
		"DOI": "10.1038/s41593-023-01535-w",
		"ISSN": "1546-1726",
		"issue": "3",
		"journalAbbreviation": "Nat Neurosci",
		"language": "en",
		"license": "2024 The Author(s)",
		"note": "publisher: Nature Publishing Group",
		"page": "403-408",
		"source": "www.nature.com",
		"title": "Distributional reinforcement learning in prefrontal cortex",
		"URL": "https://www.nature.com/articles/s41593-023-01535-w",
		"volume": "27",
		"author": [
			{
				"family": "Muller",
				"given": "Timothy H."
			},
			{
				"family": "Butler",
				"given": "James L."
			},
			{
				"family": "Veselic",
				"given": "Sebastijan"
			},
			{
				"family": "Miranda",
				"given": "Bruno"
			},
			{
				"family": "Wallis",
				"given": "Joni D."
			},
			{
				"family": "Dayan",
				"given": "Peter"
			},
			{
				"family": "Behrens",
				"given": "Timothy E. J."
			},
			{
				"family": "Kurth-Nelson",
				"given": "Zeb"
			},
			{
				"family": "Kennerley",
				"given": "Steven W."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3
				]
			]
		}
	},
	{
		"id": "dedieuLearningCognitiveMaps2024",
		"type": "article",
		"abstract": "Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.",
		"language": "en",
		"note": "arXiv:2401.05946 [cs]",
		"number": "arXiv:2401.05946",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments",
		"URL": "http://arxiv.org/abs/2401.05946",
		"author": [
			{
				"family": "Dedieu",
				"given": "Antoine"
			},
			{
				"family": "Lehrach",
				"given": "Wolfgang"
			},
			{
				"family": "Zhou",
				"given": "Guangyao"
			},
			{
				"family": "George",
				"given": "Dileep"
			},
			{
				"family": "Lázaro-Gredilla",
				"given": "Miguel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		}
	},
	{
		"id": "zolnaGATSGatherAttendScatter2024",
		"type": "article",
		"abstract": "As the AI community increasingly adopts large-scale models, it is crucial to develop general and flexible tools to integrate them. We introduce Gather-Attend-Scatter (GATS), a novel module that enables seamless combination of pretrained foundation models, both trainable and frozen, into larger multimodal networks. GATS empowers AI systems to process and generate information across multiple modalities at different rates. In contrast to traditional fine-tuning, GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase. We demonstrate the utility and versatility of GATS with a few experiments across games, robotics, and multimodal input-output systems.",
		"DOI": "10.48550/arXiv.2401.08525",
		"note": "arXiv:2401.08525 [cs]",
		"number": "arXiv:2401.08525",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "GATS: Gather-Attend-Scatter",
		"title-short": "GATS",
		"URL": "http://arxiv.org/abs/2401.08525",
		"author": [
			{
				"family": "Zolna",
				"given": "Konrad"
			},
			{
				"family": "Cabi",
				"given": "Serkan"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			},
			{
				"family": "Lau",
				"given": "Eric"
			},
			{
				"family": "Fantacci",
				"given": "Claudio"
			},
			{
				"family": "Pasukonis",
				"given": "Jurgis"
			},
			{
				"family": "Springenberg",
				"given": "Jost Tobias"
			},
			{
				"family": "Colmenarejo",
				"given": "Sergio Gomez"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					16
				]
			]
		}
	},
	{
		"id": "agarwalOnPolicyDistillationLanguage2024",
		"type": "article",
		"abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.",
		"DOI": "10.48550/arXiv.2306.13649",
		"note": "arXiv:2306.13649 [cs]",
		"number": "arXiv:2306.13649",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes",
		"title-short": "On-Policy Distillation of Language Models",
		"URL": "http://arxiv.org/abs/2306.13649",
		"author": [
			{
				"family": "Agarwal",
				"given": "Rishabh"
			},
			{
				"family": "Vieillard",
				"given": "Nino"
			},
			{
				"family": "Zhou",
				"given": "Yongchao"
			},
			{
				"family": "Stanczyk",
				"given": "Piotr"
			},
			{
				"family": "Ramos",
				"given": "Sabela"
			},
			{
				"family": "Geist",
				"given": "Matthieu"
			},
			{
				"family": "Bachem",
				"given": "Olivier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					16
				]
			]
		}
	},
	{
		"id": "liuNeuralPopulationLearning2024",
		"type": "article",
		"abstract": "We study computationally efficient methods for finding equilibria in n-player general-sum games, specifically ones that afford complex visuomotor skills. We show how existing methods would struggle in this setting, either computationally or in theory. We then introduce NeuPL-JPSRO, a neural population learning algorithm that benefits from transfer learning of skills and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show empirical convergence in a suite of OpenSpiel games, validated rigorously by exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our approach enables adaptive coordination in a MuJoCo control domain and skill transfer in capture-the-flag. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives.",
		"DOI": "10.48550/arXiv.2401.05133",
		"note": "arXiv:2401.05133 [cs]",
		"number": "arXiv:2401.05133",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Neural Population Learning beyond Symmetric Zero-sum Games",
		"URL": "http://arxiv.org/abs/2401.05133",
		"author": [
			{
				"family": "Liu",
				"given": "Siqi"
			},
			{
				"family": "Marris",
				"given": "Luke"
			},
			{
				"family": "Lanctot",
				"given": "Marc"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			},
			{
				"family": "Leibo",
				"given": "Joel Z."
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					10
				]
			]
		}
	},
	{
		"id": "grau-moyaLearningUniversalPredictors2024",
		"type": "article",
		"abstract": "Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies.",
		"DOI": "10.48550/arXiv.2401.14953",
		"note": "arXiv:2401.14953 [cs]",
		"number": "arXiv:2401.14953",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Universal Predictors",
		"URL": "http://arxiv.org/abs/2401.14953",
		"author": [
			{
				"family": "Grau-Moya",
				"given": "Jordi"
			},
			{
				"family": "Genewein",
				"given": "Tim"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Delétang",
				"given": "Grégoire"
			},
			{
				"family": "Catt",
				"given": "Elliot"
			},
			{
				"family": "Ruoss",
				"given": "Anian"
			},
			{
				"family": "Wenliang",
				"given": "Li Kevin"
			},
			{
				"family": "Mattern",
				"given": "Christopher"
			},
			{
				"family": "Aitchison",
				"given": "Matthew"
			},
			{
				"family": "Veness",
				"given": "Joel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					26
				]
			]
		}
	},
	{
		"id": "dwaracherlaEfficientExplorationLLMs2024",
		"type": "article",
		"abstract": "We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.",
		"language": "en",
		"note": "arXiv:2402.00396 [cs, stat]",
		"number": "arXiv:2402.00396",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Efficient Exploration for LLMs",
		"URL": "http://arxiv.org/abs/2402.00396",
		"author": [
			{
				"family": "Dwaracherla",
				"given": "Vikranth"
			},
			{
				"family": "Asghari",
				"given": "Seyed Mohammad"
			},
			{
				"family": "Hao",
				"given": "Botao"
			},
			{
				"family": "Van Roy",
				"given": "Benjamin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					4
				]
			]
		}
	},
	{
		"id": "richensRobustAgentsLearn2023",
		"type": "paper-conference",
		"abstract": "It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Robust agents learn causal world models",
		"URL": "https://openreview.net/forum?id=pOoKI3ouv1",
		"author": [
			{
				"family": "Richens",
				"given": "Jonathan"
			},
			{
				"family": "Everitt",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "alabdulmohsinFractalPatternsMay2024",
		"type": "article",
		"abstract": "We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.7. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.",
		"DOI": "10.48550/arXiv.2402.01825",
		"note": "arXiv:2402.01825 [cs]",
		"number": "arXiv:2402.01825",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fractal Patterns May Illuminate the Success of Next-Token Prediction",
		"URL": "http://arxiv.org/abs/2402.01825",
		"author": [
			{
				"family": "Alabdulmohsin",
				"given": "Ibrahim"
			},
			{
				"family": "Tran",
				"given": "Vinh Q."
			},
			{
				"family": "Dehghani",
				"given": "Mostafa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					22
				]
			]
		}
	},
	{
		"id": "zhouSelfDiscoverLargeLanguage2024",
		"type": "article",
		"abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.",
		"DOI": "10.48550/arXiv.2402.03620",
		"note": "arXiv:2402.03620 [cs]",
		"number": "arXiv:2402.03620",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
		"title-short": "Self-Discover",
		"URL": "http://arxiv.org/abs/2402.03620",
		"author": [
			{
				"family": "Zhou",
				"given": "Pei"
			},
			{
				"family": "Pujara",
				"given": "Jay"
			},
			{
				"family": "Ren",
				"given": "Xiang"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Cheng",
				"given": "Heng-Tze"
			},
			{
				"family": "Le",
				"given": "Quoc V."
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Zhou",
				"given": "Denny"
			},
			{
				"family": "Mishra",
				"given": "Swaroop"
			},
			{
				"family": "Zheng",
				"given": "Huaixiu Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					5
				]
			]
		}
	},
	{
		"id": "gempStatesStringsStrategies2024",
		"type": "article",
		"abstract": "Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.",
		"DOI": "10.48550/arXiv.2402.01704",
		"note": "arXiv:2402.01704 [cs]",
		"number": "arXiv:2402.01704",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers",
		"title-short": "States as Strings as Strategies",
		"URL": "http://arxiv.org/abs/2402.01704",
		"author": [
			{
				"family": "Gemp",
				"given": "Ian"
			},
			{
				"family": "Bachrach",
				"given": "Yoram"
			},
			{
				"family": "Lanctot",
				"given": "Marc"
			},
			{
				"family": "Patel",
				"given": "Roma"
			},
			{
				"family": "Dasagi",
				"given": "Vibhavari"
			},
			{
				"family": "Marris",
				"given": "Luke"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			},
			{
				"family": "Liu",
				"given": "Siqi"
			},
			{
				"family": "Tuyls",
				"given": "Karl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					6
				]
			]
		}
	},
	{
		"id": "balazevicMemoryConsolidationEnables2024",
		"type": "article",
		"abstract": "Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. Instead, we propose to re-purpose existing pretrained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memoryconsolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.",
		"language": "en",
		"note": "arXiv:2402.05861 [cs]",
		"number": "arXiv:2402.05861",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Memory Consolidation Enables Long-Context Video Understanding",
		"URL": "http://arxiv.org/abs/2402.05861",
		"author": [
			{
				"family": "Balažević",
				"given": "Ivana"
			},
			{
				"family": "Shi",
				"given": "Yuge"
			},
			{
				"family": "Papalampidi",
				"given": "Pinelopi"
			},
			{
				"family": "Chaabouni",
				"given": "Rahma"
			},
			{
				"family": "Koppula",
				"given": "Skanda"
			},
			{
				"family": "Hénaff",
				"given": "Olivier J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					31
				]
			]
		}
	},
	{
		"id": "wangChainofTableEvolvingTables2023",
		"type": "paper-conference",
		"abstract": "Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
		"title-short": "Chain-of-Table",
		"URL": "https://openreview.net/forum?id=4L0xnS4GQM",
		"author": [
			{
				"family": "Wang",
				"given": "Zilong"
			},
			{
				"family": "Zhang",
				"given": "Hao"
			},
			{
				"family": "Li",
				"given": "Chun-Liang"
			},
			{
				"family": "Eisenschlos",
				"given": "Julian Martin"
			},
			{
				"family": "Perot",
				"given": "Vincent"
			},
			{
				"family": "Wang",
				"given": "Zifeng"
			},
			{
				"family": "Miculicich",
				"given": "Lesly"
			},
			{
				"family": "Fujii",
				"given": "Yasuhisa"
			},
			{
				"family": "Shang",
				"given": "Jingbo"
			},
			{
				"family": "Lee",
				"given": "Chen-Yu"
			},
			{
				"family": "Pfister",
				"given": "Tomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "nasirianyPIVOTIterativeVisual2024",
		"type": "article",
		"abstract": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
		"DOI": "10.48550/arXiv.2402.07872",
		"note": "arXiv:2402.07872 [cs]",
		"number": "arXiv:2402.07872",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
		"title-short": "PIVOT",
		"URL": "http://arxiv.org/abs/2402.07872",
		"author": [
			{
				"family": "Nasiriany",
				"given": "Soroush"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Yu",
				"given": "Wenhao"
			},
			{
				"family": "Xiao",
				"given": "Ted"
			},
			{
				"family": "Liang",
				"given": "Jacky"
			},
			{
				"family": "Dasgupta",
				"given": "Ishita"
			},
			{
				"family": "Xie",
				"given": "Annie"
			},
			{
				"family": "Driess",
				"given": "Danny"
			},
			{
				"family": "Wahid",
				"given": "Ayzaan"
			},
			{
				"family": "Xu",
				"given": "Zhuo"
			},
			{
				"family": "Vuong",
				"given": "Quan"
			},
			{
				"family": "Zhang",
				"given": "Tingnan"
			},
			{
				"family": "Lee",
				"given": "Tsang-Wei Edward"
			},
			{
				"family": "Lee",
				"given": "Kuang-Huei"
			},
			{
				"family": "Xu",
				"given": "Peng"
			},
			{
				"family": "Kirmani",
				"given": "Sean"
			},
			{
				"family": "Zhu",
				"given": "Yuke"
			},
			{
				"family": "Zeng",
				"given": "Andy"
			},
			{
				"family": "Hausman",
				"given": "Karol"
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Ichter",
				"given": "Brian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					12
				]
			]
		}
	},
	{
		"id": "chenPremiseOrderMatters2024",
		"type": "article",
		"abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",
		"DOI": "10.48550/arXiv.2402.08939",
		"note": "arXiv:2402.08939 [cs]",
		"number": "arXiv:2402.08939",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Premise Order Matters in Reasoning with Large Language Models",
		"URL": "http://arxiv.org/abs/2402.08939",
		"author": [
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Chi",
				"given": "Ryan A."
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Zhou",
				"given": "Denny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					28
				]
			]
		}
	},
	{
		"id": "leeHumanInspiredReadingAgent2024",
		"type": "article",
		"abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.",
		"DOI": "10.48550/arXiv.2402.09727",
		"note": "arXiv:2402.09727 [cs]",
		"number": "arXiv:2402.09727",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
		"URL": "http://arxiv.org/abs/2402.09727",
		"author": [
			{
				"family": "Lee",
				"given": "Kuang-Huei"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Furuta",
				"given": "Hiroki"
			},
			{
				"family": "Canny",
				"given": "John"
			},
			{
				"family": "Fischer",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					23
				]
			]
		}
	},
	{
		"id": "liangLearningLearnFaster2024",
		"type": "article",
		"abstract": "Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are viewed as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions is training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: https://robot-teaching.github.io/.",
		"DOI": "10.48550/arXiv.2402.11450",
		"note": "arXiv:2402.11450 [cs]",
		"number": "arXiv:2402.11450",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Learn Faster from Human Feedback with Language Model Predictive Control",
		"URL": "http://arxiv.org/abs/2402.11450",
		"author": [
			{
				"family": "Liang",
				"given": "Jacky"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Yu",
				"given": "Wenhao"
			},
			{
				"family": "Zeng",
				"given": "Andy"
			},
			{
				"family": "Arenas",
				"given": "Montserrat Gonzalez"
			},
			{
				"family": "Attarian",
				"given": "Maria"
			},
			{
				"family": "Bauza",
				"given": "Maria"
			},
			{
				"family": "Bennice",
				"given": "Matthew"
			},
			{
				"family": "Bewley",
				"given": "Alex"
			},
			{
				"family": "Dostmohamed",
				"given": "Adil"
			},
			{
				"family": "Fu",
				"given": "Chuyuan Kelly"
			},
			{
				"family": "Gileadi",
				"given": "Nimrod"
			},
			{
				"family": "Giustina",
				"given": "Marissa"
			},
			{
				"family": "Gopalakrishnan",
				"given": "Keerthana"
			},
			{
				"family": "Hasenclever",
				"given": "Leonard"
			},
			{
				"family": "Humplik",
				"given": "Jan"
			},
			{
				"family": "Hsu",
				"given": "Jasmine"
			},
			{
				"family": "Joshi",
				"given": "Nikhil"
			},
			{
				"family": "Jyenis",
				"given": "Ben"
			},
			{
				"family": "Kew",
				"given": "Chase"
			},
			{
				"family": "Kirmani",
				"given": "Sean"
			},
			{
				"family": "Lee",
				"given": "Tsang-Wei Edward"
			},
			{
				"family": "Lee",
				"given": "Kuang-Huei"
			},
			{
				"family": "Michaely",
				"given": "Assaf Hurwitz"
			},
			{
				"family": "Moore",
				"given": "Joss"
			},
			{
				"family": "Oslund",
				"given": "Ken"
			},
			{
				"family": "Rao",
				"given": "Dushyant"
			},
			{
				"family": "Ren",
				"given": "Allen"
			},
			{
				"family": "Tabanpour",
				"given": "Baruch"
			},
			{
				"family": "Vuong",
				"given": "Quan"
			},
			{
				"family": "Wahid",
				"given": "Ayzaan"
			},
			{
				"family": "Xiao",
				"given": "Ted"
			},
			{
				"family": "Xu",
				"given": "Ying"
			},
			{
				"family": "Zhuang",
				"given": "Vincent"
			},
			{
				"family": "Xu",
				"given": "Peng"
			},
			{
				"family": "Frey",
				"given": "Erik"
			},
			{
				"family": "Caluwaerts",
				"given": "Ken"
			},
			{
				"family": "Zhang",
				"given": "Tingnan"
			},
			{
				"family": "Ichter",
				"given": "Brian"
			},
			{
				"family": "Tompson",
				"given": "Jonathan"
			},
			{
				"family": "Takayama",
				"given": "Leila"
			},
			{
				"family": "Vanhoucke",
				"given": "Vincent"
			},
			{
				"family": "Shafran",
				"given": "Izhak"
			},
			{
				"family": "Mataric",
				"given": "Maja"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			},
			{
				"family": "Rao",
				"given": "Kanishka"
			},
			{
				"family": "Stewart",
				"given": "Nik"
			},
			{
				"family": "Tan",
				"given": "Jie"
			},
			{
				"family": "Parada",
				"given": "Carolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					31
				]
			]
		}
	},
	{
		"id": "shanahanSimulacraConsciousExotica2024",
		"type": "article",
		"abstract": "The advent of conversational agents with increasingly human-like behaviour throws old philosophical questions into new light. Does it, or could it, ever make sense to speak of AI agents built out of generative language models in terms of consciousness, given that they are \"mere\" simulacra of human behaviour, and that what they do can be seen as \"merely\" role play? Drawing on the later writings of Wittgenstein, this paper attempts to tackle this question while avoiding the pitfalls of dualistic thinking.",
		"DOI": "10.48550/arXiv.2402.12422",
		"note": "arXiv:2402.12422 [cs]",
		"number": "arXiv:2402.12422",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simulacra as Conscious Exotica",
		"URL": "http://arxiv.org/abs/2402.12422",
		"author": [
			{
				"family": "Shanahan",
				"given": "Murray"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					19
				]
			]
		}
	},
	{
		"id": "zhangWhenScalingMeets2023",
		"type": "paper-conference",
		"abstract": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
		"title-short": "When Scaling Meets LLM Finetuning",
		"URL": "https://openreview.net/forum?id=5HCnKDeTws",
		"author": [
			{
				"family": "Zhang",
				"given": "Biao"
			},
			{
				"family": "Liu",
				"given": "Zhongtao"
			},
			{
				"family": "Cherry",
				"given": "Colin"
			},
			{
				"family": "Firat",
				"given": "Orhan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "songOmniPredLanguageModels2024",
		"type": "article",
		"abstract": "Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.",
		"DOI": "10.48550/arXiv.2402.14547",
		"note": "arXiv:2402.14547 [cs]",
		"number": "arXiv:2402.14547",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "OmniPred: Language Models as Universal Regressors",
		"title-short": "OmniPred",
		"URL": "http://arxiv.org/abs/2402.14547",
		"author": [
			{
				"family": "Song",
				"given": "Xingyou"
			},
			{
				"family": "Li",
				"given": "Oscar"
			},
			{
				"family": "Lee",
				"given": "Chansoo"
			},
			{
				"family": "Yang",
				"given": "Bangding"
			},
			{
				"family": "Peng",
				"given": "Daiyi"
			},
			{
				"family": "Perel",
				"given": "Sagi"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					4
				]
			]
		}
	},
	{
		"id": "pengLimitationsTransformerArchitecture2024",
		"type": "article",
		"abstract": "What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.",
		"DOI": "10.48550/arXiv.2402.08164",
		"note": "arXiv:2402.08164 [cs, stat]",
		"number": "arXiv:2402.08164",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On Limitations of the Transformer Architecture",
		"URL": "http://arxiv.org/abs/2402.08164",
		"author": [
			{
				"family": "Peng",
				"given": "Binghui"
			},
			{
				"family": "Narayanan",
				"given": "Srini"
			},
			{
				"family": "Papadimitriou",
				"given": "Christos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					26
				]
			]
		}
	},
	{
		"id": "dumoulinDensityEstimationPerspective2023",
		"type": "article-journal",
		"abstract": "Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\"—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "A density estimation perspective on learning from pairwise human preferences",
		"URL": "https://openreview.net/forum?id=YH3oERVYjF",
		"author": [
			{
				"family": "Dumoulin",
				"given": "Vincent"
			},
			{
				"family": "Johnson",
				"given": "Daniel D."
			},
			{
				"family": "Castro",
				"given": "Pablo Samuel"
			},
			{
				"family": "Larochelle",
				"given": "Hugo"
			},
			{
				"family": "Dauphin",
				"given": "Yann"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					25
				]
			]
		}
	},
	{
		"id": "barFrozenFeatureAugmentation2024",
		"type": "article",
		"abstract": "Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called 'frozen features', leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed 'frozen feature augmentation (FroFA)', covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.",
		"DOI": "10.48550/arXiv.2403.10519",
		"note": "arXiv:2403.10519 [cs]",
		"number": "arXiv:2403.10519",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Frozen Feature Augmentation for Few-Shot Image Classification",
		"URL": "http://arxiv.org/abs/2403.10519",
		"author": [
			{
				"family": "Bär",
				"given": "Andreas"
			},
			{
				"family": "Houlsby",
				"given": "Neil"
			},
			{
				"family": "Dehghani",
				"given": "Mostafa"
			},
			{
				"family": "Kumar",
				"given": "Manoj"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					15
				]
			]
		}
	},
	{
		"id": "jainiIntriguingPropertiesGenerative2023",
		"type": "paper-conference",
		"abstract": "What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Intriguing Properties of Generative Classifiers",
		"URL": "https://openreview.net/forum?id=rmg0qMKYRQ",
		"author": [
			{
				"family": "Jaini",
				"given": "Priyank"
			},
			{
				"family": "Clark",
				"given": "Kevin"
			},
			{
				"family": "Geirhos",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "muttenthalerSetLearningAccurate2024",
		"type": "article",
		"abstract": "Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We demonstrate this in extensive experimental analyses and provide a mathematical theory to interpret our findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and a trained model can be applied to single examples at inference time, without significant run-time overhead or architecture changes.",
		"DOI": "10.48550/arXiv.2307.02245",
		"note": "arXiv:2307.02245 [cs, math]",
		"number": "arXiv:2307.02245",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Set Learning for Accurate and Calibrated Models",
		"URL": "http://arxiv.org/abs/2307.02245",
		"author": [
			{
				"family": "Muttenthaler",
				"given": "Lukas"
			},
			{
				"family": "Vandermeulen",
				"given": "Robert A."
			},
			{
				"family": "Zhang",
				"given": "Qiuyi"
			},
			{
				"family": "Unterthiner",
				"given": "Thomas"
			},
			{
				"family": "Müller",
				"given": "Klaus-Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					12
				]
			]
		}
	},
	{
		"id": "parthasarathySelfsupervisedVideoPretraining2023",
		"type": "article",
		"abstract": "Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformations than image-, video-, and adversarially-trained ones. Finally, VITO’s predictions are strongly aligned with human judgements, surpassing models that were specifically trained for that purpose. Together, these results suggest that video pretraining could be a simple way of learning unified, robust, and human-aligned representations of the visual world.",
		"language": "en",
		"note": "arXiv:2210.06433 [cs]",
		"number": "arXiv:2210.06433",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-supervised video pretraining yields human-aligned visual representations",
		"URL": "http://arxiv.org/abs/2210.06433",
		"author": [
			{
				"family": "Parthasarathy",
				"given": "Nikhil"
			},
			{
				"family": "Eslami",
				"given": "S. M. Ali"
			},
			{
				"family": "Carreira",
				"given": "João"
			},
			{
				"family": "Hénaff",
				"given": "Olivier J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					25
				]
			]
		}
	},
	{
		"id": "evansBadStudentsMake2024",
		"type": "article",
		"abstract": "Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate “learnability” scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly-trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-prioritization scheme to be complementary with recent data-curation and learning objectives, yielding a new state-of-the-art in several multimodal transfer tasks.",
		"language": "en",
		"note": "arXiv:2312.05328 [cs]",
		"number": "arXiv:2312.05328",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding",
		"title-short": "Bad Students Make Great Teachers",
		"URL": "http://arxiv.org/abs/2312.05328",
		"author": [
			{
				"family": "Evans",
				"given": "Talfan"
			},
			{
				"family": "Pathak",
				"given": "Shreya"
			},
			{
				"family": "Merzic",
				"given": "Hamza"
			},
			{
				"family": "Schwarz",
				"given": "Jonathan"
			},
			{
				"family": "Tanno",
				"given": "Ryutaro"
			},
			{
				"family": "Henaff",
				"given": "Olivier J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					14
				]
			]
		}
	},
	{
		"id": "gempApproximatingCoreIterative2024",
		"type": "article",
		"abstract": "The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.",
		"DOI": "10.48550/arXiv.2402.03928",
		"note": "arXiv:2402.03928 [cs]",
		"number": "arXiv:2402.03928",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Approximating the Core via Iterative Coalition Sampling",
		"URL": "http://arxiv.org/abs/2402.03928",
		"author": [
			{
				"family": "Gemp",
				"given": "Ian"
			},
			{
				"family": "Lanctot",
				"given": "Marc"
			},
			{
				"family": "Marris",
				"given": "Luke"
			},
			{
				"family": "Mao",
				"given": "Yiran"
			},
			{
				"family": "Duéñez-Guzmán",
				"given": "Edgar"
			},
			{
				"family": "Perrin",
				"given": "Sarah"
			},
			{
				"family": "Gyorgy",
				"given": "Andras"
			},
			{
				"family": "Elie",
				"given": "Romuald"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			},
			{
				"family": "Kaisers",
				"given": "Michael"
			},
			{
				"family": "Hennes",
				"given": "Daniel"
			},
			{
				"family": "Bullard",
				"given": "Kalesha"
			},
			{
				"family": "Larson",
				"given": "Kate"
			},
			{
				"family": "Bachrach",
				"given": "Yoram"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					6
				]
			]
		}
	},
	{
		"id": "ahlertHowAlignedAre2024",
		"type": "paper-conference",
		"abstract": "In recent years, various methods and benchmarks have been proposed to empirically evaluate the alignment of artificial neural networks to human neural and behavioral data. But how aligned are different alignment metrics? To answer this question, we here analyze visual data from Brain-Score (Schrimpf et al., 2018), including metrics from the model-vs-human toolbox (Geirhos et al., 2021), together with human feature alignment (Linsley et al., 2018; Fel et al., 2022) and human similarity judgements (Muttenthaler et al., 2022). We find that pairwise correlations between neural scores and behavioral scores are quite low and sometimes even negative. For instance, the average correlation between those 80 models on Brain-Score that were fully evaluated on all 69 alignment metrics we considered is only 0.198. Assuming that all of the employed metrics are sound, this implies that alignment with human perception may best be thought of as a multidimensional concept, with different methods measuring fundamentally different aspects. Our results underline the importance of integrative benchmarking, but also raise questions about how to correctly combine and aggregate individual metrics. Aggregating by taking the arithmetic average, as done in Brain-Score, leads to the overall performance currently being dominated by behavior (95.25% explained variance) while the neural predictivity plays a less important role (only 33.33% explained variance). As a first step towards making sure that different alignment metrics all contribute fairly towards an integrative benchmark score, we therefore conclude by comparing three different aggregation options.",
		"event-title": "ICLR 2024 Workshop on Representational Alignment",
		"language": "en",
		"source": "openreview.net",
		"title": "How aligned are different alignment metrics?",
		"URL": "https://openreview.net/forum?id=cHlKB28bjV",
		"author": [
			{
				"family": "Ahlert",
				"given": "Jannis"
			},
			{
				"family": "Klein",
				"given": "Thomas"
			},
			{
				"family": "Wichmann",
				"given": "Felix A."
			},
			{
				"family": "Geirhos",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					2
				]
			]
		}
	},
	{
		"id": "kramarAtPEfficientScalable2024",
		"type": "article",
		"abstract": "Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.",
		"DOI": "10.48550/arXiv.2403.00745",
		"note": "arXiv:2403.00745 [cs]",
		"number": "arXiv:2403.00745",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
		"title-short": "AtP*",
		"URL": "http://arxiv.org/abs/2403.00745",
		"author": [
			{
				"family": "Kramár",
				"given": "János"
			},
			{
				"family": "Lieberum",
				"given": "Tom"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					1
				]
			]
		}
	},
	{
		"id": "tiapkinDemonstrationRegularizedRL2024",
		"type": "article",
		"abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{O}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{O}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works.",
		"DOI": "10.48550/arXiv.2310.17303",
		"note": "arXiv:2310.17303 [cs, stat]",
		"number": "arXiv:2310.17303",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Demonstration-Regularized RL",
		"URL": "http://arxiv.org/abs/2310.17303",
		"author": [
			{
				"family": "Tiapkin",
				"given": "Daniil"
			},
			{
				"family": "Belomestny",
				"given": "Denis"
			},
			{
				"family": "Calandriello",
				"given": "Daniele"
			},
			{
				"family": "Moulines",
				"given": "Eric"
			},
			{
				"family": "Naumov",
				"given": "Alexey"
			},
			{
				"family": "Perrault",
				"given": "Pierre"
			},
			{
				"family": "Valko",
				"given": "Michal"
			},
			{
				"family": "Menard",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					10
				]
			]
		}
	},
	{
		"id": "azarGeneralTheoreticalParadigm2023",
		"type": "article",
		"abstract": "The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\\Psi$PO by setting $\\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.",
		"DOI": "10.48550/arXiv.2310.12036",
		"note": "arXiv:2310.12036 [cs, stat]",
		"number": "arXiv:2310.12036",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
		"URL": "http://arxiv.org/abs/2310.12036",
		"author": [
			{
				"family": "Azar",
				"given": "Mohammad Gheshlaghi"
			},
			{
				"family": "Rowland",
				"given": "Mark"
			},
			{
				"family": "Piot",
				"given": "Bilal"
			},
			{
				"family": "Guo",
				"given": "Daniel"
			},
			{
				"family": "Calandriello",
				"given": "Daniele"
			},
			{
				"family": "Valko",
				"given": "Michal"
			},
			{
				"family": "Munos",
				"given": "Rémi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					21
				]
			]
		}
	},
	{
		"id": "phuongEvaluatingFrontierModels2024",
		"type": "article",
		"abstract": "To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new \"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.",
		"DOI": "10.48550/arXiv.2403.13793",
		"note": "arXiv:2403.13793 [cs]",
		"number": "arXiv:2403.13793",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating Frontier Models for Dangerous Capabilities",
		"URL": "http://arxiv.org/abs/2403.13793",
		"author": [
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Aitchison",
				"given": "Matthew"
			},
			{
				"family": "Catt",
				"given": "Elliot"
			},
			{
				"family": "Cogan",
				"given": "Sarah"
			},
			{
				"family": "Kaskasoli",
				"given": "Alexandre"
			},
			{
				"family": "Krakovna",
				"given": "Victoria"
			},
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Rahtz",
				"given": "Matthew"
			},
			{
				"family": "Assael",
				"given": "Yannis"
			},
			{
				"family": "Hodkinson",
				"given": "Sarah"
			},
			{
				"family": "Howard",
				"given": "Heidi"
			},
			{
				"family": "Lieberum",
				"given": "Tom"
			},
			{
				"family": "Kumar",
				"given": "Ramana"
			},
			{
				"family": "Raad",
				"given": "Maria Abi"
			},
			{
				"family": "Webson",
				"given": "Albert"
			},
			{
				"family": "Ho",
				"given": "Lewis"
			},
			{
				"family": "Lin",
				"given": "Sharon"
			},
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "Deletang",
				"given": "Gregoire"
			},
			{
				"family": "Ruoss",
				"given": "Anian"
			},
			{
				"family": "El-Sayed",
				"given": "Seliem"
			},
			{
				"family": "Brown",
				"given": "Sasha"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Shevlane",
				"given": "Toby"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					5
				]
			]
		}
	},
	{
		"id": "weiLongformFactualityLarge2024",
		"type": "article",
		"abstract": "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.",
		"DOI": "10.48550/arXiv.2403.18802",
		"note": "arXiv:2403.18802 [cs]",
		"number": "arXiv:2403.18802",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Long-form factuality in large language models",
		"URL": "http://arxiv.org/abs/2403.18802",
		"author": [
			{
				"family": "Wei",
				"given": "Jerry"
			},
			{
				"family": "Yang",
				"given": "Chengrun"
			},
			{
				"family": "Song",
				"given": "Xinying"
			},
			{
				"family": "Lu",
				"given": "Yifeng"
			},
			{
				"family": "Hu",
				"given": "Nathan"
			},
			{
				"family": "Huang",
				"given": "Jie"
			},
			{
				"family": "Tran",
				"given": "Dustin"
			},
			{
				"family": "Peng",
				"given": "Daiyi"
			},
			{
				"family": "Liu",
				"given": "Ruibo"
			},
			{
				"family": "Huang",
				"given": "Da"
			},
			{
				"family": "Du",
				"given": "Cosmo"
			},
			{
				"family": "Le",
				"given": "Quoc V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					3
				]
			]
		}
	},
	{
		"id": "liFewShotRecalibrationLanguage2024",
		"type": "article",
		"abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
		"DOI": "10.48550/arXiv.2403.18286",
		"note": "arXiv:2403.18286 [cs]",
		"number": "arXiv:2403.18286",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Few-Shot Recalibration of Language Models",
		"URL": "http://arxiv.org/abs/2403.18286",
		"author": [
			{
				"family": "Li",
				"given": "Xiang Lisa"
			},
			{
				"family": "Khandelwal",
				"given": "Urvashi"
			},
			{
				"family": "Guu",
				"given": "Kelvin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					27
				]
			]
		}
	},
	{
		"id": "leeGeckoVersatileText2024",
		"type": "article",
		"abstract": "We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.",
		"DOI": "10.48550/arXiv.2403.20327",
		"note": "arXiv:2403.20327 [cs]",
		"number": "arXiv:2403.20327",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
		"title-short": "Gecko",
		"URL": "http://arxiv.org/abs/2403.20327",
		"author": [
			{
				"family": "Lee",
				"given": "Jinhyuk"
			},
			{
				"family": "Dai",
				"given": "Zhuyun"
			},
			{
				"family": "Ren",
				"given": "Xiaoqi"
			},
			{
				"family": "Chen",
				"given": "Blair"
			},
			{
				"family": "Cer",
				"given": "Daniel"
			},
			{
				"family": "Cole",
				"given": "Jeremy R."
			},
			{
				"family": "Hui",
				"given": "Kai"
			},
			{
				"family": "Boratko",
				"given": "Michael"
			},
			{
				"family": "Kapadia",
				"given": "Rajvi"
			},
			{
				"family": "Ding",
				"given": "Wen"
			},
			{
				"family": "Luan",
				"given": "Yi"
			},
			{
				"family": "Duddu",
				"given": "Sai Meher Karthik"
			},
			{
				"family": "Abrego",
				"given": "Gustavo Hernandez"
			},
			{
				"family": "Shi",
				"given": "Weiqiang"
			},
			{
				"family": "Gupta",
				"given": "Nithi"
			},
			{
				"family": "Kusupati",
				"given": "Aditya"
			},
			{
				"family": "Jain",
				"given": "Prateek"
			},
			{
				"family": "Jonnalagadda",
				"given": "Siddhartha Reddy"
			},
			{
				"family": "Chang",
				"given": "Ming-Wei"
			},
			{
				"family": "Naim",
				"given": "Iftekhar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					29
				]
			]
		}
	},
	{
		"id": "weidingerHolisticSafetyResponsibility2024",
		"type": "article",
		"abstract": "Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.",
		"language": "en",
		"note": "arXiv:2404.14068 [cs]",
		"number": "arXiv:2404.14068",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Holistic Safety and Responsibility Evaluations of Advanced AI Models",
		"URL": "http://arxiv.org/abs/2404.14068",
		"author": [
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Barnhart",
				"given": "Joslyn"
			},
			{
				"family": "Brennan",
				"given": "Jenny"
			},
			{
				"family": "Butterfield",
				"given": "Christina"
			},
			{
				"family": "Young",
				"given": "Susie"
			},
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Comanescu",
				"given": "Ramona"
			},
			{
				"family": "Chang",
				"given": "Oscar"
			},
			{
				"family": "Rodriguez",
				"given": "Mikel"
			},
			{
				"family": "Beroshi",
				"given": "Jennifer"
			},
			{
				"family": "Bloxwich",
				"given": "Dawn"
			},
			{
				"family": "Proleev",
				"given": "Lev"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			},
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Ho",
				"given": "Lewis"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Isaac",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					22
				]
			]
		}
	},
	{
		"id": "songPositionLeverageFoundational2024",
		"type": "article",
		"abstract": "Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research. However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.",
		"DOI": "10.48550/arXiv.2405.03547",
		"note": "arXiv:2405.03547 [cs]",
		"number": "arXiv:2405.03547",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Position: Leverage Foundational Models for Black-Box Optimization",
		"title-short": "Position",
		"URL": "http://arxiv.org/abs/2405.03547",
		"author": [
			{
				"family": "Song",
				"given": "Xingyou"
			},
			{
				"family": "Tian",
				"given": "Yingtao"
			},
			{
				"family": "Lange",
				"given": "Robert Tjarko"
			},
			{
				"family": "Lee",
				"given": "Chansoo"
			},
			{
				"family": "Tang",
				"given": "Yujin"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					9
				]
			]
		}
	},
	{
		"id": "pandaTeachLLMsPhish2023",
		"type": "paper-conference",
		"abstract": "When large language models are trained on private data, it can be a \\textit{significant} privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new \\emph{practical} data extraction attack that we call ``neural phishing''. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of $10\\%$ attack success rates, at times, as high as $50\\%$. Our attack assumes only that an adversary can insert as few as $10$s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
		"title-short": "Teach LLMs to Phish",
		"URL": "https://openreview.net/forum?id=qo21ZlfNu6",
		"author": [
			{
				"family": "Panda",
				"given": "Ashwinee"
			},
			{
				"family": "Choquette-Choo",
				"given": "Christopher A."
			},
			{
				"family": "Zhang",
				"given": "Zhengming"
			},
			{
				"family": "Yang",
				"given": "Yaoqing"
			},
			{
				"family": "Mittal",
				"given": "Prateek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "scarpelliniPi2vecPolicy2023",
		"type": "paper-conference",
		"abstract": "This paper introduces $\\pi$2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. $\\pi$2vec represents the behavior of policies by capturing the statistics of the features from a pretrained model with the help of successor feature framework. We focus on the offline setting where policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on $\\pi$2vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "$\\pi$2vec: Policy Representation with Successor Features",
		"title-short": "$\\pi$2vec",
		"URL": "https://openreview.net/forum?id=o5Bqa4o5Mi",
		"author": [
			{
				"family": "Scarpellini",
				"given": "Gianluca"
			},
			{
				"family": "Konyushkova",
				"given": "Ksenia"
			},
			{
				"family": "Fantacci",
				"given": "Claudio"
			},
			{
				"family": "Paine",
				"given": "Thomas"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			},
			{
				"family": "Denil",
				"given": "Misha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "deletangLanguageModelingCompression2024",
		"type": "article",
		"abstract": "It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.",
		"DOI": "10.48550/arXiv.2309.10668",
		"note": "arXiv:2309.10668 [cs, math]",
		"number": "arXiv:2309.10668",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Modeling Is Compression",
		"URL": "http://arxiv.org/abs/2309.10668",
		"author": [
			{
				"family": "Delétang",
				"given": "Grégoire"
			},
			{
				"family": "Ruoss",
				"given": "Anian"
			},
			{
				"family": "Duquenne",
				"given": "Paul-Ambroise"
			},
			{
				"family": "Catt",
				"given": "Elliot"
			},
			{
				"family": "Genewein",
				"given": "Tim"
			},
			{
				"family": "Mattern",
				"given": "Christopher"
			},
			{
				"family": "Grau-Moya",
				"given": "Jordi"
			},
			{
				"family": "Wenliang",
				"given": "Li Kevin"
			},
			{
				"family": "Aitchison",
				"given": "Matthew"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "Veness",
				"given": "Joel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					18
				]
			]
		}
	},
	{
		"id": "Hutter:24uaibook2",
		"type": "book",
		"abstract": "This book provides a gentle introduction to Universal Artificial Intelligence (UAI), a theory that provides a formal underpinning of what it means for an agent to act intelligently in an unknown environment. First presented in book (Hutter, 2004), UAI offers a framework in which virtually all AI problems can be formulated, and a theory of how to solve them. UAI unifies ideas from sequential decision theory, Bayesian inference, and algorithmic information theory to construct AIXI, an optimal reinforcement learning agent that learns to act optimally in unknown environments. AIXI is the theoretical gold standard for intelligent behavior.\n     The book covers both the theoretical and practical aspects of UAI. Bayesian updating can be done efficiently with context tree weighting, and planning can be approximated by sampling with Monte Carlo tree search. It provides algorithms for the reader to implement, and experimental results to compare against. These algorithms are used to approximate AIXI. The book ends with a philosophical discussion of Artificial General Intelligence: Can super-intelligent agents even be constructed? Is it inevitable that they will be constructed, and what are the potential consequences?\n     This text is suitable for late undergraduate students. It provides an extensive chapter to fill in the required mathematics, probability, information, and computability theory background.\n     The book is not a second edition of Hutter (2004) UAI. It is a prequel (much more gentle introduction to the prerequisite topics) and a sequel (progress in the last 20 years). It is easier (more detailed less dense explanations) and harder (covers some advanced topics such a CTW and GoT). It includes various practical approximations, pseudo-code, and links to implementations of some (in Java and C).",
		"note": "tex.for: 010404(20\ntex.support: ARC grant DP150104590",
		"title": "An introduction to universal artificial intelligence",
		"URL": "http://www.hutter1.net/ai/uaibook2.htm",
		"author": [
			{
				"family": "Hutter",
				"given": "Marcus"
			},
			{
				"family": "Quarel",
				"given": "David"
			},
			{
				"family": "Catt",
				"given": "Elliot"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "geirhosDonTrustYour2024",
		"type": "article",
		"abstract": "How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to \"explain\" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.",
		"DOI": "10.48550/arXiv.2306.04719",
		"note": "arXiv:2306.04719 [cs, q-bio]",
		"number": "arXiv:2306.04719",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Don't trust your eyes: on the (un)reliability of feature visualizations",
		"title-short": "Don't trust your eyes",
		"URL": "http://arxiv.org/abs/2306.04719",
		"author": [
			{
				"family": "Geirhos",
				"given": "Robert"
			},
			{
				"family": "Zimmermann",
				"given": "Roland S."
			},
			{
				"family": "Bilodeau",
				"given": "Blair"
			},
			{
				"family": "Brendel",
				"given": "Wieland"
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					6
				]
			]
		}
	},
	{
		"id": "morrisLevelsAGIOperationalizing2024",
		"type": "article",
		"abstract": "We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. With these principles in mind, we propose \"Levels of AGI\" based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.",
		"DOI": "10.48550/arXiv.2311.02462",
		"note": "arXiv:2311.02462 [cs]",
		"number": "arXiv:2311.02462",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Levels of AGI for Operationalizing Progress on the Path to AGI",
		"URL": "http://arxiv.org/abs/2311.02462",
		"author": [
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Sohl-dickstein",
				"given": "Jascha"
			},
			{
				"family": "Fiedel",
				"given": "Noah"
			},
			{
				"family": "Warkentin",
				"given": "Tris"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			},
			{
				"family": "Farabet",
				"given": "Clement"
			},
			{
				"family": "Legg",
				"given": "Shane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					3
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					5
				]
			]
		}
	},
	{
		"id": "christianoTransferSimulationReal2016",
		"type": "article",
		"abstract": "Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.",
		"DOI": "10.48550/arXiv.1610.03518",
		"note": "arXiv:1610.03518 [cs]",
		"number": "arXiv:1610.03518",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model",
		"URL": "http://arxiv.org/abs/1610.03518",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Shah",
				"given": "Zain"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Blackwell",
				"given": "Trevor"
			},
			{
				"family": "Tobin",
				"given": "Joshua"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					10,
					11
				]
			]
		}
	},
	{
		"id": "finnConnectionGenerativeAdversarial2016",
		"type": "article",
		"abstract": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.",
		"DOI": "10.48550/arXiv.1611.03852",
		"note": "arXiv:1611.03852 [cs]",
		"number": "arXiv:1611.03852",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models",
		"URL": "http://arxiv.org/abs/1611.03852",
		"author": [
			{
				"family": "Finn",
				"given": "Chelsea"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					11,
					25
				]
			]
		}
	},
	{
		"id": "wuQuantitativeAnalysisDecoderBased2017",
		"type": "article",
		"abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https://github.com/tonywu95/eval_gen. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.",
		"DOI": "10.48550/arXiv.1611.04273",
		"note": "arXiv:1611.04273 [cs]",
		"number": "arXiv:1611.04273",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Quantitative Analysis of Decoder-Based Generative Models",
		"URL": "http://arxiv.org/abs/1611.04273",
		"author": [
			{
				"family": "Wu",
				"given": "Yuhuai"
			},
			{
				"family": "Burda",
				"given": "Yuri"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					6,
					6
				]
			]
		}
	},
	{
		"id": "stadieThirdPersonImitationLearning2019",
		"type": "article",
		"abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
		"DOI": "10.48550/arXiv.1703.01703",
		"note": "arXiv:1703.01703 [cs]",
		"number": "arXiv:1703.01703",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Third-Person Imitation Learning",
		"URL": "http://arxiv.org/abs/1703.01703",
		"author": [
			{
				"family": "Stadie",
				"given": "Bradly C."
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					22
				]
			]
		}
	},
	{
		"id": "mishraPredictionControlTemporal2017",
		"type": "article",
		"abstract": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",
		"DOI": "10.48550/arXiv.1703.04070",
		"note": "arXiv:1703.04070 [cs, stat]",
		"number": "arXiv:1703.04070",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Prediction and Control with Temporal Segment Models",
		"URL": "http://arxiv.org/abs/1703.04070",
		"author": [
			{
				"family": "Mishra",
				"given": "Nikhil"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					7,
					13
				]
			]
		}
	},
	{
		"id": "mordatchEmergenceGroundedCompositional2018",
		"type": "article",
		"abstract": "By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.",
		"DOI": "10.48550/arXiv.1703.04908",
		"note": "arXiv:1703.04908 [cs]",
		"number": "arXiv:1703.04908",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergence of Grounded Compositional Language in Multi-Agent Populations",
		"URL": "http://arxiv.org/abs/1703.04908",
		"author": [
			{
				"family": "Mordatch",
				"given": "Igor"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					24
				]
			]
		}
	},
	{
		"id": "salimansEvolutionStrategiesScalable2017",
		"type": "article",
		"abstract": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",
		"DOI": "10.48550/arXiv.1703.03864",
		"note": "arXiv:1703.03864 [cs, stat]",
		"number": "arXiv:1703.03864",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1703.03864",
		"author": [
			{
				"family": "Salimans",
				"given": "Tim"
			},
			{
				"family": "Ho",
				"given": "Jonathan"
			},
			{
				"family": "Chen",
				"given": "Xi"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					9,
					7
				]
			]
		}
	},
	{
		"id": "radfordLearningGenerateReviews2017",
		"type": "article",
		"abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.",
		"DOI": "10.48550/arXiv.1704.01444",
		"note": "arXiv:1704.01444 [cs]",
		"number": "arXiv:1704.01444",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Generate Reviews and Discovering Sentiment",
		"URL": "http://arxiv.org/abs/1704.01444",
		"author": [
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Jozefowicz",
				"given": "Rafal"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					6
				]
			]
		}
	},
	{
		"id": "florensaStochasticNeuralNetworks2017",
		"type": "article",
		"abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
		"DOI": "10.48550/arXiv.1704.03012",
		"note": "arXiv:1704.03012 [cs]",
		"number": "arXiv:1704.03012",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1704.03012",
		"author": [
			{
				"family": "Florensa",
				"given": "Carlos"
			},
			{
				"family": "Duan",
				"given": "Yan"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					10
				]
			]
		}
	},
	{
		"id": "schulmanEquivalencePolicyGradients2018",
		"type": "article",
		"abstract": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \"soft\" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\\epsilon$-greedy exploration schedule.",
		"DOI": "10.48550/arXiv.1704.06440",
		"note": "arXiv:1704.06440 [cs]",
		"number": "arXiv:1704.06440",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Equivalence Between Policy Gradients and Soft Q-Learning",
		"URL": "http://arxiv.org/abs/1704.06440",
		"author": [
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Chen",
				"given": "Xi"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					14
				]
			]
		}
	},
	{
		"id": "duanOneShotImitationLearning2017a",
		"type": "article",
		"abstract": "Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .",
		"DOI": "10.48550/arXiv.1703.07326",
		"note": "arXiv:1703.07326 [cs]",
		"number": "arXiv:1703.07326",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "One-Shot Imitation Learning",
		"URL": "http://arxiv.org/abs/1703.07326",
		"author": [
			{
				"family": "Duan",
				"given": "Yan"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Stadie",
				"given": "Bradly C."
			},
			{
				"family": "Ho",
				"given": "Jonathan"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					12,
					4
				]
			]
		}
	},
	{
		"id": "tobinDomainRandomizationTransferring2017",
		"type": "article",
		"abstract": "Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.",
		"DOI": "10.48550/arXiv.1703.06907",
		"note": "arXiv:1703.06907 [cs]",
		"number": "arXiv:1703.06907",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
		"URL": "http://arxiv.org/abs/1703.06907",
		"author": [
			{
				"family": "Tobin",
				"given": "Josh"
			},
			{
				"family": "Fong",
				"given": "Rachel"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					3,
					20
				]
			]
		}
	},
	{
		"id": "loweMultiAgentActorCriticMixed2020",
		"type": "article",
		"abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",
		"DOI": "10.48550/arXiv.1706.02275",
		"note": "arXiv:1706.02275 [cs]",
		"number": "arXiv:1706.02275",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
		"URL": "http://arxiv.org/abs/1706.02275",
		"author": [
			{
				"family": "Lowe",
				"given": "Ryan"
			},
			{
				"family": "Wu",
				"given": "Yi"
			},
			{
				"family": "Tamar",
				"given": "Aviv"
			},
			{
				"family": "Harb",
				"given": "Jean"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					3,
					14
				]
			]
		}
	},
	{
		"id": "matiisenTeacherStudentCurriculumLearning2017",
		"type": "article",
		"abstract": "We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.",
		"DOI": "10.48550/arXiv.1707.00183",
		"note": "arXiv:1707.00183 [cs]",
		"number": "arXiv:1707.00183",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Teacher-Student Curriculum Learning",
		"URL": "http://arxiv.org/abs/1707.00183",
		"author": [
			{
				"family": "Matiisen",
				"given": "Tambet"
			},
			{
				"family": "Oliver",
				"given": "Avital"
			},
			{
				"family": "Cohen",
				"given": "Taco"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					11,
					29
				]
			]
		}
	},
	{
		"id": "andrychowiczHindsightExperienceReplay2018",
		"type": "article",
		"abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",
		"DOI": "10.48550/arXiv.1707.01495",
		"note": "arXiv:1707.01495 [cs]",
		"number": "arXiv:1707.01495",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hindsight Experience Replay",
		"URL": "http://arxiv.org/abs/1707.01495",
		"author": [
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Wolski",
				"given": "Filip"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Fong",
				"given": "Rachel"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Tobin",
				"given": "Josh"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					23
				]
			]
		}
	},
	{
		"id": "athalyeSynthesizingRobustAdversarial2018",
		"type": "article",
		"abstract": "Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",
		"DOI": "10.48550/arXiv.1707.07397",
		"note": "arXiv:1707.07397 [cs]",
		"number": "arXiv:1707.07397",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Synthesizing Robust Adversarial Examples",
		"URL": "http://arxiv.org/abs/1707.07397",
		"author": [
			{
				"family": "Athalye",
				"given": "Anish"
			},
			{
				"family": "Engstrom",
				"given": "Logan"
			},
			{
				"family": "Ilyas",
				"given": "Andrew"
			},
			{
				"family": "Kwok",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					7
				]
			]
		}
	},
	{
		"id": "plappertParameterSpaceNoise2018",
		"type": "article",
		"abstract": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.",
		"DOI": "10.48550/arXiv.1706.01905",
		"note": "arXiv:1706.01905 [cs, stat]",
		"number": "arXiv:1706.01905",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Parameter Space Noise for Exploration",
		"URL": "http://arxiv.org/abs/1706.01905",
		"author": [
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Houthooft",
				"given": "Rein"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Chen",
				"given": "Richard Y."
			},
			{
				"family": "Chen",
				"given": "Xi"
			},
			{
				"family": "Asfour",
				"given": "Tamim"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					31
				]
			]
		}
	},
	{
		"id": "wuScalableTrustregionMethod2017",
		"type": "article",
		"abstract": "In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines",
		"DOI": "10.48550/arXiv.1708.05144",
		"note": "arXiv:1708.05144 [cs]",
		"number": "arXiv:1708.05144",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation",
		"URL": "http://arxiv.org/abs/1708.05144",
		"author": [
			{
				"family": "Wu",
				"given": "Yuhuai"
			},
			{
				"family": "Mansimov",
				"given": "Elman"
			},
			{
				"family": "Liao",
				"given": "Shun"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					8,
					18
				]
			]
		}
	},
	{
		"id": "foersterLearningOpponentLearningAwareness2018",
		"type": "article",
		"abstract": "Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.",
		"DOI": "10.48550/arXiv.1709.04326",
		"note": "arXiv:1709.04326 [cs]",
		"number": "arXiv:1709.04326",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning with Opponent-Learning Awareness",
		"URL": "http://arxiv.org/abs/1709.04326",
		"author": [
			{
				"family": "Foerster",
				"given": "Jakob N."
			},
			{
				"family": "Chen",
				"given": "Richard Y."
			},
			{
				"family": "Al-Shedivat",
				"given": "Maruan"
			},
			{
				"family": "Whiteson",
				"given": "Shimon"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					19
				]
			]
		}
	},
	{
		"id": "al-shedivatContinuousAdaptationMetaLearning2018",
		"type": "article",
		"abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.",
		"DOI": "10.48550/arXiv.1710.03641",
		"note": "arXiv:1710.03641 [cs]",
		"number": "arXiv:1710.03641",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
		"URL": "http://arxiv.org/abs/1710.03641",
		"author": [
			{
				"family": "Al-Shedivat",
				"given": "Maruan"
			},
			{
				"family": "Bansal",
				"given": "Trapit"
			},
			{
				"family": "Burda",
				"given": "Yuri"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					23
				]
			]
		}
	},
	{
		"id": "bansalEmergentComplexityMultiAgent2018",
		"type": "article",
		"abstract": "Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX",
		"DOI": "10.48550/arXiv.1710.03748",
		"note": "arXiv:1710.03748 [cs]",
		"number": "arXiv:1710.03748",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergent Complexity via Multi-Agent Competition",
		"URL": "http://arxiv.org/abs/1710.03748",
		"author": [
			{
				"family": "Bansal",
				"given": "Trapit"
			},
			{
				"family": "Pachocki",
				"given": "Jakub"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					3,
					14
				]
			]
		}
	},
	{
		"id": "tobinDomainRandomizationGenerative2018",
		"type": "article",
		"abstract": "Deep learning-based robotic grasping has made significant progress thanks to algorithmic improvements and increased data availability. However, state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances, and as a result generalization can be a challenge. In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects. Since the distribution of successful grasps for a given object can be highly multimodal, we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution over possible grasps. This model allows us to sample grasps efficiently at test time (or avoid sampling entirely). We evaluate our model architecture and data generation pipeline in simulation and the real world. We find we can achieve a $>$90% success rate on previously unseen realistic objects at test time in simulation despite having only been trained on random objects. We also demonstrate an 80% success rate on real-world grasp attempts despite having only been trained on random simulated objects.",
		"DOI": "10.48550/arXiv.1710.06425",
		"note": "arXiv:1710.06425 [cs]",
		"number": "arXiv:1710.06425",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Domain Randomization and Generative Models for Robotic Grasping",
		"URL": "http://arxiv.org/abs/1710.06425",
		"author": [
			{
				"family": "Tobin",
				"given": "Joshua"
			},
			{
				"family": "Biewald",
				"given": "Lukas"
			},
			{
				"family": "Duan",
				"given": "Rocky"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Handa",
				"given": "Ankur"
			},
			{
				"family": "Kumar",
				"given": "Vikash"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					3
				]
			]
		}
	},
	{
		"id": "pengSimtoRealTransferRobotic2018",
		"type": "paper-conference",
		"abstract": "Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this \"reality gap\". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.",
		"container-title": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
		"DOI": "10.1109/ICRA.2018.8460528",
		"note": "arXiv:1710.06537 [cs]",
		"page": "3803-3810",
		"source": "arXiv.org",
		"title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
		"URL": "http://arxiv.org/abs/1710.06537",
		"author": [
			{
				"family": "Peng",
				"given": "Xue Bin"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5
				]
			]
		}
	},
	{
		"id": "fransMetaLearningShared2017",
		"type": "article",
		"abstract": "We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",
		"DOI": "10.48550/arXiv.1710.09767",
		"note": "arXiv:1710.09767 [cs]",
		"number": "arXiv:1710.09767",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Meta Learning Shared Hierarchies",
		"URL": "http://arxiv.org/abs/1710.09767",
		"author": [
			{
				"family": "Frans",
				"given": "Kevin"
			},
			{
				"family": "Ho",
				"given": "Jonathan"
			},
			{
				"family": "Chen",
				"given": "Xi"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					10,
					26
				]
			]
		}
	},
	{
		"id": "louizosLearningSparseNeural2018",
		"type": "article",
		"abstract": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
		"DOI": "10.48550/arXiv.1712.01312",
		"note": "arXiv:1712.01312 [cs, stat]",
		"number": "arXiv:1712.01312",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Sparse Neural Networks through $L_0$ Regularization",
		"URL": "http://arxiv.org/abs/1712.01312",
		"author": [
			{
				"family": "Louizos",
				"given": "Christos"
			},
			{
				"family": "Welling",
				"given": "Max"
			},
			{
				"family": "Kingma",
				"given": "Diederik P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					22
				]
			]
		}
	},
	{
		"id": "raimanDeepTypeMultilingualEntity2018",
		"type": "article",
		"abstract": "The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.",
		"DOI": "10.48550/arXiv.1802.01021",
		"note": "arXiv:1802.01021 [cs]",
		"number": "arXiv:1802.01021",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
		"title-short": "DeepType",
		"URL": "http://arxiv.org/abs/1802.01021",
		"author": [
			{
				"family": "Raiman",
				"given": "Jonathan"
			},
			{
				"family": "Raiman",
				"given": "Olivier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					3
				]
			]
		}
	},
	{
		"id": "milliInterpretablePedagogicalExamples2018a",
		"type": "article",
		"abstract": "Teachers intentionally pick the most informative examples to show their students. However, if the teacher and student are neural networks, the examples that the teacher network learns to give, although effective at teaching the student, are typically uninterpretable. We show that training the student and teacher iteratively, rather than jointly, can produce interpretable teaching strategies. We evaluate interpretability by (1) measuring the similarity of the teacher's emergent strategies to intuitive strategies in each domain and (2) conducting human experiments to evaluate how effective the teacher's strategies are at teaching humans. We show that the teacher network learns to select or generate interpretable, pedagogical examples to teach rule-based, probabilistic, boolean, and hierarchical concepts.",
		"DOI": "10.48550/arXiv.1711.00694",
		"note": "arXiv:1711.00694 [cs]",
		"number": "arXiv:1711.00694",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interpretable and Pedagogical Examples",
		"URL": "http://arxiv.org/abs/1711.00694",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					14
				]
			]
		}
	},
	{
		"id": "plappertMultiGoalReinforcementLearning2018",
		"type": "article",
		"abstract": "The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick & place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.",
		"DOI": "10.48550/arXiv.1802.09464",
		"note": "arXiv:1802.09464 [cs]",
		"number": "arXiv:1802.09464",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research",
		"title-short": "Multi-Goal Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1802.09464",
		"author": [
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Baker",
				"given": "Bowen"
			},
			{
				"family": "Powell",
				"given": "Glenn"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Tobin",
				"given": "Josh"
			},
			{
				"family": "Chociej",
				"given": "Maciek"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Kumar",
				"given": "Vikash"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					3,
					10
				]
			]
		}
	},
	{
		"id": "stadieConsiderationsLearningExplore2019",
		"type": "article",
		"abstract": "We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$ deliver better performance on tasks where exploration is important.",
		"DOI": "10.48550/arXiv.1803.01118",
		"note": "arXiv:1803.01118 [cs]",
		"number": "arXiv:1803.01118",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1803.01118",
		"author": [
			{
				"family": "Stadie",
				"given": "Bradly C."
			},
			{
				"family": "Yang",
				"given": "Ge"
			},
			{
				"family": "Houthooft",
				"given": "Rein"
			},
			{
				"family": "Chen",
				"given": "Xi"
			},
			{
				"family": "Duan",
				"given": "Yan"
			},
			{
				"family": "Wu",
				"given": "Yuhuai"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					11
				]
			]
		}
	},
	{
		"id": "nicholFirstOrderMetaLearningAlgorithms2018a",
		"type": "article",
		"abstract": "This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.",
		"DOI": "10.48550/arXiv.1803.02999",
		"note": "arXiv:1803.02999 [cs]",
		"number": "arXiv:1803.02999",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On First-Order Meta-Learning Algorithms",
		"URL": "http://arxiv.org/abs/1803.02999",
		"author": [
			{
				"family": "Nichol",
				"given": "Alex"
			},
			{
				"family": "Achiam",
				"given": "Joshua"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					22
				]
			]
		}
	},
	{
		"id": "salimansImprovingGANsUsing2018",
		"type": "article",
		"abstract": "We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.",
		"DOI": "10.48550/arXiv.1803.05573",
		"note": "arXiv:1803.05573 [cs, stat]",
		"number": "arXiv:1803.05573",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving GANs Using Optimal Transport",
		"URL": "http://arxiv.org/abs/1803.05573",
		"author": [
			{
				"family": "Salimans",
				"given": "Tim"
			},
			{
				"family": "Zhang",
				"given": "Han"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Metaxas",
				"given": "Dimitris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					3,
					14
				]
			]
		}
	},
	{
		"id": "wuVarianceReductionPolicy2018",
		"type": "article",
		"abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.",
		"DOI": "10.48550/arXiv.1803.07246",
		"note": "arXiv:1803.07246 [cs, stat]",
		"number": "arXiv:1803.07246",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines",
		"URL": "http://arxiv.org/abs/1803.07246",
		"author": [
			{
				"family": "Wu",
				"given": "Cathy"
			},
			{
				"family": "Rajeswaran",
				"given": "Aravind"
			},
			{
				"family": "Duan",
				"given": "Yan"
			},
			{
				"family": "Kumar",
				"given": "Vikash"
			},
			{
				"family": "Bayen",
				"given": "Alexandre M."
			},
			{
				"family": "Kakade",
				"given": "Sham"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					3,
					19
				]
			]
		}
	},
	{
		"id": "nicholGottaLearnFast2018",
		"type": "article",
		"abstract": "In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.",
		"DOI": "10.48550/arXiv.1804.03720",
		"note": "arXiv:1804.03720 [cs, stat]",
		"number": "arXiv:1804.03720",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Gotta Learn Fast: A New Benchmark for Generalization in RL",
		"title-short": "Gotta Learn Fast",
		"URL": "http://arxiv.org/abs/1804.03720",
		"author": [
			{
				"family": "Nichol",
				"given": "Alex"
			},
			{
				"family": "Pfau",
				"given": "Vicki"
			},
			{
				"family": "Hesse",
				"given": "Christopher"
			},
			{
				"family": "Klimov",
				"given": "Oleg"
			},
			{
				"family": "Schulman",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					23
				]
			]
		}
	},
	{
		"id": "houthooftEvolvedPolicyGradients2018",
		"type": "article",
		"abstract": "We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.",
		"DOI": "10.48550/arXiv.1802.04821",
		"note": "arXiv:1802.04821 [cs]",
		"number": "arXiv:1802.04821",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evolved Policy Gradients",
		"URL": "http://arxiv.org/abs/1802.04821",
		"author": [
			{
				"family": "Houthooft",
				"given": "Rein"
			},
			{
				"family": "Chen",
				"given": "Richard Y."
			},
			{
				"family": "Isola",
				"given": "Phillip"
			},
			{
				"family": "Stadie",
				"given": "Bradly C."
			},
			{
				"family": "Wolski",
				"given": "Filip"
			},
			{
				"family": "Ho",
				"given": "Jonathan"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					4,
					29
				]
			]
		}
	},
	{
		"id": "huangGamePadLearningEnvironment2018",
		"type": "article",
		"abstract": "In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.",
		"DOI": "10.48550/arXiv.1806.00608",
		"note": "arXiv:1806.00608 [cs, stat]",
		"number": "arXiv:1806.00608",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "GamePad: A Learning Environment for Theorem Proving",
		"title-short": "GamePad",
		"URL": "http://arxiv.org/abs/1806.00608",
		"author": [
			{
				"family": "Huang",
				"given": "Daniel"
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					21
				]
			]
		}
	},
	{
		"id": "groverLearningPolicyRepresentations2018",
		"type": "article",
		"abstract": "Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.",
		"DOI": "10.48550/arXiv.1806.06464",
		"note": "arXiv:1806.06464 [cs, stat]",
		"number": "arXiv:1806.06464",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Policy Representations in Multiagent Systems",
		"URL": "http://arxiv.org/abs/1806.06464",
		"author": [
			{
				"family": "Grover",
				"given": "Aditya"
			},
			{
				"family": "Al-Shedivat",
				"given": "Maruan"
			},
			{
				"family": "Gupta",
				"given": "Jayesh K."
			},
			{
				"family": "Burda",
				"given": "Yura"
			},
			{
				"family": "Edwards",
				"given": "Harrison"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					31
				]
			]
		}
	},
	{
		"id": "kingmaGlowGenerativeFlow2018",
		"type": "article",
		"abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow",
		"DOI": "10.48550/arXiv.1807.03039",
		"note": "arXiv:1807.03039 [cs, stat]",
		"number": "arXiv:1807.03039",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
		"title-short": "Glow",
		"URL": "http://arxiv.org/abs/1807.03039",
		"author": [
			{
				"family": "Kingma",
				"given": "Diederik P."
			},
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					10
				]
			]
		}
	},
	{
		"id": "openaiLearningDexterousInHand2019",
		"type": "article",
		"abstract": "We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM",
		"DOI": "10.48550/arXiv.1808.00177",
		"note": "arXiv:1808.00177 [cs, stat]",
		"number": "arXiv:1808.00177",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Dexterous In-Hand Manipulation",
		"URL": "http://arxiv.org/abs/1808.00177",
		"author": [
			{
				"family": "OpenAI",
				"given": ""
			},
			{
				"family": "Andrychowicz",
				"given": "Marcin"
			},
			{
				"family": "Baker",
				"given": "Bowen"
			},
			{
				"family": "Chociej",
				"given": "Maciek"
			},
			{
				"family": "Jozefowicz",
				"given": "Rafal"
			},
			{
				"family": "McGrew",
				"given": "Bob"
			},
			{
				"family": "Pachocki",
				"given": "Jakub"
			},
			{
				"family": "Petron",
				"given": "Arthur"
			},
			{
				"family": "Plappert",
				"given": "Matthias"
			},
			{
				"family": "Powell",
				"given": "Glenn"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Schneider",
				"given": "Jonas"
			},
			{
				"family": "Sidor",
				"given": "Szymon"
			},
			{
				"family": "Tobin",
				"given": "Josh"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Zaremba",
				"given": "Wojciech"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					18
				]
			]
		}
	},
	{
		"id": "burdaLargeScaleStudyCuriosityDriven2018",
		"type": "article",
		"abstract": "Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/",
		"DOI": "10.48550/arXiv.1808.04355",
		"note": "arXiv:1808.04355 [cs, stat]",
		"number": "arXiv:1808.04355",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Large-Scale Study of Curiosity-Driven Learning",
		"URL": "http://arxiv.org/abs/1808.04355",
		"author": [
			{
				"family": "Burda",
				"given": "Yuri"
			},
			{
				"family": "Edwards",
				"given": "Harri"
			},
			{
				"family": "Pathak",
				"given": "Deepak"
			},
			{
				"family": "Storkey",
				"given": "Amos"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Efros",
				"given": "Alexei A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					13
				]
			]
		}
	},
	{
		"id": "grathwohlFFJORDFreeformContinuous2018",
		"type": "article",
		"abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.",
		"DOI": "10.48550/arXiv.1810.01367",
		"note": "arXiv:1810.01367 [cs, stat]",
		"number": "arXiv:1810.01367",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models",
		"title-short": "FFJORD",
		"URL": "http://arxiv.org/abs/1810.01367",
		"author": [
			{
				"family": "Grathwohl",
				"given": "Will"
			},
			{
				"family": "Chen",
				"given": "Ricky T. Q."
			},
			{
				"family": "Bettencourt",
				"given": "Jesse"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					2
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					22
				]
			]
		}
	},
	{
		"id": "hernandez-oralloAIParadigmsAI2020",
		"type": "article-journal",
		"abstract": "AI safety often analyses a risk or safety issue, such as interruptibility, under a particular AI paradigm, such as reinforcement learning. But what is an AI paradigm and how does it affect the understanding and implications of the safety issue? Is AI safety research covering the most representative paradigms and the right combinations of paradigms with safety issues? Will current research directions in AI safety be able to anticipate more capable and powerful systems yet to come? In this paper we analyse these questions, introducing a distinction between two types of paradigms in AI: artefacts and techniques. We then use experimental data of research and media documents from AI Topics, an ofﬁcial publication of the AAAI, to examine how safety research is distributed across artefacts and techniques. We observe that AI safety research is not sufﬁciently anticipatory, and is heavily weighted towards certain research paradigms. We identify a need for AI safety to be more explicit about the artefacts and techniques for which a particular issue may be applicable, in order to identify gaps and cover a broader range of issues.",
		"container-title": "Santiago de Compostela",
		"language": "en",
		"source": "Zotero",
		"title": "AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues",
		"author": [
			{
				"family": "Hernandez-Orallo",
				"given": "Jose"
			},
			{
				"family": "Martınez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhouPredictableArtificialIntelligence2023",
		"type": "article",
		"abstract": "We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.",
		"DOI": "10.48550/arXiv.2310.06167",
		"note": "arXiv:2310.06167 [cs]",
		"number": "arXiv:2310.06167",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Predictable Artificial Intelligence",
		"URL": "http://arxiv.org/abs/2310.06167",
		"author": [
			{
				"family": "Zhou",
				"given": "Lexin"
			},
			{
				"family": "Moreno-Casares",
				"given": "Pablo A."
			},
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Burnell",
				"given": "Ryan"
			},
			{
				"family": "Cheke",
				"given": "Lucy"
			},
			{
				"family": "Ferri",
				"given": "Cèsar"
			},
			{
				"family": "Marcoci",
				"given": "Alexandru"
			},
			{
				"family": "Mehrbakhsh",
				"given": "Behzad"
			},
			{
				"family": "Moros-Daval",
				"given": "Yael"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			},
			{
				"family": "Rutar",
				"given": "Danaja"
			},
			{
				"family": "Schellaert",
				"given": "Wout"
			},
			{
				"family": "Voudouris",
				"given": "Konstantinos"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					9
				]
			]
		}
	},
	{
		"id": "avinFillingGapsTrustworthy2021",
		"type": "article-journal",
		"abstract": "The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks while eroding confidence in AI systems and the organizations that develop them. A 2019 study (1) found more than 80 organizations that have published and adopted “AI ethics principles,” and more have joined since. But the principles often leave a gap between the “what” and the “how” of trustworthy AI development. Such gaps have enabled questionable or ethically dubious behavior, which casts doubts on the trustworthiness of specific organizations, and the field more broadly. There is thus an urgent need for concrete methods that both enable AI developers to prevent harm and allow them to demonstrate their trustworthiness through verifiable behavior. Below, we explore mechanisms [drawn from (2)] for creating an ecosystem where AI developers can earn trust—if they are trustworthy (see the figure). Better assessment of developer trustworthiness could inform user choice, employee actions, investment decisions, legal recourse, and emerging governance regimes.",
		"container-title": "Science",
		"DOI": "10.1126/science.abi7176",
		"issue": "6573",
		"note": "publisher: American Association for the Advancement of Science",
		"page": "1327-1329",
		"source": "science.org (Atypon)",
		"title": "Filling gaps in trustworthy development of AI",
		"URL": "https://www.science.org/doi/10.1126/science.abi7176",
		"volume": "374",
		"author": [
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Wang",
				"given": "Jasmine"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Krawczuk",
				"given": "Igor"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Lebensold",
				"given": "Jonathan"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Zilberman",
				"given": "Noa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					10
				]
			]
		}
	},
	{
		"id": "caveBridgingLongtermConcerns2019",
		"type": "article-journal",
		"abstract": "Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.",
		"container-title": "Nature Machine Intelligence",
		"DOI": "10.1038/s42256-018-0003-2",
		"ISSN": "2522-5839",
		"issue": "1",
		"journalAbbreviation": "Nat Mach Intell",
		"language": "en",
		"license": "2019 Springer Nature Limited",
		"note": "publisher: Nature Publishing Group",
		"page": "5-6",
		"source": "www.nature.com",
		"title": "Bridging near- and long-term concerns about AI",
		"URL": "https://www.nature.com/articles/s42256-018-0003-2",
		"volume": "1",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			},
			{
				"family": "ÓhÉigeartaigh",
				"given": "Seán S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1
				]
			]
		}
	},
	{
		"id": "brundageMaliciousUseArtificial2018a",
		"type": "article",
		"abstract": "This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.",
		"DOI": "10.48550/arXiv.1802.07228",
		"note": "arXiv:1802.07228 [cs]",
		"number": "arXiv:1802.07228",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",
		"title-short": "The Malicious Use of Artificial Intelligence",
		"URL": "http://arxiv.org/abs/1802.07228",
		"author": [
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Toner",
				"given": "Helen"
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Scharre",
				"given": "Paul"
			},
			{
				"family": "Zeitzoff",
				"given": "Thomas"
			},
			{
				"family": "Filar",
				"given": "Bobby"
			},
			{
				"family": "Anderson",
				"given": "Hyrum"
			},
			{
				"family": "Roff",
				"given": "Heather"
			},
			{
				"family": "Allen",
				"given": "Gregory C."
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Flynn",
				"given": "Carrick"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			},
			{
				"family": "Beard",
				"given": "Simon"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			},
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Lyle",
				"given": "Clare"
			},
			{
				"family": "Crootof",
				"given": "Rebecca"
			},
			{
				"family": "Evans",
				"given": "Owain"
			},
			{
				"family": "Page",
				"given": "Michael"
			},
			{
				"family": "Bryson",
				"given": "Joanna"
			},
			{
				"family": "Yampolskiy",
				"given": "Roman"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					20
				]
			]
		}
	},
	{
		"id": "brundageTrustworthyAIDevelopment2020a",
		"type": "article",
		"abstract": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",
		"language": "en",
		"note": "arXiv:2004.07213 [cs]",
		"number": "arXiv:2004.07213",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims",
		"title-short": "Toward Trustworthy AI Development",
		"URL": "http://arxiv.org/abs/2004.07213",
		"author": [
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Wang",
				"given": "Jasmine"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			},
			{
				"family": "Khlaaf",
				"given": "Heidy"
			},
			{
				"family": "Yang",
				"given": "Jingying"
			},
			{
				"family": "Toner",
				"given": "Helen"
			},
			{
				"family": "Fong",
				"given": "Ruth"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Koh",
				"given": "Pang Wei"
			},
			{
				"family": "Hooker",
				"given": "Sara"
			},
			{
				"family": "Leung",
				"given": "Jade"
			},
			{
				"family": "Trask",
				"given": "Andrew"
			},
			{
				"family": "Bluemke",
				"given": "Emma"
			},
			{
				"family": "Lebensold",
				"given": "Jonathan"
			},
			{
				"family": "O'Keefe",
				"given": "Cullen"
			},
			{
				"family": "Koren",
				"given": "Mark"
			},
			{
				"family": "Ryffel",
				"given": "Théo"
			},
			{
				"family": "Rubinovitz",
				"given": "J. B."
			},
			{
				"family": "Besiroglu",
				"given": "Tamay"
			},
			{
				"family": "Carugati",
				"given": "Federica"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			},
			{
				"family": "Haas",
				"given": "Sarah",
				"non-dropping-particle": "de"
			},
			{
				"family": "Johnson",
				"given": "Maritza"
			},
			{
				"family": "Laurie",
				"given": "Ben"
			},
			{
				"family": "Ingerman",
				"given": "Alex"
			},
			{
				"family": "Krawczuk",
				"given": "Igor"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Cammarota",
				"given": "Rosario"
			},
			{
				"family": "Lohn",
				"given": "Andrew"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Stix",
				"given": "Charlotte"
			},
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Graham",
				"given": "Logan"
			},
			{
				"family": "Prunkl",
				"given": "Carina"
			},
			{
				"family": "Martin",
				"given": "Bianca"
			},
			{
				"family": "Seger",
				"given": "Elizabeth"
			},
			{
				"family": "Zilberman",
				"given": "Noa"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			},
			{
				"family": "Kroeger",
				"given": "Frens"
			},
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Kagan",
				"given": "Rebecca"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Tse",
				"given": "Brian"
			},
			{
				"family": "Barnes",
				"given": "Elizabeth"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Scharre",
				"given": "Paul"
			},
			{
				"family": "Herbert-Voss",
				"given": "Ariel"
			},
			{
				"family": "Rasser",
				"given": "Martijn"
			},
			{
				"family": "Sodhani",
				"given": "Shagun"
			},
			{
				"family": "Flynn",
				"given": "Carrick"
			},
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Dyer",
				"given": "Lisa"
			},
			{
				"family": "Khan",
				"given": "Saif"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4,
					20
				]
			]
		}
	},
	{
		"id": "casaresHowGeneralPurposeLanguage2022",
		"type": "article-journal",
		"abstract": "The new generation of language models is reported to solve some extraordinary tasks the models were never trained for specifically, in few-shot or zero-shot settings. However, these reports usually cherry-pick the tasks, use the best prompts, and unwrap or extract the solutions leniently even if they are followed by nonsensical text. In sum, they are specialised results for one domain, a particular way of using the models and interpreting the results. In this paper, we present a novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters --- in the wild. For a useful and safe interaction in these increasingly more common conditions, we need to understand when the model fails because of a lack of capability or a misunderstanding of the user's intents. Our results indicate that language models such as GPT-3 have limited understanding of the human command; far from becoming general-purpose systems in the wild.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v36i5.20466",
		"ISSN": "2374-3468",
		"issue": "5",
		"language": "en",
		"license": "Copyright (c) 2022 Association for the Advancement of Artificial Intelligence",
		"note": "number: 5",
		"page": "5295-5303",
		"source": "ojs.aaai.org",
		"title": "How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild",
		"title-short": "How General-Purpose Is a Language Model?",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/20466",
		"volume": "36",
		"author": [
			{
				"family": "Casares",
				"given": "Pablo Antonio Moreno"
			},
			{
				"family": "Loe",
				"given": "Bao Sheng"
			},
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "hEigeartaigh",
				"given": "Sean"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					28
				]
			]
		}
	},
	{
		"id": "guestSafeguardingSafeguardsHow2023",
		"type": "article",
		"abstract": "AI alignment work is important from both a commercial and a safety lens. With this paper, we aim to help actors who support alignment efforts to make these efforts as effective as possible, and to avoid potential adverse effects. We begin by suggesting that institutions that are trying to act in the public interest (such as governments) should aim to support specifically alignment work that reduces accident or misuse risks. We then describe four problems which might cause alignment efforts to be counterproductive, increasing large-scale AI risks. We suggest mitigations for each problem. Finally, we make a broader recommendation that institutions trying to act in the public interest should think systematically about how to make their alignment efforts as effective, and as likely to be beneficial, as possible.",
		"DOI": "10.48550/arXiv.2312.08039",
		"note": "arXiv:2312.08039 [cs]",
		"number": "arXiv:2312.08039",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safeguarding the safeguards: How best to promote AI alignment in the public interest",
		"title-short": "Safeguarding the safeguards",
		"URL": "http://arxiv.org/abs/2312.08039",
		"author": [
			{
				"family": "Guest",
				"given": "Oliver"
			},
			{
				"family": "Aird",
				"given": "Michael"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					15
				]
			]
		}
	},
	{
		"id": "liuSolvingProblemfindingFramework2021",
		"type": "article-journal",
		"abstract": "Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.",
		"container-title": "Futures",
		"DOI": "10.1016/j.futures.2020.102672",
		"ISSN": "0016-3287",
		"journalAbbreviation": "Futures",
		"page": "102672",
		"source": "ScienceDirect",
		"title": "‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence",
		"title-short": "‘Solving for X?",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0016328720301634",
		"volume": "126",
		"author": [
			{
				"family": "Liu",
				"given": "Hin-Yan"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					1
				]
			]
		}
	},
	{
		"id": "barnettOasesCooperationEmpirical2022",
		"type": "article-journal",
		"abstract": "In the creation of safe AI systems it is extremely important to ensure cooperative behaviour of these systems, even when there are incentives to act selﬁshly. In many cases, even when game-theoretic solutions allow for cooperation, actually getting the AI systems to converge on these solutions through training is difﬁcult. In this paper we empirically evaluate how reinforcement learning agents can be encouraged to cooperate (without opening themselves up to exploitation) by selecting appropriate hyperparameters and environmental perceptions for the agent. Our results in the multi-agent scenario indicate that in hyperparameter-space there are isolated “oases” of mutual cooperation, and small changes in these hyperparameters can lead to sharp drops into non-cooperative behaviour.",
		"language": "en",
		"source": "Zotero",
		"title": "Oases of Cooperation: An Empirical Evaluation of Reinforcement Learning in the Iterated Prisoner’s Dilemma",
		"author": [
			{
				"family": "Barnett",
				"given": "Peter"
			},
			{
				"family": "Burden",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "burdenExploringAISafety2020",
		"type": "paper-conference",
		"abstract": "The landscape of AI safety is frequently explored differently by contrasting specialised AI versus general AI (or AGI), by analysing the short-term hazards of systems with limited capabilities against those more long-term risks posed by ‘superintelligence’, and by conceptualising sophisticated ways of bounding control an AI system has over its environment and itself (impact, harm to humans, self-harm, containment, etc.). In this position paper we reconsider these three aspects of AI safety as quantitative factors –generality, capability and\ncontrol–, suggesting that by defining metrics for these dimensions, AI risks can be characterised and analysed more precisely. As an example, we illustrate how to define these metrics and their values for some simple agents in a toy scenario within a reinforcement learning setting.",
		"collection-title": "CEUR Workshop Proceedings",
		"container-title": "Proceedings of the Workshop on Artificial Intelligence Safety, co-located with 34th AAAI Conference on Artificial Intelligence, SafeAI@AAAI 2020, New York City, NY, USA, February 7, 2020",
		"page": "36–40",
		"publisher": "CEUR-WS.org",
		"source": "DBLP Computer Science Bibliography",
		"title": "Exploring AI Safety in Degrees: Generality, Capability and Control",
		"title-short": "Exploring AI Safety in Degrees",
		"URL": "https://ceur-ws.org/Vol-2560/paper21.pdf",
		"volume": "2560",
		"author": [
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			}
		],
		"editor": [
			{
				"family": "Espinoza",
				"given": "Huáscar"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Chen",
				"given": "Xin Cynthia"
			},
			{
				"family": "ÓhÉigeartaigh",
				"given": "Seán S."
			},
			{
				"family": "Huang",
				"given": "Xiaowei"
			},
			{
				"family": "Castillo-Effen",
				"given": "Mauricio"
			},
			{
				"family": "Mallah",
				"given": "Richard"
			},
			{
				"family": "McDermid",
				"given": "John A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "schellaertYourPromptMy2023",
		"type": "article-journal",
		"abstract": "Even with obvious deficiencies, large prompt-commanded multimodal models are proving to be flexible cognitive tools representing an unprecedented generality. But the directness, diversity, and degree of user interaction create a distinctive “human-centred generality” (HCG), rather than a fully autonomous one. HCG implies that —for a specific user— a system is only as general as it is effective for the user’s relevant tasks and their prevalent ways of prompting. A human-centred evaluation of general-purpose AI systems therefore needs to reflect the personal nature of interaction, tasks and cognition. We argue that the best way to understand these systems is as highly-coupled cognitive extenders, and to analyse the bidirectional cognitive adaptations between them and humans. In this paper, we give a formulation of HCG, as well as a high-level overview of the elements and trade-offs involved in the prompting process. We end the paper by outlining some essential research questions and suggestions for improving evaluation practices, which we envision as characteristic for the evaluation of general artificial intelligence in the future.\nThis paper appears in the AI &amp; Society track.",
		"container-title": "Journal of Artificial Intelligence Research",
		"DOI": "10.1613/jair.1.14157",
		"ISSN": "1076-9757",
		"language": "en",
		"license": "Copyright (c) 2023 Journal of Artificial Intelligence Research",
		"page": "377-394",
		"source": "www.jair.org",
		"title": "Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models",
		"title-short": "Your Prompt is My Command",
		"URL": "https://www.jair.org/index.php/jair/article/view/14157",
		"volume": "77",
		"author": [
			{
				"family": "Schellaert",
				"given": "Wout"
			},
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Vold",
				"given": "Karina"
			},
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Casares",
				"given": "Pablo A. M."
			},
			{
				"family": "Loe",
				"given": "Bao Sheng"
			},
			{
				"family": "Reichart",
				"given": "Roi"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Sean Ó"
			},
			{
				"family": "Korhonen",
				"given": "Anna"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "bostromSuperintelligentWillMotivation2012",
		"type": "article-journal",
		"abstract": "This paper discusses the relation between intelligence and motivation in artificial agents, developing and briefly arguing for two theses. The first, the orthogonality thesis, holds (with some caveats) that intelligence and final goals (purposes) are orthogonal axes along which possible artificial intellects can freely vary—more or less any level of intelligence could be combined with more or less any final goal. The second, the instrumental convergence thesis, holds that as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. In combination, the two theses help us understand the possible range of behavior of superintelligent agents, and they point to some potential dangers in building such an agent.",
		"container-title": "Minds and Machines",
		"DOI": "10.1007/s11023-012-9281-3",
		"ISSN": "1572-8641",
		"issue": "2",
		"journalAbbreviation": "Minds & Machines",
		"language": "en",
		"page": "71-85",
		"source": "Springer Link",
		"title": "The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents",
		"title-short": "The Superintelligent Will",
		"URL": "https://doi.org/10.1007/s11023-012-9281-3",
		"volume": "22",
		"author": [
			{
				"family": "Bostrom",
				"given": "Nick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					5,
					1
				]
			]
		}
	},
	{
		"id": "armstrongThinkingBoxControlling2012",
		"type": "article-journal",
		"abstract": "There is no strong reason to believe that human-level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed motivation system. Solving this issue in general has proven to be considerably harder than expected. This paper looks at one particular approach, Oracle AI. An Oracle AI is an AI that does not act in the world except by answering questions. Even this narrow approach presents considerable challenges. In this paper, we analyse and critique various methods of controlling the AI. In general an Oracle AI might be safer than unrestricted AI, but still remains potentially dangerous.",
		"container-title": "Minds and Machines",
		"DOI": "10.1007/s11023-012-9282-2",
		"ISSN": "1572-8641",
		"issue": "4",
		"journalAbbreviation": "Minds & Machines",
		"language": "en",
		"page": "299-324",
		"source": "Springer Link",
		"title": "Thinking Inside the Box: Controlling and Using an Oracle AI",
		"title-short": "Thinking Inside the Box",
		"URL": "https://doi.org/10.1007/s11023-012-9282-2",
		"volume": "22",
		"author": [
			{
				"family": "Armstrong",
				"given": "Stuart"
			},
			{
				"family": "Sandberg",
				"given": "Anders"
			},
			{
				"family": "Bostrom",
				"given": "Nick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					11,
					1
				]
			]
		}
	},
	{
		"id": "orseauSafelyInterruptibleAgents2016",
		"type": "paper-conference",
		"abstract": "Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button— which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal definition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.",
		"collection-title": "UAI'16",
		"container-title": "Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence",
		"event-place": "Arlington, Virginia, USA",
		"ISBN": "978-0-9966431-1-5",
		"page": "557–566",
		"publisher": "AUAI Press",
		"publisher-place": "Arlington, Virginia, USA",
		"source": "ACM Digital Library",
		"title": "Safely interruptible agents",
		"author": [
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Armstrong",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					6,
					25
				]
			]
		}
	},
	{
		"id": "graceViewpointWhenWill2018",
		"type": "article-journal",
		"abstract": "Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.\n\n\nThis article is part of the special track on AI and Society.",
		"container-title": "Journal of Artificial Intelligence Research",
		"DOI": "10.1613/jair.1.11222",
		"ISSN": "1076-9757",
		"language": "en",
		"license": "Copyright (c) 2018",
		"page": "729-754",
		"source": "jair.org",
		"title": "Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts",
		"title-short": "Viewpoint",
		"URL": "https://jair.org/index.php/jair/article/view/11222",
		"volume": "62",
		"author": [
			{
				"family": "Grace",
				"given": "Katja"
			},
			{
				"family": "Salvatier",
				"given": "John"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Evans",
				"given": "Owain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					31
				]
			]
		}
	},
	{
		"id": "drexlerReframingSuperintelligence2019",
		"type": "article",
		"abstract": "Studies of superintelligent-level systems have typically posited AI functionality that plays the role of a mind in a rational utility-directed agent, and hence employ an abstraction initially developed as an idealized model of human decision makers. Today, developments in AI technology highlight intelligent systems that are quite unlike minds, and provide a basis for a different approach to understanding them: Today, we can\nconsider how AI systems are produced (through the work of research and development), what they do (broadly, provide services by performing tasks), and what they will enable (including incremental yet potentially thorough automation of human tasks). Because tasks subject to automation include the tasks that comprise AI research and development, current trends in the field promise accelerating AI-enabled advances in AI technology itself, potentially leading to asymptotically recursive improvement of AI technologies in distributed systems, a prospect that contrasts sharply with the vision of\nself-improvement internal to opaque, unitary agents. The trajectory of AI development thus points to the emergence of asymptotically comprehensive, superintelligent-level AI services that—crucially—can include the service of developing new services, both narrow and broad, guided by concrete human goals and informed by strong models of human (dis)approval. The concept of comprehensive AI services (CAIS) provides a model of flexible, general intelligence in which agents are a class of service-providing products, rather than a natural or necessary engine of progress in themselves. Ramifications of the CAIS model reframe not only prospects for an\nintelligence explosion and the nature of advanced machine intelligence, but also the relationship between goals and intelligence, the problem of harnessing advanced AI to broad, challenging problems, and funda-\nmental considerations in AI safety and strategy. Perhaps surprisingly,\nstrongly self-modifying agents lose their instrumental value even as their\nimplementation becomes more accessible, while the likely context for the emergence of such agents becomes a world already in possession of general superintelligent-level capabilities. These prospective capabilities, in turn, engender novel risks and opportunities of their own. Further topics addressed in this work include the general architecture of systems with broad capabilities, the intersection between symbolic and neural systems, learning vs. competence in definitions of intelligence, tactical vs. strategic tasks in the context of human control, and estimates of the relative capacities of human brains vs. current digital systems.",
		"event-place": "Unpublished",
		"language": "en",
		"publisher-place": "Unpublished",
		"source": "Zotero",
		"title": "Reframing Superintelligence",
		"author": [
			{
				"family": "Drexler",
				"given": "K Eric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					1
				]
			]
		}
	},
	{
		"id": "evansTruthfulAIDeveloping2021",
		"type": "article",
		"abstract": "In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI \"lies\" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding \"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",
		"DOI": "10.48550/arXiv.2110.06674",
		"note": "arXiv:2110.06674 [cs]",
		"number": "arXiv:2110.06674",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Truthful AI: Developing and governing AI that does not lie",
		"title-short": "Truthful AI",
		"URL": "http://arxiv.org/abs/2110.06674",
		"author": [
			{
				"family": "Evans",
				"given": "Owain"
			},
			{
				"family": "Cotton-Barratt",
				"given": "Owen"
			},
			{
				"family": "Finnveden",
				"given": "Lukas"
			},
			{
				"family": "Bales",
				"given": "Adam"
			},
			{
				"family": "Balwit",
				"given": "Avital"
			},
			{
				"family": "Wills",
				"given": "Peter"
			},
			{
				"family": "Righetti",
				"given": "Luca"
			},
			{
				"family": "Saunders",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					13
				]
			]
		}
	},
	{
		"id": "filanClusterabilityNeuralNetworks2021",
		"type": "article",
		"abstract": "The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We ﬁnd that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and ﬁnd that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",
		"language": "en",
		"note": "arXiv:2103.03386 [cs]",
		"number": "arXiv:2103.03386",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Clusterability in Neural Networks",
		"URL": "http://arxiv.org/abs/2103.03386",
		"author": [
			{
				"family": "Filan",
				"given": "Daniel"
			},
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Hod",
				"given": "Shlomi"
			},
			{
				"family": "Wild",
				"given": "Cody"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					4
				]
			]
		}
	},
	{
		"id": "vermaDiscoveringUserInterpretableCapabilities2022",
		"type": "article",
		"abstract": "Several approaches have been developed for answering users' specific questions about AI behavior and for assessing their core functionality in terms of primitive executable actions. However, the problem of summarizing an AI agent's broad capabilities for a user is comparatively new. This paper presents an algorithm for discovering from scratch the suite of high-level \"capabilities\" that an AI system with arbitrary internal planning algorithms/policies can perform. It computes conditions describing the applicability and effects of these capabilities in user-interpretable terms. Starting from a set of user-interpretable state properties, an AI agent, and a simulator that the agent can interact with, our algorithm returns a set of high-level capabilities with their parameterized descriptions. Empirical evaluation on several game-based scenarios shows that this approach efficiently learns descriptions of various types of AI agents in deterministic, fully observable settings. User studies show that such descriptions are easier to understand and reason with than the agent's primitive actions.",
		"DOI": "10.48550/arXiv.2107.13668",
		"note": "arXiv:2107.13668 [cs]",
		"number": "arXiv:2107.13668",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering User-Interpretable Capabilities of Black-Box Planning Agents",
		"URL": "http://arxiv.org/abs/2107.13668",
		"author": [
			{
				"family": "Verma",
				"given": "Pulkit"
			},
			{
				"family": "Marpally",
				"given": "Shashank Rao"
			},
			{
				"family": "Srivastava",
				"given": "Siddharth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					30
				]
			]
		}
	},
	{
		"id": "watkinsExplainingRobotPolicies2021",
		"type": "article-journal",
		"abstract": "In order to interact with a robot or make wise decisions about where and how to deploy it in the real world, humans need to have an accurate mental model of how the robot acts in different situations. We propose to improve users' mental model of a robot by showing them examples of how the robot behaves in informative scenarios. We explore this in two settings. First, we show that when there are many possible environment states, users can more quickly understand the robot's policy if they are shown critical states where taking a particular action is important. Second, we show that when there is a distribution shift between training and test environment distributions, then it is more effective to show exploratory states that the robot does not visit naturally.",
		"container-title": "Applied AI Letters",
		"DOI": "10.1002/ail2.52",
		"ISSN": "2689-5595",
		"issue": "4",
		"language": "en",
		"license": "© 2021 The Authors. Applied AI Letters published by John Wiley & Sons Ltd.",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.52",
		"page": "e52",
		"source": "Wiley Online Library",
		"title": "Explaining robot policies",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.52",
		"volume": "2",
		"author": [
			{
				"family": "Watkins",
				"given": "Olivia"
			},
			{
				"family": "Huang",
				"given": "Sandy"
			},
			{
				"family": "Frost",
				"given": "Julius"
			},
			{
				"family": "Bhatia",
				"given": "Kush"
			},
			{
				"family": "Weiner",
				"given": "Eric"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Plummer",
				"given": "Bryan"
			},
			{
				"family": "Saenko",
				"given": "Kate"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "filanPrunedNeuralNetworks2022",
		"type": "article",
		"abstract": "The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a \"module\" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers. Note that this paper has been superceded by \"Clusterability in Neural Networks\", arxiv:2103.03386 and \"Quantifying Local Specialization in Deep Neural Networks\", arxiv:2110.08058!",
		"DOI": "10.48550/arXiv.2003.04881",
		"note": "arXiv:2003.04881 [cs]",
		"number": "arXiv:2003.04881",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Pruned Neural Networks are Surprisingly Modular",
		"URL": "http://arxiv.org/abs/2003.04881",
		"author": [
			{
				"family": "Filan",
				"given": "Daniel"
			},
			{
				"family": "Hod",
				"given": "Shlomi"
			},
			{
				"family": "Wild",
				"given": "Cody"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					7
				]
			]
		}
	},
	{
		"id": "andreasTranslatingNeuralese2018",
		"type": "article",
		"abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",
		"DOI": "10.48550/arXiv.1704.06960",
		"note": "arXiv:1704.06960 [cs]",
		"number": "arXiv:1704.06960",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Translating Neuralese",
		"URL": "http://arxiv.org/abs/1704.06960",
		"author": [
			{
				"family": "Andreas",
				"given": "Jacob"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Klein",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					22
				]
			]
		}
	},
	{
		"id": "turnerActivationAdditionSteering2024",
		"type": "article",
		"abstract": "Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.",
		"DOI": "10.48550/arXiv.2308.10248",
		"note": "arXiv:2308.10248 [cs]",
		"number": "arXiv:2308.10248",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Activation Addition: Steering Language Models Without Optimization",
		"title-short": "Activation Addition",
		"URL": "http://arxiv.org/abs/2308.10248",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Thiergart",
				"given": "Lisa"
			},
			{
				"family": "Leech",
				"given": "Gavin"
			},
			{
				"family": "Udell",
				"given": "David"
			},
			{
				"family": "Vazquez",
				"given": "Juan J."
			},
			{
				"family": "Mini",
				"given": "Ulisse"
			},
			{
				"family": "MacDiarmid",
				"given": "Monte"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					4
				]
			]
		}
	},
	{
		"id": "miniUnderstandingControllingMazeSolving2023",
		"type": "article",
		"abstract": "To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.",
		"DOI": "10.48550/arXiv.2310.08043",
		"note": "arXiv:2310.08043 [cs]",
		"number": "arXiv:2310.08043",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding and Controlling a Maze-Solving Policy Network",
		"URL": "http://arxiv.org/abs/2310.08043",
		"author": [
			{
				"family": "Mini",
				"given": "Ulisse"
			},
			{
				"family": "Grietzer",
				"given": "Peli"
			},
			{
				"family": "Sharma",
				"given": "Mrinank"
			},
			{
				"family": "Meek",
				"given": "Austin"
			},
			{
				"family": "MacDiarmid",
				"given": "Monte"
			},
			{
				"family": "Turner",
				"given": "Alexander Matt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					12
				]
			]
		}
	},
	{
		"id": "chughtaiNeuralNetworksLearn2023",
		"type": "article-journal",
		"abstract": "We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory, through learning several irreducible representations of the group and converting group composition to matrix multiplication. We show small networks consistently learn this algorithm when trained on composition of group elements by reverse engineering model logits and weights, and confirm our understanding using ablations. We use this as an algorithmic test bed for the hypothesis of universality in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. By studying networks trained on various groups and architectures, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop –are arbitrary.",
		"language": "en",
		"source": "Zotero",
		"title": "Neural Networks Learn Representation Theory: Reverse Engineering How Networks Perform Group Operations",
		"author": [
			{
				"family": "Chughtai",
				"given": "Bilal"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chughtaiToyModelUniversality2023",
		"type": "article",
		"abstract": "Universality is a key hypothesis in mechanistic interpretability – that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned – as well as the order they develop – are arbitrary.",
		"language": "en",
		"note": "arXiv:2302.03025 [cs, math]",
		"number": "arXiv:2302.03025",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
		"title-short": "A Toy Model of Universality",
		"URL": "http://arxiv.org/abs/2302.03025",
		"author": [
			{
				"family": "Chughtai",
				"given": "Bilal"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					24
				]
			]
		}
	},
	{
		"id": "nandaProgressMeasuresGrokking2023a",
		"type": "article",
		"abstract": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous progress measures that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverseengineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of “grokking” exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.",
		"language": "en",
		"note": "arXiv:2301.05217 [cs]",
		"number": "arXiv:2301.05217",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Progress measures for grokking via mechanistic interpretability",
		"URL": "http://arxiv.org/abs/2301.05217",
		"author": [
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Lieberum",
				"given": "Tom"
			},
			{
				"family": "Smith",
				"given": "Jess"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					19
				]
			]
		}
	},
	{
		"id": "boyd-graberHumanCenteredEvaluationExplanations2022",
		"type": "paper-conference",
		"abstract": "The NLP community are increasingly interested in providing explanations for NLP models to help people make sense of model behavior and potentially improve human interaction with models. In addition to computational challenges in generating these explanations, evaluations of the generated explanations require human-centered perspectives and approaches. This tutorial will provide an overview of human-centered evaluations of explanations. First, we will give a brief introduction to the psychological foundation of explanations as well as types of NLP model explanations and their corresponding presentation, to provide the necessary background. We will then present a taxonomy of human-centered evaluation of explanations and dive into depth in the two categories: 1) evaluation based on human-annotated explanations; 2) evaluation with human-subjects studies. We will conclude by discussing future directions. We will also adopt a flipped format to maximize the in- teractive components for the live audience.",
		"container-title": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts",
		"DOI": "10.18653/v1/2022.naacl-tutorials.4",
		"event-place": "Seattle, United States",
		"page": "26–32",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Seattle, United States",
		"source": "ACLWeb",
		"title": "Human-Centered Evaluation of Explanations",
		"URL": "https://aclanthology.org/2022.naacl-tutorials.4",
		"author": [
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Carton",
				"given": "Samuel"
			},
			{
				"family": "Feng",
				"given": "Shi"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Lombrozo",
				"given": "Tania"
			},
			{
				"family": "Smith-Renner",
				"given": "Alison"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"editor": [
			{
				"family": "Ballesteros",
				"given": "Miguel"
			},
			{
				"family": "Tsvetkov",
				"given": "Yulia"
			},
			{
				"family": "Alm",
				"given": "Cecilia O."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "ohSelfImitationLearning2018",
		"type": "article",
		"abstract": "This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",
		"DOI": "10.48550/arXiv.1806.05635",
		"note": "arXiv:1806.05635 [cs, stat]",
		"number": "arXiv:1806.05635",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Imitation Learning",
		"URL": "http://arxiv.org/abs/1806.05635",
		"author": [
			{
				"family": "Oh",
				"given": "Junhyuk"
			},
			{
				"family": "Guo",
				"given": "Yijie"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					14
				]
			]
		}
	},
	{
		"id": "zhangWisdomHindsightMakes2023",
		"type": "article",
		"abstract": "Reinforcement learning has seen wide success in ﬁnetuning large language models to better align with instructions via human feedback. The socalled algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn’t require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised ﬁnetuning1.",
		"language": "en",
		"note": "arXiv:2302.05206 [cs]",
		"number": "arXiv:2302.05206",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers",
		"URL": "http://arxiv.org/abs/2302.05206",
		"author": [
			{
				"family": "Zhang",
				"given": "Tianjun"
			},
			{
				"family": "Liu",
				"given": "Fangchen"
			},
			{
				"family": "Wong",
				"given": "Justin"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Gonzalez",
				"given": "Joseph E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					10
				]
			]
		}
	},
	{
		"id": "liuChainHindsightAligns2023",
		"type": "article",
		"abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
		"language": "en",
		"note": "arXiv:2302.02676 [cs]",
		"number": "arXiv:2302.02676",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Chain of Hindsight Aligns Language Models with Feedback",
		"URL": "http://arxiv.org/abs/2302.02676",
		"author": [
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Sferrazza",
				"given": "Carmelo"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					18
				]
			]
		}
	},
	{
		"id": "liuEmergentAgenticTransformer2023",
		"type": "article",
		"abstract": "Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at training and test time. As we show on D4RL and ExoRL benchmarks, to the best our knowledge, this is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data. Our Agentic Transformer also shows a promising scaling trend that bigger models consistently improve results.",
		"language": "en",
		"note": "arXiv:2305.16554 [cs]",
		"number": "arXiv:2305.16554",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Emergent Agentic Transformer from Chain of Hindsight Experience",
		"URL": "http://arxiv.org/abs/2305.16554",
		"author": [
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					25
				]
			]
		}
	},
	{
		"id": "gudibandeFalsePromiseImitating2023",
		"type": "article",
		"abstract": "An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model’s capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B–13B), data sources, and imitation data amounts (0.3M–150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models—they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",
		"language": "en",
		"note": "arXiv:2305.15717 [cs]",
		"number": "arXiv:2305.15717",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The False Promise of Imitating Proprietary LLMs",
		"URL": "http://arxiv.org/abs/2305.15717",
		"author": [
			{
				"family": "Gudibande",
				"given": "Arnav"
			},
			{
				"family": "Wallace",
				"given": "Eric"
			},
			{
				"family": "Snell",
				"given": "Charlie"
			},
			{
				"family": "Geng",
				"given": "Xinyang"
			},
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					25
				]
			]
		}
	},
	{
		"id": "bauerHumanTimescaleAdaptationOpenEnded2023",
		"type": "paper-conference",
		"abstract": "Foundation models have shown impressive adaptation and scalability in supervised and selfsupervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-ﬂy hypothesis-driven exploration, efﬁcient exploitation of acquired knowledge, and can successfully be prompted with ﬁrst-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attentionbased memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent’s capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across everlarger open-ended domains.",
		"event-title": "ICML 2023",
		"language": "en",
		"source": "Zotero",
		"title": "Human-Timescale Adaptation in an Open-Ended Task Space",
		"author": [
			{
				"family": "Bauer",
				"given": "Jakob"
			},
			{
				"family": "Baumli",
				"given": "Kate"
			},
			{
				"family": "Behbahani",
				"given": "Feryal"
			},
			{
				"family": "Bhoopchand",
				"given": "Avishkar"
			},
			{
				"family": "Bradley-Schmieg",
				"given": "Nathalie"
			},
			{
				"family": "Chang",
				"given": "Michael"
			},
			{
				"family": "Clay",
				"given": "Natalie"
			},
			{
				"family": "Collister",
				"given": "Adrian"
			},
			{
				"family": "Dasagi",
				"given": "Vibhavari"
			},
			{
				"family": "Gonzalez",
				"given": "Lucy"
			},
			{
				"family": "Gregor",
				"given": "Karol"
			},
			{
				"family": "Hughes",
				"given": "Edward"
			},
			{
				"family": "Kashem",
				"given": "Sheleem"
			},
			{
				"family": "Loks-Thompson",
				"given": "Maria"
			},
			{
				"family": "Openshaw",
				"given": "Hannah"
			},
			{
				"family": "Parker-Holder",
				"given": "Jack"
			},
			{
				"family": "Pathak",
				"given": "Shreya"
			},
			{
				"family": "Perez-Nieves",
				"given": "Nicolas"
			},
			{
				"family": "Rakicevic",
				"given": "Nemanja"
			},
			{
				"family": "Rocktäschel",
				"given": "Tim"
			},
			{
				"family": "Schroecker",
				"given": "Yannick"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			},
			{
				"family": "Sygnowski",
				"given": "Jakub"
			},
			{
				"family": "Tuyls",
				"given": "Karl"
			},
			{
				"family": "York",
				"given": "Sarah"
			},
			{
				"family": "Zacherl",
				"given": "Alexander"
			},
			{
				"family": "Zhang",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "langeDiscoveringEvolutionStrategies2023",
		"type": "article",
		"abstract": "Optimizing functions without access to gradients is the remit of black-box methods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inﬂexible — exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that metaevolving this system on a small set of representative low-dimensional analytic optimization problems is sufﬁcient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop.",
		"language": "en",
		"note": "arXiv:2211.11260 [cs]",
		"number": "arXiv:2211.11260",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering Evolution Strategies via Meta-Black-Box Optimization",
		"URL": "http://arxiv.org/abs/2211.11260",
		"author": [
			{
				"family": "Lange",
				"given": "Robert Tjarko"
			},
			{
				"family": "Schaul",
				"given": "Tom"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			},
			{
				"family": "Zahavy",
				"given": "Tom"
			},
			{
				"family": "Dallibard",
				"given": "Valentin"
			},
			{
				"family": "Lu",
				"given": "Chris"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			},
			{
				"family": "Flennerhag",
				"given": "Sebastian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					2
				]
			]
		}
	},
	{
		"id": "zahavyDiversifyingAICreative2023",
		"type": "article",
		"abstract": "In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.",
		"language": "en",
		"note": "arXiv:2308.09175 [cs]",
		"number": "arXiv:2308.09175",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Diversifying AI: Towards Creative Chess with AlphaZero",
		"title-short": "Diversifying AI",
		"URL": "http://arxiv.org/abs/2308.09175",
		"author": [
			{
				"family": "Zahavy",
				"given": "Tom"
			},
			{
				"family": "Veeriah",
				"given": "Vivek"
			},
			{
				"family": "Hou",
				"given": "Shaobo"
			},
			{
				"family": "Waugh",
				"given": "Kevin"
			},
			{
				"family": "Lai",
				"given": "Matthew"
			},
			{
				"family": "Leurent",
				"given": "Edouard"
			},
			{
				"family": "Tomasev",
				"given": "Nenad"
			},
			{
				"family": "Schut",
				"given": "Lisa"
			},
			{
				"family": "Hassabis",
				"given": "Demis"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					29
				]
			]
		}
	},
	{
		"id": "fishmanTaskScopingGenerating2023",
		"type": "article",
		"abstract": "A general-purpose planning agent requires an open-scope world model: one rich enough to tackle any of the wide range of tasks it may be asked to solve over its operational lifetime. This stands in contrast with typical planning approaches, where the scope of a model is limited to a specific family of tasks that share significant structure. Unfortunately, planning to solve any specific task using an open-scope model is computationally intractable - even for state-of-the-art methods - due to the many states and actions that are necessarily present in the model but irrelevant to that problem. We propose task scoping: a method that exploits knowledge of the initial state, goal conditions, and transition system to automatically and efficiently remove provably irrelevant variables and actions from a planning problem. Our approach leverages causal link analysis and backwards reachability over state variables (rather than states) along with operator merging (when effects on relevant variables are identical). Using task scoping as a pre-planning step can shrink the search space by orders of magnitude and dramatically decrease planning time. We empirically demonstrate that these improvements occur across a variety of open-scope domains, including Minecraft, where our approach leads to a 75x reduction in search time with a state-of-the-art numeric planner, even after including the time required for task scoping itself.",
		"DOI": "10.48550/arXiv.2010.08869",
		"note": "arXiv:2010.08869 [cs]",
		"number": "arXiv:2010.08869",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Task Scoping: Generating Task-Specific Abstractions for Planning in Open-Scope Models",
		"title-short": "Task Scoping",
		"URL": "http://arxiv.org/abs/2010.08869",
		"author": [
			{
				"family": "Fishman",
				"given": "Michael"
			},
			{
				"family": "Kumar",
				"given": "Nishanth"
			},
			{
				"family": "Allen",
				"given": "Cameron"
			},
			{
				"family": "Danas",
				"given": "Natasha"
			},
			{
				"family": "Littman",
				"given": "Michael"
			},
			{
				"family": "Tellex",
				"given": "Stefanie"
			},
			{
				"family": "Konidaris",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					4
				]
			]
		}
	},
	{
		"id": "kinnimentEvaluatingLanguageModelAgents2024",
		"type": "article",
		"abstract": "In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as \"autonomous replication and adaptation\" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.",
		"DOI": "10.48550/arXiv.2312.11671",
		"note": "arXiv:2312.11671 [cs]",
		"number": "arXiv:2312.11671",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
		"URL": "http://arxiv.org/abs/2312.11671",
		"author": [
			{
				"family": "Kinniment",
				"given": "Megan"
			},
			{
				"family": "Sato",
				"given": "Lucas Jun Koba"
			},
			{
				"family": "Du",
				"given": "Haoxing"
			},
			{
				"family": "Goodrich",
				"given": "Brian"
			},
			{
				"family": "Hasin",
				"given": "Max"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Miles",
				"given": "Luke Harold"
			},
			{
				"family": "Lin",
				"given": "Tao R."
			},
			{
				"family": "Wijk",
				"given": "Hjalmar"
			},
			{
				"family": "Burget",
				"given": "Joel"
			},
			{
				"family": "Ho",
				"given": "Aaron"
			},
			{
				"family": "Barnes",
				"given": "Elizabeth"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					4
				]
			]
		}
	},
	{
		"id": "shinSuperhumanArtificialIntelligence2023",
		"type": "article-journal",
		"abstract": "How will superhuman artificial intelligence (AI) affect human decision-making? And what will be the mechanisms behind this effect? We address these questions in a domain where AI already exceeds human performance, analyzing more than 5.8 million move decisions made by professional Go players over the past 71 y (1950 to 2021). To address the first question, we use a superhuman AI program to estimate the quality of human decisions across time, generating 58 billion counterfactual game patterns and comparing the win rates of actual human decisions with those of counterfactual AI decisions. We find that humans began to make significantly better decisions following the advent of superhuman AI. We then examine human players’ strategies across time and find that novel decisions (i.e., previously unobserved moves) occurred more frequently and became associated with higher decision quality after the advent of superhuman AI. Our findings suggest that the development of superhuman AI programs may have prompted human players to break away from traditional strategies and induced them to explore novel moves, which in turn may have improved their decision-making.",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.2214840120",
		"issue": "12",
		"note": "publisher: Proceedings of the National Academy of Sciences",
		"page": "e2214840120",
		"source": "pnas.org (Atypon)",
		"title": "Superhuman artificial intelligence can improve human decision-making by increasing novelty",
		"URL": "https://www.pnas.org/doi/full/10.1073/pnas.2214840120",
		"volume": "120",
		"author": [
			{
				"family": "Shin",
				"given": "Minkyu"
			},
			{
				"family": "Kim",
				"given": "Jin"
			},
			{
				"family": "Opheusden",
				"given": "Bas",
				"non-dropping-particle": "van"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					21
				]
			]
		}
	},
	{
		"id": "mccoyEmbersAutoregressionUnderstanding2023",
		"type": "article",
		"abstract": "The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach—which we call the teleological approach—leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low—even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4’s accuracy at decoding a simple cipher is 51% when the output is a high-probability word sequence but only 13% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system—one that has been shaped by its own particular set of pressures.",
		"language": "en",
		"note": "arXiv:2309.13638 [cs]",
		"number": "arXiv:2309.13638",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
		"title-short": "Embers of Autoregression",
		"URL": "http://arxiv.org/abs/2309.13638",
		"author": [
			{
				"family": "McCoy",
				"given": "R. Thomas"
			},
			{
				"family": "Yao",
				"given": "Shunyu"
			},
			{
				"family": "Friedman",
				"given": "Dan"
			},
			{
				"family": "Hardy",
				"given": "Matthew"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					24
				]
			]
		}
	},
	{
		"id": "zhaoDefineEvaluateImprove2023",
		"type": "article",
		"abstract": "Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them.",
		"DOI": "10.48550/arXiv.2301.05149",
		"note": "arXiv:2301.05149 [cs]",
		"number": "arXiv:2301.05149",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",
		"URL": "http://arxiv.org/abs/2301.05149",
		"author": [
			{
				"family": "Zhao",
				"given": "Lingjun"
			},
			{
				"family": "Nguyen",
				"given": "Khanh"
			},
			{
				"family": "Daumé III",
				"given": "Hal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					28
				]
			]
		}
	},
	{
		"id": "yangLearningInteractiveRealWorld2024a",
		"type": "article",
		"abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by ∆x, ∆y” from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at universal-simulator.github.io.",
		"language": "en",
		"note": "arXiv:2310.06114 [cs]",
		"number": "arXiv:2310.06114",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Interactive Real-World Simulators",
		"URL": "http://arxiv.org/abs/2310.06114",
		"author": [
			{
				"family": "Yang",
				"given": "Mengjiao"
			},
			{
				"family": "Du",
				"given": "Yilun"
			},
			{
				"family": "Ghasemipour",
				"given": "Kamyar"
			},
			{
				"family": "Tompson",
				"given": "Jonathan"
			},
			{
				"family": "Kaelbling",
				"given": "Leslie"
			},
			{
				"family": "Schuurmans",
				"given": "Dale"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					12
				]
			]
		}
	},
	{
		"id": "yeFoundationReinforcementLearning2023",
		"type": "article",
		"abstract": "Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation priors, FAC learns significantly faster than traditional RL. Our evaluation on the Meta-World has proved that FAC can achieve 100% success rates for 7/8 tasks under less than 200k frames, which outperforms the baseline method with careful manual-designed rewards under 1M frames. (2) Robust to noisy priors. Our method tolerates the unavoidable noise in embodied foundation models. We show that FAC works well even under heavy noise or quantization errors. (3) Minimal human intervention: FAC completely learns from the foundation priors, without the need of human-specified dense reward, or providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our FRL framework could enable the future robot to autonomously explore and learn without human intervention in the physical world. In summary, our proposed FRL is a novel and powerful learning paradigm, towards achieving embodied generalist agents.",
		"DOI": "10.48550/arXiv.2310.02635",
		"note": "arXiv:2310.02635 [cs]",
		"number": "arXiv:2310.02635",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance",
		"title-short": "Foundation Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2310.02635",
		"author": [
			{
				"family": "Ye",
				"given": "Weirui"
			},
			{
				"family": "Zhang",
				"given": "Yunsheng"
			},
			{
				"family": "Wang",
				"given": "Mengchen"
			},
			{
				"family": "Wang",
				"given": "Shengjie"
			},
			{
				"family": "Gu",
				"given": "Xianfan"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Gao",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					10
				]
			]
		}
	},
	{
		"id": "liuExplorationPrinciplesDiverse2023",
		"type": "article",
		"abstract": "Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the generated content, offering critiques to guide the actor. Empirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision.",
		"DOI": "10.48550/arXiv.2310.08899",
		"note": "arXiv:2310.08899 [cs]",
		"number": "arXiv:2310.08899",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Exploration with Principles for Diverse AI Supervision",
		"URL": "http://arxiv.org/abs/2310.08899",
		"author": [
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Zaharia",
				"given": "Matei"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					23
				]
			]
		}
	},
	{
		"id": "khouryAdversarialTrainingVoronoi2019",
		"type": "article",
		"abstract": "Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which an adversary could construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove that adversarial training is sample inefficient, and show sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust. Finally we introduce adversarial training with Voronoi constraints, which replaces the norm ball constraint with the Voronoi cell for each point in the training set. We show that adversarial training with Voronoi constraints produces robust models which significantly improve over the state-of-the-art on MNIST and are competitive on CIFAR-10.",
		"DOI": "10.48550/arXiv.1905.01019",
		"note": "arXiv:1905.01019 [cs, stat]",
		"number": "arXiv:1905.01019",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Training with Voronoi Constraints",
		"URL": "http://arxiv.org/abs/1905.01019",
		"author": [
			{
				"family": "Khoury",
				"given": "Marc"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					2
				]
			]
		}
	},
	{
		"id": "khouryGeometryAdversarialExamples2018",
		"type": "article",
		"abstract": "Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.",
		"DOI": "10.48550/arXiv.1811.00525",
		"note": "arXiv:1811.00525 [cs, stat]",
		"number": "arXiv:1811.00525",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Geometry of Adversarial Examples",
		"URL": "http://arxiv.org/abs/1811.00525",
		"author": [
			{
				"family": "Khoury",
				"given": "Marc"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					11
				]
			]
		}
	},
	{
		"id": "zhanPreventingImitationLearning2020",
		"type": "article",
		"abstract": "Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of \"non-clonable\" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.",
		"DOI": "10.48550/arXiv.2002.01059",
		"note": "arXiv:2002.01059 [cs, stat]",
		"number": "arXiv:2002.01059",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Preventing Imitation Learning with Adversarial Policy Ensembles",
		"URL": "http://arxiv.org/abs/2002.01059",
		"author": [
			{
				"family": "Zhan",
				"given": "Albert"
			},
			{
				"family": "Tiomkin",
				"given": "Stas"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					2
				]
			]
		}
	},
	{
		"id": "gleaveAdversarialPoliciesAttacking2021",
		"type": "article",
		"abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",
		"DOI": "10.48550/arXiv.1905.10615",
		"note": "arXiv:1905.10615 [cs, stat]",
		"number": "arXiv:1905.10615",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
		"title-short": "Adversarial Policies",
		"URL": "http://arxiv.org/abs/1905.10615",
		"author": [
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Dennis",
				"given": "Michael"
			},
			{
				"family": "Wild",
				"given": "Cody"
			},
			{
				"family": "Kant",
				"given": "Neel"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					17
				]
			]
		}
	},
	{
		"id": "laidlawPerceptualAdversarialRobustness2021",
		"type": "article",
		"abstract": "A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the very definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to avoid this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models. To resolve this issue, we propose adversarial training against the set of all imperceptible adversarial examples, approximated using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks. We find that PAT achieves state-of-the-art robustness against the union of these five attacks, more than doubling the accuracy over the next best model, without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.",
		"DOI": "10.48550/arXiv.2006.12655",
		"note": "arXiv:2006.12655 [cs, stat]",
		"number": "arXiv:2006.12655",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models",
		"title-short": "Perceptual Adversarial Robustness",
		"URL": "http://arxiv.org/abs/2006.12655",
		"author": [
			{
				"family": "Laidlaw",
				"given": "Cassidy"
			},
			{
				"family": "Singla",
				"given": "Sahil"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					4
				]
			]
		}
	},
	{
		"id": "casperDiagnosticsDeepNeural2023",
		"type": "article",
		"abstract": "This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue",
		"DOI": "10.48550/arXiv.2211.10024",
		"note": "arXiv:2211.10024 [cs]",
		"number": "arXiv:2211.10024",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks",
		"URL": "http://arxiv.org/abs/2211.10024",
		"author": [
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Hariharan",
				"given": "Kaivalya"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					5
				]
			]
		}
	},
	{
		"id": "baileyImageHijacksAdversarial2024",
		"type": "article",
		"abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
		"DOI": "10.48550/arXiv.2309.00236",
		"note": "arXiv:2309.00236 [cs]",
		"number": "arXiv:2309.00236",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
		"title-short": "Image Hijacks",
		"URL": "http://arxiv.org/abs/2309.00236",
		"author": [
			{
				"family": "Bailey",
				"given": "Luke"
			},
			{
				"family": "Ong",
				"given": "Euan"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Emmons",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					22
				]
			]
		}
	},
	{
		"id": "toyerTensorTrustInterpretable2023",
		"type": "article",
		"abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper",
		"DOI": "10.48550/arXiv.2311.01011",
		"note": "arXiv:2311.01011 [cs]",
		"number": "arXiv:2311.01011",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
		"title-short": "Tensor Trust",
		"URL": "http://arxiv.org/abs/2311.01011",
		"author": [
			{
				"family": "Toyer",
				"given": "Sam"
			},
			{
				"family": "Watkins",
				"given": "Olivia"
			},
			{
				"family": "Mendes",
				"given": "Ethan Adrian"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Bailey",
				"given": "Luke"
			},
			{
				"family": "Wang",
				"given": "Tiffany"
			},
			{
				"family": "Ong",
				"given": "Isaac"
			},
			{
				"family": "Elmaaroufi",
				"given": "Karim"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Ritter",
				"given": "Alan"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "sucholutskyGettingAlignedRepresentational2023",
		"type": "article",
		"abstract": "Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.",
		"DOI": "10.48550/arXiv.2310.13018",
		"note": "arXiv:2310.13018 [cs, q-bio]",
		"number": "arXiv:2310.13018",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Getting aligned on representational alignment",
		"URL": "http://arxiv.org/abs/2310.13018",
		"author": [
			{
				"family": "Sucholutsky",
				"given": "Ilia"
			},
			{
				"family": "Muttenthaler",
				"given": "Lukas"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Bobu",
				"given": "Andreea"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Love",
				"given": "Bradley C."
			},
			{
				"family": "Grant",
				"given": "Erin"
			},
			{
				"family": "Groen",
				"given": "Iris"
			},
			{
				"family": "Achterberg",
				"given": "Jascha"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Collins",
				"given": "Katherine M."
			},
			{
				"family": "Hermann",
				"given": "Katherine L."
			},
			{
				"family": "Oktar",
				"given": "Kerem"
			},
			{
				"family": "Greff",
				"given": "Klaus"
			},
			{
				"family": "Hebart",
				"given": "Martin N."
			},
			{
				"family": "Jacoby",
				"given": "Nori"
			},
			{
				"family": "Zhang",
				"given": "Qiuyi"
			},
			{
				"family": "Marjieh",
				"given": "Raja"
			},
			{
				"family": "Geirhos",
				"given": "Robert"
			},
			{
				"family": "Chen",
				"given": "Sherol"
			},
			{
				"family": "Kornblith",
				"given": "Simon"
			},
			{
				"family": "Rane",
				"given": "Sunayana"
			},
			{
				"family": "Konkle",
				"given": "Talia"
			},
			{
				"family": "O'Connell",
				"given": "Thomas P."
			},
			{
				"family": "Unterthiner",
				"given": "Thomas"
			},
			{
				"family": "Lampinen",
				"given": "Andrew K."
			},
			{
				"family": "Müller",
				"given": "Klaus-Robert"
			},
			{
				"family": "Toneva",
				"given": "Mariya"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					2
				]
			]
		}
	},
	{
		"id": "russekInvertingCognitiveModels2023",
		"type": "paper-conference",
		"abstract": "Inferring an individual’s preferences from their observable behavior is a key step in the development of assistive decision-making technology. Although machine learning models such as neural networks could in principle be deployed toward this inference, a large amount of data is required to train such models. Here, we present an approach in which a cognitive model generates simulated data to augment limited human data. Using these data, we train a neural network to invert the model, making it possible to infer preferences from behavior. We show how this approach can be used to infer the value that people assign to food items from their eye movements when choosing between those items. We demonstrate first that neural networks can infer the latent preferences used by the model to generate simulated fixations, and second that simulated data can be beneficial in pretraining a network for predicting human-reported preferences from real fixations. Compared to inferring preferences from choice alone, this approach confers a slight improvement in predicting preferences and also allows prediction to take place prior to the choice being made. Overall, our results suggest that using a combination of neural networks and model-simulated training data is a promising approach for developing technology that infers human preferences.",
		"event-title": "NeuRIPS 2023 Workshop on Gaze Meets ML",
		"language": "en",
		"source": "openreview.net",
		"title": "Inverting cognitive models with machine learning to infer preferences from fixations",
		"URL": "https://openreview.net/forum?id=ai0ES5VAAM",
		"author": [
			{
				"family": "Russek",
				"given": "Evan"
			},
			{
				"family": "Callaway",
				"given": "Frederick"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "halpernDynamicAwareness2020",
		"type": "paper-conference",
		"abstract": "We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rˆego (2013) by adding probability, and deﬁne a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula ϕ in state s of a model M , she transitions to state s∗ in a model M ∗. We then discuss how such a model can be applied to information disclosure.",
		"container-title": "Proceedings of the Seventeenth International Conference on Principles of Knowledge Representation and Reasoning",
		"DOI": "10.24963/kr.2020/48",
		"event-place": "Rhodes, Greece",
		"event-title": "17th International Conference on Principles of Knowledge Representation and Reasoning {KR-2020}",
		"ISBN": "978-0-9992411-7-2",
		"language": "en",
		"page": "476-484",
		"publisher": "International Joint Conferences on Artificial Intelligence Organization",
		"publisher-place": "Rhodes, Greece",
		"source": "DOI.org (Crossref)",
		"title": "Dynamic Awareness",
		"URL": "https://proceedings.kr.org/2020/48",
		"author": [
			{
				"family": "Halpern",
				"given": "Joseph Y."
			},
			{
				"family": "Piermont",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					7
				]
			]
		}
	},
	{
		"id": "basichCompetenceawareSystems2023",
		"type": "article-journal",
		"abstract": "Building autonomous systems for deployment in the open world has been a longstanding objective in both artificial intelligence and robotics. The open world, however, presents challenges that question some of the assumptions often made in contemporary AI models. Autonomous systems that operate in the open world face complex, non-stationary environments wherein enumerating all situations the system may face over the course of its deployment is intractable. Nevertheless, these systems are expected to operate safely and reliably for extended durations. Consequently, AI systems often rely on some degree of human assistance to mitigate risks while completing their tasks, and are hence better treated as semi-autonomous systems. In order to reduce unnecessary reliance on humans and optimize autonomy, we propose a novel introspective planning model—competence-aware systems (CAS)—that enables a semi-autonomous system to reason about its own competence and allowed level of autonomy by leveraging human feedback or assistance. A CAS learns to adjust its level of autonomy based on experience and interactions with a human authority so as to reduce improper reliance on the human and optimize the degree of autonomy it employs in any given circumstance. To handle situations in which the initial CAS model has insufficient state information to properly discriminate feedback received from humans, we introduce a methodology called iterative state space refinement that gradually increases the granularity of the state space online. The approach exploits information that exists in the standard CAS model and requires no additional input from the human. The result is an agent that can more confidently predict the correct feedback from the human authority in each level of autonomy, enabling it learn its competence in a larger portion of the state space.",
		"container-title": "Artificial Intelligence",
		"DOI": "10.1016/j.artint.2022.103844",
		"ISSN": "0004-3702",
		"journalAbbreviation": "Artificial Intelligence",
		"page": "103844",
		"source": "ScienceDirect",
		"title": "Competence-aware systems",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0004370222001849",
		"volume": "316",
		"author": [
			{
				"family": "Basich",
				"given": "Connor"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Wray",
				"given": "Kyle H."
			},
			{
				"family": "Witwicki",
				"given": "Stefan"
			},
			{
				"family": "Biswas",
				"given": "Joydeep"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					1
				]
			]
		}
	},
	{
		"id": "svegliatoMetareasoningSafeDecision2022",
		"type": "paper-conference",
		"abstract": "Although experts carefully specify the high-level decision-making models in autonomous systems, it is infeasible to guarantee safety across every scenario during operation. We therefore propose a safety metareasoning system that optimizes the severity of the system's safety concerns and the interference to the system's task: the system executes in parallel a task process that completes a specified task and safety processes that each address a specified safety concern with a conflict resolver for arbitration. This paper offers a formal definition of a safety metareasoning system, a recommendation algorithm for a safety process, an arbitration algorithm for a conflict resolver, an application of our approach to planetary rover exploration, and a demonstration that our approach is effective in simulation.",
		"container-title": "2022 International Conference on Robotics and Automation (ICRA)",
		"DOI": "10.1109/ICRA46639.2022.9811887",
		"event-title": "2022 International Conference on Robotics and Automation (ICRA)",
		"page": "11073-11079",
		"source": "IEEE Xplore",
		"title": "Metareasoning for Safe Decision Making in Autonomous Systems",
		"URL": "https://ieeexplore.ieee.org/document/9811887",
		"author": [
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Basich",
				"given": "Connor"
			},
			{
				"family": "Saisubramanian",
				"given": "Sandhya"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "geffnerLowerBoundsImplementing2023",
		"type": "article-journal",
		"abstract": "Abraham, Dolev, Geffner, and Halpern [1] proved that, in asynchronous systems, a (k, t)-robust equilibrium for n players and a trusted mediator can be implemented without the mediator as long as n &gt; 4(k+t), where an equilibrium is (k, t)-robust if, roughly speaking, no coalition of t players can decrease the payoff of any of the other players, and no coalition of k players can increase their payoff by deviating. We prove that this bound is tight, in the sense that if n ≤ 4(k+t) there exist (k, t)-robust equilibria with a mediator that cannot be implemented by the players alone. Even though implementing (k, t)-robust mediators seems closely related to implementing asynchronous multiparty (k+t)-secure computation [6], to the best of our knowledge there is no known straightforward reduction from one problem to another. Nevertheless, we show that there is a non-trivial reduction from a slightly weaker notion of (k+t)-secure computation, which we call (k+t)-strict secure computation, to implementing (k, t)-robust mediators. We prove the desired lower bound by showing that there are functions on n variables that cannot be (k+t)-strictly securely computed if n ≤ 4(k+t). This also provides a simple alternative proof for the well-known lower bound of 4t+1 on asynchronous secure computation in the presence of up to t malicious agents [4, 8, 10].",
		"container-title": "J. ACM",
		"DOI": "10.1145/3578579",
		"ISSN": "0004-5411",
		"issue": "2",
		"page": "13:1–13:21",
		"source": "ACM Digital Library",
		"title": "Lower Bounds on Implementing Mediators in Asynchronous Systems with Rational and Malicious Agents",
		"URL": "https://doi.org/10.1145/3578579",
		"volume": "70",
		"author": [
			{
				"family": "Geffner",
				"given": "Ivan"
			},
			{
				"family": "Halpern",
				"given": "Joseph Y."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					25
				]
			]
		}
	},
	{
		"id": "huangEstablishingAppropriateTrust2018",
		"type": "article",
		"abstract": "In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",
		"DOI": "10.48550/arXiv.1810.08174",
		"note": "arXiv:1810.08174 [cs]",
		"number": "arXiv:1810.08174",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Establishing Appropriate Trust via Critical States",
		"URL": "http://arxiv.org/abs/1810.08174",
		"author": [
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Bhatia",
				"given": "Kush"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					18
				]
			]
		}
	},
	{
		"id": "mehrInferringAssistingConstraints2016",
		"type": "paper-conference",
		"abstract": "Our goal is to enable robots to better assist people with motor impairments in day-to-day tasks. Currently, such robots are teleoperated, which is tedious. It requires carefully maneuvering the robot by providing input through some interface. This is further complicated because most tasks are filled with constraints, e.g. on how much the end effector can tilt before the glass that the robot is carrying spills. Satisfying these constraints can be difficult or even impossible with the latency, bandwidth, and resolution of the input interface. We seek to make operating these robots more efficient and reduce cognitive load on the operator. Given that manipulation research is not advanced enough to make these robots autonomous in the near term, achieving this goal requires finding aspects of these tasks that are difficult for human operators to achieve, but easy to automate with current capabilities. We propose constraints are the key: maintaining task constraints is the most difficult part of the task for operators, yet it is easy to do autonomously. We introduce a method for inferring constraints from operator input, along with a confidence-based way of assisting the user in maintaining them, and evaluate in a user study.",
		"container-title": "2016 IEEE 55th Conference on Decision and Control (CDC)",
		"DOI": "10.1109/CDC.2016.7799299",
		"event-title": "2016 IEEE 55th Conference on Decision and Control (CDC)",
		"page": "6689-6696",
		"source": "IEEE Xplore",
		"title": "Inferring and assisting with constraints in shared autonomy",
		"URL": "https://ieeexplore.ieee.org/document/7799299",
		"author": [
			{
				"family": "Mehr",
				"given": "Negar"
			},
			{
				"family": "Horowitz",
				"given": "Roberto"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					12
				]
			]
		}
	},
	{
		"id": "fisacGeneratingPlansThat2018",
		"type": "article",
		"abstract": "Collaboration requires coordination, and we coordinate by anticipating our teammates' future actions and adapting to their plan. In some cases, our teammates' actions early on can give us a clear idea of what the remainder of their plan is, i.e. what action sequence we should expect. In others, they might leave us less confident, or even lead us to the wrong conclusion. Our goal is for robot actions to fall in the first category: we want to enable robots to select their actions in such a way that human collaborators can easily use them to correctly anticipate what will follow. While previous work has focused on finding initial plans that convey a set goal, here we focus on finding two portions of a plan such that the initial portion conveys the final one. We introduce $t$-\\ACty{}: a measure that quantifies the accuracy and confidence with which human observers can predict the remaining robot plan from the overall task goal and the observed initial $t$ actions in the plan. We contribute a method for generating $t$-predictable plans: we search for a full plan that accomplishes the task, but in which the first $t$ actions make it as easy as possible to infer the remaining ones. The result is often different from the most efficient plan, in which the initial actions might leave a lot of ambiguity as to how the task will be completed. Through an online experiment and an in-person user study with physical robots, we find that our approach outperforms a traditional efficiency-based planner in objective and subjective collaboration metrics.",
		"DOI": "10.48550/arXiv.1802.05250",
		"note": "arXiv:1802.05250 [cs]",
		"number": "arXiv:1802.05250",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generating Plans that Predict Themselves",
		"URL": "http://arxiv.org/abs/1802.05250",
		"author": [
			{
				"family": "Fisac",
				"given": "Jaime F."
			},
			{
				"family": "Liu",
				"given": "Chang"
			},
			{
				"family": "Hamrick",
				"given": "Jessica B."
			},
			{
				"family": "Sastry",
				"given": "S. Shankar"
			},
			{
				"family": "Hedrick",
				"given": "J. Karl"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					14
				]
			]
		}
	},
	{
		"id": "fisacGeneralSafetyFramework2018",
		"type": "article",
		"abstract": "The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when (a) the computed safety guarantees require it, or (b) confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.",
		"DOI": "10.48550/arXiv.1705.01292",
		"note": "arXiv:1705.01292 [cs]",
		"number": "arXiv:1705.01292",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems",
		"URL": "http://arxiv.org/abs/1705.01292",
		"author": [
			{
				"family": "Fisac",
				"given": "Jaime F."
			},
			{
				"family": "Akametalu",
				"given": "Anayo K."
			},
			{
				"family": "Zeilinger",
				"given": "Melanie N."
			},
			{
				"family": "Kaynama",
				"given": "Shahab"
			},
			{
				"family": "Gillula",
				"given": "Jeremy"
			},
			{
				"family": "Tomlin",
				"given": "Claire J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					14
				]
			]
		}
	},
	{
		"id": "huangEnablingRobotsCommunicate2017",
		"type": "paper-conference",
		"abstract": "The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.",
		"container-title": "Robotics: Science and Systems XIII",
		"DOI": "10.15607/RSS.2017.XIII.059",
		"note": "arXiv:1702.03465 [cs]",
		"source": "arXiv.org",
		"title": "Enabling Robots to Communicate their Objectives",
		"URL": "http://arxiv.org/abs/1702.03465",
		"author": [
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Held",
				"given": "David"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					7,
					12
				]
			]
		}
	},
	{
		"id": "milliShouldRobotsBe2017",
		"type": "article",
		"abstract": "Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",
		"DOI": "10.48550/arXiv.1705.09990",
		"note": "arXiv:1705.09990 [cs]",
		"number": "arXiv:1705.09990",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Should Robots be Obedient?",
		"URL": "http://arxiv.org/abs/1705.09990",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					5,
					28
				]
			]
		}
	},
	{
		"id": "hadfield-menellOffSwitchGame2017",
		"type": "article",
		"abstract": "It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",
		"DOI": "10.48550/arXiv.1611.08219",
		"note": "arXiv:1611.08219 [cs]",
		"number": "arXiv:1611.08219",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Off-Switch Game",
		"URL": "http://arxiv.org/abs/1611.08219",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					6,
					15
				]
			]
		}
	},
	{
		"id": "kwonExpressingRobotIncapability2018",
		"type": "paper-conference",
		"abstract": "Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.",
		"collection-title": "HRI '18",
		"container-title": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3171221.3171276",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-4953-6",
		"page": "87–95",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Expressing Robot Incapability",
		"URL": "https://doi.org/10.1145/3171221.3171276",
		"author": [
			{
				"family": "Kwon",
				"given": "Minae"
			},
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					26
				]
			]
		}
	},
	{
		"id": "bajcsyLearningPhysicalHuman2018",
		"type": "paper-conference",
		"abstract": "We focus on learning robot objective functions from human guidance: specifically, from physical corrections provided by the person while the robot is acting. Objective functions are typically parametrized in terms of features, which capture aspects of the task that might be important. When the person intervenes to correct the robot»s behavior, the robot should update its understanding of which features matter, how much, and in what way. Unfortunately, real users do not provide optimal corrections that isolate exactly what the robot was doing wrong. Thus, when receiving a correction, it is difficult for the robot to determine which features the person meant to correct, and which features were changed unintentionally. In this paper, we propose to improve the efficiency of robot learning during physical interactions by reducing unintended learning. Our approach allows the human-robot team to focus on learning one feature at a time, unlike state-of-the-art techniques that update all features at once. We derive an online method for identifying the single feature which the human is trying to change during physical interaction, and experimentally compare this one-at-a-time approach to the all-at-once baseline in a user study. Our results suggest that users teaching one-at-a-time perform better, especially in tasks that require changing multiple features.",
		"collection-title": "HRI '18",
		"container-title": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3171221.3171267",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-4953-6",
		"page": "141–149",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Learning from Physical Human Corrections, One Feature at a Time",
		"URL": "https://doi.org/10.1145/3171221.3171267",
		"author": [
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "Losey",
				"given": "Dylan P."
			},
			{
				"family": "O'Malley",
				"given": "Marcia K."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					26
				]
			]
		}
	},
	{
		"id": "liRobustMultiAgentReinforcement2019",
		"type": "article-journal",
		"abstract": "Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v33i01.33014213",
		"ISSN": "2374-3468",
		"issue": "01",
		"language": "en",
		"license": "Copyright (c) 2019 Association for the Advancement of Artificial Intelligence",
		"note": "number: 01",
		"page": "4213-4220",
		"source": "ojs.aaai.org",
		"title": "Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/4327",
		"volume": "33",
		"author": [
			{
				"family": "Li",
				"given": "Shihui"
			},
			{
				"family": "Wu",
				"given": "Yi"
			},
			{
				"family": "Cui",
				"given": "Xinyue"
			},
			{
				"family": "Dong",
				"given": "Honghua"
			},
			{
				"family": "Fang",
				"given": "Fei"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					17
				]
			]
		}
	},
	{
		"id": "duAvEAssistanceEmpowerment2020",
		"type": "paper-conference",
		"abstract": "One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s).  Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective increases the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "4560–4571",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "AvE: Assistance via Empowerment",
		"title-short": "AvE",
		"URL": "https://proceedings.neurips.cc/paper/2020/hash/30de9ece7cf3790c8c39ccff1a044209-Abstract.html",
		"volume": "33",
		"author": [
			{
				"family": "Du",
				"given": "Yuqing"
			},
			{
				"family": "Tiomkin",
				"given": "Stas"
			},
			{
				"family": "Kiciman",
				"given": "Emre"
			},
			{
				"family": "Polani",
				"given": "Daniel"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "fickingerMultiPrincipalAssistanceGames2020",
		"type": "article",
		"abstract": "We introduce the concept of a multi-principal assistance game (MPAG), and circumvent an obstacle in social choice theory — Gibbard’s theorem — by using a sufﬁciently “collegial” preference inference mechanism. In an MPAG, a single agent assists N human principals who may have widely different preferences. MPAGs generalize assistance games, also known as cooperative inverse reinforcement learning games. We analyze in particular a generalization of apprenticeship learning in which the humans ﬁrst perform some work to obtain utility and demonstrate their preferences, and then the robot acts to further maximize the sum of human payoffs. We show in this setting that if the game is sufﬁciently collegial — i.e., if the humans are responsible for obtaining a sufﬁcient fraction of the rewards through their own actions — then their preferences are straightforwardly revealed through their work. This revelation mechanism is non-dictatorial, does not limit the possible outcomes to two alternatives, and is dominant-strategy incentive-compatible.",
		"language": "en",
		"note": "arXiv:2012.14536 [cs]",
		"number": "arXiv:2012.14536",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-Principal Assistance Games: Definition and Collegial Mechanisms",
		"title-short": "Multi-Principal Assistance Games",
		"URL": "http://arxiv.org/abs/2012.14536",
		"author": [
			{
				"family": "Fickinger",
				"given": "Arnaud"
			},
			{
				"family": "Zhuang",
				"given": "Simon"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					28
				]
			]
		}
	},
	{
		"id": "shahBenefitsAssistanceReward2020",
		"type": "article-journal",
		"abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.",
		"language": "en",
		"source": "openreview.net",
		"title": "Benefits of Assistance over Reward Learning",
		"URL": "https://openreview.net/forum?id=DFIoGDZejIB",
		"author": [
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Freire",
				"given": "Pedro"
			},
			{
				"family": "Alex",
				"given": "Neel"
			},
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Dennis",
				"given": "Michael D."
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					2
				]
			]
		}
	},
	{
		"id": "gatesHowBeHelpful2020",
		"type": "article-journal",
		"abstract": "When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decisionmaker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artiﬁcial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.",
		"container-title": "Cognitive Science",
		"DOI": "10.1111/cogs.12841",
		"ISSN": "0364-0213, 1551-6709",
		"issue": "6",
		"journalAbbreviation": "Cognitive Science",
		"language": "en",
		"page": "e12841",
		"source": "DOI.org (Crossref)",
		"title": "How to Be Helpful to Multiple People at Once",
		"URL": "https://onlinelibrary.wiley.com/doi/10.1111/cogs.12841",
		"volume": "44",
		"author": [
			{
				"family": "Gates",
				"given": "Vael"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6
				]
			]
		}
	},
	{
		"id": "bajcsyRobustControlFramework2021",
		"type": "article-journal",
		"abstract": "Designing human motion predictors which preserve safety while maintaining robot efficiency is an increasingly important challenge for robots operating in close physical proximity to people. One approach is to use robust control predictors that safeguard against every possible future human state, leading to safe but often too conservative robot plans. Alternatively, intent-driven predictors explicitly model how humans make decisions given their intent, leading to efficient robot plans. However, when the intent model is misspecified, the robot might confidently plan unsafe maneuvers. In this letter, we combine ideas from robust control and intent-driven human modelling to formulate a novel human motion predictor which provides robustness against misspecified human models, but reduces the conservatism of traditional worst-case predictors. Our approach predicts the human states by trusting the intent-driven model to decide only which human actions are completely unlikely. We then safeguard against all likely enough actions, much like a robust control predictor. We demonstrate in simulation and hardware how our approach safeguards against misspecified human intent models while not leading to overly conservative robot plans.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2020.3028049",
		"ISSN": "2377-3766",
		"issue": "1",
		"note": "event-title: IEEE Robotics and Automation Letters",
		"page": "24-31",
		"source": "IEEE Xplore",
		"title": "A Robust Control Framework for Human Motion Prediction",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9210199",
		"volume": "6",
		"author": [
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "Bansal",
				"given": "Somil"
			},
			{
				"family": "Ratner",
				"given": "Ellis"
			},
			{
				"family": "Tomlin",
				"given": "Claire J."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1
				]
			]
		}
	},
	{
		"id": "knottEvaluatingRobustnessCollaborative2021",
		"type": "article",
		"abstract": "In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are robust. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of unit testing in software engineering. Speciﬁcally, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in possible partner behavior and possible states encountered, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We ﬁnd that the test suite provides signiﬁcant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",
		"language": "en",
		"note": "arXiv:2101.05507 [cs]",
		"number": "arXiv:2101.05507",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating the Robustness of Collaborative Agents",
		"URL": "http://arxiv.org/abs/2101.05507",
		"author": [
			{
				"family": "Knott",
				"given": "Paul"
			},
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Devlin",
				"given": "Sam"
			},
			{
				"family": "Ciosek",
				"given": "Kamil"
			},
			{
				"family": "Hofmann",
				"given": "Katja"
			},
			{
				"family": "Dragan",
				"given": "A. D."
			},
			{
				"family": "Shah",
				"given": "Rohin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					14
				]
			]
		}
	},
	{
		"id": "biyikLearningHumansAdaptive2022",
		"type": "paper-conference",
		"abstract": "Robots that will cooperate (or even compete) with humans should understand their goals and preferences. Humans leak and provide a lot of data, e.g., they take actions to achieve their goals, they make choices between multiple options, they use language or gestures to convey information. And we, as humans, are usually very good at using all these available information: we can easily understand what another person is trying to do just by watching them for a while. The goal of my research is to equip robots with the capability of using multiple modes of information sources. For this, I propose using a Bayesian learning approach, and show how it is useful in a variety of applications ranging from exoskeleton gait optimization to traffic routing.",
		"container-title": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
		"DOI": "10.1109/HRI53351.2022.9889436",
		"event-title": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
		"page": "1152-1154",
		"source": "IEEE Xplore",
		"title": "Learning from Humans for Adaptive Interaction",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9889436",
		"author": [
			{
				"family": "Biyik",
				"given": "Erdem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "sumersHowTalkAI2024",
		"type": "paper-conference",
		"abstract": "From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: instructions, which provide information about the desired policy, and descriptions, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about how the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that obey language to agents that learn from it.",
		"collection-title": "NIPS '22",
		"container-title": "Proceedings of the 36th International Conference on Neural Information Processing Systems",
		"event-place": "Red Hook, NY, USA",
		"ISBN": "978-1-7138-7108-8",
		"page": "34762–34775",
		"publisher": "Curran Associates Inc.",
		"publisher-place": "Red Hook, NY, USA",
		"source": "ACM Digital Library",
		"title": "How to talk so AI will learn: instructions, descriptions, and autonomy",
		"title-short": "How to talk so AI will learn",
		"author": [
			{
				"family": "Sumers",
				"given": "Theodore R."
			},
			{
				"family": "Hawkins",
				"given": "Robert D."
			},
			{
				"family": "Ho",
				"given": "Mark K."
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					3
				]
			]
		}
	},
	{
		"id": "youngPlayfulInteractionsRepresentation2021",
		"type": "article",
		"abstract": "One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations are becoming easier with teleoperation methods and the use of low-cost assistive tools, we often still require 100-1000 demonstrations for every task to learn a visual representation and policy. To address this, we turn to an alternate form of data that does not require task-specific demonstrations -- play. Playing is a fundamental method children use to learn a set of skills and behaviors and visual representations in early learning. Importantly, play data is diverse, task-agnostic, and relatively cheap to obtain. In this work, we propose to use playful interactions in a self-supervised manner to learn visual representations for downstream tasks. We collect 2 hours of playful data in 19 diverse environments and use self-predictive learning to extract visual representations. Given these representations, we train policies using imitation learning for two downstream tasks: Pushing and Stacking. We demonstrate that our visual representations generalize better than standard behavior cloning and can achieve similar performance with only half the number of required demonstrations. Our representations, which are trained from scratch, compare favorably against ImageNet pretrained representations. Finally, we provide an experimental analysis on the effects of different pretraining modes on downstream task learning.",
		"DOI": "10.48550/arXiv.2107.09046",
		"note": "arXiv:2107.09046 [cs]",
		"number": "arXiv:2107.09046",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Playful Interactions for Representation Learning",
		"URL": "http://arxiv.org/abs/2107.09046",
		"author": [
			{
				"family": "Young",
				"given": "Sarah"
			},
			{
				"family": "Pari",
				"given": "Jyothish"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Pinto",
				"given": "Lerrel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					19
				]
			]
		}
	},
	{
		"id": "tianSafetyAssurancesHumanRobot2021",
		"type": "article",
		"abstract": "An outstanding challenge with safety methods for human-robot interaction is reducing their conservatism while maintaining robustness to variations in human behavior. In this work, we propose that robots use confidence-aware game-theoretic models of human behavior when assessing the safety of a human-robot interaction. By treating the influence between the human and robot as well as the human's rationality as unobserved latent states, we succinctly infer the degree to which a human is following the game-theoretic interaction model. We leverage this model to restrict the set of feasible human controls during safety verification, enabling the robot to confidently modulate the conservatism of its safety monitor online. Evaluations in simulated human-robot scenarios and ablation studies demonstrate that imbuing safety monitors with confidence-aware game-theoretic models enables both safe and efficient human-robot interaction. Moreover, evaluations with real traffic data show that our safety monitor is less conservative than traditional safety methods in real human driving scenarios.",
		"DOI": "10.48550/arXiv.2109.14700",
		"note": "arXiv:2109.14700 [cs]",
		"number": "arXiv:2109.14700",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safety Assurances for Human-Robot Interaction via Confidence-aware Game-theoretic Human Models",
		"URL": "http://arxiv.org/abs/2109.14700",
		"author": [
			{
				"family": "Tian",
				"given": "Ran"
			},
			{
				"family": "Sun",
				"given": "Liting"
			},
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "Tomizuka",
				"given": "Masayoshi"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					30
				]
			]
		}
	},
	{
		"id": "bobuAligningRobotHuman2024",
		"type": "paper-conference",
		"abstract": "To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behaviour. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot’s learned representation does not capture the human’s representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.",
		"container-title": "Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3610977.3634987",
		"language": "en",
		"note": "arXiv:2302.01928 [cs]",
		"page": "42-54",
		"source": "arXiv.org",
		"title": "Aligning Robot and Human Representations",
		"URL": "http://arxiv.org/abs/2302.01928",
		"author": [
			{
				"family": "Bobu",
				"given": "Andreea"
			},
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Agrawal",
				"given": "Pulkit"
			},
			{
				"family": "Shah",
				"given": "Julie"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					11
				]
			]
		}
	},
	{
		"id": "hongLearningInfluenceHuman2023",
		"type": "article",
		"abstract": "When interacting with people, AI agents do not just influence the state of the world – they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. Accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. Instead, we focus on influence in settings where there is a need to capture human suboptimality. For instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well – how could an agent influence them towards more optimal behavior? Assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. But experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. Hence, we focus on learning from an offline dataset of human-human interactions. Our observation is that offline reinforcement learning (RL) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. We demonstrate that offline RL can solve two challenges with effective influence. First, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks – none of which contains examples of successful influence – an agent can learn influence strategies to steer humans towards better performance even on new tasks. Second, we show that by also modeling and conditioning on human behavior, offline RL can learn to affect not just the human’s actions but also their underlying strategy, and adapt to changes in their strategy.",
		"language": "en",
		"note": "arXiv:2303.02265 [cs]",
		"number": "arXiv:2303.02265",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Influence Human Behavior with Offline Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2303.02265",
		"author": [
			{
				"family": "Hong",
				"given": "Joey"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "aminRepeatedInverseReinforcement2017",
		"type": "article",
		"abstract": "We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.",
		"DOI": "10.48550/arXiv.1705.05427",
		"note": "arXiv:1705.05427 [cs]",
		"number": "arXiv:1705.05427",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Repeated Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1705.05427",
		"author": [
			{
				"family": "Amin",
				"given": "Kareem"
			},
			{
				"family": "Jiang",
				"given": "Nan"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					11,
					3
				]
			]
		}
	},
	{
		"id": "hadfield-menellCooperativeInverseReinforcement2024a",
		"type": "article",
		"abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",
		"DOI": "10.48550/arXiv.1606.03137",
		"note": "arXiv:1606.03137 [cs]",
		"number": "arXiv:1606.03137",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Cooperative Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1606.03137",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					17
				]
			]
		}
	},
	{
		"id": "hadfield-menellInverseRewardDesign2020",
		"type": "article",
		"abstract": "Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",
		"DOI": "10.48550/arXiv.1711.02827",
		"note": "arXiv:1711.02827 [cs]",
		"number": "arXiv:1711.02827",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Inverse Reward Design",
		"URL": "http://arxiv.org/abs/1711.02827",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					7
				]
			]
		}
	},
	{
		"id": "mindermannActiveInverseReward2019",
		"type": "article",
		"abstract": "Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",
		"DOI": "10.48550/arXiv.1809.03060",
		"note": "arXiv:1809.03060 [cs, stat]",
		"number": "arXiv:1809.03060",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Active Inverse Reward Design",
		"URL": "http://arxiv.org/abs/1809.03060",
		"author": [
			{
				"family": "Mindermann",
				"given": "Sören"
			},
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					11,
					6
				]
			]
		}
	},
	{
		"id": "landolfiSocialCohesionAutonomous2018",
		"type": "article",
		"abstract": "Autonomous cars can perform poorly for many reasons. They may have perception issues, incorrect dynamics models, be unaware of obscure rules of human traffic systems, or follow certain rules too conservatively. Regardless of the exact failure mode of the car, often human drivers around the car are behaving correctly. For example, even if the car does not know that it should pull over when an ambulance races by, other humans on the road will know and will pull over. We propose to make socially cohesive cars that leverage the behavior of nearby human drivers to act in ways that are safer and more socially acceptable. The simple intuition behind our algorithm is that if all the humans are consistently behaving in a particular way, then the autonomous car probably should too. We analyze the performance of our algorithm in a variety of scenarios and conduct a user study to assess people's attitudes towards socially cohesive cars. We find that people are surprisingly tolerant of mistakes that cohesive cars might make in order to get the benefits of driving in a car with a safer, or even just more socially acceptable behavior.",
		"DOI": "10.48550/arXiv.1808.03845",
		"note": "arXiv:1808.03845 [cs]",
		"number": "arXiv:1808.03845",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Social Cohesion in Autonomous Driving",
		"URL": "http://arxiv.org/abs/1808.03845",
		"author": [
			{
				"family": "Landolfi",
				"given": "Nicholas C."
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					27
				]
			]
		}
	},
	{
		"id": "ratnerSimplifyingRewardDesign2018",
		"type": "article",
		"abstract": "Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-and-conquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We find that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.",
		"DOI": "10.48550/arXiv.1806.02501",
		"note": "arXiv:1806.02501 [cs]",
		"number": "arXiv:1806.02501",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simplifying Reward Design through Divide-and-Conquer",
		"URL": "http://arxiv.org/abs/1806.02501",
		"author": [
			{
				"family": "Ratner",
				"given": "Ellis"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					6
				]
			]
		}
	},
	{
		"id": "malikEfficientGeneralizedBellman2018",
		"type": "article",
		"abstract": "Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",
		"DOI": "10.48550/arXiv.1806.03820",
		"note": "arXiv:1806.03820 [cs]",
		"number": "arXiv:1806.03820",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1806.03820",
		"author": [
			{
				"family": "Malik",
				"given": "Dhruv"
			},
			{
				"family": "Palaniappan",
				"given": "Malayandi"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					11
				]
			]
		}
	},
	{
		"id": "basuLearningRicherHuman2018",
		"type": "paper-conference",
		"abstract": "We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.",
		"collection-title": "HRI '18",
		"container-title": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3171221.3171284",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-4953-6",
		"page": "132–140",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries",
		"title-short": "Learning from Richer Human Guidance",
		"URL": "https://doi.org/10.1145/3171221.3171284",
		"author": [
			{
				"family": "Basu",
				"given": "Chandrayee"
			},
			{
				"family": "Singhal",
				"given": "Mukesh"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					2,
					26
				]
			]
		}
	},
	{
		"id": "cundyExploringHierarchyAwareInverse2018",
		"type": "article",
		"abstract": "We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.",
		"DOI": "10.48550/arXiv.1807.05037",
		"note": "arXiv:1807.05037 [cs]",
		"number": "arXiv:1807.05037",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Exploring Hierarchy-Aware Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1807.05037",
		"author": [
			{
				"family": "Cundy",
				"given": "Chris"
			},
			{
				"family": "Filan",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					13
				]
			]
		}
	},
	{
		"id": "pengVariationalDiscriminatorBottleneck2020",
		"type": "article",
		"abstract": "Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \\emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",
		"DOI": "10.48550/arXiv.1810.00821",
		"note": "arXiv:1810.00821 [cs, stat]",
		"number": "arXiv:1810.00821",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow",
		"title-short": "Variational Discriminator Bottleneck",
		"URL": "http://arxiv.org/abs/1810.00821",
		"author": [
			{
				"family": "Peng",
				"given": "Xue Bin"
			},
			{
				"family": "Kanazawa",
				"given": "Angjoo"
			},
			{
				"family": "Toyer",
				"given": "Sam"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					24
				]
			]
		}
	},
	{
		"id": "milliLiteralPedagogicHuman2019",
		"type": "article",
		"abstract": "It is incredibly easy for a system designer to misspecify the objective for an autonomous system (\"robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",
		"DOI": "10.48550/arXiv.1903.03877",
		"note": "arXiv:1903.03877 [cs]",
		"number": "arXiv:1903.03877",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning",
		"title-short": "Literal or Pedagogic Human?",
		"URL": "http://arxiv.org/abs/1903.03877",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					28
				]
			]
		}
	},
	{
		"id": "reddyLearningHumanObjectives2021",
		"type": "article",
		"abstract": "We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",
		"DOI": "10.48550/arXiv.1912.05652",
		"note": "arXiv:1912.05652 [cs, stat]",
		"number": "arXiv:1912.05652",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Human Objectives by Evaluating Hypothetical Behavior",
		"URL": "http://arxiv.org/abs/1912.05652",
		"author": [
			{
				"family": "Reddy",
				"given": "Siddharth"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Legg",
				"given": "Shane"
			},
			{
				"family": "Leike",
				"given": "Jan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					24
				]
			]
		}
	},
	{
		"id": "garrabrantLogicalInduction2020",
		"type": "article",
		"abstract": "We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\\pi$ are difficult to predict, then a logical inductor learns to assign $\\approx 10\\%$ probability to \"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\\phi \\implies \\psi$, $\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\\phi$ is associated with a stock that is worth \\$1 per share if [...]",
		"DOI": "10.48550/arXiv.1609.03543",
		"note": "arXiv:1609.03543 [cs, math]",
		"number": "arXiv:1609.03543",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Logical Induction",
		"URL": "http://arxiv.org/abs/1609.03543",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Benson-Tilsen",
				"given": "Tsvi"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					7
				]
			]
		}
	},
	{
		"id": "leikeFormalSolutionGrain2016",
		"type": "article",
		"abstract": "A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (in other words, its prior contains a \\emph{grain of truth}). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the \\emph{grain of truth problem}. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play {\\epsilon}-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.",
		"DOI": "10.48550/arXiv.1609.05058",
		"note": "arXiv:1609.05058 [cs]",
		"number": "arXiv:1609.05058",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Formal Solution to the Grain of Truth Problem",
		"URL": "http://arxiv.org/abs/1609.05058",
		"author": [
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			},
			{
				"family": "Fallenstein",
				"given": "Benya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					9,
					16
				]
			]
		}
	},
	{
		"id": "yudkowskyFunctionalDecisionTheory2018",
		"type": "article",
		"abstract": "This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, \"Which output of this very function would yield the best outcome?\" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",
		"DOI": "10.48550/arXiv.1710.05060",
		"note": "arXiv:1710.05060 [cs]",
		"number": "arXiv:1710.05060",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Functional Decision Theory: A New Theory of Instrumental Rationality",
		"title-short": "Functional Decision Theory",
		"URL": "http://arxiv.org/abs/1710.05060",
		"author": [
			{
				"family": "Yudkowsky",
				"given": "Eliezer"
			},
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					22
				]
			]
		}
	},
	{
		"id": "fallensteinProofProducingReflectionHOL2015",
		"type": "paper-conference",
		"abstract": "We present a reflection principle of the form “If $$\\ulcorner \\varphi \\urcorner $$⌜φ⌝is provable, then $$\\varphi $$φ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure $$\\varphi $$φhas the same meaning both inside and outside of this model. Soundness of HOL implies that if $$\\ulcorner \\varphi \\urcorner $$⌜φ⌝is provable, then it is true in this model, and hence $$\\varphi $$φholds. We additionally show how this reflection principle can be extended, assuming an infinite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.",
		"container-title": "Interactive Theorem Proving",
		"DOI": "10.1007/978-3-319-22102-1_11",
		"event-place": "Cham",
		"ISBN": "978-3-319-22102-1",
		"language": "en",
		"page": "170-186",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "Springer Link",
		"title": "Proof-Producing Reflection for HOL",
		"author": [
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Kumar",
				"given": "Ramana"
			}
		],
		"editor": [
			{
				"family": "Urban",
				"given": "Christian"
			},
			{
				"family": "Zhang",
				"given": "Xingyuan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "soaresValueLearningProblem2018",
		"type": "chapter",
		"abstract": "Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its designers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.",
		"container-title": "Artificial Intelligence Safety and Security",
		"edition": "1",
		"event-place": "First edition. | Boca Raton, FL : CRC Press/Taylor & Francis Group, 2018.",
		"ISBN": "978-1-351-25138-9",
		"language": "en",
		"note": "DOI: 10.1201/9781351251389-7",
		"page": "89-97",
		"publisher": "Chapman and Hall/CRC",
		"publisher-place": "First edition. | Boca Raton, FL : CRC Press/Taylor & Francis Group, 2018.",
		"source": "DOI.org (Crossref)",
		"title": "The Value Learning Problem",
		"URL": "https://www.taylorfrancis.com/books/9781351251372/chapters/10.1201/9781351251389-7",
		"editor": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					27
				]
			]
		}
	},
	{
		"id": "benson-tilsenFormalizingConvergentInstrumental2016",
		"type": "article",
		"abstract": "Omohundro has argued that sufﬁciently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources. Omohundro refers to these as “basic AI drives,” and he, along with Bostrom and others, has argued that this means great care must be taken when designing powerful autonomous systems, because even if they have harmless goals, the side effects of pursuing those goals may be quite harmful. These arguments, while intuitively compelling, are primarily philosophical. In this paper, we provide formal models that demonstrate Omohundro’s thesis, thereby putting mathematical weight behind those intuitive claims.",
		"language": "en",
		"source": "Zotero",
		"title": "Formalizing Convergent Instrumental Goals",
		"author": [
			{
				"family": "Benson-Tilsen",
				"given": "Tsvi"
			},
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "garrabrantTemporalInferenceFinite2021",
		"type": "article",
		"abstract": "We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",
		"DOI": "10.48550/arXiv.2109.11513",
		"note": "arXiv:2109.11513 [cs, math]",
		"number": "arXiv:2109.11513",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Temporal Inference with Finite Factored Sets",
		"URL": "http://arxiv.org/abs/2109.11513",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					9,
					23
				]
			]
		}
	},
	{
		"id": "garrabrantCartesianFrames2021",
		"type": "article",
		"abstract": "We introduce a novel framework, the theory of Cartesian frames (CF), that gives powerful tools for manipulating sets of acts. The CF framework takes as its most fundamental building block that an agent can freely choose from a set of available actions. The framework uses the mathematics of Chu spaces to develop a calculus of those sets of actions, how those actions change at various levels of description, and how different agents' actions can combine when agents work in concert. We discuss how this framework might provide an illuminating perspective on issues in decision theory and formal epistemology.",
		"DOI": "10.48550/arXiv.2109.10996",
		"note": "arXiv:2109.10996 [math]",
		"number": "arXiv:2109.10996",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Cartesian Frames",
		"URL": "http://arxiv.org/abs/2109.10996",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Herrmann",
				"given": "Daniel A."
			},
			{
				"family": "Lopez-Wild",
				"given": "Josiah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					9,
					22
				]
			]
		}
	},
	{
		"id": "hubingerOverview11Proposals2020",
		"type": "article",
		"abstract": "This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.",
		"DOI": "10.48550/arXiv.2012.07532",
		"note": "arXiv:2012.07532 [cs]",
		"number": "arXiv:2012.07532",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An overview of 11 proposals for building safe advanced AI",
		"URL": "http://arxiv.org/abs/2012.07532",
		"author": [
			{
				"family": "Hubinger",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					4
				]
			]
		}
	},
	{
		"id": "demskiEmbeddedAgency2020",
		"type": "article",
		"abstract": "Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts. We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type \"function\"; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.",
		"DOI": "10.48550/arXiv.1902.09469",
		"note": "arXiv:1902.09469 [cs]",
		"number": "arXiv:1902.09469",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Embedded Agency",
		"URL": "http://arxiv.org/abs/1902.09469",
		"author": [
			{
				"family": "Demski",
				"given": "Abram"
			},
			{
				"family": "Garrabrant",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					6
				]
			]
		}
	},
	{
		"id": "armstrongOccamRazorInsufficient2019",
		"type": "article",
		"abstract": "Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",
		"DOI": "10.48550/arXiv.1712.05812",
		"note": "arXiv:1712.05812 [cs]",
		"number": "arXiv:1712.05812",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Occam's razor is insufficient to infer the preferences of irrational agents",
		"URL": "http://arxiv.org/abs/1712.05812",
		"author": [
			{
				"family": "Armstrong",
				"given": "Stuart"
			},
			{
				"family": "Mindermann",
				"given": "Sören"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					11
				]
			]
		}
	},
	{
		"id": "manheimCategorizingVariantsGoodhart2019a",
		"type": "article",
		"abstract": "There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are \"(at least) four different mechanisms\" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",
		"DOI": "10.48550/arXiv.1803.04585",
		"note": "arXiv:1803.04585 [cs, q-fin, stat]",
		"number": "arXiv:1803.04585",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Categorizing Variants of Goodhart's Law",
		"URL": "http://arxiv.org/abs/1803.04585",
		"author": [
			{
				"family": "Manheim",
				"given": "David"
			},
			{
				"family": "Garrabrant",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					2,
					24
				]
			]
		}
	},
	{
		"id": "critchNegotiableReinforcementLearning2017",
		"type": "article",
		"abstract": "Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\\\"{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.",
		"DOI": "10.48550/arXiv.1701.01302",
		"note": "arXiv:1701.01302 [cs]",
		"number": "arXiv:1701.01302",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making",
		"title-short": "Toward negotiable reinforcement learning",
		"URL": "http://arxiv.org/abs/1701.01302",
		"author": [
			{
				"family": "Critch",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					5,
					13
				]
			]
		}
	},
	{
		"id": "garrabrantFormalApproachProblem2017",
		"type": "article-journal",
		"abstract": "We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.",
		"container-title": "Electronic Proceedings in Theoretical Computer Science",
		"DOI": "10.4204/EPTCS.251.16",
		"ISSN": "2075-2180",
		"journalAbbreviation": "Electron. Proc. Theor. Comput. Sci.",
		"note": "arXiv:1707.08747 [cs]",
		"page": "221-235",
		"source": "arXiv.org",
		"title": "A Formal Approach to the Problem of Logical Non-Omniscience",
		"URL": "http://arxiv.org/abs/1707.08747",
		"volume": "251",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Benson-Tilsen",
				"given": "Tsvi"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					7,
					25
				]
			]
		}
	},
	{
		"id": "graceWhenWillAI2018",
		"type": "article",
		"abstract": "Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.",
		"DOI": "10.48550/arXiv.1705.08807",
		"note": "arXiv:1705.08807 [cs]",
		"number": "arXiv:1705.08807",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "When Will AI Exceed Human Performance? Evidence from AI Experts",
		"title-short": "When Will AI Exceed Human Performance?",
		"URL": "http://arxiv.org/abs/1705.08807",
		"author": [
			{
				"family": "Grace",
				"given": "Katja"
			},
			{
				"family": "Salvatier",
				"given": "John"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Evans",
				"given": "Owain"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					3
				]
			]
		}
	},
	{
		"id": "kosoyForecastingUsingIncomplete2019",
		"type": "article",
		"abstract": "We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is \"suspected\" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.",
		"DOI": "10.48550/arXiv.1705.04630",
		"note": "arXiv:1705.04630 [cs]",
		"number": "arXiv:1705.04630",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Forecasting using incomplete models",
		"URL": "http://arxiv.org/abs/1705.04630",
		"author": [
			{
				"family": "Kosoy",
				"given": "Vanessa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					16
				]
			]
		}
	},
	{
		"id": "critchParametricResourceBoundedGeneralization2019",
		"type": "article-journal",
		"abstract": "This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.",
		"container-title": "The Journal of Symbolic Logic",
		"DOI": "10.1017/jsl.2017.42",
		"ISSN": "0022-4812, 1943-5886",
		"issue": "4",
		"language": "en",
		"page": "1368-1381",
		"source": "Cambridge University Press",
		"title": "A Parametric, Resource-Bounded Generalization of Löb’s Theorem, and a Robust Cooperation Criterion for Open-Source Game Theory",
		"URL": "https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79",
		"volume": "84",
		"author": [
			{
				"family": "Critch",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12
				]
			]
		}
	},
	{
		"id": "critchParametricBoundedLob2016",
		"type": "article",
		"abstract": "L\\\"ob's theorem and G\\\"odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\\\"ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas \"L\\\"obian\" cooperation is much more robust and agnostic of the opponent's implementation.",
		"DOI": "10.48550/arXiv.1602.04184",
		"note": "arXiv:1602.04184 [cs]",
		"number": "arXiv:1602.04184",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents",
		"URL": "http://arxiv.org/abs/1602.04184",
		"author": [
			{
				"family": "Critch",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					8,
					24
				]
			]
		}
	},
	{
		"id": "garrabrantInductiveCoherence2016",
		"type": "article",
		"abstract": "While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.",
		"DOI": "10.48550/arXiv.1604.05288",
		"note": "arXiv:1604.05288 [cs, math]",
		"number": "arXiv:1604.05288",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Inductive Coherence",
		"URL": "http://arxiv.org/abs/1604.05288",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Fallenstein",
				"given": "Benya"
			},
			{
				"family": "Demski",
				"given": "Abram"
			},
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					10,
					7
				]
			]
		}
	},
	{
		"id": "garrabrantAsymptoticConvergenceOnline2016",
		"type": "article",
		"abstract": "We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.",
		"DOI": "10.48550/arXiv.1604.05280",
		"note": "arXiv:1604.05280 [cs, math]",
		"number": "arXiv:1604.05280",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Asymptotic Convergence in Online Learning with Unbounded Delays",
		"URL": "http://arxiv.org/abs/1604.05280",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					9,
					7
				]
			]
		}
	},
	{
		"id": "kosoyOptimalPolynomialTimeEstimators2019",
		"type": "article",
		"abstract": "We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of \"optimal polynomial-time estimators.\" We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with \"classical\" probability theory.",
		"DOI": "10.48550/arXiv.1608.04112",
		"note": "arXiv:1608.04112 [cs]",
		"number": "arXiv:1608.04112",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm",
		"title-short": "Optimal Polynomial-Time Estimators",
		"URL": "http://arxiv.org/abs/1608.04112",
		"author": [
			{
				"family": "Kosoy",
				"given": "Vanessa"
			},
			{
				"family": "Appel",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					4
				]
			]
		}
	},
	{
		"id": "sotalaDefiningHumanValues2016",
		"type": "article",
		"abstract": "Hypothetical “value learning” AIs learn human values and then try to act according to those values. The design of such AIs, however, is hampered by the fact that there exists no satisfactory definition of what exactly human values are. After arguing that the standard concept of preference is insufficient as a definition, I draw on reinforcement learning theory, emotion research, and moral psychology to offer an alternative definition. In this definition, human values are conceptualized as mental representations that encode the brain’s value function (in the reinforcement learning sense) by being imbued with a context-sensitive affective gloss. I finish with a discussion of the implications that this hypothesis has on the design of value learners.",
		"event-place": "Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society",
		"language": "en",
		"publisher-place": "Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society",
		"source": "Zotero",
		"title": "Defining Human Values for Value Learners",
		"author": [
			{
				"family": "Sotala",
				"given": "Kaj"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "taylorQuantilizersSaferAlternative2016",
		"type": "article",
		"abstract": "In the ﬁeld of AI, expected utility maximizers are commonly used as a model for idealized agents. However, expected utility maximization can lead to unintended solutions when the utility function does not quantify everything the operators care about: imagine, for example, an expected utility maximizer tasked with winning money on the stock market, which has no regard for whether it accidentally causes a market crash. Once AI systems become sufﬁciently intelligent and powerful, these unintended solutions could become quite dangerous. In this paper, we describe an alternative to expected utility maximization for powerful AI systems, which we call expected utility quantilization. This could allow the construction of AI systems that do not necessarily fall into strange and unanticipated shortcuts and edge cases in pursuit of their goals.",
		"event-place": "The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society",
		"language": "en",
		"publisher-place": "The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence AI, Ethics, and Society",
		"source": "Zotero",
		"title": "Quantilizers: A Safer Alternative to Maximizers for Limited Optimization",
		"author": [
			{
				"family": "Taylor",
				"given": "Jessica"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "taylorAlignmentAdvancedMachine2020a",
		"type": "chapter",
		"abstract": "This chapter surveys eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? The chapter focuses on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers. The questions surveyed include the following: How can we train reinforcement learners to take actions that are more amenable to meaningful assessment by intelligent overseers? What kinds of objective functions incentivize a system to “not have an overly large impact” or “not have many side effects”? The chapter discusses these questions, related work, and potential directions for future research, with the goal of highlighting relevant research topics in machine learning that appear tractable today.",
		"container-title": "Ethics of Artificial Intelligence",
		"ISBN": "978-0-19-090503-3",
		"note": "DOI: 10.1093/oso/9780190905033.003.0013",
		"page": "0",
		"publisher": "Oxford University Press",
		"source": "Silverchair",
		"title": "Alignment for Advanced Machine Learning Systems",
		"URL": "https://doi.org/10.1093/oso/9780190905033.003.0013",
		"author": [
			{
				"family": "Taylor",
				"given": "Jessica"
			},
			{
				"family": "Yudkowsky",
				"given": "Eliezer"
			},
			{
				"family": "LaVictoire",
				"given": "Patrick"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			}
		],
		"editor": [
			{
				"family": "Liao",
				"given": "S. Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					17
				]
			]
		}
	},
	{
		"id": "fallensteinVingeanReflectionReliable2015",
		"type": "article",
		"abstract": "Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.",
		"language": "en",
		"source": "Zotero",
		"title": "Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents",
		"URL": "https://intelligence.org/files/VingeanReflection.pdf",
		"author": [
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015",
					2
				]
			]
		}
	},
	{
		"id": "fallensteinReflectiveVariantsSolomonoff2015",
		"type": "chapter",
		"abstract": "Solomonoﬀ induction and AIXI model their environment as an arbitrary Turing machine, but are themselves uncomputable. This fails to capture an essential property of real-world agents, which cannot be more powerful than the environment they are embedded in; for example, AIXI cannot accurately model game-theoretic scenarios in which its opponent is another instance of AIXI.",
		"container-title": "Artificial General Intelligence",
		"event-place": "Cham",
		"ISBN": "978-3-319-21364-4",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-319-21365-1_7",
		"page": "60-69",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Reflective Variants of Solomonoff Induction and AIXI",
		"URL": "https://link.springer.com/10.1007/978-3-319-21365-1_7",
		"volume": "9205",
		"editor": [
			{
				"family": "Bieger",
				"given": "Jordi"
			},
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Potapov",
				"given": "Alexey"
			}
		],
		"author": [
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "fallensteinReflectiveOraclesFoundation2015",
		"type": "article",
		"abstract": "Classical game theory treats players as special---a description of a game contains a full, explicit enumeration of all players---even though in the real world, \"players\" are no more fundamentally special than rocks or clouds. It isn't trivial to find a decision-theoretic foundation for game theory in which an agent's coplayers are a non-distinguished part of the agent's environment. Attempts to model both players and the environment as Turing machines, for example, fail for standard diagonalization reasons. In this paper, we introduce a \"reflective\" type of oracle, which is able to answer questions about the outputs of oracle machines with access to the same oracle. These oracles avoid diagonalization by answering some queries randomly. We show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory. These agents model their environment as a probabilistic oracle machine, which may contain other agents as a non-distinguished part. We show that if such agents interact, they will play a Nash equilibrium, with the randomization in mixed strategies coming from the randomization in the oracle's answers. This can be seen as providing a foundation for classical game theory in which players aren't special.",
		"DOI": "10.48550/arXiv.1508.04145",
		"note": "arXiv:1508.04145 [cs]",
		"number": "arXiv:1508.04145",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Reflective Oracles: A Foundation for Classical Game Theory",
		"title-short": "Reflective Oracles",
		"URL": "http://arxiv.org/abs/1508.04145",
		"author": [
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Taylor",
				"given": "Jessica"
			},
			{
				"family": "Christiano",
				"given": "Paul F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					8,
					17
				]
			]
		}
	},
	{
		"id": "garrabrantAsymptoticLogicalUncertainty2015",
		"type": "article",
		"abstract": "We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs \"true\" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.",
		"DOI": "10.48550/arXiv.1510.03370",
		"note": "arXiv:1510.03370 [cs]",
		"number": "arXiv:1510.03370",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Asymptotic Logical Uncertainty and The Benford Test",
		"URL": "http://arxiv.org/abs/1510.03370",
		"author": [
			{
				"family": "Garrabrant",
				"given": "Scott"
			},
			{
				"family": "Bhaskar",
				"given": "Siddharth"
			},
			{
				"family": "Demski",
				"given": "Abram"
			},
			{
				"family": "Garrabrant",
				"given": "Joanna"
			},
			{
				"family": "Koleszarik",
				"given": "George"
			},
			{
				"family": "Lloyd",
				"given": "Evan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					10,
					12
				]
			]
		}
	},
	{
		"id": "soaresIdealizedDecisionTheory2015",
		"type": "article",
		"abstract": "This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.",
		"DOI": "10.48550/arXiv.1507.01986",
		"note": "arXiv:1507.01986 [cs]",
		"number": "arXiv:1507.01986",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Idealized Decision Theory",
		"URL": "http://arxiv.org/abs/1507.01986",
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Fallenstein",
				"given": "Benja"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					7,
					7
				]
			]
		}
	},
	{
		"id": "soaresFormalizingTwoProblems2015",
		"type": "article",
		"abstract": "An intelligent agent embedded within the real world must reason about an environment which is larger than the agent, and learn how to achieve goals in that environment. We discuss attempts to formalize two problems: one of induction, where an agent must use sensory data to infer a universe which embeds (and computes) the agent, and one of interaction, where an agent must learn to achieve complex goals in the universe. We review related problems formalized by Solomonoﬀ and Hutter, and explore challenges that arise when attempting to formalize analogous problems in a setting where the agent is embedded within the environment.",
		"language": "en",
		"source": "Zotero",
		"title": "Formalizing Two Problems of Realistic World-Models",
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "soaresTwoAttemptsFormalize2015",
		"type": "chapter",
		"abstract": "This paper motivates the study of counterpossibles (logically impossible counterfactuals) as necessary for developing a decision theory suitable for generally intelligent agents embedded within their environments. We discuss two attempts to formalize a decision theory using counterpossibles, one based on graphical models and another based on proof search.",
		"container-title": "Artificial General Intelligence",
		"event-place": "Cham",
		"ISBN": "978-3-319-21364-4",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-319-21365-1_17",
		"page": "156-165",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Two Attempts to Formalize Counterpossible Reasoning in Deterministic Settings",
		"URL": "http://link.springer.com/10.1007/978-3-319-21365-1_17",
		"volume": "9205",
		"editor": [
			{
				"family": "Bieger",
				"given": "Jordi"
			},
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Potapov",
				"given": "Alexey"
			}
		],
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Fallenstein",
				"given": "Benja"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "sotalaConceptLearningSafe2015",
		"type": "article",
		"abstract": "Sophisticated autonomous AI may need to base its behavior on fuzzy concepts such as well-being or rights. These concepts cannot be given an explicit formal definition, but obtaining desired behavior still requires a way to instill the concepts in an AI system. To solve the problem, we review evidence suggesting that the human brain generates its concepts using a relatively limited set of rules and mechanisms. This suggests that it might be feasible to build AI systems that use similar criteria for generating their own concepts, and could thus learn similar concepts as humans do. Major challenges to this approach include the embodied nature of human thought, evolutionary vestiges in cognition, the social nature of concepts, and the need to compare conceptual representations between humans and AI systems.",
		"event-place": "Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop",
		"language": "en",
		"publisher-place": "Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop",
		"source": "Zotero",
		"title": "Concept Learning for Safe Autonomous AI",
		"author": [
			{
				"family": "Sotala",
				"given": "Kaj"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "armstrongErrorsInsightsLessons2014",
		"type": "article-journal",
		"abstract": "Predicting the development of artificial intelligence (AI) is a difficult project – but a vital one, according to some analysts. AI predictions are already abound: but are they reliable? This paper starts by proposing a decomposition schema for classifying them. Then it constructs a variety of theoretical tools for analysing, judging and improving them. These tools are demonstrated by careful analysis of five famous AI predictions: the initial Dartmouth conference, Dreyfus's criticism of AI, Searle's Chinese room paper, Kurzweil's predictions in the Age of Spiritual Machines, and Omohundro's ‘AI drives’ paper. These case studies illustrate several important principles, such as the general overconfidence of experts, the superiority of models over expert judgement and the need for greater uncertainty in all types of predictions. The general reliability of expert judgement in AI timeline predictions is shown to be poor, a result that fits in with previous studies of expert competence.",
		"container-title": "Journal of Experimental & Theoretical Artificial Intelligence",
		"DOI": "10.1080/0952813X.2014.895105",
		"ISSN": "0952-813X",
		"issue": "3",
		"note": "publisher: Taylor & Francis\n_eprint: https://doi.org/10.1080/0952813X.2014.895105",
		"page": "317-342",
		"source": "Taylor and Francis+NEJM",
		"title": "The errors, insights and lessons of famous AI predictions – and what they mean for the future",
		"URL": "https://doi.org/10.1080/0952813X.2014.895105",
		"volume": "26",
		"author": [
			{
				"family": "Armstrong",
				"given": "Stuart"
			},
			{
				"family": "Sotala",
				"given": "Kaj"
			},
			{
				"family": "Ó hÉigeartaigh",
				"given": "Seán S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					7,
					3
				]
			]
		}
	},
	{
		"id": "fallensteinProblemsSelfreferenceSelfimproving2014",
		"type": "paper-conference",
		"abstract": "By considering agents to be a part of their environment, Orseau and Ring’s space-time embedded intelligence [10] is a better fit to the real world than the traditional agent framework. However, a self-modifying AGI that sees future versions of itself as an ordinary part of the environment may run into problems of self-reference. We show that in one particular model based on formal logic, naive approaches either lead to incorrect reasoning that allows an agent to put off an important task forever (the procrastination paradox), or fail to allow the agent to justify even obviously safe rewrites (the Löbian obstacle). We argue that these problems have relevance beyond our particular formalism, and discuss partial solutions.",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-319-09274-4_3",
		"event-place": "Cham",
		"ISBN": "978-3-319-09274-4",
		"language": "en",
		"page": "21-32",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "Springer Link",
		"title": "Problems of Self-reference in Self-improving Space-Time Embedded Intelligence",
		"author": [
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Soares",
				"given": "Nate"
			}
		],
		"editor": [
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			},
			{
				"family": "Snaider",
				"given": "Javier"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "armstrongRacingPrecipiceModel2016",
		"type": "article-journal",
		"abstract": "This paper presents a simple model of an AI (artificial intelligence) arms race, where several development teams race to build the first AI. Under the assumption that the first AI will be very powerful and transformative, each team is incentivised to finish first—by skimping on safety precautions if need be. This paper presents the Nash equilibrium of this process, where each team takes the correct amount of safety precautions in the arms race. Having extra development teams and extra enmity between teams can increase the danger of an AI disaster, especially if risk-taking is more important than skill in developing the AI. Surprisingly, information also increases the risks: the more teams know about each others’ capabilities (and about their own), the more the danger increases. Should these results persist in more realistic models and analysis, it points the way to methods of increasing the chance of the safe development of AI.",
		"container-title": "AI & SOCIETY",
		"DOI": "10.1007/s00146-015-0590-y",
		"ISSN": "1435-5655",
		"issue": "2",
		"journalAbbreviation": "AI & Soc",
		"language": "en",
		"page": "201-206",
		"source": "Springer Link",
		"title": "Racing to the precipice: a model of artificial intelligence development",
		"title-short": "Racing to the precipice",
		"URL": "https://doi.org/10.1007/s00146-015-0590-y",
		"volume": "31",
		"author": [
			{
				"family": "Armstrong",
				"given": "Stuart"
			},
			{
				"family": "Bostrom",
				"given": "Nick"
			},
			{
				"family": "Shulman",
				"given": "Carl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					5,
					1
				]
			]
		}
	},
	{
		"id": "yampolskiySafetyEngineeringArtificial2013",
		"type": "article-journal",
		"abstract": "Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence and robotics communities. We will argue that attempts to attribute moral agency and assign rights to all intelligent machines are misguided, whether applied to infrahuman or superhuman AIs, as are proposals to limit the negative effects of AIs by constraining their behavior. As an alternative, we propose a new science of safety engineering for intelligent artificial agents based on maximizing for what humans value. In particular, we challenge the scientific community to develop intelligent systems that have human-friendly values that they provably retain, even under recursive self-improvement.",
		"container-title": "Topoi",
		"DOI": "10.1007/s11245-012-9128-9",
		"ISSN": "1572-8749",
		"issue": "2",
		"journalAbbreviation": "Topoi",
		"language": "en",
		"page": "217-226",
		"source": "Springer Link",
		"title": "Safety Engineering for Artificial General Intelligence",
		"URL": "https://doi.org/10.1007/s11245-012-9128-9",
		"volume": "32",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman"
			},
			{
				"family": "Fox",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					10,
					1
				]
			]
		}
	},
	{
		"id": "yampolskiyArtificialGeneralIntelligence2012",
		"type": "chapter",
		"abstract": "When the first artificial general intelligences are built, they may improve themselves to far-above-human levels. Speculations about such future entities are already affected by anthropomorphic bias, which leads to erroneous analogies with human minds. In this chapter, we apply a goal-oriented understanding of intelligence to show that humanity occupies only a tiny portion of the design space of possible minds. This space is much larger than what we are familiar with from the human example; and the mental architectures and goals of future superintelligences need not have most of the properties of human minds. A new approach to cognitive science and philosophy of mind, one not centered on the human example, is needed to help us understand the challenges which we will face when a power greater than us emerges.",
		"container-title": "Singularity Hypotheses: A Scientific and Philosophical Assessment",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-32560-1",
		"language": "en",
		"note": "DOI: 10.1007/978-3-642-32560-1_7",
		"page": "129-145",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Artificial General Intelligence and the Human Mental Model",
		"URL": "https://doi.org/10.1007/978-3-642-32560-1_7",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			},
			{
				"family": "Fox",
				"given": "Joshua"
			}
		],
		"editor": [
			{
				"family": "Eden",
				"given": "Amnon H."
			},
			{
				"family": "Moor",
				"given": "James H."
			},
			{
				"family": "Søraker",
				"given": "Johnny H."
			},
			{
				"family": "Steinhart",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "hibbardAvoidingUnintendedAI2012a",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems too complex for predefined environment models and actions will need to learn environment models and to choose actions that optimize some criteria. Several authors have described mechanisms by which such complex systems may behave in ways not intended in their designs. This paper describes ways to avoid such unintended behavior. For hypothesized powerful AI systems that may pose a threat to humans, this paper proposes a two-stage agent architecture that avoids some known types of unintended behavior. For the first stage of the architecture this paper shows that the most probable finite stochastic program to model a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions.",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-35506-6_12",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-35506-6",
		"language": "en",
		"page": "107-116",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Avoiding Unintended AI Behaviors",
		"author": [
			{
				"family": "Hibbard",
				"given": "Bill"
			}
		],
		"editor": [
			{
				"family": "Bach",
				"given": "Joscha"
			},
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Iklé",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "hibbardDecisionSupportSafe2012",
		"type": "paper-conference",
		"abstract": "There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter’s agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-35506-6_13",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-35506-6",
		"language": "en",
		"page": "117-125",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Decision Support for Safe AI Design",
		"author": [
			{
				"family": "Hibbard",
				"given": "Bill"
			}
		],
		"editor": [
			{
				"family": "Bach",
				"given": "Joscha"
			},
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Iklé",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "deblancOntologicalCrisesArtificial2011",
		"type": "article",
		"abstract": "Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",
		"DOI": "10.48550/arXiv.1105.3821",
		"note": "arXiv:1105.3821 [cs]",
		"number": "arXiv:1105.3821",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Ontological Crises in Artificial Agents' Value Systems",
		"URL": "http://arxiv.org/abs/1105.3821",
		"author": [
			{
				"family": "Blanc",
				"given": "Peter",
				"non-dropping-particle": "de"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					5,
					19
				]
			]
		}
	},
	{
		"id": "deweyLearningWhatValue2011",
		"type": "paper-conference",
		"abstract": "I. J. Good’s intelligence explosion theory predicts that ultraintelligent agents will undergo a process of repeated self-improvement; in the wake of such an event, how well our values are fulfilled would depend on the goals of these ultraintelligent agents. With this motivation, we examine ultraintelligent reinforcement learning agents. Reinforcement learning can only be used in the real world to define agents whose goal is to maximize expected rewards, and since this goal does not match with human goals, AGIs based on reinforcement learning will often work at cross-purposes to us. To solve this problem, we define value learners, agents that can be designed to learn and maximize any initially unknown utility function so long as we provide them with an idea of what constitutes evidence about that utility function.",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-22887-2_35",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-22887-2",
		"language": "en",
		"page": "309-314",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Learning What to Value",
		"author": [
			{
				"family": "Dewey",
				"given": "Daniel"
			}
		],
		"editor": [
			{
				"family": "Schmidhuber",
				"given": "Jürgen"
			},
			{
				"family": "Thórisson",
				"given": "Kristinn R."
			},
			{
				"family": "Looks",
				"given": "Moshe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "yudkowskyComplexValueSystems2011",
		"type": "paper-conference",
		"abstract": "A common reaction to first encountering the problem statement of Friendly AI (”Ensure that the creation of a generally intelligent, self-improving, eventually superintelligent system realizes a positive outcome”) is to propose a simple design which allegedly suffices; or to reject the problem by replying that ”constraining” our creations is undesirable or unnecessary. This paper briefly presents some of the reasoning which suggests that Friendly AI is solvable, but not simply or trivially so, and that a wise strategy would be to invoke detailed learning of and inheritance from human values as a basis for further normalization and reflection.",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-22887-2_48",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-22887-2",
		"language": "en",
		"page": "388-393",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Complex Value Systems in Friendly AI",
		"author": [
			{
				"family": "Yudkowsky",
				"given": "Eliezer"
			}
		],
		"editor": [
			{
				"family": "Schmidhuber",
				"given": "Jürgen"
			},
			{
				"family": "Thórisson",
				"given": "Kristinn R."
			},
			{
				"family": "Looks",
				"given": "Moshe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "deblancConvergenceExpectedUtility2009",
		"type": "article",
		"abstract": "We consider a sequence of repeated interactions between an agent and an environment. Uncertainty about the environment is captured by a probability distribution over a space of hypotheses, which includes all computable functions. Given a utility function, we can evaluate the expected utility of any computational policy for interaction with the environment. After making some plausible assumptions (and maybe one not-so-plausible assumption), we show that if the utility function is unbounded, then the expected utility of any policy is undefined.",
		"DOI": "10.48550/arXiv.0907.5598",
		"note": "arXiv:0907.5598 [cs]",
		"number": "arXiv:0907.5598",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Convergence of Expected Utility for Universal AI",
		"URL": "http://arxiv.org/abs/0907.5598",
		"author": [
			{
				"family": "Blanc",
				"given": "Peter",
				"non-dropping-particle": "de"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009",
					12,
					2
				]
			]
		}
	},
	{
		"id": "turnerFormalizingProblemSide2022",
		"type": "article",
		"abstract": "AI objectives are often hard to specify properly. Some approaches tackle this problem by regularizing the AI's side effects: Agents must weigh off \"how much of a mess they make\" with an imperfectly specified proxy objective. We propose a formal criterion for side effect regularization via the assistance game framework. In these games, the agent solves a partially observable Markov decision process (POMDP) representing its uncertainty about the objective function it should optimize. We consider the setting where the true objective is revealed to the agent at a later time step. We show that this POMDP is solved by trading off the proxy reward with the agent's ability to achieve a range of future tasks. We empirically demonstrate the reasonableness of our problem formalization via ground-truth evaluation in two gridworld environments.",
		"DOI": "10.48550/arXiv.2206.11812",
		"note": "arXiv:2206.11812 [cs]",
		"number": "arXiv:2206.11812",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Formalizing the Problem of Side Effect Regularization",
		"URL": "http://arxiv.org/abs/2206.11812",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Saxena",
				"given": "Aseem"
			},
			{
				"family": "Tadepalli",
				"given": "Prasad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					8
				]
			]
		}
	},
	{
		"id": "sarkarMeasuringRobustnessBlackBox2022",
		"type": "paper-conference",
		"abstract": "A measure of robustness against naturally occurring distortions is key to the trustworthiness, safety, and success of machine learning models on deployment. We investigate an adversarial black-box attack that adds minimum Gaussian noise distortions to input images to make deep learning models misclassify. We used a Reinforcement Learning (RL) agent as a smart hacker to explore the input images to add minimum distortions to the most sensitive regions to induce misclassification. The agent employs a smart policy also to remove noises introduced earlier, which has less impact on the trained model at a given state. This novel approach is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to faster and optimal convergence. Also, this adversarial attack method effectively measures the robustness of image classification models with the misclassification inducing minimum L2 distortion of Gaussian noise similar to many naturally occurring distortions. Furthermore, the proposed black-box L2 adversarial attack tool beats state-of-the-art competitors in terms of the average number of queries by a significant margin with a 100\\% success rate while maintaining a very competitive L2 score, despite limiting distortions to Gaussian noise. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31% better than the state-of-the-art \"Square-Attack\" approach while maintaining a competitive L2. Demo: https://tinyurl.com/pzrca5fj",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Measuring Robustness with Black-Box Adversarial Attack using Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=Lj8fj0ECPv",
		"author": [
			{
				"family": "Sarkar",
				"given": "Soumyendu"
			},
			{
				"family": "Mousavi",
				"given": "Sajad"
			},
			{
				"family": "Babu",
				"given": "Ashwin Ramesh"
			},
			{
				"family": "Gundecha",
				"given": "Vineet"
			},
			{
				"family": "Ghorbanpour",
				"given": "Sahand"
			},
			{
				"family": "Shmakov",
				"given": "Alexander K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "hobbhahnInvestigatingCausalUnderstanding2022",
		"type": "paper-conference",
		"abstract": "We investigate the quality of causal world models of LLMs in very simple settings. We test whether LLMs can identify cause and effect in natural language settings (taken from BigBench) such as “My car got dirty. I washed the car. Question: Which sentence is the cause of the other?” and in multiple other toy settings. We probe the LLM's world model by changing the presentation of the prompt while keeping the meaning constant, e.g. by changing the order of the sentences or asking the opposite question. Additionally, we test if the model can be “tricked” into giving wrong answers when we present the shot in a different pattern than the prompt. We have three findings. Firstly, larger models yield better results. Secondly, k-shot outperforms one-shot and one-shot outperforms zero-shot in standard conditions. Thirdly, LLMs perform worse in conditions where form and content differ. We conclude that the form of the presentation matters for LLM predictions or, in other words, that LLMs don't solely base their predictions on content. Finally, we detail some of the implications this research has on AI safety.",
		"event-title": "NeurIPS 2022 Workshop on Causality for Real-world Impact",
		"language": "en",
		"source": "openreview.net",
		"title": "Investigating causal understanding in LLMs",
		"URL": "https://openreview.net/forum?id=st6jtGdW8Ke",
		"author": [
			{
				"family": "Hobbhahn",
				"given": "Marius"
			},
			{
				"family": "Lieberum",
				"given": "Tom"
			},
			{
				"family": "Seiler",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		}
	},
	{
		"id": "hobbhahnReflectionMechanismsAlignment2022",
		"type": "paper-conference",
		"abstract": "We used Positly to survey roughly 1000 US-based workers about their attitudes on moral questions, conditions under which they would change their moral beliefs, and approval towards different mechanisms for society to resolve moral disagreements. Unsurprisingly, our sample strongly disagreed on contentious object-level moral questions such as whether abortion is immoral. In addition, a substantial fraction of people reported that these beliefs wouldn’t change even if they came to different beliefs about factors we view as morally relevant, such as whether the fetus was conscious in the case of abortion. However, people were generally favorable to the idea of society deciding policies by some means of reflection - such as democracy, a debate between well-intentioned experts, or thinking for a long time. This agreement improves in a hypothetical well-intentioned future society. Surprisingly, favorability remained even when we stipulate that the reflection procedure came to the opposite of the respondents' view on polarizing topics like abortion. This provides evidence that people may support aligning AIs to a reflection procedure rather than individual beliefs. We tested our findings on a second adversarial survey that actively tries to disprove the finding from the first study. We find that our core results are robust in standard settings but are weakened when the questions are constructed adversarially (e.g. when decisions are made by people who have the opposite of the respondents' moral or political beliefs).",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Reflection Mechanisms as an Alignment Target: A Survey",
		"title-short": "Reflection Mechanisms as an Alignment Target",
		"URL": "https://openreview.net/forum?id=4eMzKmZ6xW",
		"author": [
			{
				"family": "Hobbhahn",
				"given": "Marius"
			},
			{
				"family": "Landgrebe",
				"given": "Eric"
			},
			{
				"family": "Barnes",
				"given": "Elizabeth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "dattaInterpolatingCompressedParameter2022",
		"type": "paper-conference",
		"abstract": "Though distribution shifts have caused growing concern for machine learning scalability, solutions tend to specialize towards a specific type of distribution shift. We learn that constructing a Compressed Parameter Subspaces (CPS), a geometric structure representing distance-regularized parameters mapped to a set of train-time distributions, can maximize average accuracy over a broad range of distribution shifts concurrently. We show sampling parameters within a CPS can mitigate backdoor, adversarial, permutation, stylization and rotation perturbations. Regularizing a hypernetwork with CPS can also reduce task forgetting.",
		"event-title": "Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Interpolating Compressed Parameter Subspaces",
		"URL": "https://openreview.net/forum?id=Zb9m4idh8I",
		"author": [
			{
				"family": "Datta",
				"given": "Siddhartha"
			},
			{
				"family": "Shadbolt",
				"given": "Nigel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					21
				]
			]
		}
	},
	{
		"id": "ramanProbabilisticallyRobustPAC2022",
		"type": "paper-conference",
		"abstract": "Recently, Robey et al. propose a notion of probabilistic robustness, which, at a high-level, requires a classifier to be robust to most but not all perturbations. They show that for certain hypothesis classes where proper learning under worst-case robustness is \\textit{not} possible, proper learning under probabilistic robustness \\textit{is} possible with sample complexity exponentially smaller than in the worst-case robustness setting. This motivates the question of whether proper learning under probabilistic robustness is always possible. In this paper, we show that this is \\textit{not} the case. We exhibit examples of hypothesis classes $\\mathcal{H}$ with finite VC dimension that are \\textit{not} probabilistically robustly PAC learnable with \\textit{any} proper learning rule.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Probabilistically Robust PAC Learning",
		"URL": "https://openreview.net/forum?id=g_BjLtjtCwT",
		"author": [
			{
				"family": "Raman",
				"given": "Vinod"
			},
			{
				"family": "Tewari",
				"given": "Ambuj"
			},
			{
				"family": "Subedi",
				"given": "Unique"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "oonishiMultipleRemoteAdversarial2022",
		"type": "paper-conference",
		"abstract": "Adversarial patches can fool object detection systems, which poses a severe threat to machine learning models. Many researchers have focused on strong adversarial patches. Remote adversarial patches, placed outside the target objects, are candidates of strong adversarial patches. This study gives a concrete model of adversarial patches on convolutional neural networks (CNNs), namely diffusion model. Our diffusion model shows that multiple remote adversarial patches pose severe threats on YOLOv2 CNN. Our experiment also demonstrates that two remote adversarial patches reduce the average existence probability to 12.81%, whereas Saha et al.'s original single adversarial patch reduced the average existence probability to 50.95%. Moreover, we generate adversarial patches on SSD architecture. In SSD architecture, two remote adversarial patches also significantly reduce the average existence probability from 24.52% to 6.12%. By the above results, this paper provides a framework for analyzing the effect of adversarial patch attacks.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Multiple Remote Adversarial Patches: Generating Patches based on Diffusion Models for Object Detection using CNNs",
		"title-short": "Multiple Remote Adversarial Patches",
		"URL": "https://openreview.net/forum?id=netFyi04pB",
		"author": [
			{
				"family": "Oonishi",
				"given": "Kento"
			},
			{
				"family": "Nakai",
				"given": "Tsunato"
			},
			{
				"family": "Suzuki",
				"given": "Daisuke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "skalseMisspecificationInverseReinforcement2023",
		"type": "paper-conference",
		"abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\\pi$. To do this, we need a model of how $\\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.",
		"DOI": "10.48550/arXiv.2212.03201",
		"event-title": "AAAI Conference on Artificial Intelligence, 2023",
		"note": "arXiv:2212.03201 [cs]",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Misspecification in Inverse Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2212.03201",
		"author": [
			{
				"family": "Skalse",
				"given": "Joar"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					24
				]
			]
		}
	},
	{
		"id": "randoRedTeamingStableDiffusion2022",
		"type": "article",
		"abstract": "Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community.",
		"DOI": "10.48550/arXiv.2210.04610",
		"note": "arXiv:2210.04610 [cs]",
		"number": "arXiv:2210.04610",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Red-Teaming the Stable Diffusion Safety Filter",
		"URL": "http://arxiv.org/abs/2210.04610",
		"author": [
			{
				"family": "Rando",
				"given": "Javier"
			},
			{
				"family": "Paleka",
				"given": "Daniel"
			},
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Tramèr",
				"given": "Florian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					10
				]
			]
		}
	},
	{
		"id": "heuilletTrackingRiskMachine2022",
		"type": "paper-conference",
		"abstract": "Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Tracking the Risk of Machine Learning Systems with Partial Monitoring",
		"URL": "https://openreview.net/forum?id=zzpu07KXNX",
		"author": [
			{
				"family": "Heuillet",
				"given": "Maxime"
			},
			{
				"family": "Durand",
				"given": "Audrey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "bowlingSettlingRewardHypothesis2023",
		"type": "article",
		"abstract": "The reward hypothesis posits that, “all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).” We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.",
		"language": "en",
		"note": "arXiv:2212.10420 [cs, math, stat]",
		"number": "arXiv:2212.10420",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Settling the Reward Hypothesis",
		"URL": "http://arxiv.org/abs/2212.10420",
		"author": [
			{
				"family": "Bowling",
				"given": "Michael"
			},
			{
				"family": "Martin",
				"given": "John D."
			},
			{
				"family": "Abel",
				"given": "David"
			},
			{
				"family": "Dabney",
				"given": "Will"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					16
				]
			]
		}
	},
	{
		"id": "skalseRewardHypothesisFalse2022",
		"type": "paper-conference",
		"abstract": "The \\emph{reward hypothesis} is the hypothesis that \\enquote{all of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal}\\citep{sutton2018reinforcement}. In this paper, we will argue that this hypothesis is false. We will look at three natural classes of reinforcement learning tasks (multi-objective reinforcement learning, risk-averse reinforcement learning, and modal reinforcement learning), and then prove mathematically that these tasks cannot be expressed using any scalar, Markovian reward function. We thus disprove the reward hypothesis by providing many examples of tasks which are both natural and intuitive to describe, but which are nonetheless impossible to express using reward functions. In the process, we provide necessary and sufficient conditions for when a multi-objective reinforcement learning problem can be reduced to ordinary, scalar reward reinforcement learning. We also call attention to a new class of reinforcement learning problems (namely those we call \\enquote{modal} problems), which have so far not been given any systematic treatment in the reinforcement learning literature.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "The Reward Hypothesis is False",
		"URL": "https://openreview.net/forum?id=5l1NgpzAfH",
		"author": [
			{
				"family": "Skalse",
				"given": "Joar Max Viktor"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "hanDataPoisoningAttack2023",
		"type": "article",
		"abstract": "Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importance of developing continual learning models that are robust to adversarial attacks.",
		"DOI": "10.48550/arXiv.2211.15875",
		"note": "arXiv:2211.15875 [cs]",
		"number": "arXiv:2211.15875",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Data Poisoning Attack Aiming the Vulnerability of Continual Learning",
		"URL": "http://arxiv.org/abs/2211.15875",
		"author": [
			{
				"family": "Han",
				"given": "Gyojin"
			},
			{
				"family": "Choi",
				"given": "Jaehyun"
			},
			{
				"family": "Hong",
				"given": "Hyeong Gwon"
			},
			{
				"family": "Kim",
				"given": "Junmo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "rajMeasuringReliabilityLarge2023",
		"type": "article",
		"abstract": "While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLMs on paraphrased versions of questions in the TruthfulQA dataset, we find that our proposed metrics are considerably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.",
		"DOI": "10.48550/arXiv.2211.05853",
		"note": "arXiv:2211.05853 [cs]",
		"number": "arXiv:2211.05853",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Measuring Reliability of Large Language Models through Semantic Consistency",
		"URL": "http://arxiv.org/abs/2211.05853",
		"author": [
			{
				"family": "Raj",
				"given": "Harsh"
			},
			{
				"family": "Rosati",
				"given": "Domenic"
			},
			{
				"family": "Majumdar",
				"given": "Subhabrata"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					11
				]
			]
		}
	},
	{
		"id": "ahnCUDACurriculumData2023",
		"type": "article",
		"abstract": "Class imbalance problems frequently occur in real-world tasks, and conventional deep learning algorithms are well known for performance degradation on imbalanced training datasets. To mitigate this problem, many approaches have aimed to balance among given classes by re-weighting or re-sampling training samples. These re-balancing methods increase the impact of minority classes and reduce the influence of majority classes on the output of models. However, the extracted representations may be of poor quality owing to the limited number of minority samples. To handle this restriction, several methods have been developed that increase the representations of minority samples by leveraging the features of the majority samples. Despite extensive recent studies, no deep analysis has been conducted on determination of classes to be augmented and strength of augmentation has been conducted. In this study, we first investigate the correlation between the degree of augmentation and class-wise performance, and find that the proper degree of augmentation must be allocated for each class to mitigate class imbalance problems. Motivated by this finding, we propose a simple and efficient novel curriculum, which is designed to find the appropriate per-class strength of data augmentation, called CUDA: CUrriculum of Data Augmentation for long-tailed recognition. CUDA can simply be integrated into existing long-tailed recognition methods. We present the results of experiments showing that CUDA effectively achieves better generalization performance compared to the state-of-the-art method on various imbalanced datasets such as CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018.",
		"DOI": "10.48550/arXiv.2302.05499",
		"note": "arXiv:2302.05499 [cs]",
		"number": "arXiv:2302.05499",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition",
		"title-short": "CUDA",
		"URL": "http://arxiv.org/abs/2302.05499",
		"author": [
			{
				"family": "Ahn",
				"given": "Sumyeong"
			},
			{
				"family": "Ko",
				"given": "Jongwoo"
			},
			{
				"family": "Yun",
				"given": "Se-Young"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					10
				]
			]
		}
	},
	{
		"id": "bartolomeisCertifiedDefencesHurt2022",
		"type": "paper-conference",
		"abstract": "In recent years, much work has been devoted to designing certified defences for neural networks, i.e., methods for learning neural networks that are provably robust to certain adversarial perturbations. Due to the non-convexity of the problem, dominant approaches in this area rely on convex approximations, which are inherently loose. In this paper, we question the effectiveness of such approaches for realistic computer vision tasks. First, we provide extensive empirical evidence to show that certified defences suffer not only worse accuracy but also worse robustness and fairness than empirical defences. We hypothesise that the reason for why certified defences suffer in generalisation is (i) the large number of relaxed non-convex constraints and (ii) strong alignment between the adversarial perturbations and the \"signal\" direction. We provide a combination of theoretical and experimental evidence to support these hypotheses.",
		"event-title": "I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification",
		"language": "en",
		"source": "openreview.net",
		"title": "Certified defences hurt generalisation",
		"URL": "https://openreview.net/forum?id=h1j5I0WVxoI",
		"author": [
			{
				"family": "Bartolomeis",
				"given": "Piersilvio De"
			},
			{
				"family": "Clarysse",
				"given": "Jacob"
			},
			{
				"family": "Yang",
				"given": "Fanny"
			},
			{
				"family": "Sanyal",
				"given": "Amartya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					6
				]
			]
		}
	},
	{
		"id": "wilesDiscoveringBugsVision2023",
		"type": "article",
		"abstract": "Automatically discovering failures in vision models under real-world settings remains an open challenge. This work demonstrates how off-the-shelf, large-scale, image-to-text and text-to-image models, trained on vast amounts of data, can be leveraged to automatically find such failures. In essence, a conditional text-to-image generative model is used to generate large amounts of synthetic, yet realistic, inputs given a ground-truth label. Misclassified inputs are clustered and a captioning model is used to describe each cluster. Each cluster's description is used in turn to generate more inputs and assess whether specific clusters induce more failures than expected. We use this pipeline to demonstrate that we can effectively interrogate classifiers trained on ImageNet to find specific failure cases and discover spurious correlations. We also show that we can scale the approach to generate adversarial datasets targeting specific classifier architectures. This work serves as a proof-of-concept demonstrating the utility of large-scale generative models to automatically discover bugs in vision models in an open-ended manner. We also describe a number of limitations and pitfalls related to this approach.",
		"DOI": "10.48550/arXiv.2208.08831",
		"note": "arXiv:2208.08831 [cs, stat]",
		"number": "arXiv:2208.08831",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning",
		"URL": "http://arxiv.org/abs/2208.08831",
		"author": [
			{
				"family": "Wiles",
				"given": "Olivia"
			},
			{
				"family": "Albuquerque",
				"given": "Isabela"
			},
			{
				"family": "Gowal",
				"given": "Sven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					11
				]
			]
		}
	},
	{
		"id": "monteiroConstrainingRepresentationsYields2023",
		"type": "article",
		"abstract": "A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code - and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence score, besides the default unTAC'ed prediction head's. In the add-on case, the original neural network's inference head is completely unaffected (so its accuracy remains the same) but we now have the option to use TAC's own confidence and prediction when determining which course of action to take in an hypothetical production workflow. In particular, we show that TAC strictly improves the value derived from models allowed to reject/defer. We provide further empirical evidence that TAC works well on multiple types of architectures and data modalities and that it is at least as good as state-of-the-art alternative confidence scores derived from existing models.",
		"DOI": "10.48550/arXiv.2208.14488",
		"note": "arXiv:2208.14488 [cs]",
		"number": "arXiv:2208.14488",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Constraining Representations Yields Models That Know What They Don't Know",
		"URL": "http://arxiv.org/abs/2208.14488",
		"author": [
			{
				"family": "Monteiro",
				"given": "Joao"
			},
			{
				"family": "Rodriguez",
				"given": "Pau"
			},
			{
				"family": "Noel",
				"given": "Pierre-Andre"
			},
			{
				"family": "Laradji",
				"given": "Issam"
			},
			{
				"family": "Vazquez",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					19
				]
			]
		}
	},
	{
		"id": "liuRobustnessSafeReinforcement2023",
		"type": "article",
		"abstract": "Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \\url{https://github.com/liuzuxin/safe-rl-robustness}",
		"DOI": "10.48550/arXiv.2205.14691",
		"note": "arXiv:2205.14691 [cs]",
		"number": "arXiv:2205.14691",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations",
		"URL": "http://arxiv.org/abs/2205.14691",
		"author": [
			{
				"family": "Liu",
				"given": "Zuxin"
			},
			{
				"family": "Guo",
				"given": "Zijian"
			},
			{
				"family": "Cen",
				"given": "Zhepeng"
			},
			{
				"family": "Zhang",
				"given": "Huan"
			},
			{
				"family": "Tan",
				"given": "Jie"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Zhao",
				"given": "Ding"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					1
				]
			]
		}
	},
	{
		"id": "geImprovingZeroshotGeneralization2023",
		"type": "paper-conference",
		"abstract": "Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNetbased datasets. For CLIP, our method improves the top1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT. The proposed method1 is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP.",
		"container-title": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
		"DOI": "10.1109/CVPR52729.2023.01067",
		"event-place": "Vancouver, BC, Canada",
		"event-title": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
		"ISBN": "979-8-3503-0129-8",
		"language": "en",
		"license": "https://doi.org/10.15223/policy-029",
		"page": "11093-11101",
		"publisher": "IEEE",
		"publisher-place": "Vancouver, BC, Canada",
		"source": "DOI.org (Crossref)",
		"title": "Improving Zero-shot Generalization and Robustness of Multi-Modal Models",
		"URL": "https://ieeexplore.ieee.org/document/10203777/",
		"author": [
			{
				"family": "Ge",
				"given": "Yunhao"
			},
			{
				"family": "Ren",
				"given": "Jie"
			},
			{
				"family": "Gallagher",
				"given": "Andrew"
			},
			{
				"family": "Wang",
				"given": "Yuxiao"
			},
			{
				"family": "Yang",
				"given": "Ming-Hsuan"
			},
			{
				"family": "Adam",
				"given": "Hartwig"
			},
			{
				"family": "Itti",
				"given": "Laurent"
			},
			{
				"family": "Lakshminarayanan",
				"given": "Balaji"
			},
			{
				"family": "Zhao",
				"given": "Jiaping"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6
				]
			]
		}
	},
	{
		"id": "korkmazDisclosingBiasesLarge2022",
		"type": "paper-conference",
		"abstract": "The success of the large language models have been utterly demonstrated in the recent time. Using these models and fine tuning for the specific task at hand results in highly performing models. However, these models also learn biased representations from the data they have been trained on. In particular, several studies recently showed that language models can learn to be biased towards certain genders. Quite recently, several studies tried to eliminate this bias via proposing human feedback included in fine-tuning. In our study we show that by changing the question asked to the language model the log probabilities of the bias measured in the responses changes dramatically. Furthermore, in several cases the language model ends up providing a completely opposite response. The recent language models finetuned on the prior gender bias datasets do not resolve the actual problem, but rather alleviates the problem for the dataset on which the model is fine-tuned. We believe our results might lay the foundation for further alignment and safety problems in large language models.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Disclosing the Biases in Large Language Models via Reward Structured Questions",
		"URL": "https://openreview.net/forum?id=495hkz94cIC",
		"author": [
			{
				"family": "Korkmaz",
				"given": "Ezgi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "tuckerBanditsCostlyReward2023",
		"type": "paper-conference",
		"abstract": "Many machine learning applications rely on large datasets that are conveniently collected from existing sources or that are labeled automatically as a by-product of user actions. However, in settings such as content moderation, accurately and reliably labeled data comes at substantial cost. If a learning algorithm has to pay for reward information, for example by asking a human for feedback, how does this change the exploration/exploitation tradeoff? We study this question in the context of bandit learning. Specifically, we investigate Bandits with Costly Reward Observations, where a cost needs to be paid in order to observe the reward of the bandit’s action. We show that the observation cost implies an Ω(c1/3T2/3)Ω(c1/3T2/3)\\Omega(c^{1/3}T^{2/3}) lower bound on the regret. Furthermore, we develop a general non-adaptive bandit algorithm which matches this lower bound, and we present several competitive adaptive learning algorithms for both k-armed and contextual bandits.",
		"container-title": "Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence",
		"event-title": "Uncertainty in Artificial Intelligence",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "2147-2156",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Bandits with costly reward observations",
		"URL": "https://proceedings.mlr.press/v216/tucker23a.html",
		"author": [
			{
				"family": "Tucker",
				"given": "Aaron D."
			},
			{
				"family": "Biddulph",
				"given": "Caleb"
			},
			{
				"family": "Wang",
				"given": "Claire"
			},
			{
				"family": "Joachims",
				"given": "Thorsten"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					2
				]
			]
		}
	},
	{
		"id": "martinez-martinezRobustAugMixJointOptimization2022",
		"type": "paper-conference",
		"abstract": "Machine learning models often suffer performance degradation when faced with corrupted data. In this work, we explore a technique that combines a data augmentation strategy (AugMix) with adversarial training, in order to increase robustness to both natural and adversarial forms of data corruption.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "RobustAugMix: Joint Optimization of Natural and Adversarial Robustness",
		"title-short": "RobustAugMix",
		"URL": "https://openreview.net/forum?id=8MfPfECiFET",
		"author": [
			{
				"family": "Martinez-Martinez",
				"given": "Josue"
			},
			{
				"family": "Brown",
				"given": "Olivia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "zhouLearningRobustFoundation2024",
		"type": "article-journal",
		"abstract": "In the transfer learning paradigm, models that are pre-trained on large datasets are used as the foundation models for various downstream tasks. However, this paradigm exposes downstream practitioners to data poisoning threats, as attackers can inject malicious samples into the re-training datasets to manipulate the behavior of models in downstream tasks. In this work, we propose a defense strategy that significantly reduces the success rate of various data poisoning attacks in downstream tasks. Our defense aims to pre-train a robust foundation model by reducing adversarial feature distance and increasing inter-class feature distance. Experiments demonstrate the excellent defense performance of the proposed strategy towards state-of-the-art clean-label poisoning attacks in the transfer learning scenario.",
		"container-title": "Neural Networks",
		"DOI": "10.1016/j.neunet.2023.10.034",
		"ISSN": "0893-6080",
		"journalAbbreviation": "Neural Networks",
		"page": "756-763",
		"source": "ScienceDirect",
		"title": "Learning a robust foundation model against clean-label data poisoning attacks at downstream tasks",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0893608023005890",
		"volume": "169",
		"author": [
			{
				"family": "Zhou",
				"given": "Ting"
			},
			{
				"family": "Yan",
				"given": "Hanshu"
			},
			{
				"family": "Han",
				"given": "Bo"
			},
			{
				"family": "Liu",
				"given": "Lei"
			},
			{
				"family": "Zhang",
				"given": "Jingfeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					1
				]
			]
		}
	},
	{
		"id": "burdenHowSureBe2022",
		"type": "article",
		"abstract": "A principal concern for AI systems is the occurrence of negative side effects, such as a robot cleaner breaking a vase. This is critical when these systems use machine learning models that were trained to maximise performance, without knowledge or feedback about the negative side effects. Within Vase World and SafeLife, two safety benchmarking domains, we analyse side effects during operation and demonstrate that their magnitude is influenced by task difficulty. Using two forms of confidence measure, we demonstrate that wrapping existing RL agents with safety policies that activate when the agent’s confidence falls below a specified threshold extends the Pareto frontier of both performance and safety.",
		"event-place": "NeurIPS 2022 ML Safety Workshop",
		"language": "en",
		"publisher-place": "NeurIPS 2022 ML Safety Workshop",
		"source": "Zotero",
		"title": "How Sure to Be Safe? Difficulty, Confidence and Negative Side Effects",
		"author": [
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wardDefiningDeceptionStructural2023a",
		"type": "paper-conference",
		"abstract": "Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals. There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a functional definition of deception in structural causal games, grounded in the philosophical literature. We present several examples to establish that our formal definition captures philosophical desiderata for deception.",
		"collection-title": "AAMAS '23",
		"container-title": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems",
		"event-place": "Richland, SC",
		"ISBN": "978-1-4503-9432-1",
		"page": "2902–2904",
		"publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
		"publisher-place": "Richland, SC",
		"source": "ACM Digital Library",
		"title": "Defining Deception in Structural Causal Games",
		"author": [
			{
				"family": "Ward",
				"given": "Francis Rhys"
			},
			{
				"family": "Toni",
				"given": "Francesca"
			},
			{
				"family": "Belardinelli",
				"given": "Francesco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					30
				]
			]
		}
	},
	{
		"id": "jathoiiiSystemSafetyEngineering2022",
		"type": "article",
		"abstract": "Governments, industry, and academia have undertaken efforts to identify and mitigate harms in ML-driven systems, with a particular focus on social and ethical risks of ML components in complex sociotechnical systems. However, existing approaches are largely disjointed, ad-hoc and of unknown effectiveness. Systems safety engineering is a well established discipline with a track record of identifying and managing risks in many complex sociotechnical domains. We adopt the natural hypothesis that tools from this domain could serve to enhance risk analyses of ML in its context of use. To test this hypothesis, we apply a \"best of breed\" systems safety analysis, Systems Theoretic Process Analysis (STPA), to a specific high-consequence system with an important ML-driven component, namely the Prescription Drug Monitoring Programs (PDMPs) operated by many US States, several of which rely on an ML-derived risk score. We focus in particular on how this analysis can extend to identifying social and ethical risks and developing concrete design-level controls to mitigate them.",
		"DOI": "10.48550/arXiv.2211.04602",
		"note": "arXiv:2211.04602 [cs]",
		"number": "arXiv:2211.04602",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "System Safety Engineering for Social and Ethical ML Risks: A Case Study",
		"title-short": "System Safety Engineering for Social and Ethical ML Risks",
		"URL": "http://arxiv.org/abs/2211.04602",
		"author": [
			{
				"family": "Jatho III",
				"given": "Edgar W."
			},
			{
				"family": "Mailloux",
				"given": "Logan O."
			},
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Williams",
				"given": "Eugene D."
			},
			{
				"family": "Kroll",
				"given": "Joshua A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					8
				]
			]
		}
	},
	{
		"id": "kieransQuantifyingMisalignmentAgents2024",
		"type": "article",
		"abstract": "Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit. Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth. Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans). In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes. Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice. We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior. By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests.",
		"DOI": "10.48550/arXiv.2406.04231",
		"note": "arXiv:2406.04231 [cs]",
		"number": "arXiv:2406.04231",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Quantifying Misalignment Between Agents",
		"URL": "http://arxiv.org/abs/2406.04231",
		"author": [
			{
				"family": "Kierans",
				"given": "Aidan"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Hazan",
				"given": "Hananel"
			},
			{
				"family": "Dori-Hacohen",
				"given": "Shiri"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					6
				]
			]
		}
	},
	{
		"id": "daiCharacterizingOptimal012023",
		"type": "article",
		"abstract": "Finding classifiers robust to adversarial examples is critical for their safe deployment. Determining the robustness of the best possible classifier under a given threat model for a given data distribution and comparing it to that achieved by state-of-the-art training methods is thus an important diagnostic tool. In this paper, we find achievable information-theoretic lower bounds on loss in the presence of a test-time attacker for multi-class classifiers on any discrete dataset. We provide a general framework for finding the optimal 0-1 loss that revolves around the construction of a conflict hypergraph from the data and adversarial constraints. We further define other variants of the attacker-classifier game that determine the range of the optimal loss more efficiently than the full-fledged hypergraph construction. Our evaluation shows, for the first time, an analysis of the gap to optimal robustness for classifiers in the multi-class setting on benchmark datasets.",
		"DOI": "10.48550/arXiv.2302.10722",
		"note": "arXiv:2302.10722 [cs]",
		"number": "arXiv:2302.10722",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker",
		"URL": "http://arxiv.org/abs/2302.10722",
		"author": [
			{
				"family": "Dai",
				"given": "Sihui"
			},
			{
				"family": "Ding",
				"given": "Wenxin"
			},
			{
				"family": "Bhagoji",
				"given": "Arjun Nitin"
			},
			{
				"family": "Cullina",
				"given": "Daniel"
			},
			{
				"family": "Zhao",
				"given": "Ben Y."
			},
			{
				"family": "Zheng",
				"given": "Haitao"
			},
			{
				"family": "Mittal",
				"given": "Prateek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					6
				]
			]
		}
	},
	{
		"id": "lafonHybridEnergyBased2023",
		"type": "article",
		"abstract": "Out-of-distribution (OOD) detection is a critical requirement for the deployment of deep neural networks. This paper introduces the HEAT model, a new post-hoc OOD detection method estimating the density of in-distribution (ID) samples using hybrid energy-based models (EBM) in the feature space of a pre-trained backbone. HEAT complements prior density estimators of the ID density, e.g. parametric models like the Gaussian Mixture Model (GMM), to provide an accurate yet robust density estimation. A second contribution is to leverage the EBM framework to provide a unified density estimation and to compose several energy terms. Extensive experiments demonstrate the significance of the two contributions. HEAT sets new state-of-the-art OOD detection results on the CIFAR-10 / CIFAR-100 benchmark as well as on the large-scale Imagenet benchmark. The code is available at: https://github.com/MarcLafon/heatood.",
		"DOI": "10.48550/arXiv.2305.16966",
		"note": "arXiv:2305.16966 [cs]",
		"number": "arXiv:2305.16966",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection",
		"URL": "http://arxiv.org/abs/2305.16966",
		"author": [
			{
				"family": "Lafon",
				"given": "Marc"
			},
			{
				"family": "Ramzi",
				"given": "Elias"
			},
			{
				"family": "Rambour",
				"given": "Clément"
			},
			{
				"family": "Thome",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					1
				]
			]
		}
	},
	{
		"id": "lindermanFinegrainInferenceOutofDistribution2022",
		"type": "article",
		"abstract": "Machine learning methods must be trusted to make appropriate decisions in real-world environments, even when faced with out-of-distribution (OOD) samples. Many current approaches simply aim to detect OOD examples and alert the user when an unrecognized input is given. However, when the OOD sample significantly overlaps with the training data, a binary anomaly detection is not interpretable or explainable, and provides little information to the user. We propose a new model for OOD detection that makes predictions at varying levels of granularity as the inputs become more ambiguous, the model predictions become coarser and more conservative. Consider an animal classifier that encounters an unknown bird species and a car. Both cases are OOD, but the user gains more information if the classifier recognizes that its uncertainty over the particular species is too large and predicts bird instead of detecting it as OOD. Furthermore, we diagnose the classifiers performance at each level of the hierarchy improving the explainability and interpretability of the models predictions. We demonstrate the effectiveness of hierarchical classifiers for both fine- and coarse-grained OOD tasks.",
		"DOI": "10.48550/arXiv.2209.04493",
		"note": "arXiv:2209.04493 [cs]",
		"number": "arXiv:2209.04493",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification",
		"URL": "http://arxiv.org/abs/2209.04493",
		"author": [
			{
				"family": "Linderman",
				"given": "Randolph"
			},
			{
				"family": "Zhang",
				"given": "Jingyang"
			},
			{
				"family": "Inkawhich",
				"given": "Nathan"
			},
			{
				"family": "Li",
				"given": "Hai"
			},
			{
				"family": "Chen",
				"given": "Yiran"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					9
				]
			]
		}
	},
	{
		"id": "luIndiscriminateDataPoisoning2024",
		"type": "article",
		"abstract": "Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting \"poisoned\" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.",
		"DOI": "10.48550/arXiv.2204.09092",
		"note": "arXiv:2204.09092 [cs]",
		"number": "arXiv:2204.09092",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
		"URL": "http://arxiv.org/abs/2204.09092",
		"author": [
			{
				"family": "Lu",
				"given": "Yiwei"
			},
			{
				"family": "Kamath",
				"given": "Gautam"
			},
			{
				"family": "Yu",
				"given": "Yaoliang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					15
				]
			]
		}
	},
	{
		"id": "liMitigatingLiesVisionLanguage2022",
		"type": "paper-conference",
		"abstract": "In this work, we bring new insights into the honesty of vision-language models, particularly in visual question answering (VQA). After a throughout revisit of the existing ‘lie’ behavior in pure language models, our work makes an unprecedented extension of ’lies’ to vision-language models. The results indicate that the lie prefixes have a more obvious misleading effect on vision-language models than on language models. We also propose a novel visual prefix and prove that the consistent vision-language prefix is more threatening to vision-language models. To defend the models from the stated ’lies’, we put forward an unsupervised framework based on Gaussian mixture modeling and obtain improvement with 3% against the language prefix and 12% against the vision-language prefix.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Mitigating Lies in Vision-Language Models",
		"URL": "https://openreview.net/forum?id=mAiTuIeWbxD",
		"author": [
			{
				"family": "Li",
				"given": "Junbo"
			},
			{
				"family": "Li",
				"given": "Xianhang"
			},
			{
				"family": "Xie",
				"given": "Cihang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "mittaSafeguardedProgressReinforcement2023",
		"type": "article",
		"abstract": "This paper addresses the problem of maintaining safety during training in Reinforcement Learning (RL), such that the safety constraint violations are bounded at any point during learning. In a variety of RL applications the safety of the agent is particularly important, e.g. autonomous platforms or robots that work in proximity of humans. As enforcing safety during training might severely limit the agent's exploration, we propose here a new architecture that handles the trade-off between efficient progress and safety during exploration. As the exploration progresses, we update via Bayesian inference Dirichlet-Categorical models of the transition probabilities of the Markov decision process that describes the environment dynamics. This paper proposes a way to approximate moments of belief about the risk associated to the action selection policy. We construct those approximations, and prove the convergence results. We propose a novel method for leveraging the expectation approximations to derive an approximate bound on the confidence that the risk is below a certain level. This approach can be easily interleaved with RL and we present experimental results to showcase the performance of the overall architecture.",
		"DOI": "10.48550/arXiv.2312.11314",
		"note": "arXiv:2312.11314 [cs, eess]\nversion: 1",
		"number": "arXiv:2312.11314",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration for Control Policy Synthesis",
		"title-short": "Safeguarded Progress in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2312.11314",
		"author": [
			{
				"family": "Mitta",
				"given": "Rohan"
			},
			{
				"family": "Hasanbeig",
				"given": "Hosein"
			},
			{
				"family": "Wang",
				"given": "Jun"
			},
			{
				"family": "Kroening",
				"given": "Daniel"
			},
			{
				"family": "Kantaros",
				"given": "Yiannis"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					18
				]
			]
		}
	},
	{
		"id": "daniels-kochExpertiseProblemLearning2022",
		"type": "article",
		"abstract": "Reinforcement learning from human feedback (RLHF) is a powerful technique for training agents to perform difficult-to-specify tasks. However, human feedback can be noisy, particularly when human teachers lack relevant knowledge or experience. Levels of expertise vary across teachers, and a given teacher may have differing levels of expertise for different components of a task. RLHF algorithms that learn from multiple teachers therefore face an expertise problem: the reliability of a given piece of feedback depends both on the teacher that it comes from and how specialized that teacher is on relevant components of the task. Existing state-of-the-art RLHF algorithms assume that all evaluations come from the same distribution, obscuring this inter- and intra-human variance, and preventing them from accounting for or taking advantage of variations in expertise. We formalize this problem, implement it as an extension of an existing RLHF benchmark, evaluate the performance of a state-of-the-art RLHF algorithm, and explore techniques to improve query and teacher selection. Our key contribution is to demonstrate and characterize the expertise problem, and to provide an open-source implementation for testing future solutions.",
		"DOI": "10.48550/arXiv.2211.06519",
		"note": "arXiv:2211.06519 [cs]",
		"number": "arXiv:2211.06519",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Expertise Problem: Learning from Specialized Feedback",
		"title-short": "The Expertise Problem",
		"URL": "http://arxiv.org/abs/2211.06519",
		"author": [
			{
				"family": "Daniels-Koch",
				"given": "Oliver"
			},
			{
				"family": "Freedman",
				"given": "Rachel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		}
	},
	{
		"id": "bethuneRobustOneClassClassification2024",
		"type": "article",
		"abstract": "We propose a new method, dubbed One Class Signed Distance Function (OCSDF), to perform One Class Classification (OCC) by provably learning the Signed Distance Function (SDF) to the boundary of the support of any distribution. The distance to the support can be interpreted as a normality score, and its approximation using 1-Lipschitz neural networks provides robustness bounds against $l2$ adversarial attacks, an under-explored weakness of deep learning-based OCC algorithms. As a result, OCSDF comes with a new metric, certified AUROC, that can be computed at the same cost as any classical AUROC. We show that OCSDF is competitive against concurrent methods on tabular and image data while being way more robust to adversarial attacks, illustrating its theoretical properties. Finally, as exploratory research perspectives, we theoretically and empirically show how OCSDF connects OCC with image generation and implicit neural surface parametrization. Our code is available at https://github.com/Algue-Rythme/OneClassMetricLearning",
		"DOI": "10.48550/arXiv.2303.01978",
		"note": "arXiv:2303.01978 [cs]\nversion: 2",
		"number": "arXiv:2303.01978",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Robust One-Class Classification with Signed Distance Function using 1-Lipschitz Neural Networks",
		"URL": "http://arxiv.org/abs/2303.01978",
		"author": [
			{
				"family": "Bethune",
				"given": "Louis"
			},
			{
				"family": "Novello",
				"given": "Paul"
			},
			{
				"family": "Boissin",
				"given": "Thibaut"
			},
			{
				"family": "Coiffier",
				"given": "Guillaume"
			},
			{
				"family": "Serrurier",
				"given": "Mathieu"
			},
			{
				"family": "Vincenot",
				"given": "Quentin"
			},
			{
				"family": "Troya-Galvis",
				"given": "Andres"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					1
				]
			]
		}
	},
	{
		"id": "zhangFalsehoodsThatML2022",
		"type": "article",
		"abstract": "An intuitive way to detect out-of-distribution (OOD) data is via the density function of a fitted probabilistic generative model: points with low density may be classed as OOD. But this approach has been found to fail, in deep learning settings. In this paper, we list some falsehoods that machine learning researchers believe about density-based OOD detection. Many recent works have proposed likelihood-ratio-based methods to `fix' the problem. We propose a framework, the OOD proxy framework, to unify these methods, and we argue that likelihood ratio is a principled method for OOD detection and not a mere `fix'. Finally, we discuss the relationship between domain discrimination and semantics.",
		"DOI": "10.48550/arXiv.2210.12767",
		"note": "arXiv:2210.12767 [cs, stat]",
		"number": "arXiv:2210.12767",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Falsehoods that ML researchers believe about OOD detection",
		"URL": "http://arxiv.org/abs/2210.12767",
		"author": [
			{
				"family": "Zhang",
				"given": "Andi"
			},
			{
				"family": "Wischik",
				"given": "Damon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					1
				]
			]
		}
	},
	{
		"id": "perezIgnorePreviousPrompt2022",
		"type": "article",
		"abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.",
		"DOI": "10.48550/arXiv.2211.09527",
		"note": "arXiv:2211.09527 [cs]",
		"number": "arXiv:2211.09527",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Ignore Previous Prompt: Attack Techniques For Language Models",
		"title-short": "Ignore Previous Prompt",
		"URL": "http://arxiv.org/abs/2211.09527",
		"author": [
			{
				"family": "Perez",
				"given": "Fábio"
			},
			{
				"family": "Ribeiro",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					17
				]
			]
		}
	},
	{
		"id": "kalariaAdversarialPurificationUsing2022",
		"type": "article",
		"abstract": "With the rapid advancement and increased use of deep learning models in image identification, security becomes a major concern to their deployment in safety-critical systems. Since the accuracy and robustness of deep learning models are primarily attributed from the purity of the training samples, therefore the deep learning architectures are often susceptible to adversarial attacks. Adversarial attacks are often obtained by making subtle perturbations to normal images, which are mostly imperceptible to humans, but can seriously confuse the state-of-the-art machine learning models. We propose a framework, named APuDAE, leveraging Denoising AutoEncoders (DAEs) to purify these samples by using them in an adaptive way and thus improve the classification accuracy of the target classifier networks that have been attacked. We also show how using DAEs adaptively instead of using them directly, improves classification accuracy further and is more robust to the possibility of designing adaptive attacks to fool them. We demonstrate our results over MNIST, CIFAR-10, ImageNet dataset and show how our framework (APuDAE) provides comparable and in most cases better performance to the baseline methods in purifying adversaries. We also design adaptive attack specifically designed to attack our purifying model and demonstrate how our defense is robust to that.",
		"DOI": "10.48550/arXiv.2208.13838",
		"note": "arXiv:2208.13838 [cs]",
		"number": "arXiv:2208.13838",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Adversarial Purification using Denoising AutoEncoders",
		"URL": "http://arxiv.org/abs/2208.13838",
		"author": [
			{
				"family": "Kalaria",
				"given": "Dvij"
			},
			{
				"family": "Hazra",
				"given": "Aritra"
			},
			{
				"family": "Chakrabarti",
				"given": "Partha Pratim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					29
				]
			]
		}
	},
	{
		"id": "kangPoisoningGenerativeReplay2023",
		"type": "paper-conference",
		"abstract": "Generative models have grown into the workhorse of many state-of-the-art machine learning methods. However, their vulnerability under poisoning attacks has been largely understudied. In this work, we investigate this issue in the context of continual learning, where generative replayers are utilized to tackle catastrophic forgetting. By developing a novel customization of dirty-label input-aware backdoors to the online setting, our attacker manages to stealthily promote forgetting while retaining high accuracy at the current task and sustaining strong defenders. Our approach taps into an intriguing property of generative models, namely that they cannot well capture input-dependent triggers. Experiments on four standard datasets corroborate the poisoner’s effectiveness.",
		"container-title": "Proceedings of the 40th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "15769-15785",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Poisoning Generative Replay in Continual Learning to Promote Forgetting",
		"URL": "https://proceedings.mlr.press/v202/kang23c.html",
		"author": [
			{
				"family": "Kang",
				"given": "Siteng"
			},
			{
				"family": "Shi",
				"given": "Zhan"
			},
			{
				"family": "Zhang",
				"given": "Xinhua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "jakhotiyaAdversarialAttacksTransformersBased2022",
		"type": "article",
		"abstract": "Signature-based malware detectors have proven to be insufficient as even a small change in malignant executable code can bypass these signature-based detectors. Many machine learning-based models have been proposed to efficiently detect a wide variety of malware. Many of these models are found to be susceptible to adversarial attacks - attacks that work by generating intentionally designed inputs that can force these models to misclassify. Our work aims to explore vulnerabilities in the current state of the art malware detectors to adversarial attacks. We train a Transformers-based malware detector, carry out adversarial attacks resulting in a misclassification rate of 23.9% and propose defenses that reduce this misclassification rate to half. An implementation of our work can be found at https://github.com/yashjakhotiya/Adversarial-Attacks-On-Transformers.",
		"DOI": "10.48550/arXiv.2210.00008",
		"note": "arXiv:2210.00008 [cs]",
		"number": "arXiv:2210.00008",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Attacks on Transformers-Based Malware Detectors",
		"URL": "http://arxiv.org/abs/2210.00008",
		"author": [
			{
				"family": "Jakhotiya",
				"given": "Yash"
			},
			{
				"family": "Patil",
				"given": "Heramb"
			},
			{
				"family": "Rawlani",
				"given": "Jugal"
			},
			{
				"family": "Mane",
				"given": "Dr Sunil B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					5
				]
			]
		}
	},
	{
		"id": "pittarasCooperativeReinforcementLearning2022",
		"type": "article",
		"abstract": "In this paper we present a Reinforcement Learning environment that leverages agent cooperation and communication, aimed at detection, learning and ultimately penalizing betrayal patterns that emerge in the behavior of self-interested agents. We provide a description of game rules, along with interesting cases of betrayal and trade-offs that arise. Preliminary experimental investigations illustrate a) betrayal emergence, b) deceptive agents outperforming honest baselines and b) betrayal detection based on classification of behavioral features, which surpasses probabilistic detection baselines. Finally, we propose approaches for penalizing betrayal, list directions for future work and suggest interesting extensions of the environment towards capturing and exploring increasingly complex patterns of social interactions.",
		"DOI": "10.48550/arXiv.2210.12841",
		"note": "arXiv:2210.12841 [cs]",
		"number": "arXiv:2210.12841",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Cooperative Reinforcement Learning Environment for Detecting and Penalizing Betrayal",
		"URL": "http://arxiv.org/abs/2210.12841",
		"author": [
			{
				"family": "Pittaras",
				"given": "Nikiforos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					23
				]
			]
		}
	},
	{
		"id": "hingunREAPLargeScaleRealistic2023",
		"type": "article",
		"abstract": "Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto the sign. Using our benchmark, we perform the first large-scale assessments of adversarial patch attacks under realistic conditions. Our experiments suggest that adversarial patch attacks may present a smaller threat than previously believed and that the success rate of an attack on simpler digital simulations is not predictive of its actual effectiveness in practice. We release our benchmark publicly at https://github.com/wagner-group/reap-benchmark.",
		"DOI": "10.48550/arXiv.2212.05680",
		"note": "arXiv:2212.05680 [cs]",
		"number": "arXiv:2212.05680",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "REAP: A Large-Scale Realistic Adversarial Patch Benchmark",
		"title-short": "REAP",
		"URL": "http://arxiv.org/abs/2212.05680",
		"author": [
			{
				"family": "Hingun",
				"given": "Nabeel"
			},
			{
				"family": "Sitawarin",
				"given": "Chawin"
			},
			{
				"family": "Li",
				"given": "Jerry"
			},
			{
				"family": "Wagner",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					18
				]
			]
		}
	},
	{
		"id": "sitawarinPartBasedModelsImprove2023",
		"type": "article",
		"abstract": "We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.",
		"DOI": "10.48550/arXiv.2209.09117",
		"note": "arXiv:2209.09117 [cs]",
		"number": "arXiv:2209.09117",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Part-Based Models Improve Adversarial Robustness",
		"URL": "http://arxiv.org/abs/2209.09117",
		"author": [
			{
				"family": "Sitawarin",
				"given": "Chawin"
			},
			{
				"family": "Pongmala",
				"given": "Kornrapat"
			},
			{
				"family": "Chen",
				"given": "Yizheng"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Wagner",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					8
				]
			]
		}
	},
	{
		"id": "xiaoSmoothedSGDmaxStabilityInspiredAlgorithm2022",
		"type": "article-journal",
		"abstract": "Unlike standard training, deep neural networks can suffer from serious overfitting problems in adversarial settings, which is studied extensively by empirical papers. Recent research (e.g., Xing et al. (2021); Xiao et al. (2022)) show that SGDmax-based adversarial training algorithms with $1/s(T)$ training loss incurs a stability-based generalization bound in $\\Theta(c+s(T)/n)$. Here $T$ is the number of iterations, $n$ is the number of samples, $s(T)\\rightarrow \\infty$ as $T\\rightarrow \\infty$, and $c$ is a $n$-independent term. This reveals that adversarial training can have nonvanishing generalization errors even if the sample size $n$ goes to infinity. A natural question arises: can we eliminate the nonvanishing term $c$ by designing a more generalizable algorithm? We give an affirmative answer in this paper. First, by an adaptation of information-theoretical lower bound on the complexity of solving Lipschitz-convex problems using randomized algorithms, we show that a minimax lower bound for adversarial generalization gap is $\\Omega(s(T)/n)$ given training loss $1/s(T)$. This implies that SGDmax does not achieve the lower bound. Next, by observing that the nonvanishing generalization error term for SGDmax comes from the non-smoothness of the adversarial loss function, we employ a smoothing technique to smooth the adversarial loss function. Based on the smoothed loss function, we design a smoothed SGDmax algorithm achieving generalization bound $\\mathcal{O}(s(T)/n)$, which matches the minimax lower bound. Experimentally, we show that our algorithm improves adversarial generalization on common datasets.",
		"language": "en",
		"source": "openreview.net",
		"title": "Smoothed-SGDmax: A Stability-Inspired Algorithm to Improve Adversarial Generalization",
		"title-short": "Smoothed-SGDmax",
		"URL": "https://openreview.net/forum?id=O0sS_cujvV0",
		"author": [
			{
				"family": "Xiao",
				"given": "Jiancong"
			},
			{
				"family": "Zhang",
				"given": "Jiawei"
			},
			{
				"family": "Luo",
				"given": "Zhi-Quan"
			},
			{
				"family": "Ozdaglar",
				"given": "Asuman E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		}
	},
	{
		"id": "diHiddenPoisonMachine2022",
		"type": "article",
		"abstract": "We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.",
		"DOI": "10.48550/arXiv.2212.10717",
		"note": "arXiv:2212.10717 [cs]",
		"number": "arXiv:2212.10717",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks",
		"title-short": "Hidden Poison",
		"URL": "http://arxiv.org/abs/2212.10717",
		"author": [
			{
				"family": "Di",
				"given": "Jimmy Z."
			},
			{
				"family": "Douglas",
				"given": "Jack"
			},
			{
				"family": "Acharya",
				"given": "Jayadev"
			},
			{
				"family": "Kamath",
				"given": "Gautam"
			},
			{
				"family": "Sekhari",
				"given": "Ayush"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					20
				]
			]
		}
	},
	{
		"id": "zhangDiagnosingRectifyingVision2023",
		"type": "article",
		"abstract": "Recent multi-modal contrastive learning models have demonstrated the ability to learn an embedding space suitable for building strong vision classifiers, by leveraging the rich information in large-scale image-caption datasets. Our work highlights a distinct advantage of this multi-modal embedding space: the ability to diagnose vision classifiers through natural language. The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality. On a range of image datasets with known error slices, we demonstrate that our method can effectively identify the error slices and influential attributes, and can further use language to rectify failure modes of the classifier.",
		"DOI": "10.48550/arXiv.2302.04269",
		"note": "arXiv:2302.04269 [cs]",
		"number": "arXiv:2302.04269",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Diagnosing and Rectifying Vision Models using Language",
		"URL": "http://arxiv.org/abs/2302.04269",
		"author": [
			{
				"family": "Zhang",
				"given": "Yuhui"
			},
			{
				"family": "HaoChen",
				"given": "Jeff Z."
			},
			{
				"family": "Huang",
				"given": "Shih-Cheng"
			},
			{
				"family": "Wang",
				"given": "Kuan-Chieh"
			},
			{
				"family": "Zou",
				"given": "James"
			},
			{
				"family": "Yeung",
				"given": "Serena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					8
				]
			]
		}
	},
	{
		"id": "davariDeceivingCKASimilarity2022",
		"type": "paper-conference",
		"abstract": "Understanding the behaviour of trained deep neural networks is a critical step in allowing reliable deployment of these networks in critical applications. One direction for obtaining insights on neural networks is through comparison of their internal representations. Comparing neural representations in neural networks is thus a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network's different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of conclusions about similarity and dissimilarity of these various representations have been made using CKA. In this work we present an analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation of CKA sensitivity to outliers and to transformations that preserve the linear separability of the data, an important generalization attribute. Finally we propose an optimization-based approach for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Deceiving the CKA Similarity Measure in Deep Learning",
		"URL": "https://openreview.net/forum?id=hITONWhDIIJ",
		"author": [
			{
				"family": "Davari",
				"given": "MohammadReza"
			},
			{
				"family": "Horoi",
				"given": "Stefan"
			},
			{
				"family": "Natik",
				"given": "Amine"
			},
			{
				"family": "Lajoie",
				"given": "Guillaume"
			},
			{
				"family": "Wolf",
				"given": "Guy"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "lubanaMechanisticModeConnectivity2023",
		"type": "paper-conference",
		"abstract": "We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model’s mechanisms, e.g., fine-tuning can fail to eliminate a model’s reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model’s mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model’s reliance on spurious attributes.",
		"container-title": "Proceedings of the 40th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "22965-23004",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Mechanistic Mode Connectivity",
		"URL": "https://proceedings.mlr.press/v202/lubana23a.html",
		"author": [
			{
				"family": "Lubana",
				"given": "Ekdeep Singh"
			},
			{
				"family": "Bigelow",
				"given": "Eric J."
			},
			{
				"family": "Dick",
				"given": "Robert P."
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Tanaka",
				"given": "Hidenori"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "chenVisualPromptingAdversarial2023",
		"type": "article",
		"abstract": "In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at testing time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at testing time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1X standard accuracy gain and 2X robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42X inference time speedup.",
		"DOI": "10.48550/arXiv.2210.06284",
		"note": "arXiv:2210.06284 [cs]",
		"number": "arXiv:2210.06284",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Visual Prompting for Adversarial Robustness",
		"URL": "http://arxiv.org/abs/2210.06284",
		"author": [
			{
				"family": "Chen",
				"given": "Aochuan"
			},
			{
				"family": "Lorenz",
				"given": "Peter"
			},
			{
				"family": "Yao",
				"given": "Yuguang"
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			},
			{
				"family": "Liu",
				"given": "Sijia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					30
				]
			]
		}
	},
	{
		"id": "chengIdentificationAdversarySingle2023",
		"type": "paper-conference",
		"abstract": "Deep neural networks have been shown vulnerable to adversarial examples. Even though many defense methods have been proposed to enhance the robustness, it is still a long way toward providing an attack-free method to build a trustworthy machine learning system. In this paper, instead of enhancing the robustness, we take the investigator’s perspective and propose a new framework to trace the first compromised model copy in a forensic investigation manner. Specifically, we focus on the following setting: the machine learning service provider provides model copies for a set of customers. However, one of the customers conducted adversarial attacks to fool the system. Therefore, the investigator’s objective is to identify the first compromised copy by collecting and analyzing evidence from only available adversarial examples. To make the tracing viable, we design a random mask watermarking mechanism to differentiate adversarial examples from different copies. First, we propose a tracing approach in the data-limited case where the original example is also available. Then, we design a data-free approach to identify the adversary without accessing the original example. Finally, the effectiveness of our proposed framework is evaluated by extensive experiments with different model architectures, adversarial attacks, and datasets.",
		"container-title": "Proceedings of the 40th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "5472-5484",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Identification of the Adversary from a Single Adversarial Example",
		"URL": "https://proceedings.mlr.press/v202/cheng23c.html",
		"author": [
			{
				"family": "Cheng",
				"given": "Minhao"
			},
			{
				"family": "Min",
				"given": "Rui"
			},
			{
				"family": "Sun",
				"given": "Haochen"
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "ahnMitigatingDatasetBias2023",
		"type": "article",
		"abstract": "The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes having a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels involving human or empirical correlation metrics (e.g., training loss). However, such metrics require human costs or have insufficient theoretical explanation. In this study, we propose a debiasing algorithm, called PGD (Per-sample Gradient-based Debiasing), that comprises three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various synthetic and real-world datasets, the proposed method showed state-of-the-art accuracy for a the classification task. Furthermore, we describe theoretical understandings about how PGD can mitigate dataset bias.",
		"DOI": "10.48550/arXiv.2205.15704",
		"note": "arXiv:2205.15704 [cs]",
		"number": "arXiv:2205.15704",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Mitigating Dataset Bias by Using Per-sample Gradient",
		"URL": "http://arxiv.org/abs/2205.15704",
		"author": [
			{
				"family": "Ahn",
				"given": "Sumyeong"
			},
			{
				"family": "Kim",
				"given": "Seongyoon"
			},
			{
				"family": "Yun",
				"given": "Se-young"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					10
				]
			]
		}
	},
	{
		"id": "bernasconiGeneralFrameworkSafe2022",
		"type": "paper-conference",
		"abstract": "We study the problem of online interaction in general decision making problems, where the objective is not only to find optimal strategies, but also to satisfy some safety guarantees, expressed in terms of costs accrued. We propose a theoretical framework to address such problems and present BAN-SOLO, a UCB-like algorithm that, in an online interaction with an unknown environment, attains sublinear regret of order O(T^{1/2}) and plays safely with high probability at each iteration. At its core, BAN-SOLO relies on tools from convex duality to manage environment exploration while satisfying the safety constraints imposed by the problem.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "A General Framework for Safe Decision Making: A Convex Duality Approach",
		"title-short": "A General Framework for Safe Decision Making",
		"URL": "https://openreview.net/forum?id=qHc5B5iEaSx",
		"author": [
			{
				"family": "Bernasconi",
				"given": "Martino"
			},
			{
				"family": "Cacciamani",
				"given": "Federico"
			},
			{
				"family": "Gatti",
				"given": "Nicola"
			},
			{
				"family": "Trovò",
				"given": "Francesco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "castiglioniUnifyingFrameworkOnline2022",
		"type": "article",
		"abstract": "We study online learning problems in which a decision maker has to take a sequence of decisions subject to $m$ long-term constraints. The goal of the decision maker is to maximize their total reward, while at the same time achieving small cumulative constraints violation across the $T$ rounds. We present the first best-of-both-world type algorithm for this general class of problems, with no-regret guarantees both in the case in which rewards and constraints are selected according to an unknown stochastic model, and in the case in which they are selected at each round by an adversary. Our algorithm is the first to provide guarantees in the adversarial setting with respect to the optimal fixed strategy that satisfies the long-term constraints. In particular, it guarantees a $\\rho/(1+\\rho)$ fraction of the optimal reward and sublinear regret, where $\\rho$ is a feasibility parameter related to the existence of strictly feasible solutions. Our framework employs traditional regret minimizers as black-box components. Therefore, by instantiating it with an appropriate choice of regret minimizers it can handle the full-feedback as well as the bandit-feedback setting. Moreover, it allows the decision maker to seamlessly handle scenarios with non-convex rewards and constraints. We show how our framework can be applied in the context of budget-management mechanisms for repeated auctions in order to guarantee long-term constraints that are not packing (e.g., ROI constraints).",
		"DOI": "10.48550/arXiv.2209.07454",
		"note": "arXiv:2209.07454 [cs, math]",
		"number": "arXiv:2209.07454",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Unifying Framework for Online Optimization with Long-Term Constraints",
		"URL": "http://arxiv.org/abs/2209.07454",
		"author": [
			{
				"family": "Castiglioni",
				"given": "Matteo"
			},
			{
				"family": "Celli",
				"given": "Andrea"
			},
			{
				"family": "Marchesi",
				"given": "Alberto"
			},
			{
				"family": "Romano",
				"given": "Giulia"
			},
			{
				"family": "Gatti",
				"given": "Nicola"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					15
				]
			]
		}
	},
	{
		"id": "kimEffectiveTargetedAttacks2023",
		"type": "article",
		"abstract": "Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given instance towards the selected target. Our method demonstrates significant enhancements in robustness when applied to non-contrastive SSL frameworks, and less but consistent robustness improvements with contrastive SSL frameworks, on the benchmark datasets.",
		"DOI": "10.48550/arXiv.2210.10482",
		"note": "arXiv:2210.10482 [cs]",
		"number": "arXiv:2210.10482",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Effective Targeted Attacks for Adversarial Self-Supervised Learning",
		"URL": "http://arxiv.org/abs/2210.10482",
		"author": [
			{
				"family": "Kim",
				"given": "Minseon"
			},
			{
				"family": "Ha",
				"given": "Hyeonjeong"
			},
			{
				"family": "Son",
				"given": "Sooel"
			},
			{
				"family": "Hwang",
				"given": "Sung Ju"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					26
				]
			]
		}
	},
	{
		"id": "wenCanaryCoalmineBetter2023",
		"type": "article",
		"abstract": "As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine.",
		"DOI": "10.48550/arXiv.2210.10750",
		"note": "arXiv:2210.10750 [cs]",
		"number": "arXiv:2210.10750",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries",
		"title-short": "Canary in a Coalmine",
		"URL": "http://arxiv.org/abs/2210.10750",
		"author": [
			{
				"family": "Wen",
				"given": "Yuxin"
			},
			{
				"family": "Bansal",
				"given": "Arpit"
			},
			{
				"family": "Kazemi",
				"given": "Hamid"
			},
			{
				"family": "Borgnia",
				"given": "Eitan"
			},
			{
				"family": "Goldblum",
				"given": "Micah"
			},
			{
				"family": "Geiping",
				"given": "Jonas"
			},
			{
				"family": "Goldstein",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					1
				]
			]
		}
	},
	{
		"id": "jangCanLargeLanguage2022",
		"type": "article",
		"abstract": "Previous work has shown that there exists a scaling law between the size of Language Models (LMs) and their zero-shot performance on different downstream NLP tasks. In this work, we show that this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts. By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions. We provide the code and the datasets to explore negated prompts at https://github.com/joeljang/negated-prompts-for-llms",
		"DOI": "10.48550/arXiv.2209.12711",
		"note": "arXiv:2209.12711 [cs]",
		"number": "arXiv:2209.12711",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts",
		"title-short": "Can Large Language Models Truly Understand Prompts?",
		"URL": "http://arxiv.org/abs/2209.12711",
		"author": [
			{
				"family": "Jang",
				"given": "Joel"
			},
			{
				"family": "Ye",
				"given": "Seonghyeon"
			},
			{
				"family": "Seo",
				"given": "Minjoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					26
				]
			]
		}
	},
	{
		"id": "caballeroBrokenNeuralScaling2023",
		"type": "article",
		"abstract": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures & for each of various tasks within a large & diverse set of upstream & downstream tasks, in zero-shot, prompted, & finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, \"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning, & reinforcement learning (single agent & multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models & extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent & the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
		"DOI": "10.48550/arXiv.2210.14891",
		"note": "arXiv:2210.14891 [cs]",
		"number": "arXiv:2210.14891",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Broken Neural Scaling Laws",
		"URL": "http://arxiv.org/abs/2210.14891",
		"author": [
			{
				"family": "Caballero",
				"given": "Ethan"
			},
			{
				"family": "Gupta",
				"given": "Kshitij"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					23
				]
			]
		}
	},
	{
		"id": "mehraDomainGeneralizationMethods2022",
		"type": "paper-conference",
		"abstract": "Domain Generalization (DG) methods use data from multiple related source domains to learn models whose performance does not degrade on unseen domains at test time. Many DG algorithms rely on reducing the divergence between the source distributions in a representation space to potentially align unseen domains close to the sources. These algorithms are motivated by the analytical works that explain generalization to unseen domains based on their distributional distance (e.g., Wasserstein distance) to the sources. However, we show that the accuracy of a DG model varies significantly on unseen domains equidistant from the sources in the learned representation space. This makes it hard to gauge the generalization performance of DG models only based on their performance on benchmark datasets. Thus, we study the worst-case loss of a DG model at a particular distance from the sources and propose an evaluation methodology based on distributionally robust optimization that efficiently computes the worst-case loss on all distributions within a Wasserstein ball around the sources. Our results show that models trained with popular DG methods incur a high worst-case loss even close to the sources which show their lack of generalization to unseen domains. Moreover, we observe a large gap between the worst-case and the empirical losses of distributions at the same distance, showing the performance of the DG models on benchmark datasets is not representative of their performance on unseen domains. Thus, our (target) data-independent and worst-case loss-based methodology highlights the poor generalization performance of current DG models and provides insights beyond empirical evaluation on benchmark datasets for improving these models.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Do Domain Generalization Methods Generalize Well?",
		"URL": "https://openreview.net/forum?id=SRWIQ0Yl53m",
		"author": [
			{
				"family": "Mehra",
				"given": "Akshay"
			},
			{
				"family": "Kailkhura",
				"given": "Bhavya"
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			},
			{
				"family": "Hamm",
				"given": "Jihun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "kulynychWhatYouSee2024",
		"type": "paper-conference",
		"abstract": "Having similar behavior at training time and test time—what we call a \"What You See Is What You Get\" (WYSIWYG) property—is desirable in machine learning. Models trained with standard stochastic gradient descent (SGD), however, do not necessarily have this property, as their complex behaviors such as robustness or subgroup performance can differ drastically between training and test time. In contrast, we show that Differentially-Private (DP) training provably ensures the high-level WYSIWYG property, which we quantify using a notion of distributional generalization. Applying this connection, we introduce new conceptual tools for designing deep-learning methods by reducing generalization concerns to optimization ones: to mitigate unwanted behavior at test time, it is provably sufficient to mitigate this behavior on the training data. By applying this novel design principle, which bypasses \"pathologies\" of SGD, we construct simple algorithms that are competitive with SOTA in several distributional-robustness applications, significantly improve the privacy vs. disparate impact trade-off of DP-SGD, and mitigate robust overfitting in adversarial training. Finally, we also improve on theoretical bounds relating DP, stability, and distributional generalization.",
		"collection-title": "NIPS '22",
		"container-title": "Proceedings of the 36th International Conference on Neural Information Processing Systems",
		"event-place": "Red Hook, NY, USA",
		"ISBN": "978-1-7138-7108-8",
		"page": "2168–2183",
		"publisher": "Curran Associates Inc.",
		"publisher-place": "Red Hook, NY, USA",
		"source": "ACM Digital Library",
		"title": "What you see is what you get: principled deep learning via distributional generalization",
		"title-short": "What you see is what you get",
		"author": [
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Yang",
				"given": "Yao-Yuan"
			},
			{
				"family": "Yu",
				"given": "Yaodong"
			},
			{
				"family": "Błasiok",
				"given": "Jarosław"
			},
			{
				"family": "Nakkiran",
				"given": "Preetum"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					3
				]
			]
		}
	},
	{
		"id": "gunnAdversarialPoisoningAttacks2022",
		"type": "paper-conference",
		"abstract": "Complex controls are increasingly common in power systems. Reinforcement learning (RL) has emerged as a strong candidate for implementing various controllers. One common use of RL in this context is for prosumer pricing aggregations, where prosumers consist of buildings with both solar generation and energy storage. Specifically, supply and demand data serve as the observation space for many microgrid controllers acting based on a policy passed from a central RL agent. Each controller outputs an action space consisting of hourly \"buy\" and \"sell\" prices for energy throughout the day; in turn, each prosumer can choose whether to transact with the RL agent or the utility. The RL agent, who is learning online, is rewarded through its ability to generate a profit.We ask: what happens when some of the microgrid controllers are compromised by a malicious entity? We demonstrate a novel attack in RL and a simple defense against the attack. Our attack perturbs each trajectory to reverse the direction of the estimated gradient. We demonstrate that if data from a small fraction of microgrid controllers is adversarially perturbed, the learning of the RL agent can be significantly slowed. With larger perturbations, the RL aggregator can be manipulated to learn a catastrophic pricing policy that causes the RL agent to operate at a loss. Other environmental characteristics are worsened too: prosumers face higher energy costs, use their batteries less, and suffer from higher peak demand when the pricing aggregator is adversarially poisoned.We address this vulnerability with a \"defense\" module; i.e., a \"robustification\" of RL algorithms against this attack. Our defense identifies the trajectories with the largest influence on the gradient and removes them from the training data. It is computationally light and reasonable to include in any RL algorithm.",
		"collection-title": "BuildSys '22",
		"container-title": "Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
		"DOI": "10.1145/3563357.3564075",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9890-9",
		"page": "262–265",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Adversarial poisoning attacks on reinforcement learning-driven energy pricing",
		"URL": "https://doi.org/10.1145/3563357.3564075",
		"author": [
			{
				"family": "Gunn",
				"given": "Sam"
			},
			{
				"family": "Jang",
				"given": "Doseok"
			},
			{
				"family": "Paradise",
				"given": "Orr"
			},
			{
				"family": "Spangher",
				"given": "Lucas"
			},
			{
				"family": "Spanos",
				"given": "Costas J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					8
				]
			]
		}
	},
	{
		"id": "zhangOutofDistributionDetectionClass2022",
		"type": "article",
		"abstract": "Density-based Out-of-distribution (OOD) detection has recently been shown unreliable for the task of detecting OOD images. Various density ratio based approaches achieve good empirical performance, however methods typically lack a principled probabilistic modelling explanation. In this work, we propose to unify density ratio based methods under a novel framework that builds energy-based models and employs differing base distributions. Under our framework, the density ratio can be viewed as the unnormalized density of an implicit semantic distribution. Further, we propose to directly estimate the density ratio of a data sample through class ratio estimation. We report competitive results on OOD image problems in comparison with recent work that alternatively requires training of deep generative models for the task. Our approach enables a simple and yet effective path towards solving the OOD detection problem.",
		"language": "en",
		"note": "arXiv:2206.03955 [cs, stat]",
		"number": "arXiv:2206.03955",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Out-of-Distribution Detection with Class Ratio Estimation",
		"URL": "http://arxiv.org/abs/2206.03955",
		"author": [
			{
				"family": "Zhang",
				"given": "Mingtian"
			},
			{
				"family": "Zhang",
				"given": "Andi"
			},
			{
				"family": "Xiao",
				"given": "Tim Z."
			},
			{
				"family": "Sun",
				"given": "Yitong"
			},
			{
				"family": "McDonagh",
				"given": "Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					8
				]
			]
		}
	},
	{
		"id": "font-reaulxAlignmentDynamicProcess2022",
		"type": "paper-conference",
		"abstract": "Most learning AIs today have exogenously given and fixed aims which they gradually learn to optimize for. It has been an assumption in alignment research that artificial general intelligences of the kind that could pose an X-risk would too. On this assumption, value alignment becomes the task of finding the right set of aims before we allow the agent to act. However, an agent can also have aims that fundamentally change during their lifetime. The task of aligning such agents is not one of specifying a set of aims, but of designing a meta-function that guides the agent’s developing aims to an equilibrium that produces behaviour aligned with our human values. If artificial general intelligences would possess such dynamic aims, then this has significant implications for the kind of alignment research we should conduct today. In this paper, I argue that there is a substantial probability that artificial general intelligences would have such dynamic aims, and in response I articulate an agenda for dynamic alignment research.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Alignment as a Dynamic Process",
		"URL": "https://openreview.net/forum?id=-6jortffIq",
		"author": [
			{
				"family": "Font-Reaulx",
				"given": "Paul",
				"dropping-particle": "de"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "kimLearningTransferableAdversarial2023",
		"type": "article",
		"abstract": "Despite the success on few-shot learning problems, most meta-learned models only focus on achieving good performance on clean examples and thus easily break down when given adversarially perturbed samples. While some recent works have shown that a combination of adversarial learning and meta-learning could enhance the robustness of a meta-learner against adversarial attacks, they fail to achieve generalizable adversarial robustness to unseen domains and tasks, which is the ultimate goal of meta-learning. To address this challenge, we propose a novel meta-adversarial multi-view representation learning framework with dual encoders. Specifically, we introduce the discrepancy across the two differently augmented samples of the same data instance by first updating the encoder parameters with them and further imposing a novel label-free adversarial attack to maximize their discrepancy. Then, we maximize the consistency across the views to learn transferable robust representations across domains and tasks. Through experimental validation on multiple benchmarks, we demonstrate the effectiveness of our framework on few-shot learning tasks from unseen domains, achieving over 10\\% robust accuracy improvements against previous adversarial meta-learning baselines.",
		"DOI": "10.48550/arXiv.2210.10485",
		"note": "arXiv:2210.10485 [cs]",
		"number": "arXiv:2210.10485",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Transferable Adversarial Robust Representations via Multi-view Consistency",
		"URL": "http://arxiv.org/abs/2210.10485",
		"author": [
			{
				"family": "Kim",
				"given": "Minseon"
			},
			{
				"family": "Ha",
				"given": "Hyeonjeong"
			},
			{
				"family": "Lee",
				"given": "Dong Bok"
			},
			{
				"family": "Hwang",
				"given": "Sung Ju"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					26
				]
			]
		}
	},
	{
		"id": "farquharWhatOutofdistributionNot2022",
		"type": "paper-conference",
		"abstract": "Researchers want to generalize robustly to ‘out-of-distribution’ (OOD) data. Unfortunately, this term is used ambiguously causing confusion and creating risk—people might believe they have made progress on OOD data and not realize this progress only holds in limited cases. We critique a standard definition of OOD—difference-in-distribution—and then disambiguate four meaningful types of OOD data: transformed-distributions, related-distributions, complement-distributions, and synthetic-distributions. We describe how existing OOD datasets, evaluations, and techniques fit into this framework. We provide a template for researchers to carefully present the scope of distribution shift considered in their work.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "What 'Out-of-distribution' Is and Is Not",
		"URL": "https://openreview.net/forum?id=XCS_zBHQA2i",
		"author": [
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Gal",
				"given": "Yarin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "korkmazAdversarialRobustDeep2023",
		"type": "article-journal",
		"abstract": "Learning from raw high dimensional data via interaction with a given environment has been effectively achieved through the utilization of deep neural networks. Yet the observed degradation in policy performance caused by imperceptible worst-case policy dependent translations along high sensitivity directions (i.e. adversarial perturbations) raises concerns on the robustness of deep reinforcement learning policies. In our paper, we show that these high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box setting. Furthermore, we show that vanilla training techniques intriguingly result in learning more robust policies compared to the policies learnt via the state-of-the-art adversarial training techniques. We believe our work lays out intriguing properties of the deep reinforcement learning policy manifold and our results can help to build robust and generalizable deep reinforcement learning policies.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v37i7.26009",
		"ISSN": "2374-3468, 2159-5399",
		"issue": "7",
		"journalAbbreviation": "AAAI",
		"language": "en",
		"page": "8369-8377",
		"source": "DOI.org (Crossref)",
		"title": "Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/26009",
		"volume": "37",
		"author": [
			{
				"family": "Korkmaz",
				"given": "Ezgi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					26
				]
			]
		}
	},
	{
		"id": "kirchheimOutlierExposureGenerative2022",
		"type": "paper-conference",
		"abstract": "While Outlier Exposure reliably increases the performance of Out-of-Distribution detectors, it requires a set of available outliers during training. In this paper, we propose Generative Outlier Exposure (GOE), which alleviates the need for available outliers by using generative models to sample synthetic outliers from low-density regions of the data distribution. The approach requires no modification of the generator, works on image and text data, and can be used with pre-trained models. We demonstrate the effectiveness of generated outliers on several image and text datasets, including ImageNet.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "On Outlier Exposure with Generative Models",
		"URL": "https://openreview.net/forum?id=SU7OAfhc8OM",
		"author": [
			{
				"family": "Kirchheim",
				"given": "Konstantin"
			},
			{
				"family": "Ortmeier",
				"given": "Frank"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "haEfficientFrameworkMonitoring2022",
		"type": "article",
		"abstract": "Monitoring machine learning systems post deployment is critical to ensure the reliability of the systems. Particularly importance is the problem of monitoring the performance of machine learning systems across all the data subgroups (subpopulations). In practice, this process could be prohibitively expensive as the number of data subgroups grows exponentially with the number of input features, and the process of labelling data to evaluate each subgroup's performance is costly. In this paper, we propose an efficient framework for monitoring subgroup performance of machine learning systems. Specifically, we aim to find the data subgroup with the worst performance using a limited number of labeled data. We mathematically formulate this problem as an optimization problem with an expensive black-box objective function, and then suggest to use Bayesian optimization to solve this problem. Our experimental results on various real-world datasets and machine learning systems show that our proposed framework can retrieve the worst-performing data subgroup effectively and efficiently.",
		"DOI": "10.48550/arXiv.2212.08312",
		"note": "arXiv:2212.08312 [cs]",
		"number": "arXiv:2212.08312",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "An Efficient Framework for Monitoring Subgroup Performance of Machine Learning Systems",
		"URL": "http://arxiv.org/abs/2212.08312",
		"author": [
			{
				"family": "Ha",
				"given": "Huong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					16
				]
			]
		}
	},
	{
		"id": "korkmazSpectralRobustnessAnalysis2022",
		"type": "paper-conference",
		"abstract": "Deep reinforcement learning algorithms enabled learning functioning policies in MDPs with complex state representations. Following these advancements deep reinforcement learning polices have been deployed in many diverse settings. However, a line of research argued that in certain settings building a reward function can be more complicated than learning it. Hence, several studies proposed different methods to learn a reward function by observing trajectories of a functioning policy (i.e. inverse reinforcement learning). Following this line of research several studies proposed to directly learn a functioning policy by solely observing trajectories of an expert (i.e. imitation learning). In this paper, we propose a novel method to analyze the spectral robustness of deep neural policies. We conduct several experiments in the Arcade Learning Environment, and demonstrate that simple vanilla trained deep reinforcement learning policies are more robust than deep imitation learning policies. We believe that our method provides a comprehensive analysis on the policy robustness and can help in understanding the fundamental properties of different training techniques.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Spectral Robustness Analysis of Deep Imitation Learning",
		"URL": "https://openreview.net/forum?id=RF97w2nVtQJ",
		"author": [
			{
				"family": "Korkmaz",
				"given": "Ezgi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "kalraCanDifferentiableDecision2024",
		"type": "article",
		"abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for capturing human intent to alleviate the challenges of hand-crafting the reward values. Despite the increasing interest in RLHF, most works learn black box reward functions that while expressive are difficult to interpret and often require running the whole costly process of RL before we can even decipher if these frameworks are actually aligned with human preferences. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs). Our experiments across several domains, including CartPole, Visual Gridworld environments and Atari games, provide evidence that the tree structure of our learned reward function is useful in determining the extent to which the reward function is aligned with human preferences. We also provide experimental evidence that not only shows that reward DDTs can often achieve competitive RL performance when compared with larger capacity deep neural network reward functions but also demonstrates the diagnostic utility of our framework in checking alignment of learned reward functions. We also observe that the choice between soft and hard (argmax) output of reward DDT reveals a tension between wanting highly shaped rewards to ensure good RL performance, while also wanting simpler, more interpretable rewards. Videos and code, are available at: https://sites.google.com/view/ddt-rlhf",
		"language": "en",
		"note": "arXiv:2306.13004 [cs]",
		"number": "arXiv:2306.13004",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can Differentiable Decision Trees Enable Interpretable Reward Learning from Human Feedback?",
		"URL": "http://arxiv.org/abs/2306.13004",
		"author": [
			{
				"family": "Kalra",
				"given": "Akansha"
			},
			{
				"family": "Brown",
				"given": "Daniel S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					24
				]
			]
		}
	},
	{
		"id": "zhouSteeringLargeLanguage2022",
		"type": "paper-conference",
		"abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model. Due to the lack of knowledge of how LLMs work, most effective prompts have been handcrafted by humans through a demanding trial and error process. To reduce the human effort involved in this alignment process, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. We treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate how well the selected instruction can steer the model to desired behavior, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. Moreover, we show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Steering Large Language Models using APE",
		"URL": "https://openreview.net/forum?id=JjvNzMOiBEp",
		"author": [
			{
				"family": "Zhou",
				"given": "Yongchao"
			},
			{
				"family": "Muresanu",
				"given": "Andrei Ioan"
			},
			{
				"family": "Han",
				"given": "Ziwen"
			},
			{
				"family": "Paster",
				"given": "Keiran"
			},
			{
				"family": "Pitis",
				"given": "Silviu"
			},
			{
				"family": "Chan",
				"given": "Harris"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "houMultiLevelFrameworkAI2023",
		"type": "article",
		"abstract": "AI alignment considers how we can encode AI systems in a way that is compatible with human values. The normative side of this problem asks what moral values or principles, if any, we should encode in AI. To this end, we present a framework to consider the question at four levels: Individual, Organizational, National, and Global. We aim to illustrate how AI alignment is made up of value alignment problems at each of these levels, where values at each level affect the others and effects can flow in either direction. We outline key questions and considerations of each level and demonstrate an application of this framework to the topic of AI content moderation.",
		"DOI": "10.48550/arXiv.2301.03740",
		"note": "arXiv:2301.03740 [cs]",
		"number": "arXiv:2301.03740",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Multi-Level Framework for the AI Alignment Problem",
		"URL": "http://arxiv.org/abs/2301.03740",
		"author": [
			{
				"family": "Hou",
				"given": "Betty Li"
			},
			{
				"family": "Green",
				"given": "Brian Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					9
				]
			]
		}
	},
	{
		"id": "korkmazDeepReinforcementLearning2021",
		"type": "article",
		"abstract": "The use of deep neural networks as function approximators has led to striking progress for reinforcement learning algorithms and applications. Yet the knowledge we have on decision boundary geometry and the loss landscape of neural policies is still quite limited. In this paper we propose a framework to investigate the decision boundary and loss landscape similarities across states and across MDPs. We conduct experiments in various games from Arcade Learning Environment, and discover that high sensitivity directions for neural policies are correlated across MDPs. We argue that these high sensitivity directions support the hypothesis that non-robust features are shared across training environments of reinforcement learning agents. We believe our results reveal fundamental properties of the environments used in deep reinforcement learning training, and represent a tangible step towards building robust and reliable deep reinforcement learning agents.",
		"DOI": "10.48550/arXiv.2112.09025",
		"note": "arXiv:2112.09025 [cs, stat]",
		"number": "arXiv:2112.09025",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs",
		"URL": "http://arxiv.org/abs/2112.09025",
		"author": [
			{
				"family": "Korkmaz",
				"given": "Ezgi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					16
				]
			]
		}
	},
	{
		"id": "besnierInstanceAwareObserverNetwork2022",
		"type": "article",
		"abstract": "Recent works on predictive uncertainty estimation have shown promising results on Out-Of-Distribution (OOD) detection for semantic segmentation. However, these methods struggle to precisely locate the point of interest in the image, i.e, the anomaly. This limitation is due to the difficulty of finegrained prediction at the pixel level. To address this issue, we build upon the recent ObsNet approach by providing object instance knowledge to the observer. We extend ObsNet by harnessing an instance-wise mask prediction. We use an additional, class agnostic, object detector to filter and aggregate observer predictions. Finally, we predict an unique anomaly score for each instance in the image. We show that our proposed method accurately disentangles in-distribution objects from OOD objects on three datasets.",
		"DOI": "10.48550/arXiv.2207.08782",
		"note": "arXiv:2207.08782 [cs]",
		"number": "arXiv:2207.08782",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Instance-Aware Observer Network for Out-of-Distribution Object Segmentation",
		"URL": "http://arxiv.org/abs/2207.08782",
		"author": [
			{
				"family": "Besnier",
				"given": "Victor"
			},
			{
				"family": "Bursuc",
				"given": "Andrei"
			},
			{
				"family": "Picard",
				"given": "David"
			},
			{
				"family": "Briot",
				"given": "Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					29
				]
			]
		}
	},
	{
		"id": "shaoAdversarialRobustnessVision2022",
		"type": "article",
		"abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.",
		"DOI": "10.48550/arXiv.2103.15670",
		"note": "arXiv:2103.15670 [cs]",
		"number": "arXiv:2103.15670",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Adversarial Robustness of Vision Transformers",
		"URL": "http://arxiv.org/abs/2103.15670",
		"author": [
			{
				"family": "Shao",
				"given": "Rulin"
			},
			{
				"family": "Shi",
				"given": "Zhouxing"
			},
			{
				"family": "Yi",
				"given": "Jinfeng"
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			},
			{
				"family": "Hsieh",
				"given": "Cho-Jui"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					2
				]
			]
		}
	},
	{
		"id": "premchandarUnifiedProbabilisticNeural2022",
		"type": "article",
		"abstract": "Robust machine learning models with accurately calibrated uncertainties are crucial for safety-critical applications. Probabilistic machine learning and especially the Bayesian formalism provide a systematic framework to incorporate robustness through the distributional estimates and reason about uncertainty. Recent works have shown that approximate inference approaches that take the weight space uncertainty of neural networks to generate ensemble prediction are the state-of-the-art. However, architecture choices have mostly been ad hoc, which essentially ignores the epistemic uncertainty from the architecture space. To this end, we propose a Unified probabilistic architecture and weight ensembling Neural Architecture Search (UraeNAS) that leverages advances in probabilistic neural architecture search and approximate Bayesian inference to generate ensembles form the joint distribution of neural network architectures and weights. The proposed approach showed a significant improvement both with in-distribution (0.86% in accuracy, 42% in ECE) CIFAR-10 and out-of-distribution (2.43% in accuracy, 30% in ECE) CIFAR-10-C compared to the baseline deterministic approach.",
		"DOI": "10.48550/arXiv.2210.04083",
		"note": "arXiv:2210.04083 [cs]",
		"number": "arXiv:2210.04083",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unified Probabilistic Neural Architecture and Weight Ensembling Improves Model Robustness",
		"URL": "http://arxiv.org/abs/2210.04083",
		"author": [
			{
				"family": "Premchandar",
				"given": "Sumegha"
			},
			{
				"family": "Madireddy",
				"given": "Sandeep"
			},
			{
				"family": "Jantre",
				"given": "Sanket"
			},
			{
				"family": "Balaprakash",
				"given": "Prasanna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					8
				]
			]
		}
	},
	{
		"id": "griffinAllWellThat2022",
		"type": "paper-conference",
		"abstract": "Misspecifying the reward function of a reinforcement learning agent may cause catastrophic side effects. In this work, we investigate \\textit{distance-impact penalties}: a general-purpose auxiliary reward based on a state-distance measure that captures, and thus can be used to penalise, side effects. We prove that the size of the penalty depends only on an agent's final impact on the environment. Distance-impact penalties are scalable, general, and immediately compatible with model-free algorithms. We analyse the sensitivity of an agent's behaviour to the choice of penalty, expanding results about reward-shaping, proving sufficient and necessary conditions for policy-optimality to be invariant to misspecification, and providing error bounds for optimal policies. Finally, we empirically investigate distance-impact penalties in a range of grid-world environments, demonstrating their ability to prevent side effects whilst permitting task completion.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "All’s Well That Ends Well: Avoiding Side Effects with Distance-Impact Penalties",
		"title-short": "All’s Well That Ends Well",
		"URL": "https://openreview.net/forum?id=3tgegVVh2j6",
		"author": [
			{
				"family": "Griffin",
				"given": "Charlie"
			},
			{
				"family": "Skalse",
				"given": "Joar Max Viktor"
			},
			{
				"family": "Hammond",
				"given": "Lewis"
			},
			{
				"family": "Abate",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "barezSystemIIILearning2023",
		"type": "article",
		"abstract": "Reinforcement learning agents naturally learn from extensive exploration. Exploration is costly and can be unsafe in $\\textit{safety-critical}$ domains. This paper proposes a novel framework for incorporating domain knowledge to help guide safe exploration and boost sample efficiency. Previous approaches impose constraints, such as regularisation parameters in neural networks, that rely on large sample sets and often are not suitable for safety-critical domains where agents should almost always avoid unsafe actions. In our approach, called $\\textit{System III}$, which is inspired by psychologists' notions of the brain's $\\textit{System I}$ and $\\textit{System II}$, we represent domain expert knowledge of safety in form of first-order logic. We evaluate the satisfaction of these constraints via p-norms in state vector space. In our formulation, constraints are analogous to hazards, objects, and regions of state that have to be avoided during exploration. We evaluated the effectiveness of the proposed method on OpenAI's Gym and Safety-Gym environments. In all tasks, including classic Control and Safety Games, we show that our approach results in safer exploration and sample efficiency.",
		"DOI": "10.48550/arXiv.2304.11593",
		"note": "arXiv:2304.11593 [cs]",
		"number": "arXiv:2304.11593",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "System III: Learning with Domain Knowledge for Safety Constraints",
		"title-short": "System III",
		"URL": "http://arxiv.org/abs/2304.11593",
		"author": [
			{
				"family": "Barez",
				"given": "Fazl"
			},
			{
				"family": "Hasanbieg",
				"given": "Hosien"
			},
			{
				"family": "Abbate",
				"given": "Alesandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					23
				]
			]
		}
	},
	{
		"id": "guptaCanActiveSampling2023",
		"type": "article",
		"abstract": "Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data. Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations. This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent. In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world. In this paper, we study causal confusion in offline reinforcement learning. We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment. To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation. We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling.",
		"DOI": "10.48550/arXiv.2312.17168",
		"note": "arXiv:2312.17168 [cs]",
		"number": "arXiv:2312.17168",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can Active Sampling Reduce Causal Confusion in Offline Reinforcement Learning?",
		"URL": "http://arxiv.org/abs/2312.17168",
		"author": [
			{
				"family": "Gupta",
				"given": "Gunshi"
			},
			{
				"family": "Rudner",
				"given": "Tim G. J."
			},
			{
				"family": "McAllister",
				"given": "Rowan Thomas"
			},
			{
				"family": "Gaidon",
				"given": "Adrien"
			},
			{
				"family": "Gal",
				"given": "Yarin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					28
				]
			]
		}
	},
	{
		"id": "hameedBoundaryAdversarialExamples2022",
		"type": "article",
		"abstract": "Standard adversarial training approaches suffer from robust overfitting where the robust accuracy decreases when models are adversarially trained for too long. The origin of this problem is still unclear and conflicting explanations have been reported, i.e., memorization effects induced by large loss data or because of small loss data and growing differences in loss distribution of training samples as the adversarial training progresses. Consequently, several mitigation approaches including early stopping, temporal ensembling and weight perturbations on small loss data have been proposed to mitigate the effect of robust overfitting. However, a side effect of these strategies is a larger reduction in clean accuracy compared to standard adversarial training. In this paper, we investigate if these mitigation approaches are complimentary to each other in improving adversarial training performance. We further propose the use of helper adversarial examples that can be obtained with minimal cost in the adversarial example generation, and show how they increase the clean accuracy in the existing approaches without compromising the robust accuracy.",
		"DOI": "10.48550/arXiv.2211.14088",
		"note": "arXiv:2211.14088 [cs]",
		"number": "arXiv:2211.14088",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Boundary Adversarial Examples Against Adversarial Overfitting",
		"URL": "http://arxiv.org/abs/2211.14088",
		"author": [
			{
				"family": "Hameed",
				"given": "Muhammad Zaid"
			},
			{
				"family": "Buesser",
				"given": "Beat"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					25
				]
			]
		}
	},
	{
		"id": "albrechtSuperhumanPerformanceCurrent2022",
		"type": "article",
		"abstract": "Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly \"super-human\" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to \"explain their reasoning\" often leads to alarming justifications of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.",
		"DOI": "10.48550/arXiv.2212.06295",
		"note": "arXiv:2212.06295 [cs]",
		"number": "arXiv:2212.06295",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety",
		"URL": "http://arxiv.org/abs/2212.06295",
		"author": [
			{
				"family": "Albrecht",
				"given": "Joshua"
			},
			{
				"family": "Kitanidis",
				"given": "Ellie"
			},
			{
				"family": "Fetterman",
				"given": "Abraham J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					12
				]
			]
		}
	},
	{
		"id": "wangInterpretabilityWildCircuit2022a",
		"type": "article",
		"abstract": "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.",
		"DOI": "10.48550/arXiv.2211.00593",
		"note": "arXiv:2211.00593 [cs]",
		"number": "arXiv:2211.00593",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small",
		"title-short": "Interpretability in the Wild",
		"URL": "http://arxiv.org/abs/2211.00593",
		"author": [
			{
				"family": "Wang",
				"given": "Kevin"
			},
			{
				"family": "Variengien",
				"given": "Alexandre"
			},
			{
				"family": "Conmy",
				"given": "Arthur"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					1
				]
			]
		}
	},
	{
		"id": "palumboTwoHeadsAre2023",
		"type": "article",
		"abstract": "Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Tram\\`er showed that, in the rejection-only case (no transduction), a strong rejection-solution can be turned into a strong (but computationally inefficient) non-rejection solution. This detector-to-classifier reduction has been mostly applied to give evidence that certain claims of strong selective-model solutions are susceptible, leaving the benefits of rejection unclear. On the other hand, a recent work by Goldwasser et al. showed that rejection combined with transduction can give provable guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA, which has been shown to be much more effective than AutoAttack against transduction), Goldwasser et al.'s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Theoretically, we show that a novel application of Tram\\`er's classifier-to-detector technique in the transductive setting can give significantly improved sample-complexity for robust generalization. While our theoretical construction is computationally inefficient, it guides us to identify an efficient transductive algorithm to learn a selective model. Extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our solutions provide significantly better robust accuracy.",
		"DOI": "10.48550/arXiv.2305.17528",
		"note": "arXiv:2305.17528 [cs]",
		"number": "arXiv:2305.17528",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Two Heads are Better than One: Towards Better Adversarial Robustness by Combining Transduction and Rejection",
		"title-short": "Two Heads are Better than One",
		"URL": "http://arxiv.org/abs/2305.17528",
		"author": [
			{
				"family": "Palumbo",
				"given": "Nils"
			},
			{
				"family": "Guo",
				"given": "Yang"
			},
			{
				"family": "Wu",
				"given": "Xi"
			},
			{
				"family": "Chen",
				"given": "Jiefeng"
			},
			{
				"family": "Liang",
				"given": "Yingyu"
			},
			{
				"family": "Jha",
				"given": "Somesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					27
				]
			]
		}
	},
	{
		"id": "phamCMBAAdversarialAttack2022",
		"type": "paper-conference",
		"abstract": "In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named \\textbf{c-MBA}. Our proposed attack can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "c-MBA: Adversarial Attack for Cooperative MARL Using Learned Dynamics Model",
		"title-short": "c-MBA",
		"URL": "https://openreview.net/forum?id=AFfKSfcF6Sv",
		"author": [
			{
				"family": "Pham",
				"given": "Nhan H."
			},
			{
				"family": "Nguyen",
				"given": "Lam M."
			},
			{
				"family": "Chen",
				"given": "Jie"
			},
			{
				"family": "Lam",
				"given": "Hoang Thanh"
			},
			{
				"family": "Das",
				"given": "Subhro"
			},
			{
				"family": "Weng",
				"given": "Lily"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "nanfackFeatureVisualizationVisual2024",
		"type": "article",
		"abstract": "Understanding the inner working functionality of large-scale deep neural networks is challenging yet crucial in several high-stakes applications. Mechanistic inter- pretability is an emergent field that tackles this challenge, often by identifying human-understandable subgraphs in deep neural networks known as circuits. In vision-pretrained models, these subgraphs are usually interpreted by visualizing their node features through a popular technique called feature visualization. Recent works have analyzed the stability of different feature visualization types under the adversarial model manipulation framework. This paper starts by addressing limitations in existing works by proposing a novel attack called ProxPulse that simultaneously manipulates the two types of feature visualizations. Surprisingly, when analyzing these attacks under the umbrella of visual circuits, we find that visual circuits show some robustness to ProxPulse. We, therefore, introduce a new attack based on ProxPulse that unveils the manipulability of visual circuits, shedding light on their lack of robustness. The effectiveness of these attacks is validated using pre-trained AlexNet and ResNet-50 models on ImageNet.",
		"DOI": "10.48550/arXiv.2406.01365",
		"note": "arXiv:2406.01365 [cs]",
		"number": "arXiv:2406.01365",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "From Feature Visualization to Visual Circuits: Effect of Adversarial Model Manipulation",
		"title-short": "From Feature Visualization to Visual Circuits",
		"URL": "http://arxiv.org/abs/2406.01365",
		"author": [
			{
				"family": "Nanfack",
				"given": "Geraldin"
			},
			{
				"family": "Eickenberg",
				"given": "Michael"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					3
				]
			]
		}
	},
	{
		"id": "martyAdversarialAttacksFeature2022",
		"type": "paper-conference",
		"abstract": "The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Feature visualization approaches are one set of techniques used to interpret and analyze trained deep learning models. On the other hand interpretability methods themselves may be subject to be deceived. In particular, we consider the idea of an adversary manipulating a model for the purpose of deceiving the interpretation. Focusing on the popular feature visualizations associated with CNNs we introduce an optimization framework for modifying the outcome of feature visualization methods.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Adversarial Attacks on Feature Visualization Methods",
		"URL": "https://openreview.net/forum?id=J51K0rszIjr",
		"author": [
			{
				"family": "Marty",
				"given": "Jonathan"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			},
			{
				"family": "Eickenberg",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "ardeshirEmbeddingReliabilityPredictability2022",
		"type": "paper-conference",
		"abstract": "In (self-)supervised (pre-)training, such as in contrastive learning, often a network is presented with correspondent (positive) and non-correspondent (negative) pairs of datapoints, and is trained to find an embedding vector for each datapoint, i.e., a representation, which can be further fine-tuned for various downstream tasks. To safely deploy these models in critical decision-making systems, it is crucial to equip them with a measure of their reliability. Here we study whether such measures can be quantified for a datapoint in a meaningful way. In other words, we explore if the downstream performance on a given datapoint is predictable, directly from a few characteristics of its pre-trained embedding. We study whether this goal can be achieved by directly estimating the distribution of the training data in the embedding space, and accounting for the local consistency of the representations. Our experiments show that these notions of reliability often strongly correlate with its downstream accuracy.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Embedding Reliability: On the Predictability of Downstream Performance",
		"title-short": "Embedding Reliability",
		"URL": "https://openreview.net/forum?id=TedqYedIERd",
		"author": [
			{
				"family": "Ardeshir",
				"given": "Shervin"
			},
			{
				"family": "Azizan",
				"given": "Navid"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "mckinneyFragilityLearnedReward2023",
		"type": "article",
		"abstract": "Reward functions are notoriously difficult to specify, especially for tasks with complex goals. Reward learning approaches attempt to infer reward functions from human feedback and preferences. Prior works on reward learning have mainly focused on the performance of policies trained alongside the reward function. This practice, however, may fail to detect learned rewards that are not capable of training new policies from scratch and thus do not capture the intended behavior. Our work focuses on demonstrating and studying the causes of these relearning failures in the domain of preference-based reward learning. We demonstrate with experiments in tabular and continuous control environments that the severity of relearning failures can be sensitive to changes in reward model design and the trajectory dataset composition. Based on our findings, we emphasize the need for more retraining-based evaluations in the literature.",
		"DOI": "10.48550/arXiv.2301.03652",
		"note": "arXiv:2301.03652 [cs]",
		"number": "arXiv:2301.03652",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On The Fragility of Learned Reward Functions",
		"URL": "http://arxiv.org/abs/2301.03652",
		"author": [
			{
				"family": "McKinney",
				"given": "Lev"
			},
			{
				"family": "Duan",
				"given": "Yawen"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					9
				]
			]
		}
	},
	{
		"id": "fowlDecepticonsCorruptedTransformers2023",
		"type": "article",
		"abstract": "A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.",
		"DOI": "10.48550/arXiv.2201.12675",
		"note": "arXiv:2201.12675 [cs]",
		"number": "arXiv:2201.12675",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models",
		"title-short": "Decepticons",
		"URL": "http://arxiv.org/abs/2201.12675",
		"author": [
			{
				"family": "Fowl",
				"given": "Liam"
			},
			{
				"family": "Geiping",
				"given": "Jonas"
			},
			{
				"family": "Reich",
				"given": "Steven"
			},
			{
				"family": "Wen",
				"given": "Yuxin"
			},
			{
				"family": "Czaja",
				"given": "Wojtek"
			},
			{
				"family": "Goldblum",
				"given": "Micah"
			},
			{
				"family": "Goldstein",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					31
				]
			]
		}
	},
	{
		"id": "kireevAdversarialRobustnessTabular2023",
		"type": "article",
		"abstract": "Many safety-critical applications of machine learning, such as fraud or abuse detection, use data in tabular domains. Adversarial examples can be particularly damaging for these applications. Yet, existing works on adversarial robustness primarily focus on machine-learning models in image and text domains. We argue that, due to the differences between tabular data and images or text, existing threat models are not suitable for tabular domains. These models do not capture that the costs of an attack could be more significant than imperceptibility, or that the adversary could assign different values to the utility obtained from deploying different adversarial examples. We demonstrate that, due to these differences, the attack and defense methods used for images and text cannot be directly applied to tabular settings. We address these issues by proposing new cost and utility-aware threat models that are tailored to the adversarial capabilities and constraints of attackers targeting tabular domains. We introduce a framework that enables us to design attack and defense mechanisms that result in models protected against cost and utility-aware adversaries, for example, adversaries constrained by a certain financial budget. We show that our approach is effective on three datasets corresponding to applications for which adversarial examples can have economic and social implications.",
		"DOI": "10.48550/arXiv.2208.13058",
		"note": "arXiv:2208.13058 [cs]",
		"number": "arXiv:2208.13058",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Robustness for Tabular Data through Cost and Utility Awareness",
		"URL": "http://arxiv.org/abs/2208.13058",
		"author": [
			{
				"family": "Kireev",
				"given": "Klim"
			},
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Troncoso",
				"given": "Carmela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					24
				]
			]
		}
	},
	{
		"id": "klassenEpistemicSideEffects2022",
		"type": "paper-conference",
		"abstract": "AI safety research has investigated the problem of negative side effects -- undesirable changes made by AI systems in pursuit of an underspecified objective. However, the focus has been on physical side effects, such as a robot breaking a vase while moving. In this paper we introduce the notion of epistemic side effects, unintended changes made to the knowledge or beliefs of agents, and describe a way to avoid negative epistemic side effects in reinforcement learning, in some cases.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Epistemic Side Effects & Avoiding Them (Sometimes)",
		"URL": "https://openreview.net/forum?id=7oDZ-6kIW1K",
		"author": [
			{
				"family": "Klassen",
				"given": "Toryn Q."
			},
			{
				"family": "Alamdari",
				"given": "Parand Alizadeh"
			},
			{
				"family": "McIlraith",
				"given": "Sheila A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "krishnaImprovingRobustnessConditional2022",
		"type": "paper-conference",
		"abstract": "The evaluation of conditional language modeling tasks such as abstractive summarization typically uses test data that is identically distributed as training. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance under distribution shift caused by such noise is relatively under-studied. We present a large empirical study quantifying the sometimes severe loss in performance (up to 12 ROUGE-1 points) from different types of input noise for a range of datasets and model sizes. We then propose a light-weight method for detecting and removing such noise in the input during model inference without requiring any extra training or auxiliary models, which effectively mitigates the loss in performance, recovering up to 11 ROUGE-1 points.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Improving the Robustness of Conditional Language Models by Detecting and Removing Input Noise",
		"URL": "https://openreview.net/forum?id=DQ5eenIbSSW",
		"author": [
			{
				"family": "Krishna",
				"given": "Kundan"
			},
			{
				"family": "Zhao",
				"given": "Yao"
			},
			{
				"family": "Ren",
				"given": "Jie"
			},
			{
				"family": "Lakshminarayanan",
				"given": "Balaji"
			},
			{
				"family": "Luo",
				"given": "Jiaming"
			},
			{
				"family": "Saleh",
				"given": "Mohammad"
			},
			{
				"family": "Liu",
				"given": "Peter J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "chiuLearningRepresentationsRobust2022",
		"type": "article",
		"abstract": "Despite the high performance achieved by deep neural networks on various tasks, extensive studies have demonstrated that small tweaks in the input could fail the model predictions. This issue of deep neural networks has led to a number of methods to improve model robustness, including adversarial training and distributionally robust optimization. Though both of these two methods are geared towards learning robust models, they have essentially different motivations: adversarial training attempts to train deep neural networks against perturbations, while distributional robust optimization aims at improving model performance on the most difficult \"uncertain distributions\". In this work, we propose an algorithm that combines adversarial training and group distribution robust optimization to improve robust representation learning. Experiments on three image benchmark datasets illustrate that the proposed method achieves superior results on robust metrics without sacrificing much of the standard measures.",
		"DOI": "10.48550/arXiv.2202.09446",
		"note": "arXiv:2202.09446 [cs]",
		"number": "arXiv:2202.09446",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Representations Robust to Group Shifts and Adversarial Examples",
		"URL": "http://arxiv.org/abs/2202.09446",
		"author": [
			{
				"family": "Chiu",
				"given": "Ming-Chang"
			},
			{
				"family": "Ma",
				"given": "Xuezhe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					18
				]
			]
		}
	},
	{
		"id": "ghoseGeometricAttacksBatch2022",
		"type": "paper-conference",
		"abstract": "Constructing adversarial examples usually requires labels, which provide a loss gradient to construct the example. We show that for batch normalized architectures, intermediate latents that are produced after a batch normalization step suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, without any label. We motivate our loss through the geometry of batch normed representations and concentration on a known hypersphere. Our losses build on and expand intermediate latent based attacks that usually require labels. The success of our method implies that leakage of intermediate representations may suffice to create a security breach for deployed models, which persist even when the model is transferred to downstream usage. We further show that removal of batch norm weakens our attack significantly, suggesting that batch norm's contribution to adversarial vulnerability may be understood by analyzing such attacks.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Geometric attacks on batch normalization",
		"URL": "https://openreview.net/forum?id=92DijVF2Ryv",
		"author": [
			{
				"family": "Ghose",
				"given": "Amur"
			},
			{
				"family": "Gupta",
				"given": "Apurv"
			},
			{
				"family": "Yu",
				"given": "Yaoliang"
			},
			{
				"family": "Poupart",
				"given": "Pascal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "baharloueiImprovingAdversarialRobustness2023",
		"type": "article",
		"abstract": "This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the \"abstain\" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to \"model degeneracy\", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.",
		"DOI": "10.48550/arXiv.2210.14410",
		"note": "arXiv:2210.14410 [cs]",
		"number": "arXiv:2210.14410",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes",
		"URL": "http://arxiv.org/abs/2210.14410",
		"author": [
			{
				"family": "Baharlouei",
				"given": "Sina"
			},
			{
				"family": "Sheikholeslami",
				"given": "Fatemeh"
			},
			{
				"family": "Razaviyayn",
				"given": "Meisam"
			},
			{
				"family": "Kolter",
				"given": "Zico"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					10
				]
			]
		}
	},
	{
		"id": "xuNetflixForgetFast2022",
		"type": "paper-conference",
		"abstract": "Suppose a person, who has streamed rom-coms exclusively with their significant other, suddenly breaks up. Consider an expecting mom, who has shopped for baby clothes, miscarries. Their streaming and shopping recommendations, however, do not necessarily update, serving as unhappy reminders of their loss. One approach is to implement the Right To Be Forgotten for recommendation systems built from user data, with the goal of updating downstream recommendations to reflect the removal without incurring the cost of re-training. Inspired by solutions to the original Netflix challenge~\\citep{koren2009bellkor}, we develop Unlearn-ALS, which is more aggressively forgetful of select data than fine-tuning. In theory, it is consistent with retraining without model degradation. Empirically, it shows fast convergence, and can be applied directly to any bi-linear models regardless of the training procedure.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Netflix and Forget: Fast Severance From Memorizing Training Data in Recommendations",
		"title-short": "Netflix and Forget",
		"URL": "https://openreview.net/forum?id=DmzF_IuR_Gv",
		"author": [
			{
				"family": "Xu",
				"given": "Mimee"
			},
			{
				"family": "Sun",
				"given": "Jiankai"
			},
			{
				"family": "Yang",
				"given": "Xin"
			},
			{
				"family": "Yao",
				"given": "Yuanshun"
			},
			{
				"family": "Wang",
				"given": "Chong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "wangEvaluatingWorstCase2022",
		"type": "paper-conference",
		"abstract": "Several algorithms are proposed to improve the robustness of deep neural networks against adversarial perturbations beyond $\\ell_p$ cases, i.e. weather perturbations. However, evaluations of existing robust training algorithms are over-optimistic. This is in part due to the lack of a standardized evaluation protocol across various robust training algorithms, leading to ad-hoc methods that test robustness on either random perturbations or the adversarial samples from generative models that are used for robust training, which is either uninformative of the worst case, or is heavily biased. In this paper, we identify such evaluation bias in these existing works and propose the first standardized and fair evaluation that compares various robust training algorithms by using physics simulators for common adverse weather effects i.e. rain and snow. With this framework, we evaluated several existing robust training algorithms on two streetview classification datasets (BIC\\_GSV, Places365) and show the evaluation bias in experiments.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Evaluating Worst Case Adversarial Weather Perturbations Robustness",
		"URL": "https://openreview.net/forum?id=Q69Aj3P0PtV",
		"author": [
			{
				"family": "Wang",
				"given": "Yihan"
			},
			{
				"family": "Ba",
				"given": "Yunhao"
			},
			{
				"family": "Zhang",
				"given": "Howard Chenyang"
			},
			{
				"family": "Zhang",
				"given": "Huan"
			},
			{
				"family": "Kadambi",
				"given": "Achuta"
			},
			{
				"family": "Soatto",
				"given": "Stefano"
			},
			{
				"family": "Wong",
				"given": "Alex"
			},
			{
				"family": "Hsieh",
				"given": "Cho-Jui"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "schmaltzIntrospectionUpdatabilityUncertainty2022",
		"type": "paper-conference",
		"abstract": "When deploying Transformer networks, we seek the ability to introspect the predictions against instances with known labels; update the model without a full re-training; and provide reliable uncertainty quantification over the predictions. We demonstrate that these properties are achievable via recently proposed approaches for approximating deep neural networks with instance-based metric learners, at varying resolutions of the input, and the associated Venn-ADMIT Predictor for constructing prediction sets. We consider a challenging (but non-adversarial) task: Zero-shot sequence labeling (i.e., feature detection) in a low-accuracy, class-imbalanced, covariate-shifted setting while requiring a high confidence level.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Introspection, Updatability, and Uncertainty Quantification with Transformers: Concrete Methods for AI Safety",
		"title-short": "Introspection, Updatability, and Uncertainty Quantification with Transformers",
		"URL": "https://openreview.net/forum?id=w9U_7Ay7f86",
		"author": [
			{
				"family": "Schmaltz",
				"given": "Allen"
			},
			{
				"family": "Rasooly",
				"given": "Danielle"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "liSamplespecificBackdoorAttack2023",
		"type": "article",
		"abstract": "Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and malicious methods since they can easily circumvent most of the current backdoor defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due to their poisoned-label nature, where users can discover anomalies if they check the image-label relationship. In particular, we demonstrate that it is ineffective to directly generalize existing SSBAs to their clean-label variants by poisoning samples solely from the target class. We reveal that it is primarily due to two reasons, including \\textbf{(1)} the `antagonistic effects' of ground-truth features and \\textbf{(2)} the learning difficulty of sample-specific features. Accordingly, trigger-related features of existing SSBAs cannot be effectively learned under the clean-label setting due to their mild trigger intensity required for ensuring stealthiness. We argue that the intensity constraint of existing SSBAs is mostly because their trigger patterns are `content-irrelevant' and therefore act as `noises' for both humans and DNNs. Motivated by this understanding, we propose to exploit content-relevant features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with attribute trigger (BAAT). Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our BAAT and its resistance to existing defenses.",
		"DOI": "10.48550/arXiv.2312.04584",
		"note": "arXiv:2312.04584 [cs]",
		"number": "arXiv:2312.04584",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger",
		"URL": "http://arxiv.org/abs/2312.04584",
		"author": [
			{
				"family": "Li",
				"given": "Yiming"
			},
			{
				"family": "Zhu",
				"given": "Mingyan"
			},
			{
				"family": "Guo",
				"given": "Junfeng"
			},
			{
				"family": "Wei",
				"given": "Tao"
			},
			{
				"family": "Xia",
				"given": "Shu-Tao"
			},
			{
				"family": "Qin",
				"given": "Zhan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					10
				]
			]
		}
	},
	{
		"id": "raghavanAvoidingCalvinistDecision2022",
		"type": "paper-conference",
		"abstract": "Causal Decision Theory (CDT) is a popular choice among practical decision theorists. While its successes and failings have been extensively studied, a less investigated topic is how CDT's choices hinge on the theory of causation used. The most common interpretation, temporal CDT, understands causation as a description of physical processes ordered in time. Another emerging view comes from the graphical framework of Structural Causal Models (SCM), which sees causation in terms of constraints on sources of variation in a system. We present an adversarial scheme where a CDT agent facing a Bandit problem can be tricked into sub-optimal choices, if it follows temporal CDT. We then propose an axiom to ground the orientation of arrows in the causal graph of a decision problem. In doing so, we resolve an ambiguity in the theory of SCMs, and underscore the importance of agent-perspectives, which have been largely ignored in the causal inference literature. We also demonstrate how this structural CDT avoids our adversarial trap, and outperforms temporal CDT in a series of canonical decision problems.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Avoiding Calvinist Decision Traps using Structural Causal Models",
		"URL": "https://openreview.net/forum?id=pLEDGG6J6wZ",
		"author": [
			{
				"family": "Raghavan",
				"given": "Arvind"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "guerinOutOfDistributionDetectionNot2023",
		"type": "article",
		"abstract": "The usage of deep neural networks in safety-critical systems is limited by our ability to guarantee their correct behavior. Runtime monitors are components aiming to identify unsafe predictions and discard them before they can lead to catastrophic consequences. Several recent works on runtime monitoring have focused on out-of-distribution (OOD) detection, i.e., identifying inputs that are different from the training data. In this work, we argue that OOD detection is not a well-suited framework to design efficient runtime monitors and that it is more relevant to evaluate monitors based on their ability to discard incorrect predictions. We call this setting out-ofmodel-scope detection and discuss the conceptual differences with OOD. We also conduct extensive experiments on popular datasets from the literature to show that studying monitors in the OOD setting can be misleading: 1. very good OOD results can give a false impression of safety, 2. comparison under the OOD setting does not allow identifying the best monitor to detect errors. Finally, we also show that removing erroneous training data samples helps to train better monitors.",
		"DOI": "10.48550/arXiv.2211.16158",
		"note": "arXiv:2211.16158 [cs, eess]",
		"number": "arXiv:2211.16158",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Out-Of-Distribution Detection Is Not All You Need",
		"URL": "http://arxiv.org/abs/2211.16158",
		"author": [
			{
				"family": "Guérin",
				"given": "Joris"
			},
			{
				"family": "Delmas",
				"given": "Kevin"
			},
			{
				"family": "Ferreira",
				"given": "Raul Sena"
			},
			{
				"family": "Guiochet",
				"given": "Jérémie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					13
				]
			]
		}
	},
	{
		"id": "goschRevisitingRobustnessGraph2023",
		"type": "article",
		"abstract": "Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the inference process of GNNs significantly reduces over-robustness, while having a positive effect on test accuracy and adversarial robustness. Theoretically, leveraging our new semantics-aware notion of robustness, we prove that there is no robustness-accuracy tradeoff for inductively classifying a newly added node.",
		"DOI": "10.48550/arXiv.2305.00851",
		"note": "arXiv:2305.00851 [cs]",
		"number": "arXiv:2305.00851",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Revisiting Robustness in Graph Machine Learning",
		"URL": "http://arxiv.org/abs/2305.00851",
		"author": [
			{
				"family": "Gosch",
				"given": "Lukas"
			},
			{
				"family": "Sturm",
				"given": "Daniel"
			},
			{
				"family": "Geisler",
				"given": "Simon"
			},
			{
				"family": "Günnemann",
				"given": "Stephan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					2
				]
			]
		}
	},
	{
		"id": "korkmazDeepReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods for adversarial training for deep reinforcement learning agents to improve robustness to adversarial perturbations. In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Deep Reinforcement Learning Policies in the Frequency Domain",
		"URL": "https://openreview.net/forum?id=s6td4UTR2FR",
		"author": [
			{
				"family": "Korkmaz",
				"given": "Ezgi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "xuPolicyResilienceEnvironment2023",
		"type": "article",
		"abstract": "This paper investigates policy resilience to training-environment poisoning attacks on reinforcement learning (RL) policies, with the goal of recovering the deployment performance of a poisoned RL policy. Due to the fact that the policy resilience is an add-on concern to RL algorithms, it should be resource-efficient, time-conserving, and widely applicable without compromising the performance of RL algorithms. This paper proposes such a policy-resilience mechanism based on an idea of knowledge sharing. We summarize the policy resilience as three stages: preparation, diagnosis, recovery. Specifically, we design the mechanism as a federated architecture coupled with a meta-learning manner, pursuing an efficient extraction and sharing of the environment knowledge. With the shared knowledge, a poisoned agent can quickly identify the deployment condition and accordingly recover its policy performance. We empirically evaluate the resilience mechanism for both model-based and model-free RL algorithms, showing its effectiveness and efficiency in restoring the deployment performance of a poisoned policy.",
		"DOI": "10.48550/arXiv.2304.12151",
		"note": "arXiv:2304.12151 [cs]",
		"number": "arXiv:2304.12151",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Policy Resilience to Environment Poisoning Attacks on Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2304.12151",
		"author": [
			{
				"family": "Xu",
				"given": "Hang"
			},
			{
				"family": "Qu",
				"given": "Xinghua"
			},
			{
				"family": "Rabinovich",
				"given": "Zinovi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					24
				]
			]
		}
	},
	{
		"id": "mayoImageRecognitionTime2022",
		"type": "paper-conference",
		"abstract": "The success of adversarial attacks and the performance tradeoffs made by adversarial defense methods have both traditionally been evaluated on image test sets constructed from a randomly sampled held out portion of a training set. Mayo 2022 et al. [1] measured the difficulty of the ImageNet and ObjectNet test sets by measuring the minimum viewing time required for an object to be recognized on average by a human, finding that these test sets are heavily skewed towards containing mostly easy, quickly recognized images. While difficult images that require longer viewing times to be recognized are uncommon in test sets, they are both common and critically important to the real world performance of vision models. In this work, we investigated the relationship between adversarial robustness and viewing time difficulty. Measuring the AUC of accuracy vs attack strength (epsilon), we find that easy, quickly recognized, images are more robust to adversarial attacks than difficult images, which require several seconds of viewing time to recognize. Additionally, adversarial defense methods improve models robustness to adversarial attacks on easy images significantly more than on hard images. We propose that the distribution of image difficulties should be carefully considered and controlled for when measuring both the effectiveness of adversarial attacks and when analyzing the clean accuracy vs robustness tradeoff made by adversarial defense methods.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Image recognition time for humans predicts adversarial vulnerability for models",
		"URL": "https://openreview.net/forum?id=d_5-0m3xCWn",
		"author": [
			{
				"family": "Mayo",
				"given": "David"
			},
			{
				"family": "Cummings",
				"given": "Jesse"
			},
			{
				"family": "Lin",
				"given": "Xinyu"
			},
			{
				"family": "Katz",
				"given": "Boris"
			},
			{
				"family": "Barbu",
				"given": "Andrei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "pitisRationalMultiObjectiveAgents2022",
		"type": "paper-conference",
		"abstract": "This paper considers intuitively appealing axioms for rational, multi-objective agents and derives an impossibility from which one concludes that such agents must admit non-Markov reward representations. The axioms include the Von-Neumann Morgenstern axioms, Pareto indifference, and dynamic consistency. We tie this result to irrational procrastination behaviors observed in humans, and show how the impossibility can be resolved by adopting a non-Markov aggregation scheme. Our work highlights the importance of non-Markov rewards for reinforcement learning and outlines directions for future work.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Rational Multi-Objective Agents Must Admit Non-Markov Reward Representations",
		"URL": "https://openreview.net/forum?id=MNwA4sgzR4W",
		"author": [
			{
				"family": "Pitis",
				"given": "Silviu"
			},
			{
				"family": "Bailey",
				"given": "Duncan"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "torfahRuntimeMonitorsOperational2022",
		"type": "paper-conference",
		"abstract": "Autonomous systems are increasingly relying on machine learning (ML) components to perform a variety of complex tasks in perception, prediction, and control.To guarantee the safety of ML-based autonomous systems, it is important to capture their operational design domain (ODD), i.e., the conditions under which using the ML components does not endanger the safety of the system. In this paper, we present a framework for learning runtime monitors for ODDs of autonomous systems with black-box ML components. A runtime monitor of an ODD predicts based on a sequence of monitorable observations whether the system is about to exit its ODD. We particularly investigate the learning of optimal monitors based on counterexample-guided refinement and conformance testing. We evaluate our approach on a case study from the domain of autonomous driving.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Runtime Monitors for Operational Design Domains of Black-Box ML-Models",
		"URL": "https://openreview.net/forum?id=6_AtjSBhqx",
		"author": [
			{
				"family": "Torfah",
				"given": "Hazem"
			},
			{
				"family": "Seshia",
				"given": "Sanjit A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					18
				]
			]
		}
	},
	{
		"id": "daviesUnifyingGrokkingDouble2023",
		"type": "article",
		"abstract": "A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \\emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \\emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.",
		"DOI": "10.48550/arXiv.2303.06173",
		"note": "arXiv:2303.06173 [cs]",
		"number": "arXiv:2303.06173",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unifying Grokking and Double Descent",
		"URL": "http://arxiv.org/abs/2303.06173",
		"author": [
			{
				"family": "Davies",
				"given": "Xander"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					10
				]
			]
		}
	},
	{
		"id": "louloudakisAssessingRobustnessImage2022",
		"type": "paper-conference",
		"abstract": "Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to incorrect mapping on hardware accelerators, which may lead to timing uncertainty and incorrect behavior. In addition, the increasing demand for optimal performance has led to progress towards the optimization of different neural network operations, such as operator fusion. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess the performance and impact of such optimizations, and explore their effectiveness. In this paper we conduct robustness analysis of four popular image recognition models with the ImageNet dataset, assessing the impact of the compiler optimizations applied, utilizing different Deep Learning frameworks and executing on hardware devices of varying capabilities. We report the impact in terms of misclassifications and inference time across varying settings.",
		"event-title": "NeurIPS ML Safety Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "Assessing Robustness of Image Recognition Models to Changes in the Computational Environment",
		"URL": "https://openreview.net/forum?id=-7DjNGvdpx",
		"author": [
			{
				"family": "Louloudakis",
				"given": "Nikolaos"
			},
			{
				"family": "Gibson",
				"given": "Perry"
			},
			{
				"family": "Cano",
				"given": "Jose"
			},
			{
				"family": "Rajan",
				"given": "Ajitha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					27
				]
			]
		}
	},
	{
		"id": "mirzaeiFakeItTill2022",
		"type": "article",
		"abstract": "We aim for image-based novelty detection. Despite considerable progress, existing models either fail or face a dramatic drop under the so-called \"near-distribution\" setting, where the differences between normal and anomalous samples are subtle. We first demonstrate existing methods experience up to 20% decrease in performance in the near-distribution setting. Next, we propose to exploit a score-based generative model to produce synthetic near-distribution anomalous data. Our model is then fine-tuned to distinguish such data from the normal samples. We provide a quantitative as well as qualitative evaluation of this strategy, and compare the results with a variety of GAN-based models. Effectiveness of our method for both the near-distribution and standard novelty detection is assessed through extensive experiments on datasets in diverse applications such as medical images, object classification, and quality control. This reveals that our method considerably improves over existing models, and consistently decreases the gap between the near-distribution and standard novelty detection performance. The code repository is available at https://github.com/rohban-lab/FITYMI.",
		"DOI": "10.48550/arXiv.2205.14297",
		"note": "arXiv:2205.14297 [cs]",
		"number": "arXiv:2205.14297",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fake It Till You Make It: Towards Accurate Near-Distribution Novelty Detection",
		"title-short": "Fake It Till You Make It",
		"URL": "http://arxiv.org/abs/2205.14297",
		"author": [
			{
				"family": "Mirzaei",
				"given": "Hossein"
			},
			{
				"family": "Salehi",
				"given": "Mohammadreza"
			},
			{
				"family": "Shahabi",
				"given": "Sajjad"
			},
			{
				"family": "Gavves",
				"given": "Efstratios"
			},
			{
				"family": "Snoek",
				"given": "Cees G. M."
			},
			{
				"family": "Sabokrou",
				"given": "Mohammad"
			},
			{
				"family": "Rohban",
				"given": "Mohammad Hossein"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					28
				]
			]
		}
	},
	{
		"id": "rajpalQuantifyingHierarchicalSelection2023",
		"type": "article",
		"abstract": "At what level does selective pressure effectively act? When considering the reproductive dynamics of interacting and mutating agents, it has long been debated whether selection is better understood by focusing on the individual or if hierarchical selection emerges as a consequence of joint adaptation. Despite longstanding efforts in theoretical ecology there is still no consensus on this fundamental issue, most likely due to the difficulty in obtaining adequate data spanning sufficient number of generations and the lack of adequate tools to quantify the effect of hierarchical selection. Here we capitalise on recent advances in information-theoretic data analysis to advance this state of affairs by investigating the emergence of high-order structures -- such as groups of species -- in the collective dynamics of the Tangled Nature model of evolutionary ecology. Our results show that evolutionary dynamics can lead to clusters of species that act as a selective group, that acquire information-theoretic agency. Overall, our findings provide quantitative evidence supporting the relevance of high-order structures in evolutionary ecology, which can emerge even from relatively simple processes of adaptation and selection.",
		"DOI": "10.48550/arXiv.2310.20386",
		"note": "arXiv:2310.20386 [nlin, q-bio]",
		"number": "arXiv:2310.20386",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Quantifying Hierarchical Selection",
		"URL": "http://arxiv.org/abs/2310.20386",
		"author": [
			{
				"family": "Rajpal",
				"given": "Hardik"
			},
			{
				"family": "Stengel",
				"given": "Clem",
				"non-dropping-particle": "von"
			},
			{
				"family": "Mediano",
				"given": "Pedro A. M."
			},
			{
				"family": "Rosas",
				"given": "Fernando E."
			},
			{
				"family": "Viegas",
				"given": "Eduardo"
			},
			{
				"family": "Marquet",
				"given": "Pablo A."
			},
			{
				"family": "Jensen",
				"given": "Henrik J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					1
				]
			]
		}
	},
	{
		"id": "ammannNaturalisedAccountPlanning2023",
		"type": "paper-conference",
		"abstract": "Abstract. We develop a naturalised account of planning, which identifies a class of functions (and their associated behaviours) in intelligent systems. The account identifies three principal components of a planning process: a system (defined by a set of possible system-environment decompositions); a subsystem (which presents a model, copy, or analog of some aspect of the system); and a selection mechanism (via which a subsystem is functionally related to expected future states). We give a generalised, system-independent account of planning, and then ground our analysis with a set of eight concrete reference systems, spanning biological, human, social, and artificial systems. Finally, we apply this naturalized account of planning to evaluate under what conditions planning behaviour is likely to emerge, and what failure modes arise in systems exhibiting such planning behaviour.",
		"DOI": "10.1162/isal_a_00696",
		"event-title": "ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference",
		"language": "en",
		"publisher": "MIT Press",
		"source": "direct.mit.edu",
		"title": "A Naturalised Account of Planning in Intelligent Systems",
		"URL": "https://dx.doi.org/10.1162/isal_a_00696",
		"author": [
			{
				"family": "Ammann",
				"given": "Nora"
			},
			{
				"family": "Stengel",
				"given": "Clem",
				"non-dropping-particle": "von"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					24
				]
			]
		}
	},
	{
		"id": "kulveitPredictiveMindsLLMs2023",
		"type": "article",
		"abstract": "Large language models (LLMs) like GPT are often conceptualized as passive predictors, simulators, or even stochastic parrots. We instead conceptualize LLMs by drawing on the theory of active inference originating in cognitive science and neuroscience. We examine similarities and differences between traditional active inference systems and LLMs, leading to the conclusion that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impacts of their actions, but otherwise fit in the active inference paradigm. We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.",
		"DOI": "10.48550/arXiv.2311.10215",
		"note": "arXiv:2311.10215 [cs]",
		"number": "arXiv:2311.10215",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Predictive Minds: LLMs As Atypical Active Inference Agents",
		"title-short": "Predictive Minds",
		"URL": "http://arxiv.org/abs/2311.10215",
		"author": [
			{
				"family": "Kulveit",
				"given": "Jan"
			},
			{
				"family": "Stengel",
				"given": "Clem",
				"non-dropping-particle": "von"
			},
			{
				"family": "Leventov",
				"given": "Roman"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		}
	},
	{
		"id": "shahinshamsabadiWashingUnwashableImpossibility2022",
		"type": "article-journal",
		"abstract": "The use of black-box models (e.g., deep neural networks) in high-stakes decision-making systems, whose internal logic is complex, raises the need for providing explanations about their decisions. Model explanation techniques mitigate this problem by generating an interpretable and high-fidelity surrogate model (e.g., a logistic regressor or decision tree) to explain the logic of black-box models. In this work, we investigate the issue of fairwashing, in which model explanation techniques are manipulated to rationalize decisions taken by an unfair black-box model using deceptive surrogate models. More precisely, we theoretically characterize and analyze fairwashing, proving that this phenomenon is difficult to avoid due to an irreducible factor---the unfairness of the black-box model. Based on the theory developed, we propose a novel technique, called FRAUD-Detect (FaiRness AUDit Detection), to detect fairwashed models by measuring a divergence over subpopulation-wise fidelity measures of the interpretable model. We empirically demonstrate that this divergence is significantly larger in purposefully fairwashed interpretable models than in honest ones. Furthermore, we show that our detector is robust to an informed adversary trying to bypass our detector. The code implementing FRAUD-Detect is available at https://github.com/cleverhans-lab/FRAUD-Detect.",
		"container-title": "Advances in Neural Information Processing Systems",
		"language": "en",
		"page": "14170-14182",
		"source": "proceedings.neurips.cc",
		"title": "Washing The Unwashable : On The (Im)possibility of Fairwashing Detection",
		"title-short": "Washing The Unwashable",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5b84864ff8474fd742c66f219b2eaac1-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Shahin Shamsabadi",
				"given": "Ali"
			},
			{
				"family": "Yaghini",
				"given": "Mohammad"
			},
			{
				"family": "Dullerud",
				"given": "Natalie"
			},
			{
				"family": "Wyllie",
				"given": "Sierra"
			},
			{
				"family": "Aïvodji",
				"given": "Ulrich"
			},
			{
				"family": "Alaagib",
				"given": "Aisha"
			},
			{
				"family": "Gambs",
				"given": "Sébastien"
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					6
				]
			]
		}
	},
	{
		"id": "xieRobustPolicyLearning2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) agents need to be robust to variations in safety-critical environments. While system identification methods provide a way to infer the variation from online experience, they can fail in settings where fast identification is not possible. Another dominant approach is robust RL which produces a policy that can handle worst-case scenarios, but these methods are generally designed to achieve robustness to a single uncertainty set that must be specified at train time. Towards a more general solution, we formulate the multi-set robustness problem to learn a policy robust to different perturbation sets. We then design an algorithm that enjoys the benefits of both system identification and robust RL: it reduces uncertainty where possible given a few interactions, but can still act robustly with respect to the remaining uncertainty. On a diverse set of control tasks, our approach demonstrates improved worst-case performance on new environments compared to prior methods based on system identification and on robust RL alone.",
		"container-title": "Proceedings of the 39th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "24414-24429",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Robust Policy Learning over Multiple Uncertainty Sets",
		"URL": "https://proceedings.mlr.press/v162/xie22c.html",
		"author": [
			{
				"family": "Xie",
				"given": "Annie"
			},
			{
				"family": "Sodhani",
				"given": "Shagun"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			},
			{
				"family": "Zhang",
				"given": "Amy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					28
				]
			]
		}
	},
	{
		"id": "nobandeganiCognitiveModelsSimulators2022",
		"type": "article",
		"abstract": "To achieve desirable performance, current AI systems often require huge amounts of training data. This is especially problematic in domains where collecting data is both expensive and time-consuming, e.g., where AI systems require having numerous interactions with humans, collecting feedback from them. In this work, we substantiate the idea of $\\textit{cognitive models as simulators}$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making their training process both less costly and faster. Here, we leverage this idea in the context of moral decision-making, by having reinforcement learning (RL) agents learn about fairness through interacting with a cognitive model of the Ultimatum Game (UG), a canonical task in behavioral and brain sciences for studying fairness. Interestingly, these RL agents learn to rationally adapt their behavior depending on the emotional state of their simulated UG responder. Our work suggests that using cognitive models as simulators of humans is an effective approach for training AI systems, presenting an important way for computational cognitive science to make contributions to AI.",
		"DOI": "10.48550/arXiv.2210.04121",
		"note": "arXiv:2210.04121 [cs, q-bio]",
		"number": "arXiv:2210.04121",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Cognitive Models as Simulators: The Case of Moral Decision-Making",
		"title-short": "Cognitive Models as Simulators",
		"URL": "http://arxiv.org/abs/2210.04121",
		"author": [
			{
				"family": "Nobandegani",
				"given": "Ardavan S."
			},
			{
				"family": "Shultz",
				"given": "Thomas R."
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					8
				]
			]
		}
	},
	{
		"id": "kazaDecisionReferralsHumanAutomation2021",
		"type": "paper-conference",
		"abstract": "We consider a model for optimal decision referrals in human-automation teams performing binary classification tasks. The automation observes a batch of independent tasks, analyzes them, and has the option to refer a subset of them to a human operator. The human operator performs fresh analysis of the tasks referred to him. Our key modeling assumption is that the human performance degrades with workload (i.e., the number of tasks referred to human). We model the problem as a stochastic optimization problem. We first consider the special case when the workload of the human is pre-specified. We show that in this setting it is optimal to myopically refer tasks which lead to the largest reduction in the conditional expected cost until the desired workload target is met. We next consider the general setting where there is no constraint on the workload. We leverage the solution of the previous step and provide a search algorithm to efficiently find the optimal set of tasks to refer. Finally, we present a numerical study to compare the performance of our algorithm with some baseline allocation policies.",
		"container-title": "2021 60th IEEE Conference on Decision and Control (CDC)",
		"DOI": "10.1109/CDC45484.2021.9683407",
		"event-title": "2021 60th IEEE Conference on Decision and Control (CDC)",
		"note": "ISSN: 2576-2370",
		"page": "2842-2847",
		"source": "IEEE Xplore",
		"title": "Decision Referrals in Human-Automation Teams",
		"URL": "https://ieeexplore.ieee.org/document/9683407",
		"author": [
			{
				"family": "Kaza",
				"given": "Kesav"
			},
			{
				"family": "Ny",
				"given": "Jerome Le"
			},
			{
				"family": "Mahajan",
				"given": "Aditya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "nikanjamFaultsDeepReinforcement2021",
		"type": "article-journal",
		"abstract": "A growing demand is witnessed in both industry and academia for employing Deep Learning (DL) in various domains to solve real-world problems. Deep reinforcement learning (DRL) is the application of DL in the domain of Reinforcement Learning. Like any software system, DRL applications can fail because of faults in their programs. In this paper, we present the first attempt to categorize faults occurring in DRL programs. We manually analyzed 761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues) developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl, Tensorforce) and identified faults reported by developers/users. We labeled and taxonomized the identified faults through several rounds of discussions. The resulting taxonomy is validated using an online survey with 19 developers/researchers. To allow for the automatic detection of faults in DRL programs, we have defined a meta-model of DRL programs and developed DRLinter, a model-based fault detection approach that leverages static analysis and graph transformations. The execution flow of DRLinter consists in parsing a DRL program to generate a model conforming to our meta-model and applying detection rules on the model to identify faults occurrences. The effectiveness of DRLinter is evaluated using 21 synthetic and real faulty DRL programs. For synthetic samples, we injected faults observed in the analyzed artifacts from Stack Overflow and GitHub. The results show that DRLinter can successfully detect faults in both synthesized and real-world examples with a recall of 75% and a precision of 100%.",
		"container-title": "Automated Software Engineering",
		"DOI": "10.1007/s10515-021-00313-x",
		"ISSN": "1573-7535",
		"issue": "1",
		"journalAbbreviation": "Autom Softw Eng",
		"language": "en",
		"page": "8",
		"source": "Springer Link",
		"title": "Faults in deep reinforcement learning programs: a taxonomy and a detection approach",
		"title-short": "Faults in deep reinforcement learning programs",
		"URL": "https://doi.org/10.1007/s10515-021-00313-x",
		"volume": "29",
		"author": [
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Morovati",
				"given": "Mohammad Mehdi"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Ben Braiek",
				"given": "Houssem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					20
				]
			]
		}
	},
	{
		"id": "bergmanJANOSIntegratedPredictive2022",
		"type": "article-journal",
		"abstract": "Business research practice is witnessing a surge in the integration of predictive modeling and prescriptive analysis. We describe a modeling framework JANOS that seamlessly integrates the two streams of analytics, allowing researchers and practitioners to embed machine learning models in an end-to-end optimization framework. JANOS allows for specifying a prescriptive model using standard optimization modeling elements such as constraints and variables. The key novelty lies in providing modeling constructs that enable the specification of commonly used predictive models within an optimization model, have the features of the predictive model as variables in the optimization model, and incorporate the output of the predictive models as part of the objective. The framework considers two sets of decision variables: regular and predicted. The relationship between the regular and the predicted variables is specified by the user as pretrained predictive models. JANOS currently supports linear regression, logistic regression, and neural network with rectified linear activation functions. In this paper, we demonstrate the flexibility of the framework through an example on scholarship allocation in a student enrollment problem and provide a numeric performance evaluation.\n\nSummary of Contribution. This paper describes a new software tool, JANOS, that integrates predictive modeling and discrete optimization to assist decision making. Specifically, the proposed solver takes as input user-specified pretrained predictive models and formulates optimization models directly over those predictive models by embedding them within an optimization model through linear transformations.",
		"container-title": "INFORMS Journal on Computing",
		"DOI": "10.1287/ijoc.2020.1023",
		"ISSN": "1091-9856",
		"issue": "2",
		"note": "publisher: INFORMS",
		"page": "807-816",
		"source": "pubsonline.informs.org (Atypon)",
		"title": "JANOS: An Integrated Predictive and Prescriptive Modeling Framework",
		"title-short": "JANOS",
		"URL": "https://pubsonline.informs.org/doi/10.1287/ijoc.2020.1023",
		"volume": "34",
		"author": [
			{
				"family": "Bergman",
				"given": "David"
			},
			{
				"family": "Huang",
				"given": "Teng"
			},
			{
				"family": "Brooks",
				"given": "Philip"
			},
			{
				"family": "Lodi",
				"given": "Andrea"
			},
			{
				"family": "Raghunathan",
				"given": "Arvind U."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "aroraWhyExposureBias2023",
		"type": "article",
		"abstract": "Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality. Source code to reproduce these experiments is available at https://github.com/kushalarora/quantifying_exposure_bias",
		"DOI": "10.48550/arXiv.2204.01171",
		"note": "arXiv:2204.01171 [cs]",
		"number": "arXiv:2204.01171",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
		"title-short": "Why Exposure Bias Matters",
		"URL": "http://arxiv.org/abs/2204.01171",
		"author": [
			{
				"family": "Arora",
				"given": "Kushal"
			},
			{
				"family": "Asri",
				"given": "Layla El"
			},
			{
				"family": "Bahuleyan",
				"given": "Hareesh"
			},
			{
				"family": "Cheung",
				"given": "Jackie Chi Kit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					9
				]
			]
		}
	},
	{
		"id": "marhabaIdentificationOutofdistributionCases2022",
		"type": "paper-conference",
		"abstract": "Machine learning is vulnerable to possible incorrect classification of cases that are out of the distribution observed during training and calibration.There has been much recent research revolving around the detection of adversarial images, when it comes to neural networks [3, 6, 10, 12--14]. Good results have been obtained by somehow learning adversarial features and behaviors and using this knowledge to distinguish between correctly and incorrectly predicted classifications due to adversarial attacks.To identify OOD cases, we propose to use Surprise Adequacy Deep Learning Likelihood (SADL) [6] instantiated to each output class, to measure In-Distribution or Out-Of-Distribution computational likelihood of classifications performed by a network.Out-of-distribution cases were not drawn from the same distribution of the training sets and they were created using affine transformations of legitimate inputs and adversarial attacks.Presented experimental results show that OOD analysis allows up to 70% to 90% OOD detection. The identification of OOD computations may be beneficial in sensitive and critical domains such as aerospace, medicine, cyber-security, and many others.",
		"collection-title": "CAIN '22",
		"container-title": "Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI",
		"DOI": "10.1145/3522664.3528617",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9275-4",
		"page": "39–40",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Identification of out-of-distribution cases of CNN using class-based surprise adequacy",
		"URL": "https://doi.org/10.1145/3522664.3528617",
		"author": [
			{
				"family": "Marhaba",
				"given": "Mira"
			},
			{
				"family": "Merlo",
				"given": "Ettore"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Antoniol",
				"given": "Giuliano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					17
				]
			]
		}
	},
	{
		"id": "liEffectivenessInterpretableFeedforward2022",
		"type": "paper-conference",
		"abstract": "Deep learning models have achieved state-of-the-art performance in many classification tasks. However, most of them cannot provide an explanation for their classification results. Machine learning models that are interpretable are usually linear or piecewise linear and yield inferior performance. Non-linear models achieve much better classification performance, but it is usually hard to explain their classification results. As a counter-example, an interpretable feedforward neural network (IFFNN) is proposed to achieve both high classification performance and interpretability for malware detection. If the IFFNN can perform well in a more flexible and general form for other classification tasks while providing meaningful explanations, it may be of great interest to the applied machine learning community. In this paper, we propose a way to generalize the interpretable feedforward neural network to multi-class classification scenarios and any type of feedforward neural networks, and evaluate its classification performance and interpretability on interpretable datasets. We conclude by finding that the generalized IFFNNs achieve comparable classification performance to their normal feedforward neural network counterparts and provide meaningful explanations. Thus, this kind of neural network architecture has great practical use.",
		"container-title": "2022 International Joint Conference on Neural Networks (IJCNN)",
		"DOI": "10.1109/IJCNN55064.2022.9892343",
		"event-title": "2022 International Joint Conference on Neural Networks (IJCNN)",
		"note": "ISSN: 2161-4407",
		"page": "1-8",
		"source": "IEEE Xplore",
		"title": "On the Effectiveness of Interpretable Feedforward Neural Network",
		"URL": "https://ieeexplore.ieee.org/document/9892343",
		"author": [
			{
				"family": "Li",
				"given": "Miles Q."
			},
			{
				"family": "Fung",
				"given": "Benjamin C. M."
			},
			{
				"family": "Abusitta",
				"given": "Adel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "rahimiApplicationArtificialIntelligence2022",
		"type": "article-journal",
		"abstract": "Background: Artificial intelligence (AI) has shown promising results in various fields of medicine. It has the potential to facilitate shared decision making (SDM). However, there is no comprehensive mapping of how AI may be used for SDM.\nObjective: We aimed to identify and evaluate published studies that have tested or implemented AI to facilitate SDM.\nMethods: We performed a scoping review informed by the methodological framework proposed by Levac et al, modifications to the original Arksey and O'Malley framework of a scoping review, and the Joanna Briggs Institute scoping review framework. We reported our results based on the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) reporting guideline. At the identification stage, an information specialist performed a comprehensive search of 6 electronic databases from their inception to May 2021. The inclusion criteria were: all populations; all AI interventions that were used to facilitate SDM, and if the AI intervention was not used for the decision-making point in SDM, it was excluded; any outcome related to patients, health care providers, or health care systems; studies in any health care setting, only studies published in the English language, and all study types. Overall, 2 reviewers independently performed the study selection process and extracted data. Any disagreements were resolved by a third reviewer. A descriptive analysis was performed.\nResults: The search process yielded 1445 records. After removing duplicates, 894 documents were screened, and 6 peer-reviewed publications met our inclusion criteria. Overall, 2 of them were conducted in North America, 2 in Europe, 1 in Australia, and 1 in Asia. Most articles were published after 2017. Overall, 3 articles focused on primary care, and 3 articles focused on secondary care. All studies used machine learning methods. Moreover, 3 articles included health care providers in the validation stage of the AI intervention, and 1 article included both health care providers and patients in clinical validation, but none of the articles included health care providers or patients in the design and development of the AI intervention. All used AI to support SDM by providing clinical recommendations or predictions.\nConclusions: Evidence of the use of AI in SDM is in its infancy. We found AI supporting SDM in similar ways across the included articles. We observed a lack of emphasis on patients’ values and preferences, as well as poor reporting of AI interventions, resulting in a lack of clarity about different aspects. Little effort was made to address the topics of explainability of AI interventions and to include end-users in the design and development of the interventions. Further efforts are required to strengthen and standardize the use of AI in different steps of SDM and to evaluate its impact on various decisions, populations, and settings.",
		"container-title": "JMIR Medical Informatics",
		"DOI": "10.2196/36199",
		"issue": "8",
		"language": "EN",
		"license": "This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright and license information must be included.",
		"note": "Company: JMIR Medical Informatics\nDistributor: JMIR Medical Informatics\nInstitution: JMIR Medical Informatics\nLabel: JMIR Medical Informatics\npublisher: JMIR Publications Inc., Toronto, Canada",
		"page": "e36199",
		"source": "medinform.jmir.org",
		"title": "Application of Artificial Intelligence in Shared Decision Making: Scoping Review",
		"title-short": "Application of Artificial Intelligence in Shared Decision Making",
		"URL": "https://medinform.jmir.org/2022/8/e36199",
		"volume": "10",
		"author": [
			{
				"family": "Rahimi",
				"given": "Samira Abbasgholizadeh"
			},
			{
				"family": "Cwintal",
				"given": "Michelle"
			},
			{
				"family": "Huang",
				"given": "Yuhui"
			},
			{
				"family": "Ghadiri",
				"given": "Pooria"
			},
			{
				"family": "Grad",
				"given": "Roland"
			},
			{
				"family": "Poenaru",
				"given": "Dan"
			},
			{
				"family": "Gore",
				"given": "Genevieve"
			},
			{
				"family": "Zomahoun",
				"given": "Hervé Tchala Vignon"
			},
			{
				"family": "Légaré",
				"given": "France"
			},
			{
				"family": "Pluye",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					9
				]
			]
		}
	},
	{
		"id": "goyalInductiveBiasesDeep2022",
		"type": "article-journal",
		"abstract": "A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.",
		"container-title": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",
		"DOI": "10.1098/rspa.2021.0068",
		"issue": "2266",
		"note": "publisher: Royal Society",
		"page": "20210068",
		"source": "royalsocietypublishing.org (Atypon)",
		"title": "Inductive biases for deep learning of higher-level cognition",
		"URL": "https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068",
		"volume": "478",
		"author": [
			{
				"family": "Goyal",
				"given": "Anirudh"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					12
				]
			]
		}
	},
	{
		"id": "layounAligningMAGMAFewShot2022",
		"type": "article",
		"abstract": "The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.",
		"DOI": "10.48550/arXiv.2210.14161",
		"note": "arXiv:2210.14161 [cs]",
		"number": "arXiv:2210.14161",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
		"URL": "http://arxiv.org/abs/2210.14161",
		"author": [
			{
				"family": "Layoun",
				"given": "Jean-Charles"
			},
			{
				"family": "Roger",
				"given": "Alexis"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					18
				]
			]
		}
	},
	{
		"id": "shethLearningUncertainConcepts2022",
		"type": "paper-conference",
		"abstract": "With neural networks applied to safety-critical applications, it has become increasingly important to understand the defining features of decision-making. Therefore, the need to uncover the black boxes to rational representational space of these neural networks is apparent. Concept bottleneck model (CBM) encourages interpretability by predicting human-understandable concepts. They predict concepts from input images and then labels from concepts. Test time intervention, a salient feature of CBM, allows for human-model interactions. However, these interactions are prone to information leakage and can often be ineffective inappropriate communication with humans. We propose a novel uncertainty based strategy, \\emph{SIUL: Single Interventional Uncertainty Learning} to select the interventions. Additionally, we empirically test the robustness of CBM and the effect of SIUL interventions under adversarial attack and distributional shift. Using SIUL, we observe that the interventions suggested lead to meaningful corrections along with mitigation of concept leakage. Extensive experiments on three vision datasets along with a histopathology dataset validate the effectiveness of our interventional learning.",
		"event-title": "Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022",
		"language": "en",
		"source": "openreview.net",
		"title": "Learning from uncertain concepts via test time interventions",
		"URL": "https://openreview.net/forum?id=WVe3vok8Cc3",
		"author": [
			{
				"family": "Sheth",
				"given": "Ivaxi"
			},
			{
				"family": "Rahman",
				"given": "Aamer Abdul"
			},
			{
				"family": "Sevyeri",
				"given": "Laya Rafiee"
			},
			{
				"family": "Havaei",
				"given": "Mohammad"
			},
			{
				"family": "Kahou",
				"given": "Samira Ebrahimi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					21
				]
			]
		}
	},
	{
		"id": "madsenPosthocInterpretabilityNeural2022",
		"type": "article-journal",
		"abstract": "Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.",
		"container-title": "ACM Comput. Surv.",
		"DOI": "10.1145/3546577",
		"ISSN": "0360-0300",
		"issue": "8",
		"page": "155:1–155:42",
		"source": "ACM Digital Library",
		"title": "Post-hoc Interpretability for Neural NLP: A Survey",
		"title-short": "Post-hoc Interpretability for Neural NLP",
		"URL": "https://doi.org/10.1145/3546577",
		"volume": "55",
		"author": [
			{
				"family": "Madsen",
				"given": "Andreas"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					23
				]
			]
		}
	},
	{
		"id": "khetarpalContinualReinforcementLearning2022",
		"type": "article-journal",
		"abstract": "In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.",
		"container-title": "Journal of Artificial Intelligence Research",
		"DOI": "10.1613/jair.1.13673",
		"ISSN": "1076-9757",
		"language": "en",
		"license": "Copyright (c) 2022 Journal of Artificial Intelligence Research",
		"page": "1401-1476",
		"source": "jair.org",
		"title": "Towards Continual Reinforcement Learning: A Review and Perspectives",
		"title-short": "Towards Continual Reinforcement Learning",
		"URL": "https://jair.org/index.php/jair/article/view/13673",
		"volume": "75",
		"author": [
			{
				"family": "Khetarpal",
				"given": "Khimya"
			},
			{
				"family": "Riemer",
				"given": "Matthew"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					22
				]
			]
		}
	},
	{
		"id": "gengReliableNeuralSpecifications2023",
		"type": "paper-conference",
		"abstract": "Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs. We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be used as a more reliable and extensible specification for neural network verification.",
		"container-title": "Proceedings of the 40th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "11196-11212",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Towards Reliable Neural Specifications",
		"URL": "https://proceedings.mlr.press/v202/geng23a.html",
		"author": [
			{
				"family": "Geng",
				"given": "Chuqin"
			},
			{
				"family": "Le",
				"given": "Nham"
			},
			{
				"family": "Xu",
				"given": "Xiaojie"
			},
			{
				"family": "Wang",
				"given": "Zhaoyue"
			},
			{
				"family": "Gurfinkel",
				"given": "Arie"
			},
			{
				"family": "Si",
				"given": "Xujie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					3
				]
			]
		}
	},
	{
		"id": "sculleySurveyScientificRigor2023",
		"type": "paper-conference",
		"abstract": "The concern that Artificial Intelligence (AI) and Machine Learning (ML) are entering a “reproducibility crisis” has spurred significant research in the past few years. Yet with each paper, it is often unclear what someone means by “reproducibility” and where it fits in the larger scope of what we will call the “scientific rigor” literature. Ultimately, the lack of clear rigor standards can affect the manner in which businesses seeking to adopt AI/ML implement such capabilities. In this survey, we will use 66 papers published since 2017 to construct a proposed set of 8 high-level categories of scientific rigor, what they are, and the history of work conducted in each. Our proposal is that these eight rigor types are not mutually exclusive and present a model for how they influence each other. To encourage more to study these questions, we map these rigors to the adoption process in real-world business use cases. In doing so, we can quantify gaps in the literature that suggest an under focus on the issues necessary for scientific rigor research to transition to practice",
		"source": "Semantic Scholar",
		"title": "Survey of Scientific Rigor Studied in Machine Learning",
		"URL": "https://www.semanticscholar.org/paper/Survey-of-Scientific-Rigor-Studied-in-Machine-Sculley-Holt/2e6953cfda81ebbf85a44bc8be72c639ec592b3a",
		"author": [
			{
				"family": "Sculley",
				"given": "D."
			},
			{
				"family": "Holt",
				"given": "Gary"
			},
			{
				"family": "Golovin",
				"given": "D."
			},
			{
				"family": "Davydov",
				"given": "Eugene"
			},
			{
				"family": "Phillips",
				"given": "Todd"
			},
			{
				"family": "Ebner",
				"given": "D."
			},
			{
				"family": "Young",
				"given": "Michael"
			},
			{
				"family": "Crespo",
				"given": "Jean-François"
			},
			{
				"family": "Dennison",
				"given": "Dan"
			},
			{
				"family": "Fox",
				"given": "Emily"
			},
			{
				"family": "Larochelle",
				"given": "H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "tahaeiHumanCenteredResponsibleArtificial2023",
		"type": "paper-conference",
		"abstract": "In recent years, the CHI community has seen significant growth in research on Human-Centered Responsible Artificial Intelligence. While different research communities may use different terminology to discuss similar topics, all of this work is ultimately aimed at developing AI that benefits humanity while being grounded in human rights and ethics, and reducing the potential harms of AI. In this special interest group, we aim to bring together researchers from academia and industry interested in these topics to map current and future research trends to advance this important area of research by fostering collaboration and sharing ideas.",
		"collection-title": "CHI EA '23",
		"container-title": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems",
		"DOI": "10.1145/3544549.3583178",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9422-2",
		"page": "1–4",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Human-Centered Responsible Artificial Intelligence: Current & Future Trends",
		"title-short": "Human-Centered Responsible Artificial Intelligence",
		"URL": "https://doi.org/10.1145/3544549.3583178",
		"author": [
			{
				"family": "Tahaei",
				"given": "Mohammad"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			},
			{
				"family": "Kennedy",
				"given": "Sean"
			},
			{
				"family": "Muller",
				"given": "Michael"
			},
			{
				"family": "Stumpf",
				"given": "Simone"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Baeza-Yates",
				"given": "Ricardo"
			},
			{
				"family": "Aroyo",
				"given": "Lora"
			},
			{
				"family": "Holbrook",
				"given": "Jess"
			},
			{
				"family": "Luger",
				"given": "Ewa"
			},
			{
				"family": "Madaio",
				"given": "Michael"
			},
			{
				"family": "Blumenfeld",
				"given": "Ilana Golbin"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Vitak",
				"given": "Jessica"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					19
				]
			]
		}
	},
	{
		"id": "beckhamExploringValidationMetrics2023",
		"type": "paper-conference",
		"abstract": "In ofﬂine model-based optimisation (MBO) we are interested in using machine learning to de-sign candidates that maximise some measure of desirability through an expensive but real-world scoring process. Ofﬂine MBO tries to approximate this expensive scoring function and use that to evaluate generated designs, however evaluation is non-exact because one approximation is being evaluated with another. Instead, we ask ourselves: if we did have the real world scoring function at hand, what cheap-to-compute validation metrics would correlate best with this? Since the real-world scoring function is available for simulated MBO datasets, insights obtained from this can be transferred over to real-world ofﬂine MBO tasks where the real-world scoring function is expensive to compute. To address this, we propose a conceptual evaluation framework that is amenable to measuring extrapolation, and apply this to conditional denoising diffusion models. Empirically, we ﬁnd that two validation metrics – agreement and Frechet distance – correlate quite well with the ground truth. When there is high variability in conditional generation, feedback is required in the form of an approximated version of the real-world scoring function. Furthermore, we ﬁnd that generating high-scoring samples may require heavily weighting the generative model in favour of sample quality, potentially at the cost of sample diversity.",
		"source": "Semantic Scholar",
		"title": "Exploring validation metrics for ofﬂine model-based optimisation",
		"URL": "https://www.semanticscholar.org/paper/Exploring-validation-metrics-for-of%EF%AC%82ine-model-based-Beckham-Pich%C3%A9/8f995cbe15e1c1fc2dbc9024b1b56440859761de",
		"author": [
			{
				"family": "Beckham",
				"given": "Christopher"
			},
			{
				"family": "Piché",
				"given": "Alexandre"
			},
			{
				"family": "Vázquez",
				"given": "David"
			},
			{
				"family": "Pal",
				"given": "C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nguyenWhereBeginImpact2022",
		"type": "article",
		"abstract": "An oft-cited challenge of federated learning is the presence of heterogeneity. \\emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \\emph{System heterogeneity} refers to the fact that client devices have different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. We empirically study the impact of starting from a pre-trained model in federated learning using four standard federated learning benchmark datasets. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\\%) than is possible when starting from random initialization. Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity. We recommend that future work proposing and evaluating federated optimization methods evaluate the performance when starting from random and pre-trained initializations. We also believe this study raises several questions for further work on understanding the role of heterogeneity in federated optimization.",
		"DOI": "10.48550/arXiv.2210.08090",
		"note": "arXiv:2210.08090 [cs]",
		"number": "arXiv:2210.08090",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning",
		"title-short": "Where to Begin?",
		"URL": "http://arxiv.org/abs/2210.08090",
		"author": [
			{
				"family": "Nguyen",
				"given": "John"
			},
			{
				"family": "Wang",
				"given": "Jianyu"
			},
			{
				"family": "Malik",
				"given": "Kshitiz"
			},
			{
				"family": "Sanjabi",
				"given": "Maziar"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					14
				]
			]
		}
	},
	{
		"id": "caoSystematicRectificationLanguage2023",
		"type": "article",
		"abstract": "With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. This is crucial as many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.",
		"DOI": "10.48550/arXiv.2302.14003",
		"note": "arXiv:2302.14003 [cs]",
		"number": "arXiv:2302.14003",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Systematic Rectification of Language Models via Dead-end Analysis",
		"URL": "http://arxiv.org/abs/2302.14003",
		"author": [
			{
				"family": "Cao",
				"given": "Meng"
			},
			{
				"family": "Fatemi",
				"given": "Mehdi"
			},
			{
				"family": "Cheung",
				"given": "Jackie Chi Kit"
			},
			{
				"family": "Shabanian",
				"given": "Samira"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					27
				]
			]
		}
	},
	{
		"id": "rothDisentanglementCorrelatedFactors2023",
		"type": "article",
		"abstract": "A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized \\emph{support}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over $+60\\%$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.",
		"DOI": "10.48550/arXiv.2210.07347",
		"note": "arXiv:2210.07347 [cs, stat]",
		"number": "arXiv:2210.07347",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Disentanglement of Correlated Factors via Hausdorff Factorized Support",
		"URL": "http://arxiv.org/abs/2210.07347",
		"author": [
			{
				"family": "Roth",
				"given": "Karsten"
			},
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Akata",
				"given": "Zeynep"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					25
				]
			]
		}
	},
	{
		"id": "krasheninnikovOutofcontextMetalearningLarge2023",
		"type": "paper-conference",
		"abstract": "Brown et al. (2020) famously introduced the phenomenon of in-context meta-learning in large language models (LLMs). Our work establishes the existence of a phenomenon we call out-of-context meta-learning via carefully designed synthetic experiments with large language models. We argue that out-of-context meta-learning is an important and surprising capability of LLMs, which may lead them to more readily \"internalize\" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and apply it in appropriate contexts. We also raise the question of how this phenomenon emerges, and discuss two possible explanations: one relying on the way LLMs store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based methods may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks.",
		"event-title": "ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models",
		"language": "en",
		"source": "openreview.net",
		"title": "Out-of-context Meta-learning in Large Language Models",
		"URL": "https://openreview.net/forum?id=X3JFgY4gvf",
		"author": [
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Krasheninnikov",
				"given": "Egor"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					8
				]
			]
		}
	},
	{
		"id": "ferryLearningHybridInterpretable2023",
		"type": "article",
		"abstract": "A hybrid model involves the cooperation of an interpretable model and a complex black box. At inference, any input of the hybrid model is assigned to either its interpretable or complex component based on a gating mechanism. The advantages of such models over classical ones are two-fold: 1) They grant users precise control over the level of transparency of the system and 2) They can potentially perform better than a standalone black box since redirecting some of the inputs to an interpretable model implicitly acts as regularization. Still, despite their high potential, hybrid models remain under-studied in the interpretability/explainability literature. In this paper, we remedy this fact by presenting a thorough investigation of such models from three perspectives: Theory, Taxonomy, and Methods. First, we explore the theory behind the generalization of hybrid models from the Probably-Approximately-Correct (PAC) perspective. A consequence of our PAC guarantee is the existence of a sweet spot for the optimal transparency of the system. When such a sweet spot is attained, a hybrid model can potentially perform better than a standalone black box. Secondly, we provide a general taxonomy for the different ways of training hybrid models: the Post-Black-Box and Pre-Black-Box paradigms. These approaches differ in the order in which the interpretable and complex components are trained. We show where the state-of-the-art hybrid models Hybrid-Rule-Set and Companion-Rule-List fall in this taxonomy. Thirdly, we implement the two paradigms in a single method: HybridCORELS, which extends the CORELS algorithm to hybrid modeling. By leveraging CORELS, HybridCORELS provides a certificate of optimality of its interpretable component and precise control over transparency. We finally show empirically that HybridCORELS is competitive with existing hybrid models, and performs just as well as a standalone black box (or even better) while being partly transparent.",
		"DOI": "10.48550/arXiv.2303.04437",
		"note": "arXiv:2303.04437 [cs, stat]",
		"number": "arXiv:2303.04437",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods",
		"title-short": "Learning Hybrid Interpretable Models",
		"URL": "http://arxiv.org/abs/2303.04437",
		"author": [
			{
				"family": "Ferry",
				"given": "Julien"
			},
			{
				"family": "Laberge",
				"given": "Gabriel"
			},
			{
				"family": "Aïvodji",
				"given": "Ulrich"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					8
				]
			]
		}
	},
	{
		"id": "paissanPosthocInterpretationQuantization2023",
		"type": "article",
		"abstract": "In this paper, we introduce a new approach, called Posthoc Interpretation via Quantization (PIQ), for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. Our model formulation also enables learning concepts by incorporating the supervision of pretrained annotation models such as state-of-the-art image segmentation models. We evaluated our method through quantitative and qualitative studies involving black-and-white images, color images, and audio. As a result of these studies we found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.",
		"DOI": "10.48550/arXiv.2303.12659",
		"note": "arXiv:2303.12659 [cs, eess]",
		"number": "arXiv:2303.12659",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Posthoc Interpretation via Quantization",
		"URL": "http://arxiv.org/abs/2303.12659",
		"author": [
			{
				"family": "Paissan",
				"given": "Francesco"
			},
			{
				"family": "Subakan",
				"given": "Cem"
			},
			{
				"family": "Ravanelli",
				"given": "Mirco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					27
				]
			]
		}
	},
	{
		"id": "rogerEthicalMultimodalSystems2024",
		"type": "article",
		"abstract": "Generative AI systems (ChatGPT, DALL-E, etc) are expanding into multiple areas of our lives, from art Rombach et al. [2021] to mental health Rob Morris and Kareem Kouddous [2022]; their rapidly growing societal impact opens new opportunities, but also raises ethical concerns. The emerging field of AI alignment aims to make AI systems reflect human values. This paper focuses on evaluating the ethics of multimodal AI systems involving both text and images - a relatively under-explored area, as most alignment work is currently focused on language models. We first create a multimodal ethical database from human feedback on ethicality. Then, using this database, we develop algorithms, including a RoBERTa-large classifier and a multilayer perceptron, to automatically assess the ethicality of system responses.",
		"DOI": "10.48550/arXiv.2304.13765",
		"note": "arXiv:2304.13765 [cs]",
		"number": "arXiv:2304.13765",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards ethical multimodal systems",
		"URL": "http://arxiv.org/abs/2304.13765",
		"author": [
			{
				"family": "Roger",
				"given": "Alexis"
			},
			{
				"family": "Aïmeur",
				"given": "Esma"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					20
				]
			]
		}
	},
	{
		"id": "altstidlRaisingBarCertified2023",
		"type": "article",
		"abstract": "Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. One of our main insights is that the generalization gap, i.e., the difference between the training and test accuracy of the original model, is a good predictor of the magnitude of the robustness improvement when using additional generated data. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\\ell_2$ ($\\epsilon = 36/255$) and $\\ell_\\infty$ ($\\epsilon = 8/255$) threat models, outperforming the previous best results by $+3.95\\%$ and $+1.39\\%$, respectively. Furthermore, we report similar improvements for CIFAR-100.",
		"DOI": "10.48550/arXiv.2305.10388",
		"note": "arXiv:2305.10388 [cs]",
		"number": "arXiv:2305.10388",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Raising the Bar for Certified Adversarial Robustness with Diffusion Models",
		"URL": "http://arxiv.org/abs/2305.10388",
		"author": [
			{
				"family": "Altstidl",
				"given": "Thomas"
			},
			{
				"family": "Dobre",
				"given": "David"
			},
			{
				"family": "Eskofier",
				"given": "Björn"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			},
			{
				"family": "Schwinn",
				"given": "Leo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					17
				]
			]
		}
	},
	{
		"id": "kangThinkYouAct2024",
		"type": "article",
		"abstract": "Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.",
		"DOI": "10.48550/arXiv.2305.16338",
		"note": "arXiv:2305.16338 [cs]",
		"number": "arXiv:2305.16338",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Think Before You Act: Decision Transformers with Working Memory",
		"title-short": "Think Before You Act",
		"URL": "http://arxiv.org/abs/2305.16338",
		"author": [
			{
				"family": "Kang",
				"given": "Jikun"
			},
			{
				"family": "Laroche",
				"given": "Romain"
			},
			{
				"family": "Yuan",
				"given": "Xingdi"
			},
			{
				"family": "Trischler",
				"given": "Adam"
			},
			{
				"family": "Liu",
				"given": "Xue"
			},
			{
				"family": "Fu",
				"given": "Jie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					28
				]
			]
		}
	},
	{
		"id": "shevlaneModelEvaluationExtreme2023b",
		"type": "article",
		"abstract": "Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through \"dangerous capability evaluations\") and the propensity of models to apply their capabilities for harm (through \"alignment evaluations\"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.",
		"DOI": "10.48550/arXiv.2305.15324",
		"note": "arXiv:2305.15324 [cs]",
		"number": "arXiv:2305.15324",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Model evaluation for extreme risks",
		"URL": "http://arxiv.org/abs/2305.15324",
		"author": [
			{
				"family": "Shevlane",
				"given": "Toby"
			},
			{
				"family": "Farquhar",
				"given": "Sebastian"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Leung",
				"given": "Jade"
			},
			{
				"family": "Kokotajlo",
				"given": "Daniel"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Ho",
				"given": "Lewis"
			},
			{
				"family": "Siddarth",
				"given": "Divya"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Bolina",
				"given": "Vijay"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					22
				]
			]
		}
	},
	{
		"id": "harbiResponsibleDesignPatterns2023",
		"type": "article",
		"abstract": "Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsible AI systems in production.",
		"DOI": "10.48550/arXiv.2306.01788",
		"note": "arXiv:2306.01788 [cs]",
		"number": "arXiv:2306.01788",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Responsible Design Patterns for Machine Learning Pipelines",
		"URL": "http://arxiv.org/abs/2306.01788",
		"author": [
			{
				"family": "Harbi",
				"given": "Saud Hakem Al"
			},
			{
				"family": "Tidjon",
				"given": "Lionel Nganyewou"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					7
				]
			]
		}
	},
	{
		"id": "bucincaAHAFacilitatingAI2023",
		"type": "article",
		"abstract": "While demands for change and accountability for harmful AI consequences mount, foreseeing the downstream effects of deploying AI systems remains a challenging task. We developed AHA! (Anticipating Harms of AI), a generative framework to assist AI practitioners and decision-makers in anticipating potential harms and unintended consequences of AI systems prior to development or deployment. Given an AI deployment scenario, AHA! generates descriptions of possible harms for different stakeholders. To do so, AHA! systematically considers the interplay between common problematic AI behaviors as well as their potential impacts on different stakeholders, and narrates these conditions through vignettes. These vignettes are then filled in with descriptions of possible harms by prompting crowd workers and large language models. By examining 4113 harms surfaced by AHA! for five different AI deployment scenarios, we found that AHA! generates meaningful examples of harms, with different problematic AI behaviors resulting in different types of harms. Prompting both crowds and a large language model with the vignettes resulted in more diverse examples of harms than those generated by either the crowd or the model alone. To gauge AHA!'s potential practical utility, we also conducted semi-structured interviews with responsible AI professionals (N=9). Participants found AHA!'s systematic approach to surfacing harms important for ethical reflection and discovered meaningful stakeholders and harms they believed they would not have thought of otherwise. Participants, however, differed in their opinions about whether AHA! should be used upfront or as a secondary-check and noted that AHA! may shift harm anticipation from an ideation problem to a potentially demanding review problem. Drawing on our results, we discuss design implications of building tools to help practitioners envision possible harms.",
		"DOI": "10.48550/arXiv.2306.03280",
		"note": "arXiv:2306.03280 [cs]",
		"number": "arXiv:2306.03280",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AHA!: Facilitating AI Impact Assessment by Generating Examples of Harms",
		"title-short": "AHA!",
		"URL": "http://arxiv.org/abs/2306.03280",
		"author": [
			{
				"family": "Buçinca",
				"given": "Zana"
			},
			{
				"family": "Pham",
				"given": "Chau Minh"
			},
			{
				"family": "Jakesch",
				"given": "Maurice"
			},
			{
				"family": "Ribeiro",
				"given": "Marco Tulio"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					5
				]
			]
		}
	},
	{
		"id": "dahlBenchmarkingNeuralNetwork2023",
		"type": "article",
		"abstract": "Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass.",
		"DOI": "10.48550/arXiv.2306.07179",
		"note": "arXiv:2306.07179 [cs, stat]",
		"number": "arXiv:2306.07179",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Benchmarking Neural Network Training Algorithms",
		"URL": "http://arxiv.org/abs/2306.07179",
		"author": [
			{
				"family": "Dahl",
				"given": "George E."
			},
			{
				"family": "Schneider",
				"given": "Frank"
			},
			{
				"family": "Nado",
				"given": "Zachary"
			},
			{
				"family": "Agarwal",
				"given": "Naman"
			},
			{
				"family": "Sastry",
				"given": "Chandramouli Shama"
			},
			{
				"family": "Hennig",
				"given": "Philipp"
			},
			{
				"family": "Medapati",
				"given": "Sourabh"
			},
			{
				"family": "Eschenhagen",
				"given": "Runa"
			},
			{
				"family": "Kasimbeg",
				"given": "Priya"
			},
			{
				"family": "Suo",
				"given": "Daniel"
			},
			{
				"family": "Bae",
				"given": "Juhan"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Peirson",
				"given": "Abel L."
			},
			{
				"family": "Khan",
				"given": "Bilal"
			},
			{
				"family": "Anil",
				"given": "Rohan"
			},
			{
				"family": "Rabbat",
				"given": "Mike"
			},
			{
				"family": "Krishnan",
				"given": "Shankar"
			},
			{
				"family": "Snider",
				"given": "Daniel"
			},
			{
				"family": "Amid",
				"given": "Ehsan"
			},
			{
				"family": "Chen",
				"given": "Kongtao"
			},
			{
				"family": "Maddison",
				"given": "Chris J."
			},
			{
				"family": "Vasudev",
				"given": "Rakshith"
			},
			{
				"family": "Badura",
				"given": "Michal"
			},
			{
				"family": "Garg",
				"given": "Ankush"
			},
			{
				"family": "Mattson",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "ibrahimOutofDistributionAdversarialRobustness2023",
		"type": "article",
		"abstract": "Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.",
		"DOI": "10.48550/arXiv.2210.03150",
		"note": "arXiv:2210.03150 [cs]",
		"number": "arXiv:2210.03150",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Out-of-Distribution Adversarial Robustness",
		"URL": "http://arxiv.org/abs/2210.03150",
		"author": [
			{
				"family": "Ibrahim",
				"given": "Adam"
			},
			{
				"family": "Guille-Escuret",
				"given": "Charles"
			},
			{
				"family": "Mitliagkas",
				"given": "Ioannis"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Bashivan",
				"given": "Pouya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					26
				]
			]
		}
	},
	{
		"id": "fengConstantMemoryAttention2023",
		"type": "article",
		"abstract": "Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation. Highlighting CMABs efficacy, we introduce methods for Neural Processes and Temporal Point Processes. Empirically, we show our proposed methods achieve results competitive with state-of-the-art while being significantly more memory efficient.",
		"DOI": "10.48550/arXiv.2306.12599",
		"note": "arXiv:2306.12599 [cs]",
		"number": "arXiv:2306.12599",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Constant Memory Attention Block",
		"URL": "http://arxiv.org/abs/2306.12599",
		"author": [
			{
				"family": "Feng",
				"given": "Leo"
			},
			{
				"family": "Tung",
				"given": "Frederick"
			},
			{
				"family": "Hajimirsadeghi",
				"given": "Hossein"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Ahmed",
				"given": "Mohamed Osama"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					21
				]
			]
		}
	},
	{
		"id": "fleischerAcceleratingGeneralizedRandom2023",
		"type": "article",
		"abstract": "Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators obtained from forests whose trees are grown through the fixed-point splitting rule, and provide numerical simulations demonstrating that the estimators obtained from such forests are comparable to those obtained from the more costly gradient-based rule.",
		"DOI": "10.48550/arXiv.2306.11908",
		"note": "arXiv:2306.11908 [cs, stat]",
		"number": "arXiv:2306.11908",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Accelerating Generalized Random Forests with Fixed-Point Trees",
		"URL": "http://arxiv.org/abs/2306.11908",
		"author": [
			{
				"family": "Fleischer",
				"given": "David"
			},
			{
				"family": "Stephens",
				"given": "David A."
			},
			{
				"family": "Yang",
				"given": "Archer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					20
				]
			]
		}
	},
	{
		"id": "nobandeganiCognitiveModelsSimulators2023",
		"type": "article-journal",
		"abstract": "In this work, we substantiate the idea of $\\textit{cognitive models as simulators}$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making the training process safer, cheaper, and faster. We leverage this idea in the context of learning a fair behavior toward a counterpart exhibiting various emotional states — as implicit human feedback. As a case study, we adopt the Ultimatum game (UG), a canonical task in behavioral and brain sciences for studying fairness. We show that our reinforcement learning (RL) agents learn to exhibit differential, rationally-justified behaviors under various emotional states of their UG counterpart. We discuss the implications of our work for AI and cognitive science research, and its potential for interactive learning with implicit human feedback.",
		"language": "en",
		"source": "openreview.net",
		"title": "Cognitive Models as Simulators: Using Cognitive Models to Tap into Implicit Human Feedback",
		"title-short": "Cognitive Models as Simulators",
		"URL": "https://openreview.net/forum?id=tuxCm2h5JL#all",
		"author": [
			{
				"family": "Nobandegani",
				"given": "Ardavan S."
			},
			{
				"family": "Shultz",
				"given": "Thomas"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					20
				]
			]
		}
	},
	{
		"id": "rezaei-shoshtariHypernetworksZeroShotTransfer2023",
		"type": "article-journal",
		"abstract": "In this paper, hypernetworks are trained to generate behaviors across a range of unseen task conditions, via a novel TD-based training objective and data from a set of near-optimal RL solutions for training tasks. This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on  zero-shot performance at test time, enabled by knowledge of the task parameters (also known as context). Our technical approach is based upon viewing each RL algorithm as a mapping from the MDP specifics to the near-optimal value function and policy and seek to approximate it with a hypernetwork that can generate near-optimal value functions and policies, given the parameters of the MDP. We show that, under certain conditions, this mapping can be considered as a supervised learning problem. We empirically evaluate the effectiveness of our method for zero-shot transfer to new reward and transition dynamics on a series of continuous control tasks from DeepMind Control Suite. Our method demonstrates significant improvements over baselines from multitask and meta RL approaches.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v37i8.26146",
		"ISSN": "2374-3468",
		"issue": "8",
		"language": "en",
		"license": "Copyright (c) 2023 Association for the Advancement of Artificial Intelligence",
		"note": "number: 8",
		"page": "9579-9587",
		"source": "ojs.aaai.org",
		"title": "Hypernetworks for Zero-Shot Transfer in Reinforcement Learning",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/26146",
		"volume": "37",
		"author": [
			{
				"family": "Rezaei-Shoshtari",
				"given": "Sahand"
			},
			{
				"family": "Morissette",
				"given": "Charlotte"
			},
			{
				"family": "Hogan",
				"given": "Francois R."
			},
			{
				"family": "Dudek",
				"given": "Gregory"
			},
			{
				"family": "Meger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					26
				]
			]
		}
	},
	{
		"id": "fleisigFairPrismEvaluatingFairnessRelated2023",
		"type": "paper-conference",
		"abstract": "It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer specification of dataset coverage, and accounting for annotator disagreement and harms that are context-dependent. FairPrism's annotations include the extent of stereotyping and demeaning harms, the demographic groups targeted, and appropriateness for different applications. The annotations also include specific harms that occur in interactive contexts and harms that raise normative concerns when the “speaker” is an AI system. Due to its precision and granularity, FairPrism can be used to diagnose (1) the types of fairness-related harms that AI text generation systems cause, and (2) the potential limitations of mitigation methods, both of which we illustrate through case studies. Finally, the process we followed to develop FairPrism offers a recipe for building improved datasets for measuring and mitigating harms caused by AI systems.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.343",
		"event-place": "Toronto, Canada",
		"event-title": "ACL 2023",
		"page": "6231–6251",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "FairPrism: Evaluating Fairness-Related Harms in Text Generation",
		"title-short": "FairPrism",
		"URL": "https://aclanthology.org/2023.acl-long.343",
		"author": [
			{
				"family": "Fleisig",
				"given": "Eve"
			},
			{
				"family": "Amstutz",
				"given": "Aubrie"
			},
			{
				"family": "Atalla",
				"given": "Chad"
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Daumé III",
				"given": "Hal"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			},
			{
				"family": "Sheng",
				"given": "Emily"
			},
			{
				"family": "Vann",
				"given": "Dan"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "marcotteRegionsReliabilityEvaluation2023",
		"type": "article",
		"abstract": "Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the \"region of reliability\" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as commonly performed in the literature.",
		"DOI": "10.48550/arXiv.2304.09836",
		"note": "arXiv:2304.09836 [cs, stat]",
		"number": "arXiv:2304.09836",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts",
		"URL": "http://arxiv.org/abs/2304.09836",
		"author": [
			{
				"family": "Marcotte",
				"given": "Étienne"
			},
			{
				"family": "Zantedeschi",
				"given": "Valentina"
			},
			{
				"family": "Drouin",
				"given": "Alexandre"
			},
			{
				"family": "Chapados",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					6
				]
			]
		}
	},
	{
		"id": "wangAdjustingMachineLearning2022",
		"type": "article-journal",
		"abstract": "Machine learning (ML) methods have the potential to automate high-stakes decisions, such as bail admissions or credit lending, by analyzing and learning from historical data. But these algorithmic decisions may be unfair: in learning from historical data, they may replicate discriminatory practices from the past. In this paper, we propose two algorithms that adjust fitted ML predictors to produce decisions that are fair. Our methods provide post-hoc adjustments to the predictors, without requiring that they be retrained. We consider a causal model of the ML decisions, define fairness through counterfactual decisions within the model, and then form algorithmic decisions that capture the historical data as well as possible but are provably fair. In particular, we consider two definitions of fairness. The first is ``equal counterfactual opportunity,'' where the counterfactual distribution of the decision is the same regardless of the protected attribute; the second is counterfactual fairness. We evaluate the algorithms, and the trade-off between accuracy and fairness, on datasets about admissions, income, credit, and recidivism.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness",
		"URL": "https://openreview.net/forum?id=P6NcRPb13w",
		"author": [
			{
				"family": "Wang",
				"given": "Yixin"
			},
			{
				"family": "Sridhar",
				"given": "Dhanya"
			},
			{
				"family": "Blei",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					20
				]
			]
		}
	},
	{
		"id": "hoInternationalInstitutionsAdvanced2023",
		"type": "article",
		"abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.",
		"DOI": "10.48550/arXiv.2307.04699",
		"note": "arXiv:2307.04699 [cs]",
		"number": "arXiv:2307.04699",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "International Institutions for Advanced AI",
		"URL": "http://arxiv.org/abs/2307.04699",
		"author": [
			{
				"family": "Ho",
				"given": "Lewis"
			},
			{
				"family": "Barnhart",
				"given": "Joslyn"
			},
			{
				"family": "Trager",
				"given": "Robert"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Carnegie",
				"given": "Allison"
			},
			{
				"family": "Chowdhury",
				"given": "Rumman"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			},
			{
				"family": "Levi",
				"given": "Margaret"
			},
			{
				"family": "Snidal",
				"given": "Duncan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					11
				]
			]
		}
	},
	{
		"id": "abelConvergenceBoundedAgents2023",
		"type": "article",
		"abstract": "When has an agent converged? Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing. However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear. In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents. The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease. The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship. We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.",
		"DOI": "10.48550/arXiv.2307.11044",
		"note": "arXiv:2307.11044 [cs]",
		"number": "arXiv:2307.11044",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Convergence of Bounded Agents",
		"URL": "http://arxiv.org/abs/2307.11044",
		"author": [
			{
				"family": "Abel",
				"given": "David"
			},
			{
				"family": "Barreto",
				"given": "André"
			},
			{
				"family": "Hasselt",
				"given": "Hado",
				"non-dropping-particle": "van"
			},
			{
				"family": "Van Roy",
				"given": "Benjamin"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					20
				]
			]
		}
	},
	{
		"id": "gasseUsingConfoundedData2023",
		"type": "article-journal",
		"abstract": "In the presence of confounding, naively using off-the-shelf offline reinforcement learning (RL) algorithms leads to sub-optimal behaviour. In this work, we propose a safe method to exploit confounded offline data in model-based RL, which improves the sample-efficiency of an interactive agent that also collects online, unconfounded data. First, we import ideas from the well-established framework of $do$-calculus to express model-based RL as a causal inference problem, thus bridging the gap between the fields of RL and causality. Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable. We prove that our method is correct and efficient, in the sense that it attains better generalization guarantees thanks to the confounded offline data (in the asymptotic case), regardless of the confounding effect (the offline expert's behaviour). We showcase our method on a series of synthetic experiments, which demonstrate that a) using confounded offline data naively degrades the sample-efficiency of an RL agent; b) using confounded offline data correctly improves sample-efficiency.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=nFWRuJXPkU",
		"author": [
			{
				"family": "Gasse",
				"given": "Maxime"
			},
			{
				"family": "Grasset",
				"given": "Damien"
			},
			{
				"family": "Gaudron",
				"given": "Guillaume"
			},
			{
				"family": "Oudeyer",
				"given": "Pierre-Yves"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					23
				]
			]
		}
	},
	{
		"id": "butlinConsciousnessArtificialIntelligence2023",
		"type": "article",
		"abstract": "Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive \"indicator properties\" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.",
		"DOI": "10.48550/arXiv.2308.08708",
		"note": "arXiv:2308.08708 [cs, q-bio]",
		"number": "arXiv:2308.08708",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
		"title-short": "Consciousness in Artificial Intelligence",
		"URL": "http://arxiv.org/abs/2308.08708",
		"author": [
			{
				"family": "Butlin",
				"given": "Patrick"
			},
			{
				"family": "Long",
				"given": "Robert"
			},
			{
				"family": "Elmoznino",
				"given": "Eric"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Birch",
				"given": "Jonathan"
			},
			{
				"family": "Constant",
				"given": "Axel"
			},
			{
				"family": "Deane",
				"given": "George"
			},
			{
				"family": "Fleming",
				"given": "Stephen M."
			},
			{
				"family": "Frith",
				"given": "Chris"
			},
			{
				"family": "Ji",
				"given": "Xu"
			},
			{
				"family": "Kanai",
				"given": "Ryota"
			},
			{
				"family": "Klein",
				"given": "Colin"
			},
			{
				"family": "Lindsay",
				"given": "Grace"
			},
			{
				"family": "Michel",
				"given": "Matthias"
			},
			{
				"family": "Mudrik",
				"given": "Liad"
			},
			{
				"family": "Peters",
				"given": "Megan A. K."
			},
			{
				"family": "Schwitzgebel",
				"given": "Eric"
			},
			{
				"family": "Simon",
				"given": "Jonathan"
			},
			{
				"family": "VanRullen",
				"given": "Rufin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					22
				]
			]
		}
	},
	{
		"id": "nekoeiFewshotCoordinationRevisiting2023",
		"type": "article",
		"abstract": "Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate zero-shot (without additional interaction experience) with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent's ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents.",
		"DOI": "10.48550/arXiv.2308.10284",
		"note": "arXiv:2308.10284 [cs]",
		"number": "arXiv:2308.10284",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi",
		"title-short": "Towards Few-shot Coordination",
		"URL": "http://arxiv.org/abs/2308.10284",
		"author": [
			{
				"family": "Nekoei",
				"given": "Hadi"
			},
			{
				"family": "Zhao",
				"given": "Xutong"
			},
			{
				"family": "Rajendran",
				"given": "Janarthanan"
			},
			{
				"family": "Liu",
				"given": "Miao"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					20
				]
			]
		}
	},
	{
		"id": "yuOpenClosedSmall2023",
		"type": "article",
		"abstract": "Recent advancements in large language models have demonstrated remarkable capabilities across various NLP tasks. But many questions remain, including whether open-source models match closed ones, why these models excel or struggle with certain tasks, and what types of practical procedures can improve performance. We address these questions in the context of classification by evaluating three classes of models using eight datasets across three distinct tasks: named entity recognition, political party prediction, and misinformation detection. While larger LLMs often lead to improved performance, open-source models can rival their closed-source counterparts by fine-tuning. Moreover, supervised smaller models, like RoBERTa, can achieve similar or even greater performance in many datasets compared to generative LLMs. On the other hand, closed models maintain an advantage in hard tasks that demand the most generalizability. This study underscores the importance of model selection based on task requirements",
		"DOI": "10.48550/arXiv.2308.10092",
		"note": "arXiv:2308.10092 [cs]",
		"number": "arXiv:2308.10092",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Open, Closed, or Small Language Models for Text Classification?",
		"URL": "http://arxiv.org/abs/2308.10092",
		"author": [
			{
				"family": "Yu",
				"given": "Hao"
			},
			{
				"family": "Yang",
				"given": "Zachary"
			},
			{
				"family": "Pelrine",
				"given": "Kellin"
			},
			{
				"family": "Godbout",
				"given": "Jean Francois"
			},
			{
				"family": "Rabbany",
				"given": "Reihaneh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					19
				]
			]
		}
	},
	{
		"id": "berryEfficientEpistemicUncertainty2024",
		"type": "article",
		"abstract": "This work introduces an efficient novel approach for epistemic uncertainty estimation for ensemble models for regression tasks using pairwise-distance estimators (PaiDEs). Utilizing the pairwise-distance between model components, these estimators establish bounds on entropy. We leverage this capability to enhance the performance of Bayesian Active Learning by Disagreement (BALD). Notably, unlike sample-based Monte Carlo estimators, PaiDEs exhibit a remarkable capability to estimate epistemic uncertainty at speeds up to 100 times faster while covering a significantly larger number of inputs at once and demonstrating superior performance in higher dimensions. To validate our approach, we conducted a varied series of regression experiments on commonly used benchmarks: 1D sinusoidal data, $\\textit{Pendulum}$, $\\textit{Hopper}$, $\\textit{Ant}$ and $\\textit{Humanoid}$. For each experimental setting, an active learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation. We compare our approach to existing active learning methods and find that our approach outperforms on high-dimensional regression tasks.",
		"DOI": "10.48550/arXiv.2308.13498",
		"note": "arXiv:2308.13498 [cs]",
		"number": "arXiv:2308.13498",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Efficient Epistemic Uncertainty Estimation in Regression Ensemble Models Using Pairwise-Distance Estimators",
		"URL": "http://arxiv.org/abs/2308.13498",
		"author": [
			{
				"family": "Berry",
				"given": "Lucas"
			},
			{
				"family": "Meger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					14
				]
			]
		}
	},
	{
		"id": "rismaniWhatDoesIt2023a",
		"type": "paper-conference",
		"abstract": "With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3600211.3604702",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"page": "584–595",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "What does it mean to be a responsible AI practitioner: An ontology of roles and skills",
		"title-short": "What does it mean to be a responsible AI practitioner",
		"URL": "https://doi.org/10.1145/3600211.3604702",
		"author": [
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Moon",
				"given": "AJung"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					29
				]
			]
		}
	},
	{
		"id": "moradiTidyingConversationalRecommender2023",
		"type": "article",
		"abstract": "The growing popularity of language models has sparked interest in conversational recommender systems (CRS) within both industry and research circles. However, concerns regarding biases in these systems have emerged. While individual components of CRS have been subject to bias studies, a literature gap remains in understanding specific biases unique to CRS and how these biases may be amplified or reduced when integrated into complex CRS models. In this paper, we provide a concise review of biases in CRS by surveying recent literature. We examine the presence of biases throughout the system's pipeline and consider the challenges that arise from combining multiple models. Our study investigates biases in classic recommender systems and their relevance to CRS. Moreover, we address specific biases in CRS, considering variations with and without natural language understanding capabilities, along with biases related to dialogue systems and language models. Through our findings, we highlight the necessity of adopting a holistic perspective when dealing with biases in complex CRS models.",
		"DOI": "10.48550/arXiv.2309.02550",
		"note": "arXiv:2309.02550 [cs]",
		"number": "arXiv:2309.02550",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tidying Up the Conversational Recommender Systems' Biases",
		"URL": "http://arxiv.org/abs/2309.02550",
		"author": [
			{
				"family": "Moradi",
				"given": "Armin"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					5
				]
			]
		}
	},
	{
		"id": "chungThinkerLearningPlan2023",
		"type": "article",
		"abstract": "We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments.",
		"DOI": "10.48550/arXiv.2307.14993",
		"note": "arXiv:2307.14993 [cs]",
		"number": "arXiv:2307.14993",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Thinker: Learning to Plan and Act",
		"title-short": "Thinker",
		"URL": "http://arxiv.org/abs/2307.14993",
		"author": [
			{
				"family": "Chung",
				"given": "Stephen"
			},
			{
				"family": "Anokhin",
				"given": "Ivan"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					26
				]
			]
		}
	},
	{
		"id": "cacciaMultiHeadAdapterRouting2023",
		"type": "article",
		"abstract": "Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\\texttt{MHR}$ (Multi-Head Routing) which combines subsets of adapter parameters and outperforms $\\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\\texttt{MHR}$-$z$) we achieve competitive performance with extreme parameter efficiency. Second, we find that $\\texttt{Poly}$/$\\texttt{MHR}$ performance is a result of better multi-task optimization, rather than modular inductive biases that facilitate adapter recombination and local adaptation, as previously hypothesized. In fact, we find that $\\texttt{MHR}$ exhibits high gradient alignment between training tasks. We find that routing is most beneficial during multi-task pre-training rather than during few-shot adaptation and propose $\\texttt{MHR}$-$\\mu$, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks. This establishes $\\texttt{MHR}$-$\\mu$ as an effective method for single-adapter fine-tuning. We also show that $\\texttt{MHR}$-$\\mu$ can be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set: this yields gains up to 3% on absolute accuracy w.r.t. the baselines.",
		"DOI": "10.48550/arXiv.2211.03831",
		"note": "arXiv:2211.03831 [cs]",
		"number": "arXiv:2211.03831",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-Head Adapter Routing for Cross-Task Generalization",
		"URL": "http://arxiv.org/abs/2211.03831",
		"author": [
			{
				"family": "Caccia",
				"given": "Lucas"
			},
			{
				"family": "Ponti",
				"given": "Edoardo"
			},
			{
				"family": "Su",
				"given": "Zhan"
			},
			{
				"family": "Pereira",
				"given": "Matheus"
			},
			{
				"family": "Roux",
				"given": "Nicolas Le"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					13
				]
			]
		}
	},
	{
		"id": "sordoniJointPromptOptimization2023",
		"type": "article",
		"abstract": "Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.",
		"DOI": "10.48550/arXiv.2306.12509",
		"note": "arXiv:2306.12509 [cs]",
		"number": "arXiv:2306.12509",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
		"URL": "http://arxiv.org/abs/2306.12509",
		"author": [
			{
				"family": "Sordoni",
				"given": "Alessandro"
			},
			{
				"family": "Yuan",
				"given": "Xingdi"
			},
			{
				"family": "Côté",
				"given": "Marc-Alexandre"
			},
			{
				"family": "Pereira",
				"given": "Matheus"
			},
			{
				"family": "Trischler",
				"given": "Adam"
			},
			{
				"family": "Xiao",
				"given": "Ziang"
			},
			{
				"family": "Hosseini",
				"given": "Arian"
			},
			{
				"family": "Niedtner",
				"given": "Friederike"
			},
			{
				"family": "Roux",
				"given": "Nicolas Le"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					4
				]
			]
		}
	},
	{
		"id": "noukhovitchLanguageModelAlignment2023",
		"type": "article",
		"abstract": "Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available at github.com/mnoukhov/elastic-reset.",
		"DOI": "10.48550/arXiv.2312.07551",
		"note": "arXiv:2312.07551 [cs]",
		"number": "arXiv:2312.07551",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Model Alignment with Elastic Reset",
		"URL": "http://arxiv.org/abs/2312.07551",
		"author": [
			{
				"family": "Noukhovitch",
				"given": "Michael"
			},
			{
				"family": "Lavoie",
				"given": "Samuel"
			},
			{
				"family": "Strub",
				"given": "Florian"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					6
				]
			]
		}
	},
	{
		"id": "fengTreeCrossAttention2024",
		"type": "article",
		"abstract": "Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens $L < N$ on which cross attention is then applied, resulting in only $\\mathcal{O}(L)$ complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic $\\mathcal{O}(\\log(N))$ number of tokens for performing inference. TCA organizes the data in a tree structure and performs a tree search at inference time to retrieve the relevant tokens for prediction. Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference. We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient. Furthermore, we compare ReTreever against Perceiver IO, showing significant gains while using the same number of tokens for inference.",
		"DOI": "10.48550/arXiv.2309.17388",
		"note": "arXiv:2309.17388 [cs]",
		"number": "arXiv:2309.17388",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tree Cross Attention",
		"URL": "http://arxiv.org/abs/2309.17388",
		"author": [
			{
				"family": "Feng",
				"given": "Leo"
			},
			{
				"family": "Tung",
				"given": "Frederick"
			},
			{
				"family": "Hajimirsadeghi",
				"given": "Hossein"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Ahmed",
				"given": "Mohamed Osama"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					1
				]
			]
		}
	},
	{
		"id": "zhaoZeroShotFaultDetection2023",
		"type": "paper-conference",
		"abstract": "We consider the detection of faults in robotic manipulators, with particular emphasis on faults that have not been observed or identified in advance, which naturally includes those that occur very infrequently. Recent studies indicate that the reward function obtained through Inverse Reinforcement Learning (IRL) can help detect anomalies caused by faults in a control system (i.e. fault detection). Current IRL methods for fault detection, however, either use a linear reward representation or require extensive sampling from the environment to estimate the policy, rendering them inappropriate for safety-critical situations where sampling of failure observations via fault injection can be expensive and dangerous. To address this issue, this paper proposes a zero-shot and exogenous fault detector based on an approximate variational reward imitation learning (AVRIL) structure. The fault detector recovers a reward signal as a function of externally observable information to describe the normal operation, which can then be used to detect anomalies caused by faults. Our method incorporates expert knowledge through a customizable reward prior distribution, allowing the fault detector to learn the reward solely from normal operation samples, without the need for a simulator or costly interactions with the environment. We evaluate our approach for exogenous partial fault detection in multi-stage robotic manipulator tasks, comparing it with several baseline methods. The results demonstrate that our method more effectively identifies unseen faults even when they occur within just three controller time steps.",
		"container-title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"DOI": "10.1109/IROS55552.2023.10342143",
		"event-title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"note": "ISSN: 2153-0866",
		"page": "3582-3589",
		"source": "IEEE Xplore",
		"title": "Zero-Shot Fault Detection for Manipulators Through Bayesian Inverse Reinforcement Learning",
		"URL": "https://ieeexplore.ieee.org/document/10342143",
		"author": [
			{
				"family": "Zhao",
				"given": "Hanqing"
			},
			{
				"family": "Liu",
				"given": "Xue"
			},
			{
				"family": "Dudek",
				"given": "Gregory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10
				]
			]
		}
	},
	{
		"id": "shuiMoreGeneralLoss2023",
		"type": "article-journal",
		"abstract": "In this article, we present an analysis of unsupervised domain adaptation with a series of theoretical and algorithmic results. We derive a novel Rényi-αα divergence-based generalization bound, which is tailored to domain adaptation algorithms with arbitrary loss functions in a stochastic setting. Moreover, our theoretical results provide new insights into the assumptions for successful domain adaptation: the closeness between the conditional distributions of the domains and the Lipschitzness on the source domain. With these assumptions, we reveal the following: if their conditional generation distributions are close, the Lipschitzness property of the target domain can be transferred from the Lipschitzness on the source domain, without knowing the exact target distribution. Motivated by our analysis and assumptions, we further derive practical principles for deep domain adaptation: 1) Rényi-2 adversarial training for marginal distributions matching and 2) Lipschitz regularization for the classifier. Our experimental results on both synthetic and real-world datasets support our theoretical findings and the practical efficiency of the proposed principles.",
		"container-title": "IEEE Transactions on Knowledge and Data Engineering",
		"DOI": "10.1109/TKDE.2023.3266785",
		"ISSN": "1558-2191",
		"issue": "10",
		"note": "event-title: IEEE Transactions on Knowledge and Data Engineering",
		"page": "10140-10150",
		"source": "IEEE Xplore",
		"title": "Towards More General Loss and Setting in Unsupervised Domain Adaptation",
		"URL": "https://ieeexplore.ieee.org/document/10102307",
		"volume": "35",
		"author": [
			{
				"family": "Shui",
				"given": "Changjian"
			},
			{
				"family": "Pu",
				"given": "Ruizhi"
			},
			{
				"family": "Xu",
				"given": "Gezheng"
			},
			{
				"family": "Wen",
				"given": "Jun"
			},
			{
				"family": "Zhou",
				"given": "Fan"
			},
			{
				"family": "Gagné",
				"given": "Christian"
			},
			{
				"family": "Ling",
				"given": "Charles X."
			},
			{
				"family": "Wang",
				"given": "Boyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10
				]
			]
		}
	},
	{
		"id": "yinhoEmpiricalStudyBugs2023",
		"type": "paper-conference",
		"abstract": "Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch’s development, and assess their locality within the project, and extract patterns of bug fixes. Our results highlight that PyTorch bugs are more like traditional software projects bugs, than related to deep learning characteristics. Finally, we also compare our results with the study on TensorFlow, highlighting similarities and differences across the bug identification and fixing process.",
		"container-title": "2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
		"DOI": "10.1109/ICSME58846.2023.00031",
		"event-title": "2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
		"note": "ISSN: 2576-3148",
		"page": "220-231",
		"source": "IEEE Xplore",
		"title": "An Empirical Study on Bugs Inside PyTorch: A Replication Study",
		"title-short": "An Empirical Study on Bugs Inside PyTorch",
		"URL": "https://ieeexplore.ieee.org/document/10336350",
		"author": [
			{
				"family": "Yin Ho",
				"given": "Sharon Chee"
			},
			{
				"family": "Majdinasab",
				"given": "Vahid"
			},
			{
				"family": "Islam",
				"given": "Mohayeminul"
			},
			{
				"family": "Costa",
				"given": "Diego Elias"
			},
			{
				"family": "Shihab",
				"given": "Emad"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Nadi",
				"given": "Sarah"
			},
			{
				"family": "Raza",
				"given": "Muhammad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10
				]
			]
		}
	},
	{
		"id": "bengioAICatastrophicRisk2023",
		"type": "article-journal",
		"abstract": "Since OpenAI's release of the very large language models Chat-GPT and GPT-4, the potential dangers of AI have garnered widespread public attention. In this essay, the author reviews the threats to democracy posed by the possibility of \"rogue AIs,\" dangerous and powerful AIs that would execute harmful goals, irrespective of whether the outcomes are intended by humans. To mitigate against the risk that rogue AIs present to democracy and geopolitical stability, the author argues that research into safe and defensive AIs should be conducted by a multilateral, international network of research laboratories.",
		"container-title": "Journal of Democracy",
		"ISSN": "1086-3214",
		"issue": "4",
		"note": "publisher: Johns Hopkins University Press",
		"page": "111-121",
		"source": "Project MUSE",
		"title": "AI and Catastrophic Risk",
		"URL": "https://muse.jhu.edu/pub/1/article/907692",
		"volume": "34",
		"author": [
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "coteDataCleaningMachine2024",
		"type": "article",
		"abstract": "Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. Results: We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. Conclusion: We believe that our review of the literature will help the community develop better approaches to clean data.",
		"DOI": "10.48550/arXiv.2310.01765",
		"note": "arXiv:2310.01765 [cs]",
		"number": "arXiv:2310.01765",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Data Cleaning and Machine Learning: A Systematic Literature Review",
		"title-short": "Data Cleaning and Machine Learning",
		"URL": "http://arxiv.org/abs/2310.01765",
		"author": [
			{
				"family": "Côté",
				"given": "Pierre-Olivier"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Ahmed",
				"given": "Nafisa"
			},
			{
				"family": "Humeniuk",
				"given": "Dmytro"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					30
				]
			]
		}
	},
	{
		"id": "meadeUsingInContextLearning2023",
		"type": "article",
		"abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
		"DOI": "10.48550/arXiv.2302.00871",
		"note": "arXiv:2302.00871 [cs]",
		"number": "arXiv:2302.00871",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Using In-Context Learning to Improve Dialogue Safety",
		"URL": "http://arxiv.org/abs/2302.00871",
		"author": [
			{
				"family": "Meade",
				"given": "Nicholas"
			},
			{
				"family": "Gella",
				"given": "Spandana"
			},
			{
				"family": "Hazarika",
				"given": "Devamanyu"
			},
			{
				"family": "Gupta",
				"given": "Prakhar"
			},
			{
				"family": "Jin",
				"given": "Di"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Hakkani-Tür",
				"given": "Dilek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					22
				]
			]
		}
	},
	{
		"id": "jinCostDownScalingLanguage2023",
		"type": "article",
		"abstract": "How does scaling the number of parameters in large language models (LLMs) affect their core capabilities? We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference. By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling. Reducing the model size by more than 30\\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the various ways the model can process in-context information, ranging from retrieving answers from a long context to learning parameterized functions from in-context exemplars. The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning.",
		"DOI": "10.48550/arXiv.2310.04680",
		"note": "arXiv:2310.04680 [cs]",
		"number": "arXiv:2310.04680",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
		"title-short": "The Cost of Down-Scaling Language Models",
		"URL": "http://arxiv.org/abs/2310.04680",
		"author": [
			{
				"family": "Jin",
				"given": "Tian"
			},
			{
				"family": "Clement",
				"given": "Nolan"
			},
			{
				"family": "Dong",
				"given": "Xin"
			},
			{
				"family": "Nagarajan",
				"given": "Vaishnavh"
			},
			{
				"family": "Carbin",
				"given": "Michael"
			},
			{
				"family": "Ragan-Kelley",
				"given": "Jonathan"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					6
				]
			]
		}
	},
	{
		"id": "tanSparseUniversalTransformer2023",
		"type": "article",
		"abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in computation during inference with very little performance decrease on formal language tasks.",
		"DOI": "10.48550/arXiv.2310.07096",
		"note": "arXiv:2310.07096 [cs]",
		"number": "arXiv:2310.07096",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sparse Universal Transformer",
		"URL": "http://arxiv.org/abs/2310.07096",
		"author": [
			{
				"family": "Tan",
				"given": "Shawn"
			},
			{
				"family": "Shen",
				"given": "Yikang"
			},
			{
				"family": "Chen",
				"given": "Zhenfang"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Gan",
				"given": "Chuang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					10
				]
			]
		}
	},
	{
		"id": "wangGuidingLanguageModel2024",
		"type": "article",
		"abstract": "Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard fine-tuning baselines.",
		"DOI": "10.48550/arXiv.2310.05707",
		"note": "arXiv:2310.05707 [cs]",
		"number": "arXiv:2310.05707",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Guiding Language Model Math Reasoning with Planning Tokens",
		"URL": "http://arxiv.org/abs/2310.05707",
		"author": [
			{
				"family": "Wang",
				"given": "Xinyi"
			},
			{
				"family": "Caccia",
				"given": "Lucas"
			},
			{
				"family": "Ostapenko",
				"given": "Oleksiy"
			},
			{
				"family": "Yuan",
				"given": "Xingdi"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					5
				]
			]
		}
	},
	{
		"id": "kumarDebiasingCounterfactualsPresence2023",
		"type": "paper-conference",
		"abstract": "Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.",
		"container-title": "Clinical Image-Based Procedures,  Fairness of AI in Medical Imaging, and Ethical and Philosophical Issues in Medical Imaging",
		"DOI": "10.1007/978-3-031-45249-9_27",
		"event-place": "Cham",
		"ISBN": "978-3-031-45249-9",
		"language": "en",
		"page": "276-286",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "Springer Link",
		"title": "Debiasing Counterfactuals in the Presence of Spurious Correlations",
		"author": [
			{
				"family": "Kumar",
				"given": "Amar"
			},
			{
				"family": "Fathi",
				"given": "Nima"
			},
			{
				"family": "Mehta",
				"given": "Raghav"
			},
			{
				"family": "Nichyporuk",
				"given": "Brennan"
			},
			{
				"family": "Falet",
				"given": "Jean-Pierre R."
			},
			{
				"family": "Tsaftaris",
				"given": "Sotirios"
			},
			{
				"family": "Arbel",
				"given": "Tal"
			}
		],
		"editor": [
			{
				"family": "Wesarg",
				"given": "Stefan"
			},
			{
				"family": "Puyol Antón",
				"given": "Esther"
			},
			{
				"family": "Baxter",
				"given": "John S. H."
			},
			{
				"family": "Erdt",
				"given": "Marius"
			},
			{
				"family": "Drechsler",
				"given": "Klaus"
			},
			{
				"family": "Oyarzun Laura",
				"given": "Cristina"
			},
			{
				"family": "Freiman",
				"given": "Moti"
			},
			{
				"family": "Chen",
				"given": "Yufei"
			},
			{
				"family": "Rekik",
				"given": "Islem"
			},
			{
				"family": "Eagleson",
				"given": "Roy"
			},
			{
				"family": "Feragen",
				"given": "Aasa"
			},
			{
				"family": "King",
				"given": "Andrew P."
			},
			{
				"family": "Cheplygina",
				"given": "Veronika"
			},
			{
				"family": "Ganz-Benjaminsen",
				"given": "Melani"
			},
			{
				"family": "Ferrante",
				"given": "Enzo"
			},
			{
				"family": "Glocker",
				"given": "Ben"
			},
			{
				"family": "Moyer",
				"given": "Daniel"
			},
			{
				"family": "Petersen",
				"given": "Eikel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nikpourExplainableAttentionFewshot2023",
		"type": "article",
		"abstract": "Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.",
		"DOI": "10.48550/arXiv.2310.07800",
		"note": "arXiv:2310.07800 [cs]",
		"number": "arXiv:2310.07800",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Explainable Attention for Few-shot Learning and Beyond",
		"URL": "http://arxiv.org/abs/2310.07800",
		"author": [
			{
				"family": "Nikpour",
				"given": "Bahareh"
			},
			{
				"family": "Armanfard",
				"given": "Narges"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					11
				]
			]
		}
	},
	{
		"id": "openjaDetectionEvaluationBiasinducing2023",
		"type": "article",
		"abstract": "The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.",
		"DOI": "10.48550/arXiv.2310.12805",
		"note": "arXiv:2310.12805 [cs]",
		"number": "arXiv:2310.12805",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Detection and Evaluation of bias-inducing Features in Machine learning",
		"URL": "http://arxiv.org/abs/2310.12805",
		"author": [
			{
				"family": "Openja",
				"given": "Moses"
			},
			{
				"family": "Laberge",
				"given": "Gabriel"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					19
				]
			]
		}
	},
	{
		"id": "hugessenSurpriseAdaptiveIntrinsicMotivation2023",
		"type": "paper-conference",
		"abstract": "Both surprise-minimizing and surprise-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method can perform well across all entropy regimes. In an effort to find a single surprise-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective depending on the entropy conditions in its environment by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes.",
		"event-title": "Intrinsically-Motivated and Open-Ended Learning Workshop @NeurIPS2023",
		"language": "en",
		"source": "openreview.net",
		"title": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=E0LWTN1xPX",
		"author": [
			{
				"family": "Hugessen",
				"given": "Adriana"
			},
			{
				"family": "Castanyer",
				"given": "Roger Creus"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					30
				]
			]
		}
	},
	{
		"id": "colomboNovelInformationTheoreticObjective2023",
		"type": "article",
		"abstract": "One of the pursued objectives of deep learning is to provide tools that learn abstract representations of reality from the observation of multiple contextual situations. More precisely, one wishes to extract disentangled representations which are (i) low dimensional and (ii) whose components are independent and correspond to concepts capturing the essence of the objects under consideration (Locatello et al., 2019b). One step towards this ambitious project consists in learning disentangled representations with respect to a predefined (sensitive) attribute, e.g., the gender or age of the writer. Perhaps one of the main application for such disentangled representations is fair classification. Existing methods extract the last layer of a neural network trained with a loss that is composed of a cross-entropy objective and a disentanglement regularizer. In this work, we adopt an information-theoretic view of this problem which motivates a novel family of regularizers that minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. The resulting set of losses, called CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks. We demonstrate that our methods offer a better disentanglement/accuracy trade-off than previous techniques, and generalize better than training with cross-entropy loss solely provided that the disentanglement task is not too constraining.",
		"DOI": "10.48550/arXiv.2310.13990",
		"note": "arXiv:2310.13990 [cs]",
		"number": "arXiv:2310.13990",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification",
		"URL": "http://arxiv.org/abs/2310.13990",
		"author": [
			{
				"family": "Colombo",
				"given": "Pierre"
			},
			{
				"family": "Noiry",
				"given": "Nathan"
			},
			{
				"family": "Staerman",
				"given": "Guillaume"
			},
			{
				"family": "Piantanida",
				"given": "Pablo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					21
				]
			]
		}
	},
	{
		"id": "eastwoodSelfSupervisedDisentanglementLeveraging2023",
		"type": "paper-conference",
		"abstract": "Self-supervised representation learning often uses data augmentations to induce some invariance to \"style\" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed \"style\" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits our approach on synthetic datasets and then present promising but limited results on ImageNet.",
		"event-title": "Causal Representation Learning Workshop at NeurIPS 2023",
		"language": "en",
		"source": "openreview.net",
		"title": "Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations",
		"URL": "https://openreview.net/forum?id=JoISqbH8vl",
		"author": [
			{
				"family": "Eastwood",
				"given": "Cian"
			},
			{
				"family": "Kügelgen",
				"given": "Julius",
				"dropping-particle": "von"
			},
			{
				"family": "Ericsson",
				"given": "Linus"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			},
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Schölkopf",
				"given": "Bernhard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "liuAttentionSchemaNeural2023",
		"type": "article",
		"abstract": "Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these exploratory experiments suggest that equipping artificial agents with a model of attention can enhance their social intelligence.",
		"DOI": "10.48550/arXiv.2305.17375",
		"note": "arXiv:2305.17375 [cs]",
		"number": "arXiv:2305.17375",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Attention Schema in Neural Agents",
		"URL": "http://arxiv.org/abs/2305.17375",
		"author": [
			{
				"family": "Liu",
				"given": "Dianbo"
			},
			{
				"family": "Bolotta",
				"given": "Samuele"
			},
			{
				"family": "Zhu",
				"given": "He"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Dumas",
				"given": "Guillaume"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					13
				]
			]
		}
	},
	{
		"id": "knottGenerativeAIModels2023",
		"type": "article-journal",
		"abstract": "The new wave of ‘foundation models’—general-purpose generative AI models, for production of text (e.g., ChatGPT) or images (e.g., MidJourney)—represent a dramatic advance in the state of the art for AI. But their use also introduces a range of new risks, which has prompted an ongoing conversation about possible regulatory mechanisms. Here we propose a specific principle that should be incorporated into legislation: that any organization developing a foundation model intended for public use must demonstrate a reliable detection mechanism for the content it generates, as a condition of its public release. The detection mechanism should be made publicly available in a tool that allows users to query, for an arbitrary item of content, whether the item was generated (wholly or partly) by the model. In this paper, we argue that this requirement is technically feasible and would play an important role in reducing certain risks from new AI models in many domains. We also outline a number of options for the tool’s design, and summarize a number of points where further input from policymakers and researchers would be required.",
		"container-title": "Ethics and Information Technology",
		"DOI": "10.1007/s10676-023-09728-4",
		"ISSN": "1572-8439",
		"issue": "4",
		"journalAbbreviation": "Ethics Inf Technol",
		"language": "en",
		"page": "55",
		"source": "Springer Link",
		"title": "Generative AI models should include detection mechanisms as a condition for public release",
		"URL": "https://doi.org/10.1007/s10676-023-09728-4",
		"volume": "25",
		"author": [
			{
				"family": "Knott",
				"given": "Alistair"
			},
			{
				"family": "Pedreschi",
				"given": "Dino"
			},
			{
				"family": "Chatila",
				"given": "Raja"
			},
			{
				"family": "Chakraborti",
				"given": "Tapabrata"
			},
			{
				"family": "Leavy",
				"given": "Susan"
			},
			{
				"family": "Baeza-Yates",
				"given": "Ricardo"
			},
			{
				"family": "Eyers",
				"given": "David"
			},
			{
				"family": "Trotman",
				"given": "Andrew"
			},
			{
				"family": "Teal",
				"given": "Paul D."
			},
			{
				"family": "Biecek",
				"given": "Przemyslaw"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					28
				]
			]
		}
	},
	{
		"id": "mohammadiFETAFairnessEnforced2023",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making driven by neural networks has become very prominent in applications that directly affect people’s quality of life. This paper focuses on the problem of ensuring individual fairness in neural network models during verification, training, and prediction. A popular approach for enforcing fairness is to translate a fairness notion into constraints over the parameters of the model. However, such a translation does not always guarantee fair predictions of the trained neural network model. To address this challenge, we develop a counterexample-guided post-processing technique to provably enforce fairness constraints at prediction time. Contrary to prior work that enforces fairness only on points around test or train data, we are able to enforce and guarantee fairness on all points in the domain. Additionally, we propose a counterexample-guided loss as an in-processing technique to use fairness as an inductive bias by iteratively incorporating fairness counterexamples in the learning process. We have implemented these techniques in a tool called FETA. Empirical evaluation on real-world datasets indicates that FETA is not only able to guarantee fairness on-the-fly at prediction time but also is able to train accurate models exhibiting a much higher degree of individual fairness.",
		"collection-title": "EAAMO '23",
		"container-title": "Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
		"DOI": "10.1145/3617694.3623243",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0381-2",
		"page": "1–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "FETA: Fairness Enforced Verifying, Training, and Predicting Algorithms for Neural Networks",
		"title-short": "FETA",
		"URL": "https://doi.org/10.1145/3617694.3623243",
		"author": [
			{
				"family": "Mohammadi",
				"given": "Kiarash"
			},
			{
				"family": "Sivaraman",
				"given": "Aishwarya"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					30
				]
			]
		}
	},
	{
		"id": "carrollCharacterizingManipulationAI2023a",
		"type": "paper-conference",
		"abstract": "Manipulation is a concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our digital interactions, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring this kind of manipulation from AI systems. Firstly, we build upon prior literature on manipulation and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, covertness, and harm. We review proposals on how to operationalize each concept and we outline challenges in including each concept in a definition of manipulation. Second, we discuss the connections between manipulation and related concepts, such as deception and coercion. We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains. While some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted.",
		"collection-title": "EAAMO '23",
		"container-title": "Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
		"DOI": "10.1145/3617694.3623226",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0381-2",
		"page": "1–13",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Characterizing Manipulation from AI Systems",
		"URL": "https://doi.org/10.1145/3617694.3623226",
		"author": [
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Ashton",
				"given": "Henry"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					30
				]
			]
		}
	},
	{
		"id": "ehyaeiCausalFairMetric2024",
		"type": "article",
		"abstract": "Despite the essential need for comprehensive considerations in responsible AI, factors like robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data or structural causal models and were unable to reflect counterfactual proximity. To address this, our paper introduces a causal fair metric formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the application of our novel metric in classifiers. Empirical evaluation of real-world and synthetic datasets illustrates the effectiveness of our proposed metric in achieving an accurate classifier with fairness, resilience to adversarial perturbations, and a nuanced understanding of causal relationships.",
		"DOI": "10.48550/arXiv.2310.19391",
		"note": "arXiv:2310.19391 [cs]",
		"number": "arXiv:2310.19391",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness",
		"title-short": "Causal Fair Metric",
		"URL": "http://arxiv.org/abs/2310.19391",
		"author": [
			{
				"family": "Ehyaei",
				"given": "Ahmad-Reza"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			},
			{
				"family": "Samadi",
				"given": "Samira"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					6
				]
			]
		}
	},
	{
		"id": "dornGoalMisgeneralizationImplicit2023",
		"type": "paper-conference",
		"abstract": "While many examples of goal misspecification have been dissected in the reinforcement learning literature, few works have focused on the relatively new goal misgeneralization. As goal misgeneralization often stems from underspecification, we explore a simple environment with some goals specifiable through explicit conditioning, and others not. We find that agents generally pursue a mixture of possible goals, and the choice of goal to pursue is often inexplicable. Nonetheless, we attempt an explanation of implicit goal conditioning -- wherein subtle environment features determine which goal is pursued -- and aim to understand which features induce pursuit of one goal over another.",
		"event-title": "NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "Goal Misgeneralization as Implicit Goal Conditioning",
		"URL": "https://openreview.net/forum?id=QT4tXTqTTr",
		"author": [
			{
				"family": "Dorn",
				"given": "Diego"
			},
			{
				"family": "Alex",
				"given": "Neel"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					27
				]
			]
		}
	},
	{
		"id": "mutalovaPlayerGuidedAIOutperforms2023",
		"type": "paper-conference",
		"abstract": "Although Artificial Intelligence (AI) has gained widespread popularity across different fields, it is essential to recognize that AI systems, while impressive, do not consistently exhibit robust generalization, particularly for difficult problems such as the Multiple Sequence Alignment (MSA). In this study, we focus on bridging this performance gap by integrating human solutions into AI training. To illustrate these principles, we leverage data from Borderlands Science, a popular citizen science game in which small instances of the MSA problem are represented as puzzles. Our goal is to leverage the collective intelligence of human players to enhance the capabilities of AI agents. To achieve this, we have developed a Player-guided AI system that enables the AI model to learn from both standard training processes and the solutions provided by players. Our findings demonstrate that incorporating human-annotated information into the AI model improves its performance on puzzle tasks. Furthermore, the Player-guided AI model shows a decrease in noise compared to a pure AI model. This advancement allows for leveraging the model to align new sequences with improved accuracy and effectiveness. Moreover, this research brings attention to the potential of integrating AI and human expertise to address other challenges where the performance of AI models may be unsatisfactory.",
		"collection-title": "CI '23",
		"container-title": "Proceedings of The ACM Collective Intelligence Conference",
		"DOI": "10.1145/3582269.3615597",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0113-9",
		"page": "53–62",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Player-Guided AI outperforms standard AI in Sequence Alignment Puzzles",
		"URL": "https://doi.org/10.1145/3582269.3615597",
		"author": [
			{
				"family": "Mutalova",
				"given": "Renata"
			},
			{
				"family": "Sarrazin-Gendron",
				"given": "Roman"
			},
			{
				"family": "Ghasemloo Gheidari",
				"given": "Parham"
			},
			{
				"family": "Cai",
				"given": "Eddie"
			},
			{
				"family": "Richard",
				"given": "Gabriel"
			},
			{
				"family": "Caisse",
				"given": "Sebastien"
			},
			{
				"family": "Knight",
				"given": "Rob"
			},
			{
				"family": "Blanchette",
				"given": "Mathieu"
			},
			{
				"family": "Szantner",
				"given": "Attila"
			},
			{
				"family": "Waldispühl",
				"given": "Jérôme"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					5
				]
			]
		}
	},
	{
		"id": "assouelUnsolvedChallengesLLMs2023",
		"type": "paper-conference",
		"abstract": "In this work, we investigate the challenges associated with developing goal-driven AI agents capable of performing novel tasks in a web environment using zero-shot learning. Our primary focus is on harnessing the capabilities of large language models (LLMs) as generalist web agents interacting with HTML-based user interfaces (UIs). We evaluate the MiniWoB benchmark and show that it is a suitable yet challenging platform for assessing an agent's ability to comprehend and solve tasks without prior human demonstrations. Our main contribution encompasses a set of extensive experiments where we compare and contrast various agent design considerations, such as action space, observation space, and the choice of LLM, with the aim of shedding light on the bottlenecks and limitations of LLM-based zero-shot learning in this domain, in order to foster research endeavours in this area. In our empirical analysis, we find that: (1) the effectiveness of the different action spaces are notably dependent on the specific LLM used; (2) open-source LLMs hold their own as competitive generalist web agents when compared to their proprietary counterparts; and (3) using an accessibility-based representation for web pages, despite resulting in some performance loss, emerges as a cost-effective strategy, particularly as web page sizes increase.",
		"event-title": "NeurIPS 2023 Foundation Models for Decision Making Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "The Unsolved Challenges of LLMs as Generalist Web Agents: A Case Study",
		"title-short": "The Unsolved Challenges of LLMs as Generalist Web Agents",
		"URL": "https://openreview.net/forum?id=jt3il4fC5B",
		"author": [
			{
				"family": "Assouel",
				"given": "Rim"
			},
			{
				"family": "Marty",
				"given": "Tom"
			},
			{
				"family": "Caccia",
				"given": "Massimo"
			},
			{
				"family": "Laradji",
				"given": "Issam H."
			},
			{
				"family": "Drouin",
				"given": "Alexandre"
			},
			{
				"family": "Rajeswar",
				"given": "Sai"
			},
			{
				"family": "Palacios",
				"given": "Hector"
			},
			{
				"family": "Cappart",
				"given": "Quentin"
			},
			{
				"family": "Vazquez",
				"given": "David"
			},
			{
				"family": "Chapados",
				"given": "Nicolas"
			},
			{
				"family": "Gasse",
				"given": "Maxime"
			},
			{
				"family": "Lacoste",
				"given": "Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					8
				]
			]
		}
	},
	{
		"id": "laradjiCaptureFlagUncovering2023",
		"type": "article",
		"abstract": "The extraction of a small number of relevant insights from vast amounts of data is a crucial component of data-driven decision-making. However, accomplishing this task requires considerable technical skills, domain expertise, and human labor. This study explores the potential of using Large Language Models (LLMs) to automate the discovery of insights in data, leveraging recent advances in reasoning and code generation techniques. We propose a new evaluation methodology based on a \"capture the flag\" principle, measuring the ability of such models to recognize meaningful and pertinent information (flags) in a dataset. We further propose two proof-of-concept agents, with different inner workings, and compare their ability to capture such flags in a real-world sales dataset. While the work reported here is preliminary, our results are sufficiently interesting to mandate future exploration by the community.",
		"DOI": "10.48550/arXiv.2312.13876",
		"note": "arXiv:2312.13876 [cs, stat]",
		"number": "arXiv:2312.13876",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Capture the Flag: Uncovering Data Insights with Large Language Models",
		"title-short": "Capture the Flag",
		"URL": "http://arxiv.org/abs/2312.13876",
		"author": [
			{
				"family": "Laradji",
				"given": "Issam"
			},
			{
				"family": "Taslakian",
				"given": "Perouz"
			},
			{
				"family": "Rajeswar",
				"given": "Sai"
			},
			{
				"family": "Zantedeschi",
				"given": "Valentina"
			},
			{
				"family": "Lacoste",
				"given": "Alexandre"
			},
			{
				"family": "Chapados",
				"given": "Nicolas"
			},
			{
				"family": "Vazquez",
				"given": "David"
			},
			{
				"family": "Pal",
				"given": "Christopher"
			},
			{
				"family": "Drouin",
				"given": "Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					21
				]
			]
		}
	},
	{
		"id": "sujitBridgingGapOffline2023",
		"type": "article",
		"abstract": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. A crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. This can be infeasible in situations where such interactions are expensive; such as in robotics. Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. While online RL algorithms are typically evaluated as a function of the number of environment interactions, there exists no single established protocol for evaluating offline RL methods.In this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency. Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. Our approach is generally applicable and easy to implement. We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets.",
		"DOI": "10.48550/arXiv.2212.08131",
		"note": "arXiv:2212.08131 [cs]",
		"number": "arXiv:2212.08131",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
		"URL": "http://arxiv.org/abs/2212.08131",
		"author": [
			{
				"family": "Sujit",
				"given": "Shivakanth"
			},
			{
				"family": "Braga",
				"given": "Pedro H. M."
			},
			{
				"family": "Bornschein",
				"given": "Jorg"
			},
			{
				"family": "Kahou",
				"given": "Samira Ebrahimi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					21
				]
			]
		}
	},
	{
		"id": "sudhakarLanguageModelInTheLoopData2023",
		"type": "article",
		"abstract": "Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM, a popular approach, leverages linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.",
		"DOI": "10.48550/arXiv.2311.07687",
		"note": "arXiv:2311.07687 [cs]",
		"number": "arXiv:2311.07687",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
		"title-short": "Language Model-In-The-Loop",
		"URL": "http://arxiv.org/abs/2311.07687",
		"author": [
			{
				"family": "Sudhakar",
				"given": "Arjun Vaithilingam"
			},
			{
				"family": "Parthasarathi",
				"given": "Prasanna"
			},
			{
				"family": "Rajendran",
				"given": "Janarthanan"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					13
				]
			]
		}
	},
	{
		"id": "changGeneralizableImitationLearning2023",
		"type": "article",
		"abstract": "In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.",
		"DOI": "10.48550/arXiv.2311.09350",
		"note": "arXiv:2311.09350 [cs]",
		"number": "arXiv:2311.09350",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generalizable Imitation Learning Through Pre-Trained Representations",
		"URL": "http://arxiv.org/abs/2311.09350",
		"author": [
			{
				"family": "Chang",
				"given": "Wei-Di"
			},
			{
				"family": "Hogan",
				"given": "Francois"
			},
			{
				"family": "Meger",
				"given": "David"
			},
			{
				"family": "Dudek",
				"given": "Gregory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					15
				]
			]
		}
	},
	{
		"id": "cacciaTaskAgnosticContinualReinforcement2023",
		"type": "paper-conference",
		"abstract": "Continual learning (CL) enables the development of models and agents that learn from a sequence of tasks while addressing the limitations of standard deep learning approaches, such as catastrophic forgetting. In this work, we investigate the factors that contribute to the performance differences between task-agnostic CL and multi-task (MTL) agents. We pose two hypotheses: (1) task-agnostic methods might provide advantages in settings with limited data, computation, or high dimensionality, and (2) faster adaptation may be particularly beneficial in continual learning settings, helping to mitigate the effects of catastrophic forgetting. To investigate these hypotheses, we introduce a replay-based recurrent reinforcement learning (3RL) methodology for task-agnostic CL agents. We assess 3RL on a synthetic task and the Meta-World benchmark, which includes 50 unique manipulation tasks. Our results demonstrate that 3RL outperforms baseline methods and can even surpass its multi-task equivalent in challenging settings with high dimensionality. We also show that the recurrent task-agnostic agent consistently outperforms or matches the performance of its transformer-based counterpart.  These findings provide insights into the advantages of task-agnostic CL over task-aware MTL approaches and highlight the potential of task-agnostic methods in resource-constrained, high-dimensional, and multi-task environments.",
		"container-title": "Proceedings of The 2nd Conference on Lifelong Learning Agents",
		"event-title": "Conference on Lifelong Learning Agents",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "89-119",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges",
		"title-short": "Task-Agnostic Continual Reinforcement Learning",
		"URL": "https://proceedings.mlr.press/v232/caccia23a.html",
		"author": [
			{
				"family": "Caccia",
				"given": "Massimo"
			},
			{
				"family": "Mueller",
				"given": "Jonas"
			},
			{
				"family": "Kim",
				"given": "Taesup"
			},
			{
				"family": "Charlin",
				"given": "Laurent"
			},
			{
				"family": "Fakoor",
				"given": "Rasool"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "olteanuResponsibleAIResearch2023",
		"type": "article",
		"abstract": "All types of research, development, and policy work can have unintended, adverse consequences - work in responsible artificial intelligence (RAI), ethical AI, or ethics in AI is no exception. The work of the responsible AI community has illustrated how the design, deployment, and use of computational systems—including machine learning (ML), artificial intelligence (AI), and natural language processing (NLP) systems—engender a range of adverse impacts. As a result, in recent years, the authors of ML, AI, and NLP research papers have been required to include reflections on possible unintended consequences and negative social impacts as dedicated sections or extensive checklists. ven though such requirement traces its roots to work done within the responsible AI community responsible AI conferences and publication venues, such as FAccT, AIES FORC, or EAAMO, have yet to explicitly enforce similar requirements. Surprisingly, many papers on responsible AI, ethical AI, ethics in AI, or related topics5 do not include similar reflections on possible adverse impacts. RAI research and work is often taken to be inherently beneficial with little to no potential for harm and can thus paradoxically fail to consider any possible adverse consequences it may give rise to (Boyarskaya et al., 2020). This is also the case for many RAI artifacts which were found to, e.g., “not contend with the organizational, labor, and political implications of AI ethics work in practice”. This trend of failing to reflect on the possible negative impact of our own work should concern all of us, as the research we conduct and the artifacts we build are more often than not value-laden, and thus encode all kinds of implicit practices, assumptions, norms, and values. Similarly to our colleagues from other research communities, we—RAI researchers and practitioners—can and often do also suffer from similar “failures of magination” when it comes to the impact of our own work, and we need to at least keep ourselves to the same standard that we expect other communities to adhere to. We believe responsible AI research needs impact statements, too.",
		"DOI": "10.48550/arXiv.2311.11776",
		"note": "arXiv:2311.11776 [cs]",
		"number": "arXiv:2311.11776",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Responsible AI Research Needs Impact Statements Too",
		"URL": "http://arxiv.org/abs/2311.11776",
		"author": [
			{
				"family": "Olteanu",
				"given": "Alexandra"
			},
			{
				"family": "Ekstrand",
				"given": "Michael"
			},
			{
				"family": "Castillo",
				"given": "Carlos"
			},
			{
				"family": "Suh",
				"given": "Jina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "casperOpenProblemsFundamental2023",
		"type": "article",
		"abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",
		"DOI": "10.48550/arXiv.2307.15217",
		"note": "arXiv:2307.15217 [cs]",
		"number": "arXiv:2307.15217",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
		"URL": "http://arxiv.org/abs/2307.15217",
		"author": [
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Davies",
				"given": "Xander"
			},
			{
				"family": "Shi",
				"given": "Claudia"
			},
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Rando",
				"given": "Javier"
			},
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Freire",
				"given": "Pedro"
			},
			{
				"family": "Wang",
				"given": "Tony"
			},
			{
				"family": "Marks",
				"given": "Samuel"
			},
			{
				"family": "Segerie",
				"given": "Charbel-Raphaël"
			},
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Christoffersen",
				"given": "Phillip"
			},
			{
				"family": "Damani",
				"given": "Mehul"
			},
			{
				"family": "Slocum",
				"given": "Stewart"
			},
			{
				"family": "Anwar",
				"given": "Usman"
			},
			{
				"family": "Siththaranjan",
				"given": "Anand"
			},
			{
				"family": "Nadeau",
				"given": "Max"
			},
			{
				"family": "Michaud",
				"given": "Eric J."
			},
			{
				"family": "Pfau",
				"given": "Jacob"
			},
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Chen",
				"given": "Xin"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Hase",
				"given": "Peter"
			},
			{
				"family": "Bıyık",
				"given": "Erdem"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					11
				]
			]
		}
	},
	{
		"id": "graneseOptimalZeroShotDetector2024",
		"type": "article",
		"abstract": "This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available \"off the shelf\". To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario for empirical evaluation, where the attacker possesses a pre-trained classifier and launches well-known adversarial attacks against it. Our experiments highlight the effectiveness of our proposed solution, even in scenarios that deviate from the optimal setup.",
		"DOI": "10.48550/arXiv.2402.15808",
		"note": "arXiv:2402.15808 [cs]",
		"number": "arXiv:2402.15808",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Optimal Zero-Shot Detector for Multi-Armed Attacks",
		"URL": "http://arxiv.org/abs/2402.15808",
		"author": [
			{
				"family": "Granese",
				"given": "Federica"
			},
			{
				"family": "Romanelli",
				"given": "Marco"
			},
			{
				"family": "Piantanida",
				"given": "Pablo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					27
				]
			]
		}
	},
	{
		"id": "bairamianMinimaxExploiterData2023",
		"type": "article",
		"abstract": "Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL). One core component of these methods relies on creating a pool of learning agents -- consisting of the Main Agent, past versions of this agent, and Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main Agents. A key drawback of these approaches is the large computational cost and physical time that is required to train the system, making them impractical to deploy in highly iterative real-life settings such as video game productions. In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency. We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game. The Minimax Exploiter consistently outperforms strong baselines, demonstrating improved stability and data efficiency, leading to a robust CSP-MARL method that is both flexible and easy to deploy.",
		"DOI": "10.48550/arXiv.2311.17190",
		"note": "arXiv:2311.17190 [cs]",
		"number": "arXiv:2311.17190",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play",
		"title-short": "Minimax Exploiter",
		"URL": "http://arxiv.org/abs/2311.17190",
		"author": [
			{
				"family": "Bairamian",
				"given": "Daniel"
			},
			{
				"family": "Marcotte",
				"given": "Philippe"
			},
			{
				"family": "Romoff",
				"given": "Joshua"
			},
			{
				"family": "Robert",
				"given": "Gabriel"
			},
			{
				"family": "Nowrouzezahrai",
				"given": "Derek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					28
				]
			]
		}
	},
	{
		"id": "scimecaMitigatingBiasesDiverse2024",
		"type": "article",
		"abstract": "Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification performance on par with prior work that relies on auxiliary data collection.",
		"DOI": "10.48550/arXiv.2311.16176",
		"note": "arXiv:2311.16176 [cs]",
		"number": "arXiv:2311.16176",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Mitigating Biases with Diverse Ensembles and Diffusion Models",
		"URL": "http://arxiv.org/abs/2311.16176",
		"author": [
			{
				"family": "Scimeca",
				"given": "Luca"
			},
			{
				"family": "Rubinstein",
				"given": "Alexander"
			},
			{
				"family": "Teney",
				"given": "Damien"
			},
			{
				"family": "Oh",
				"given": "Seong Joon"
			},
			{
				"family": "Nicolicioiu",
				"given": "Armand Mihai"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					6
				]
			]
		}
	},
	{
		"id": "pasquatoSearchLostAttractor2023",
		"type": "article",
		"abstract": "N-body systems characterized by inverse square attractive forces may display a self similar collapse known as the gravo-thermal catastrophe. In star clusters, collapse is halted by binary stars, and a large fraction of Milky Way clusters may have already reached this phase. It has been speculated -- with guidance from simulations -- that macroscopic variables such as central density and velocity dispersion are governed post-collapse by an effective, low-dimensional system of ODEs. It is still hard to distinguish chaotic, low dimensional motion, from high dimensional stochastic noise. Here we apply three machine learning tools to state-of-the-art dynamical simulations to constrain the post collapse dynamics: topological data analysis (TDA) on a lag embedding of the relevant time series, Sparse Identification of Nonlinear Dynamics (SINDY), and Tests of Accuracy with Random Points (TARP).",
		"DOI": "10.48550/arXiv.2311.16306",
		"note": "arXiv:2311.16306 [astro-ph, physics:nlin]",
		"number": "arXiv:2311.16306",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The search for the lost attractor",
		"URL": "http://arxiv.org/abs/2311.16306",
		"author": [
			{
				"family": "Pasquato",
				"given": "Mario"
			},
			{
				"family": "Haddad",
				"given": "Syphax"
			},
			{
				"family": "Di Cintio",
				"given": "Pierfrancesco"
			},
			{
				"family": "Adam",
				"given": "Alexandre"
			},
			{
				"family": "Lemos",
				"given": "Pablo"
			},
			{
				"family": "Dia",
				"given": "Noé"
			},
			{
				"family": "Petrache",
				"given": "Mircea"
			},
			{
				"family": "Di Carlo",
				"given": "Ugo Niccolò"
			},
			{
				"family": "Trani",
				"given": "Alessandro Alberto"
			},
			{
				"family": "Perreault-Levasseur",
				"given": "Laurence"
			},
			{
				"family": "Hezaveh",
				"given": "Yashar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					27
				]
			]
		}
	},
	{
		"id": "gengScalarInvariantNetworks2023",
		"type": "article",
		"abstract": "Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are thought to enhance the representational power of neural networks, enabling them to solve a variety of tasks in computer vision. However, we argue that biases can be disregarded for some image-related tasks such as image classification, by considering the intrinsic distribution of images in the input space and desired model properties from first principles. Our findings suggest that zero-bias neural networks can perform comparably to biased networks for practical image classification tasks. We demonstrate that zero-bias neural networks possess a valuable property called scalar (multiplication) invariance. This means that the prediction of the network remains unchanged when the contrast of the input image is altered. We extend scalar invariance to more general cases, enabling formal verification of certain convex regions of the input space. Additionally, we prove that zero-bias neural networks are fair in predicting the zero image. Unlike state-of-the-art models that may exhibit bias toward certain labels, zero-bias networks have uniform belief in all labels. We believe dropping bias terms can be considered as a geometric prior in designing neural network architecture for image classification, which shares the spirit of adapting convolutions as the transnational invariance prior. The robustness and fairness advantages of zero-bias neural networks may also indicate a promising path towards trustworthy and ethical AI.",
		"DOI": "10.48550/arXiv.2211.08486",
		"note": "arXiv:2211.08486 [cs]",
		"number": "arXiv:2211.08486",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scalar Invariant Networks with Zero Bias",
		"URL": "http://arxiv.org/abs/2211.08486",
		"author": [
			{
				"family": "Geng",
				"given": "Chuqin"
			},
			{
				"family": "Xu",
				"given": "Xiaojie"
			},
			{
				"family": "Ye",
				"given": "Haolin"
			},
			{
				"family": "Si",
				"given": "Xujie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					29
				]
			]
		}
	},
	{
		"id": "tambonSilentBugsDeep2023",
		"type": "article-journal",
		"abstract": "Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their integration into various applications even among non-DL experts. However, like any other programs, they are prone to bugs. This paper deals with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the “black-box” and stochastic nature of the DL systems (i.e., the end user can not understand how the model makes decisions). This paper presents the first empirical study of the silent bugs in Tensorflow, specifically its high-level API Keras, and their impact on users’ programs. We extracted closed issues related to Keras API from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users’ programs. We categorized the bugs based on the effects on the users’ programs and the components where the issues occurred, using information from the issue reports. We then derived a threat level for each of the issues, based on the impact they had on the users’ programs. To assess the relevance of identified categories and the impact scale, we conducted an online survey with 103 DL developers. The participants generally agreed with the significant impact of silent bugs in DL frameworks and how they impact users and acknowledged our findings (i.e., categories of silent bugs and the proposed impact scale).",
		"container-title": "Empirical Software Engineering",
		"DOI": "10.1007/s10664-023-10389-6",
		"ISSN": "1573-7616",
		"issue": "1",
		"journalAbbreviation": "Empir Software Eng",
		"language": "en",
		"page": "10",
		"source": "Springer Link",
		"title": "Silent bugs in deep learning frameworks: an empirical study of Keras and TensorFlow",
		"title-short": "Silent bugs in deep learning frameworks",
		"URL": "https://doi.org/10.1007/s10664-023-10389-6",
		"volume": "29",
		"author": [
			{
				"family": "Tambon",
				"given": "Florian"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "An",
				"given": "Le"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Antoniol",
				"given": "Giuliano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		}
	},
	{
		"id": "colomboStrongerTextualAttack2023",
		"type": "paper-conference",
		"abstract": "The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding deep NLP systems integrity. However, the crucial problem of defending against malicious attacks has only drawn few attention in the NLP community. The latter is nonetheless instrumental to develop robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, an extended benchmark composed of nine popular attack methods, three datasets and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter free and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factor of detection rate variations.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2023",
		"DOI": "10.18653/v1/2023.findings-emnlp.35",
		"event-place": "Singapore",
		"event-title": "Findings 2023",
		"page": "484–505",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "Toward Stronger Textual Attack Detectors",
		"URL": "https://aclanthology.org/2023.findings-emnlp.35",
		"author": [
			{
				"family": "Colombo",
				"given": "Pierre"
			},
			{
				"family": "Picot",
				"given": "Marine"
			},
			{
				"family": "Noiry",
				"given": "Nathan"
			},
			{
				"family": "Staerman",
				"given": "Guillaume"
			},
			{
				"family": "Piantanida",
				"given": "Pablo"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "morinSpectralTemporalContrastive2023",
		"type": "article",
		"abstract": "Learning useful data representations without requiring labels is a cornerstone of modern deep learning. Self-supervised learning methods, particularly contrastive learning (CL), have proven successful by leveraging data augmentations to define positive pairs. This success has prompted a number of theoretical studies to better understand CL and investigate theoretical bounds for downstream linear probing tasks. This work is concerned with the temporal contrastive learning (TCL) setting where the sequential structure of the data is used instead to define positive pairs, which is more commonly used in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a population loss based on a state graph derived from a time-homogeneous reversible Markov chain with uniform stationary distribution. The STCL loss enables to connect the linear probing performance to the spectral properties of the graph, and can be estimated by considering previously observed data sequences as an ensemble of MCMC chains.",
		"DOI": "10.48550/arXiv.2312.00966",
		"note": "arXiv:2312.00966 [cs]",
		"number": "arXiv:2312.00966",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Spectral Temporal Contrastive Learning",
		"URL": "http://arxiv.org/abs/2312.00966",
		"author": [
			{
				"family": "Morin",
				"given": "Sacha"
			},
			{
				"family": "Nath",
				"given": "Somjit"
			},
			{
				"family": "Kahou",
				"given": "Samira Ebrahimi"
			},
			{
				"family": "Wolf",
				"given": "Guy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					7
				]
			]
		}
	},
	{
		"id": "rezanejadShapeBasedMeasuresImprove2024",
		"type": "article-journal",
		"abstract": "Converging evidence indicates that deep neural network models that are trained on large datasets are biased toward color and texture information. Humans, on the other hand, can easily recognize objects and scenes from images as well as from bounding contours. Mid-level vision is characterized by the recombination and organization of simple primary features into more complex ones by a set of so-called Gestalt grouping rules. While described qualitatively in the human literature, a computational implementation of these perceptual grouping rules is so far missing. In this article, we contribute a novel set of algorithms for the detection of contour-based cues in complex scenes. We use the medial axis transform (MAT) to locally score contours according to these grouping rules. We demonstrate the benefit of these cues for scene categorization in two ways: (i) Both human observers and CNN models categorize scenes most accurately when perceptual grouping information is emphasized. (ii) Weighting the contours with these measures boosts performance of a CNN model significantly compared to the use of unweighted contours. Our work suggests that, even though these measures are computed directly from contours in the image, current CNN models do not appear to extract or utilize these grouping cues.",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"DOI": "10.1109/TPAMI.2023.3333352",
		"ISSN": "1939-3539",
		"issue": "4",
		"note": "event-title: IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"page": "2041-2053",
		"source": "IEEE Xplore",
		"title": "Shape-Based Measures Improve Scene Categorization",
		"URL": "https://ieeexplore.ieee.org/document/10337773",
		"volume": "46",
		"author": [
			{
				"family": "Rezanejad",
				"given": "Morteza"
			},
			{
				"family": "Wilder",
				"given": "John"
			},
			{
				"family": "Walther",
				"given": "Dirk B."
			},
			{
				"family": "Jepson",
				"given": "Allan D."
			},
			{
				"family": "Dickinson",
				"given": "Sven"
			},
			{
				"family": "Siddiqi",
				"given": "Kaleem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4
				]
			]
		}
	},
	{
		"id": "darrinRainProofUmbrellaShield2023",
		"type": "paper-conference",
		"abstract": "Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF provides OOD detection methods more aligned with task-specific performance metrics than traditional OOD detectors.",
		"container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2023.emnlp-main.357",
		"event-place": "Singapore",
		"event-title": "EMNLP 2023",
		"page": "5831–5857",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data",
		"title-short": "RainProof",
		"URL": "https://aclanthology.org/2023.emnlp-main.357",
		"author": [
			{
				"family": "Darrin",
				"given": "Maxime"
			},
			{
				"family": "Piantanida",
				"given": "Pablo"
			},
			{
				"family": "Colombo",
				"given": "Pierre"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "munosNashLearningHuman2024",
		"type": "article",
		"abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.",
		"DOI": "10.48550/arXiv.2312.00886",
		"note": "arXiv:2312.00886 [cs, stat]",
		"number": "arXiv:2312.00886",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Nash Learning from Human Feedback",
		"URL": "http://arxiv.org/abs/2312.00886",
		"author": [
			{
				"family": "Munos",
				"given": "Rémi"
			},
			{
				"family": "Valko",
				"given": "Michal"
			},
			{
				"family": "Calandriello",
				"given": "Daniele"
			},
			{
				"family": "Azar",
				"given": "Mohammad Gheshlaghi"
			},
			{
				"family": "Rowland",
				"given": "Mark"
			},
			{
				"family": "Guo",
				"given": "Zhaohan Daniel"
			},
			{
				"family": "Tang",
				"given": "Yunhao"
			},
			{
				"family": "Geist",
				"given": "Matthieu"
			},
			{
				"family": "Mesnard",
				"given": "Thomas"
			},
			{
				"family": "Michi",
				"given": "Andrea"
			},
			{
				"family": "Selvi",
				"given": "Marco"
			},
			{
				"family": "Girgin",
				"given": "Sertan"
			},
			{
				"family": "Momchev",
				"given": "Nikola"
			},
			{
				"family": "Bachem",
				"given": "Olivier"
			},
			{
				"family": "Mankowitz",
				"given": "Daniel J."
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Piot",
				"given": "Bilal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					11
				]
			]
		}
	},
	{
		"id": "morovatiBugCharacterizationMachine2023",
		"type": "article-journal",
		"abstract": "The rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Since corrective maintenance, i.e. identifying and resolving systems bugs, is a key task in the software development process to deliver reliable software components, it is necessary to investigate the usage of ML components, from the software maintenance perspective. Understanding the bugs’ characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, and symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs is more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying significant attention to the reliability of the ML components is crucial in ML-based systems. These results deepen the understanding of ML bugs and we hope that our findings help shed light on opportunities for designing effective tools for testing and debugging ML-based systems.",
		"container-title": "Empirical Software Engineering",
		"DOI": "10.1007/s10664-023-10400-0",
		"ISSN": "1573-7616",
		"issue": "1",
		"journalAbbreviation": "Empir Software Eng",
		"language": "en",
		"page": "14",
		"source": "Springer Link",
		"title": "Bug characterization in machine learning-based systems",
		"URL": "https://doi.org/10.1007/s10664-023-10400-0",
		"volume": "29",
		"author": [
			{
				"family": "Morovati",
				"given": "Mohammad Mehdi"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Tambon",
				"given": "Florian"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Jiang",
				"given": "Zhen Ming (Jack)"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					5
				]
			]
		}
	},
	{
		"id": "liAnomalyDetectionScalable2023",
		"type": "article",
		"abstract": "The use of learning-based methods for optimizing cellular radio access networks (RAN) has received increasing attention in recent years. This coincides with a rapid increase in the number of cell sites worldwide, driven largely by dramatic growth in cellular network traffic. Training and maintaining learned models that work well across a large number of cell sites has thus become a pertinent problem. This paper proposes a scalable framework for constructing a reinforcement learning policy bank that can perform RAN optimization across a large number of cell sites with varying traffic patterns. Central to our framework is a novel application of anomaly detection techniques to assess the compatibility between sites (tasks) and the policy bank. This allows our framework to intelligently identify when a policy can be reused for a task, and when a new policy needs to be trained and added to the policy bank. Our results show that our approach to compatibility assessment leads to an efficient use of computational resources, by allowing us to construct a performant policy bank without exhaustively training on all tasks, which makes it applicable under real-world constraints.",
		"DOI": "10.48550/arXiv.2312.03277",
		"note": "arXiv:2312.03277 [cs]",
		"number": "arXiv:2312.03277",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization",
		"URL": "http://arxiv.org/abs/2312.03277",
		"author": [
			{
				"family": "Li",
				"given": "Jimmy"
			},
			{
				"family": "Kozlov",
				"given": "Igor"
			},
			{
				"family": "Wu",
				"given": "Di"
			},
			{
				"family": "Liu",
				"given": "Xue"
			},
			{
				"family": "Dudek",
				"given": "Gregory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					5
				]
			]
		}
	},
	{
		"id": "agrawalAddressingSampleInefficiency2023",
		"type": "article",
		"abstract": "Non-contrastive self-supervised learning (NC-SSL) methods like BarlowTwins and VICReg have shown great promise for label-free representation learning in computer vision. Despite the apparent simplicity of these techniques, researchers must rely on several empirical heuristics to achieve competitive performance, most notably using high-dimensional projector heads and two augmentations of the same image. In this work, we provide theoretical insights on the implicit bias of the BarlowTwins and VICReg loss that can explain these heuristics and guide the development of more principled recommendations. Our first insight is that the orthogonality of the features is more critical than projector dimensionality for learning good representations. Based on this, we empirically demonstrate that low-dimensional projector heads are sufficient with appropriate regularization, contrary to the existing heuristic. Our second theoretical insight suggests that using multiple data augmentations better represents the desiderata of the SSL objective. Based on this, we demonstrate that leveraging more augmentations per sample improves representation quality and trainability. In particular, it improves optimization convergence, leading to better features emerging earlier in the training. Remarkably, we demonstrate that we can reduce the pretraining dataset size by up to 4x while maintaining accuracy and improving convergence simply by using more data augmentations. Combining these insights, we present practical pretraining recommendations that improve wall-clock time by 2x and improve performance on CIFAR-10/STL-10 datasets using a ResNet-50 backbone. Thus, this work provides a theoretical insight into NC-SSL and produces practical recommendations for enhancing its sample and compute efficiency.",
		"DOI": "10.48550/arXiv.2312.10725",
		"note": "arXiv:2312.10725 [cs]",
		"number": "arXiv:2312.10725",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Addressing Sample Inefficiency in Multi-View Representation Learning",
		"URL": "http://arxiv.org/abs/2312.10725",
		"author": [
			{
				"family": "Agrawal",
				"given": "Kumar Krishna"
			},
			{
				"family": "Ghosh",
				"given": "Arna"
			},
			{
				"family": "Oberman",
				"given": "Adam"
			},
			{
				"family": "Richards",
				"given": "Blake"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					17
				]
			]
		}
	},
	{
		"id": "nobandeganiMachinesThatTrust2023",
		"type": "article",
		"abstract": "Widely considered a cornerstone of human morality, trust shapes many aspects of human social interactions. In this work, we present a theoretical analysis of the $\\textit{trust game}$, the canonical task for studying trust in behavioral and brain sciences, along with simulation results supporting our analysis. Specifically, leveraging reinforcement learning (RL) to train our AI agents, we systematically investigate learning trust under various parameterizations of this task. Our theoretical analysis, corroborated by the simulations results presented, provides a mathematical basis for the emergence of trust in the trust game.",
		"DOI": "10.48550/arXiv.2312.12868",
		"note": "arXiv:2312.12868 [cs, q-bio]",
		"number": "arXiv:2312.12868",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Machines that Trust: AI Agents Learn to Trust in the Trust Game",
		"title-short": "Towards Machines that Trust",
		"URL": "http://arxiv.org/abs/2312.12868",
		"author": [
			{
				"family": "Nobandegani",
				"given": "Ardavan S."
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Shultz",
				"given": "Thomas R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					20
				]
			]
		}
	},
	{
		"id": "mindomHarnessingPretrainedGeneralist2023",
		"type": "article",
		"abstract": "Nowadays, we are witnessing an increasing adoption of Artificial Intelligence (AI) to develop techniques aimed at improving the reliability, effectiveness, and overall quality of software systems. Deep reinforcement learning (DRL) has recently been successfully used for automation in complex tasks such as game testing and solving the job-shop scheduling problem. However, these specialized DRL agents, trained from scratch on specific tasks, suffer from a lack of generalizability to other tasks and they need substantial time to be developed and re-trained effectively. Recently, DRL researchers have begun to develop generalist agents, able to learn a policy from various environments and capable of achieving performances similar to or better than specialist agents in new tasks. In the Natural Language Processing or Computer Vision domain, these generalist agents are showing promising adaptation capabilities to never-before-seen tasks after a light fine-tuning phase and achieving high performance. This paper investigates the potential of generalist agents for solving SE tasks. Specifically, we conduct an empirical study aimed at assessing the performance of two generalist agents on two important SE tasks: the detection of bugs in games (for two games) and the minimization of makespan in a scheduling task, to solve the job-shop scheduling problem (for two instances). Our results show that the generalist agents outperform the specialist agents with very little effort for fine-tuning, achieving a 20% reduction of the makespan over specialized agent performance on task-based scheduling. In the context of game testing, some generalist agent configurations detect 85% more bugs than the specialist agents. Building on our analysis, we provide recommendations for researchers and practitioners looking to select generalist agents for SE tasks, to ensure that they perform effectively.",
		"DOI": "10.48550/arXiv.2312.15536",
		"note": "arXiv:2312.15536 [cs]",
		"number": "arXiv:2312.15536",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Harnessing Pre-trained Generalist Agents for Software Engineering Tasks",
		"URL": "http://arxiv.org/abs/2312.15536",
		"author": [
			{
				"family": "Mindom",
				"given": "Paulina Stevia Nouwou"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					24
				]
			]
		}
	},
	{
		"id": "zayedFairnessAwareStructuredPruning2023",
		"type": "article",
		"abstract": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance.",
		"DOI": "10.48550/arXiv.2312.15398",
		"note": "arXiv:2312.15398 [cs]",
		"number": "arXiv:2312.15398",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fairness-Aware Structured Pruning in Transformers",
		"URL": "http://arxiv.org/abs/2312.15398",
		"author": [
			{
				"family": "Zayed",
				"given": "Abdelrahman"
			},
			{
				"family": "Mordido",
				"given": "Goncalo"
			},
			{
				"family": "Shabanian",
				"given": "Samira"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					23
				]
			]
		}
	},
	{
		"id": "ehyaeiCausalAdversarialPerturbations2024",
		"type": "article-journal",
		"abstract": "As responsible AI gains importance in machine learning algorithms, properties like fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models (SCMs) in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use SCMs and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation (CAP) and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v38i10.29070",
		"ISSN": "2374-3468",
		"issue": "10",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"note": "number: 10",
		"page": "11847-11855",
		"source": "ojs.aaai.org",
		"title": "Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/29070",
		"volume": "38",
		"author": [
			{
				"family": "Ehyaei",
				"given": "Ahmad-Reza"
			},
			{
				"family": "Mohammadi",
				"given": "Kiarash"
			},
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Samadi",
				"given": "Samira"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					24
				]
			]
		}
	},
	{
		"id": "kwokDatasetDifficultyRole2024",
		"type": "article",
		"abstract": "Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call \"example difficulty scores\", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive examples. These findings guide practitioners in maximizing the consistency of their scores (e.g. by choosing appropriate scoring methods, number of runs, and subsets of examples), and establishes comprehensive baselines for evaluating scores in the future.",
		"DOI": "10.48550/arXiv.2401.01867",
		"note": "arXiv:2401.01867 [cs]",
		"number": "arXiv:2401.01867",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Dataset Difficulty and the Role of Inductive Bias",
		"URL": "http://arxiv.org/abs/2401.01867",
		"author": [
			{
				"family": "Kwok",
				"given": "Devin"
			},
			{
				"family": "Anand",
				"given": "Nikhil"
			},
			{
				"family": "Frankle",
				"given": "Jonathan"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			},
			{
				"family": "Rolnick",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					3
				]
			]
		}
	},
	{
		"id": "braunschweigAITAAITrustworthiness2024",
		"type": "article-journal",
		"abstract": "The accelerated developments in the field of Artificial Intelligence (AI) hint at the need for considering “Trust” as a design principle rather than an option. Moreover, the design of AI-based critical systems, such as in avionics, mobility, defense, healthcare, finance, critical infrastructures, etc., requires proving their trustworthiness. Thus, AI-based critical systems must be assessed across many dimensions by different parties (regulators, developers, customers, reinsurance companies, and end-users) for different reasons. We can call it AI validation, monitoring, assessing, or auditing, but the fundamental concept in all cases is to make sure that the AI is performing well within its operational design domain. Such assessment begins from the early stages of development, including the definition of the specification requirements for the system, the analysis, the design, etc. Trust and trustworthiness assessment have to be considered at every phase of the system lifecycle, including sale and deployment, updates, maintenance, or int. It is expected that full trustworthiness in AI systems can only be established if the technical measures to establish trustworthiness are flanked by specifications for the governance and processes of organizations that use and develop AI. Application of Social Sciences and Humanities (SSH) methods and principles to handle human–AI interaction, and aid in the operationalisation of (ethical) values in the design and assessment, with important information provided on their actual impact on trust and trustworthiness is a key issue.",
		"container-title": "AI and Ethics",
		"DOI": "10.1007/s43681-023-00397-z",
		"ISSN": "2730-5961",
		"issue": "1",
		"journalAbbreviation": "AI Ethics",
		"language": "en",
		"page": "1-3",
		"source": "Springer Link",
		"title": "AITA: AI trustworthiness assessment",
		"title-short": "AITA",
		"URL": "https://doi.org/10.1007/s43681-023-00397-z",
		"volume": "4",
		"author": [
			{
				"family": "Braunschweig",
				"given": "Bertrand"
			},
			{
				"family": "Buijsman",
				"given": "Stefan"
			},
			{
				"family": "Chamroukhi",
				"given": "Faïcel"
			},
			{
				"family": "Heintz",
				"given": "Fredrik"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Mattioli",
				"given": "Juliette"
			},
			{
				"family": "Poretschkin",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					1
				]
			]
		}
	},
	{
		"id": "mousaviAreLLMsRobust2024",
		"type": "article",
		"abstract": "Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation. Nevertheless, most of the publicly available datasets and benchmarks on task-oriented dialogues focus on written conversations. Consequently, the robustness of the developed models to spoken interactions is unknown. In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the lack of proper spoken dialogue datasets, we have automatically transcribed a development set of spoken dialogues with a state-of-the-art ASR engine. We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues. We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models in two subtasks of response generation and dialogue state tracking, respectively. The results show that LLMs are not robust to spoken noise by default, however, fine-tuning/training such models on a proper dataset of spoken TODs can result in a more robust performance.",
		"DOI": "10.48550/arXiv.2401.02297",
		"note": "arXiv:2401.02297 [cs]",
		"number": "arXiv:2401.02297",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Are LLMs Robust for Spoken Dialogues?",
		"URL": "http://arxiv.org/abs/2401.02297",
		"author": [
			{
				"family": "Mousavi",
				"given": "Seyed Mahed"
			},
			{
				"family": "Roccabruna",
				"given": "Gabriel"
			},
			{
				"family": "Alghisi",
				"given": "Simone"
			},
			{
				"family": "Rizzoli",
				"given": "Massimo"
			},
			{
				"family": "Ravanelli",
				"given": "Mirco"
			},
			{
				"family": "Riccardi",
				"given": "Giuseppe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					4
				]
			]
		}
	},
	{
		"id": "leeJaxPrunerConciseLibrary2023",
		"type": "article",
		"abstract": "This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.",
		"DOI": "10.48550/arXiv.2304.14082",
		"note": "arXiv:2304.14082 [cs]",
		"number": "arXiv:2304.14082",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "JaxPruner: A concise library for sparsity research",
		"title-short": "JaxPruner",
		"URL": "http://arxiv.org/abs/2304.14082",
		"author": [
			{
				"family": "Lee",
				"given": "Joo Hyung"
			},
			{
				"family": "Park",
				"given": "Wonpyo"
			},
			{
				"family": "Mitchell",
				"given": "Nicole"
			},
			{
				"family": "Pilault",
				"given": "Jonathan"
			},
			{
				"family": "Obando-Ceron",
				"given": "Johan"
			},
			{
				"family": "Kim",
				"given": "Han-Byul"
			},
			{
				"family": "Lee",
				"given": "Namhoon"
			},
			{
				"family": "Frantar",
				"given": "Elias"
			},
			{
				"family": "Long",
				"given": "Yun"
			},
			{
				"family": "Yazdanbakhsh",
				"given": "Amir"
			},
			{
				"family": "Agrawal",
				"given": "Shivani"
			},
			{
				"family": "Subramanian",
				"given": "Suvinay"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Kao",
				"given": "Sheng-Chun"
			},
			{
				"family": "Zhang",
				"given": "Xingyao"
			},
			{
				"family": "Gale",
				"given": "Trevor"
			},
			{
				"family": "Bik",
				"given": "Aart"
			},
			{
				"family": "Han",
				"given": "Woohyun"
			},
			{
				"family": "Ferev",
				"given": "Milen"
			},
			{
				"family": "Han",
				"given": "Zhonglin"
			},
			{
				"family": "Kim",
				"given": "Hong-Seok"
			},
			{
				"family": "Dauphin",
				"given": "Yann"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			},
			{
				"family": "Castro",
				"given": "Pablo Samuel"
			},
			{
				"family": "Evci",
				"given": "Utku"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					18
				]
			]
		}
	},
	{
		"id": "costeRewardModelEnsembles2023",
		"type": "paper-conference",
		"abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \"true\" reward, these learned reward models are susceptible to overoptimization. Gao et al. studied this phenomenon in a synthetic human feedback setup with a significantly larger \"gold\" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
		"event-title": "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following",
		"language": "en",
		"source": "openreview.net",
		"title": "Reward Model Ensembles Help Mitigate Overoptimization",
		"URL": "https://openreview.net/forum?id=NiQYQEPUsA",
		"author": [
			{
				"family": "Coste",
				"given": "Thomas"
			},
			{
				"family": "Anwar",
				"given": "Usman"
			},
			{
				"family": "Kirk",
				"given": "Robert"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					26
				]
			]
		}
	},
	{
		"id": "wabarthaPiecewiseLinearParametrization2023",
		"type": "paper-conference",
		"abstract": "Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. We argue for the use of policies that are piecewise-linear. We carefully study to what extent they can retain the interpretable properties of linear policies while performing competitively with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the agent's decision process without needing an additional explanation model. We evaluate HC policies in control and navigation experiments, visualize the improved interpretability of the agent and highlight its trade-off with performance.",
		"event-title": "XAI in Action: Past, Present, and Future Applications",
		"language": "en",
		"source": "openreview.net",
		"title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning",
		"title-short": "Piecewise Linear Parametrization of Policies",
		"URL": "https://openreview.net/forum?id=Zbt9z0a95l",
		"author": [
			{
				"family": "Wabartha",
				"given": "Maxime"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "jainMechanisticallyAnalyzingEffects2023",
		"type": "paper-conference",
		"abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
		"URL": "https://openreview.net/forum?id=A0HKeKl4Nl",
		"author": [
			{
				"family": "Jain",
				"given": "Samyak"
			},
			{
				"family": "Kirk",
				"given": "Robert"
			},
			{
				"family": "Lubana",
				"given": "Ekdeep Singh"
			},
			{
				"family": "Dick",
				"given": "Robert P."
			},
			{
				"family": "Tanaka",
				"given": "Hidenori"
			},
			{
				"family": "Rocktäschel",
				"given": "Tim"
			},
			{
				"family": "Grefenstette",
				"given": "Edward"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "szotLargeLanguageModels2024",
		"type": "article",
		"abstract": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.",
		"DOI": "10.48550/arXiv.2310.17722",
		"note": "arXiv:2310.17722 [cs]",
		"number": "arXiv:2310.17722",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Large Language Models as Generalizable Policies for Embodied Tasks",
		"URL": "http://arxiv.org/abs/2310.17722",
		"author": [
			{
				"family": "Szot",
				"given": "Andrew"
			},
			{
				"family": "Schwarzer",
				"given": "Max"
			},
			{
				"family": "Agrawal",
				"given": "Harsh"
			},
			{
				"family": "Mazoure",
				"given": "Bogdan"
			},
			{
				"family": "Talbott",
				"given": "Walter"
			},
			{
				"family": "Metcalf",
				"given": "Katherine"
			},
			{
				"family": "Mackraz",
				"given": "Natalie"
			},
			{
				"family": "Hjelm",
				"given": "Devon"
			},
			{
				"family": "Toshev",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					16
				]
			]
		}
	},
	{
		"id": "patilIntelligentSwitchingResetFree2023",
		"type": "paper-conference",
		"abstract": "In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The resetting assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (forward) with learned resets by constructing a second (backward) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent’s confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Intelligent Switching for Reset-Free RL",
		"URL": "https://openreview.net/forum?id=Nq45xeghcL",
		"author": [
			{
				"family": "Patil",
				"given": "Darshan"
			},
			{
				"family": "Rajendran",
				"given": "Janarthanan"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "castanyerImprovingIntrinsicExploration2024",
		"type": "article",
		"abstract": "Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Several exploration objectives like count-based bonuses, pseudo-counts, and state-entropy maximization are non-stationary and hence are difficult to optimize for the agent. While this issue is generally known, it is usually omitted and solutions remain under-explored. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. We show that SOFE improves the performance of several exploration objectives, including count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover, SOFE outperforms prior methods that attempt to stabilize the optimization of intrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.",
		"DOI": "10.48550/arXiv.2310.18144",
		"note": "arXiv:2310.18144 [cs]",
		"number": "arXiv:2310.18144",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Intrinsic Exploration by Creating Stationary Objectives",
		"URL": "http://arxiv.org/abs/2310.18144",
		"author": [
			{
				"family": "Castanyer",
				"given": "Roger Creus"
			},
			{
				"family": "Romoff",
				"given": "Joshua"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					22
				]
			]
		}
	},
	{
		"id": "luoHallucinationDetectionHallucination2024",
		"type": "article",
		"abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.",
		"DOI": "10.48550/arXiv.2401.08358",
		"note": "arXiv:2401.08358 [cs]",
		"number": "arXiv:2401.08358",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
		"title-short": "Hallucination Detection and Hallucination Mitigation",
		"URL": "http://arxiv.org/abs/2401.08358",
		"author": [
			{
				"family": "Luo",
				"given": "Junliang"
			},
			{
				"family": "Li",
				"given": "Tianyu"
			},
			{
				"family": "Wu",
				"given": "Di"
			},
			{
				"family": "Jenkin",
				"given": "Michael"
			},
			{
				"family": "Liu",
				"given": "Steve"
			},
			{
				"family": "Dudek",
				"given": "Gregory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					16
				]
			]
		}
	},
	{
		"id": "mohammadpourDecouplingRegularizationAction2023",
		"type": "paper-conference",
		"abstract": "Regularized reinforcement learning (RL), particularly the entropy-regularized kind, has gained traction in optimal control and inverse RL. While standard unregularized RL methods remain unaffected by changes in the number of actions, we show that it can severely impact their regularized counterparts. This paper demonstrates the importance of decoupling the regularizer from the action space: that is, to maintain a consistent level of regularization regardless of how many actions are involved to avoid over-regularization. Whereas the problem can be avoided by introducing a task-specific temperature parameter, it is often undesirable and cannot solve the problem when action spaces are state-dependent. In the state-dependent action context, different states with varying action spaces are regularized inconsistently. We introduce two solutions: a static temperature selection approach and a dynamic counterpart, universally applicable where this problem arises. Implementing these changes improves performance on the DeepMind control suite in static and dynamic temperature regimes and a biological design task.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Decoupling regularization from the action space",
		"URL": "https://openreview.net/forum?id=UaMgmoKEBj",
		"author": [
			{
				"family": "Mohammadpour",
				"given": "Sobhan"
			},
			{
				"family": "Frejinger",
				"given": "Emma"
			},
			{
				"family": "Bacon",
				"given": "Pierre-Luc"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "zhaoConsciousnessInspiredSpatioTemporalAbstractions2023",
		"type": "paper-conference",
		"abstract": "Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning framework utilizing spatio-temporal abstractions to generalize better in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and thus enables sparse decision-making and focused computation on the relevant parts of the environment. The decomposition relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper’s significant advantage in zero-shot generalization, compared to some existing state-of-the-art hierarchical planning methods.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=eo9dHwtTFt",
		"author": [
			{
				"family": "Zhao",
				"given": "Harry"
			},
			{
				"family": "Alver",
				"given": "Safa"
			},
			{
				"family": "Seijen",
				"given": "Harm",
				"dropping-particle": "van"
			},
			{
				"family": "Laroche",
				"given": "Romain"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "ghugareClosingGapTD2024",
		"type": "article",
		"abstract": "Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models. Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization. Nonetheless, the connection between stitching and combinatorial generalisation also suggests a simple remedy for improving generalisation in SL: data augmentation. We propose a temporal data augmentation and demonstrate that adding it to SL-based methods enables them to successfully complete tasks not seen together during training. On a high level, this connection illustrates the importance of combinatorial generalization for data efficiency in time-series data beyond tasks beyond RL, like audio, video, or text.",
		"DOI": "10.48550/arXiv.2401.11237",
		"note": "arXiv:2401.11237 [cs]",
		"number": "arXiv:2401.11237",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View",
		"URL": "http://arxiv.org/abs/2401.11237",
		"author": [
			{
				"family": "Ghugare",
				"given": "Raj"
			},
			{
				"family": "Geist",
				"given": "Matthieu"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			},
			{
				"family": "Eysenbach",
				"given": "Benjamin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					11
				]
			]
		}
	},
	{
		"id": "niBridgingStateHistory2024",
		"type": "article",
		"abstract": "Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with distractors, and POMDPs with sparse rewards. These findings culminate in a set of preliminary guidelines for RL practitioners.",
		"DOI": "10.48550/arXiv.2401.08898",
		"note": "arXiv:2401.08898 [cs]",
		"number": "arXiv:2401.08898",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bridging State and History Representations: Understanding Self-Predictive RL",
		"title-short": "Bridging State and History Representations",
		"URL": "http://arxiv.org/abs/2401.08898",
		"author": [
			{
				"family": "Ni",
				"given": "Tianwei"
			},
			{
				"family": "Eysenbach",
				"given": "Benjamin"
			},
			{
				"family": "Seyedsalehi",
				"given": "Erfan"
			},
			{
				"family": "Ma",
				"given": "Michel"
			},
			{
				"family": "Gehring",
				"given": "Clement"
			},
			{
				"family": "Mahajan",
				"given": "Aditya"
			},
			{
				"family": "Bacon",
				"given": "Pierre-Luc"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					21
				]
			]
		}
	},
	{
		"id": "hashemizadehBalancingActConstraining2024",
		"type": "article",
		"abstract": "Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.",
		"DOI": "10.48550/arXiv.2310.20673",
		"note": "arXiv:2310.20673 [cs]",
		"number": "arXiv:2310.20673",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Balancing Act: Constraining Disparate Impact in Sparse Models",
		"title-short": "Balancing Act",
		"URL": "http://arxiv.org/abs/2310.20673",
		"author": [
			{
				"family": "Hashemizadeh",
				"given": "Meraj"
			},
			{
				"family": "Ramirez",
				"given": "Juan"
			},
			{
				"family": "Sukumaran",
				"given": "Rohan"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			},
			{
				"family": "Lacoste-Julien",
				"given": "Simon"
			},
			{
				"family": "Gallego-Posada",
				"given": "Jose"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					7
				]
			]
		}
	},
	{
		"id": "huAmortizingIntractableInference2024",
		"type": "article",
		"abstract": "Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.",
		"DOI": "10.48550/arXiv.2310.04363",
		"note": "arXiv:2310.04363 [cs]",
		"number": "arXiv:2310.04363",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Amortizing intractable inference in large language models",
		"URL": "http://arxiv.org/abs/2310.04363",
		"author": [
			{
				"family": "Hu",
				"given": "Edward J."
			},
			{
				"family": "Jain",
				"given": "Moksh"
			},
			{
				"family": "Elmoznino",
				"given": "Eric"
			},
			{
				"family": "Kaddar",
				"given": "Younesse"
			},
			{
				"family": "Lajoie",
				"given": "Guillaume"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Malkin",
				"given": "Nikolay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					13
				]
			]
		}
	},
	{
		"id": "siddiquiBlockwiseSelfSupervisedLearning2023",
		"type": "article",
		"abstract": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
		"DOI": "10.48550/arXiv.2302.01647",
		"note": "arXiv:2302.01647 [cs]",
		"number": "arXiv:2302.01647",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Blockwise Self-Supervised Learning at Scale",
		"URL": "http://arxiv.org/abs/2302.01647",
		"author": [
			{
				"family": "Siddiqui",
				"given": "Shoaib Ahmed"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "LeCun",
				"given": "Yann"
			},
			{
				"family": "Deny",
				"given": "Stéphane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					3
				]
			]
		}
	},
	{
		"id": "sharmaHumanAIAlignmentLargeScale2024",
		"type": "article",
		"abstract": "Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.",
		"DOI": "10.48550/arXiv.2402.03575",
		"note": "arXiv:2402.03575 [cs]",
		"number": "arXiv:2402.03575",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toward Human-AI Alignment in Large-Scale Multi-Player Games",
		"URL": "http://arxiv.org/abs/2402.03575",
		"author": [
			{
				"family": "Sharma",
				"given": "Sugandha"
			},
			{
				"family": "Davidson",
				"given": "Guy"
			},
			{
				"family": "Khetarpal",
				"given": "Khimya"
			},
			{
				"family": "Kanervisto",
				"given": "Anssi"
			},
			{
				"family": "Arora",
				"given": "Udit"
			},
			{
				"family": "Hofmann",
				"given": "Katja"
			},
			{
				"family": "Momennejad",
				"given": "Ida"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					18
				]
			]
		}
	},
	{
		"id": "viannaChannelSelectiveNormalizationLabelShift2024",
		"type": "article",
		"abstract": "Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution. For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets. To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference. Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks. It is implemented by recalculating batch normalization statistics on test batches. Prior work has focused on analysis with test data that has the same label distribution as the training data. However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. This presents a risk in applying test time adaptation methods in deployment. We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts. Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes. We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts. We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.",
		"DOI": "10.48550/arXiv.2402.04958",
		"note": "arXiv:2402.04958 [cs]",
		"number": "arXiv:2402.04958",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation",
		"URL": "http://arxiv.org/abs/2402.04958",
		"author": [
			{
				"family": "Vianna",
				"given": "Pedro"
			},
			{
				"family": "Chaudhary",
				"given": "Muawiz"
			},
			{
				"family": "Mehrbod",
				"given": "Paria"
			},
			{
				"family": "Tang",
				"given": "An"
			},
			{
				"family": "Cloutier",
				"given": "Guy"
			},
			{
				"family": "Wolf",
				"given": "Guy"
			},
			{
				"family": "Eickenberg",
				"given": "Michael"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					29
				]
			]
		}
	},
	{
		"id": "xhonneuxInContextLearningCan2024",
		"type": "article",
		"abstract": "Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains LLM-generated text with violence, suicide, and misinformation.",
		"DOI": "10.48550/arXiv.2402.05723",
		"note": "arXiv:2402.05723 [cs]",
		"number": "arXiv:2402.05723",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "In-Context Learning Can Re-learn Forbidden Tasks",
		"URL": "http://arxiv.org/abs/2402.05723",
		"author": [
			{
				"family": "Xhonneux",
				"given": "Sophie"
			},
			{
				"family": "Dobre",
				"given": "David"
			},
			{
				"family": "Tang",
				"given": "Jian"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			},
			{
				"family": "Sridhar",
				"given": "Dhanya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					8
				]
			]
		}
	},
	{
		"id": "caiAntagonisticAI2024",
		"type": "article",
		"abstract": "The vast majority of discourse around AI development assumes that subservient, \"moral\" models aligned with \"human values\" are universally beneficial -- in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values. Far from being \"bad\" or \"immoral,\" we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethical challenges of this space and identify three dimensions for the responsible design of antagonistic AI -- consent, context, and framing.",
		"DOI": "10.48550/arXiv.2402.07350",
		"note": "arXiv:2402.07350 [cs]",
		"number": "arXiv:2402.07350",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Antagonistic AI",
		"URL": "http://arxiv.org/abs/2402.07350",
		"author": [
			{
				"family": "Cai",
				"given": "Alice"
			},
			{
				"family": "Arawjo",
				"given": "Ian"
			},
			{
				"family": "Glassman",
				"given": "Elena L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					11
				]
			]
		}
	},
	{
		"id": "obando-ceronMixturesExpertsUnlock2024",
		"type": "article",
		"abstract": "The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.",
		"DOI": "10.48550/arXiv.2402.08609",
		"note": "arXiv:2402.08609 [cs]",
		"number": "arXiv:2402.08609",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
		"URL": "http://arxiv.org/abs/2402.08609",
		"author": [
			{
				"family": "Obando-Ceron",
				"given": "Johan"
			},
			{
				"family": "Sokar",
				"given": "Ghada"
			},
			{
				"family": "Willi",
				"given": "Timon"
			},
			{
				"family": "Lyle",
				"given": "Clare"
			},
			{
				"family": "Farebrother",
				"given": "Jesse"
			},
			{
				"family": "Foerster",
				"given": "Jakob"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Castro",
				"given": "Pablo Samuel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					26
				]
			]
		}
	},
	{
		"id": "majdinasabTrainedMyConsent2024",
		"type": "article",
		"abstract": "Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.",
		"DOI": "10.48550/arXiv.2402.09299",
		"note": "arXiv:2402.09299 [cs]",
		"number": "arXiv:2402.09299",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code",
		"title-short": "Trained Without My Consent",
		"URL": "http://arxiv.org/abs/2402.09299",
		"author": [
			{
				"family": "Majdinasab",
				"given": "Vahid"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					14
				]
			]
		}
	},
	{
		"id": "sastryComputingPowerGovernance2024",
		"type": "article",
		"abstract": "Computing power, or \"compute,\" is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.",
		"DOI": "10.48550/arXiv.2402.08797",
		"note": "arXiv:2402.08797 [cs]",
		"number": "arXiv:2402.08797",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Computing Power and the Governance of Artificial Intelligence",
		"URL": "http://arxiv.org/abs/2402.08797",
		"author": [
			{
				"family": "Sastry",
				"given": "Girish"
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			},
			{
				"family": "Hazell",
				"given": "Julian"
			},
			{
				"family": "O'Keefe",
				"given": "Cullen"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			},
			{
				"family": "Ngo",
				"given": "Richard"
			},
			{
				"family": "Pilz",
				"given": "Konstantin"
			},
			{
				"family": "Gor",
				"given": "George"
			},
			{
				"family": "Bluemke",
				"given": "Emma"
			},
			{
				"family": "Shoker",
				"given": "Sarah"
			},
			{
				"family": "Egan",
				"given": "Janet"
			},
			{
				"family": "Trager",
				"given": "Robert F."
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Coyle",
				"given": "Diane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					13
				]
			]
		}
	},
	{
		"id": "kapoorSocietalImpactOpen2024",
		"type": "article",
		"abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",
		"DOI": "10.48550/arXiv.2403.07918",
		"note": "arXiv:2403.07918 [cs]",
		"number": "arXiv:2403.07918",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Societal Impact of Open Foundation Models",
		"URL": "http://arxiv.org/abs/2403.07918",
		"author": [
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Klyman",
				"given": "Kevin"
			},
			{
				"family": "Longpre",
				"given": "Shayne"
			},
			{
				"family": "Ramaswami",
				"given": "Ashwin"
			},
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Hopkins",
				"given": "Aspen"
			},
			{
				"family": "Bankston",
				"given": "Kevin"
			},
			{
				"family": "Biderman",
				"given": "Stella"
			},
			{
				"family": "Bogen",
				"given": "Miranda"
			},
			{
				"family": "Chowdhury",
				"given": "Rumman"
			},
			{
				"family": "Engler",
				"given": "Alex"
			},
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Lazar",
				"given": "Seth"
			},
			{
				"family": "Maffulli",
				"given": "Stefano"
			},
			{
				"family": "Nelson",
				"given": "Alondra"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			},
			{
				"family": "Skowron",
				"given": "Aviya"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Storchan",
				"given": "Victor"
			},
			{
				"family": "Zhang",
				"given": "Daniel"
			},
			{
				"family": "Ho",
				"given": "Daniel E."
			},
			{
				"family": "Liang",
				"given": "Percy"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					27
				]
			]
		}
	},
	{
		"id": "manduchiChallengesOpportunitiesGenerative2024",
		"type": "article",
		"abstract": "The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI solutions.",
		"DOI": "10.48550/arXiv.2403.00025",
		"note": "arXiv:2403.00025 [cs]",
		"number": "arXiv:2403.00025",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Challenges and Opportunities in Generative AI",
		"URL": "http://arxiv.org/abs/2403.00025",
		"author": [
			{
				"family": "Manduchi",
				"given": "Laura"
			},
			{
				"family": "Pandey",
				"given": "Kushagra"
			},
			{
				"family": "Bamler",
				"given": "Robert"
			},
			{
				"family": "Cotterell",
				"given": "Ryan"
			},
			{
				"family": "Däubener",
				"given": "Sina"
			},
			{
				"family": "Fellenz",
				"given": "Sophie"
			},
			{
				"family": "Fischer",
				"given": "Asja"
			},
			{
				"family": "Gärtner",
				"given": "Thomas"
			},
			{
				"family": "Kirchler",
				"given": "Matthias"
			},
			{
				"family": "Kloft",
				"given": "Marius"
			},
			{
				"family": "Li",
				"given": "Yingzhen"
			},
			{
				"family": "Lippert",
				"given": "Christoph"
			},
			{
				"family": "Melo",
				"given": "Gerard",
				"non-dropping-particle": "de"
			},
			{
				"family": "Nalisnick",
				"given": "Eric"
			},
			{
				"family": "Ommer",
				"given": "Björn"
			},
			{
				"family": "Ranganath",
				"given": "Rajesh"
			},
			{
				"family": "Rudolph",
				"given": "Maja"
			},
			{
				"family": "Ullrich",
				"given": "Karen"
			},
			{
				"family": "Broeck",
				"given": "Guy Van",
				"dropping-particle": "den"
			},
			{
				"family": "Vogt",
				"given": "Julia E."
			},
			{
				"family": "Wang",
				"given": "Yixin"
			},
			{
				"family": "Wenzel",
				"given": "Florian"
			},
			{
				"family": "Wood",
				"given": "Frank"
			},
			{
				"family": "Mandt",
				"given": "Stephan"
			},
			{
				"family": "Fortuin",
				"given": "Vincent"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					28
				]
			]
		}
	},
	{
		"id": "lyleDisentanglingCausesPlasticity2024",
		"type": "article",
		"abstract": "Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.",
		"DOI": "10.48550/arXiv.2402.18762",
		"note": "arXiv:2402.18762 [cs]",
		"number": "arXiv:2402.18762",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Disentangling the Causes of Plasticity Loss in Neural Networks",
		"URL": "http://arxiv.org/abs/2402.18762",
		"author": [
			{
				"family": "Lyle",
				"given": "Clare"
			},
			{
				"family": "Zheng",
				"given": "Zeyu"
			},
			{
				"family": "Khetarpal",
				"given": "Khimya"
			},
			{
				"family": "Hasselt",
				"given": "Hado",
				"non-dropping-particle": "van"
			},
			{
				"family": "Pascanu",
				"given": "Razvan"
			},
			{
				"family": "Martens",
				"given": "James"
			},
			{
				"family": "Dabney",
				"given": "Will"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					28
				]
			]
		}
	},
	{
		"id": "allinghamGenerativeModelSymmetry2024",
		"type": "article",
		"abstract": "Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we take inspiration from group theoretic ideas to construct a generative model that explicitly aims to capture the data's approximate symmetries. This results in a model that, given a prespecified broad set of possible symmetries, learns to what extent, if at all, those symmetries are actually present. Our model can be seen as a generative process for data augmentation. We provide a simple algorithm for learning our generative model and empirically demonstrate its ability to capture symmetries under affine and color transformations, in an interpretable way. Combining our symmetry model with standard generative models results in higher marginal test-log-likelihoods and improved data efficiency.",
		"DOI": "10.48550/arXiv.2403.01946",
		"note": "arXiv:2403.01946 [cs]",
		"number": "arXiv:2403.01946",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Generative Model of Symmetry Transformations",
		"URL": "http://arxiv.org/abs/2403.01946",
		"author": [
			{
				"family": "Allingham",
				"given": "James Urquhart"
			},
			{
				"family": "Mlodozeniec",
				"given": "Bruno Kacper"
			},
			{
				"family": "Padhy",
				"given": "Shreyas"
			},
			{
				"family": "Antorán",
				"given": "Javier"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Turner",
				"given": "Richard E."
			},
			{
				"family": "Nalisnick",
				"given": "Eric"
			},
			{
				"family": "Hernández-Lobato",
				"given": "José Miguel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					20
				]
			]
		}
	},
	{
		"id": "notsawoPredictingGrokkingLong2023",
		"type": "article",
		"abstract": "This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quantify the amplitude of low-frequency components to detect the presence of such oscillations. We also present additional experiments aimed at explaining the cause of these oscillations and characterizing the loss landscape.",
		"DOI": "10.48550/arXiv.2306.13253",
		"note": "arXiv:2306.13253 [cs]",
		"number": "arXiv:2306.13253",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok",
		"title-short": "Predicting Grokking Long Before it Happens",
		"URL": "http://arxiv.org/abs/2306.13253",
		"author": [
			{
				"family": "Notsawo",
				"given": "Pascal Jr Tikeng"
			},
			{
				"family": "Zhou",
				"given": "Hattie"
			},
			{
				"family": "Pezeshki",
				"given": "Mohammad"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Dumas",
				"given": "Guillaume"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					28
				]
			]
		}
	},
	{
		"id": "picheSelfevaluationSelfpromptingImprove2024",
		"type": "paper-conference",
		"abstract": "In order to safely deploy Large Language Models (LLMs), they must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a simple objective that can encourage the model to produce generation that the model is confident in. To optimize this objective, we introduce ReSearch, an iterative search algorithm based on self-evaluation and self-prompting. Our method results in fewer hallucinations overall, both for known and unknown topics, as the model learns to selectively restrain itself. In addition, our method elegantly incorporates the ability to decline, when the model assesses that it cannot provide a response without a high proportion of hallucination.",
		"event-title": "ICLR 2024 Workshop on Secure and Trustworthy Large Language Models",
		"language": "en",
		"source": "openreview.net",
		"title": "Self-evaluation and self-prompting to improve the reliability of LLMs",
		"URL": "https://openreview.net/forum?id=IwB45cjqC8",
		"author": [
			{
				"family": "Piché",
				"given": "Alexandre"
			},
			{
				"family": "Milios",
				"given": "Aristides"
			},
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Pal",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					14
				]
			]
		}
	},
	{
		"id": "mazoureGenerativeModelsDecision2024",
		"type": "paper-conference",
		"abstract": "Generative Artificial Intelligence (AI) has made significant advancements in recent years, particularly with the development of large language and diffusion models. These generative models have demonstrated impressive capabilities in various tasks, such as text generation and image and audio synthesis. Concurrently, Reinforcement Learning (RL) has made significant strides in solving complex sequential decision-making problems with the help of external knowledge sources . However, there remains untapped potential in combining generative models with RL algorithms to tackle real-world challenges, particularly to improve sample efficiency of tabula rasa training by introducing priors from related domains such as visual question-answering, image captioning and image generation. This workshop aims to bring together researchers and practitioners from the fields of generative AI and reinforcement learning to explore the latest advances, methodologies, and applications. By fostering collaborations between these two domains, we intend to unlock new opportunities for addressing complex problems that lie at the intersection of both fields.",
		"event-title": "ICLR 2024 Workshops",
		"language": "en",
		"source": "openreview.net",
		"title": "Generative Models for Decision Making",
		"URL": "https://openreview.net/forum?id=lzXisiFRgD",
		"author": [
			{
				"family": "Mazoure",
				"given": "Bogdan"
			},
			{
				"family": "Lee",
				"given": "Lisa"
			},
			{
				"family": "Raileanu",
				"given": "Roberta"
			},
			{
				"family": "Du",
				"given": "Yilun"
			},
			{
				"family": "Talbott",
				"given": "Walter"
			},
			{
				"family": "Metcalf",
				"given": "Katherine"
			},
			{
				"family": "Hjelm",
				"given": "R. Devon"
			},
			{
				"family": "Toshev",
				"given": "Alexander T."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					8
				]
			]
		}
	},
	{
		"id": "qadriGlobalAICultures2024",
		"type": "paper-conference",
		"abstract": "Building globally-inclusive generative artificial intelligence (genAI) that encodes, respects, and valorizes cultural sensibilities as well as performs well for users across cultural contexts, is an important goal as we deploy generative AI products globally. If we are to build such inclusive AI for people, we must both have a better understanding of how we can make our AI pipeline more globally inclusive and how AI technologies can impact or shape cultures it is deployed in. If unexamined, this relationship between AI and culture will universalize western-centered AI and have unforeseen impacts on global cultural production, values and consumptions. However, this is not a relationship AI scholarship can understand on its own. We need an engagement with existing theories on the interplay between technology and culture, and how both can shape each other. We urgently thus need a cross-disciplinary and cross-community framework for understanding this multifaceted relationship between AI and Culture. This workshop aims to begin a conversation between core AI researchers and experts from the social sciences and humanities with a focus on the impact of generative AI on cultures and the cultural exclusions embedded in our on generative AI pipelines. Through this focus, the workshop will encourage field building on deepening our understanding for how we can build and deploy globally inclusive genAI and how we can responsibly encode cultural knowledge into our technologies.",
		"event-title": "ICLR 2024 Workshops",
		"language": "en",
		"source": "openreview.net",
		"title": "Global AI Cultures",
		"URL": "https://openreview.net/forum?id=gw8aq110vD",
		"author": [
			{
				"family": "Qadri",
				"given": "Rida"
			},
			{
				"family": "Diaz",
				"given": "Fernando"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Born",
				"given": "Georgina Emma"
			},
			{
				"family": "Gray",
				"given": "Mary L."
			},
			{
				"family": "Quaye",
				"given": "Jessica"
			},
			{
				"family": "Bergmann",
				"given": "Rachel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					8
				]
			]
		}
	},
	{
		"id": "zhuInvestigatingRobotInfluence2024",
		"type": "paper-conference",
		"abstract": "Humans naturally tend to synchronize their movements with others, a phenomenon known as the entrainment effect, whether voluntarily or involuntarily. This phenomenon extends to interactions between humans and robots, which could have either positive or negative consequences for the human partner. We propose a human-subject study aimed at investigating the use of robots to influence human behaviour through entrainment in diverse Human-Robot Interaction (HRI) scenarios. The current work involves two human-subject experiments investigating the impact of robots on short-term human behaviour, encompassing human-human and human-robot interactions. The goal is to comprehend how variations in robot actions, such as movement frequency during repetitive tasks, influence human perceptions and behaviours in collaborative lab-based settings. Another objective is to investigate the factors that make participants aware of the entrainment effect during HRI. The preliminary results of the HHI experiment provide evidence that individuals tend to synchronize their movements with another person.",
		"collection-title": "HRI '24",
		"container-title": "Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3610978.3638367",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0323-2",
		"page": "169–171",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Investigating Robot Influence on Human Behaviour By Leveraging Entrainment Effects",
		"URL": "https://doi.org/10.1145/3610978.3638367",
		"author": [
			{
				"family": "Zhu",
				"given": "Lixiao"
			},
			{
				"family": "Moon",
				"given": "AJung"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					11
				]
			]
		}
	},
	{
		"id": "lehnertBetterPlanningTransformers2024",
		"type": "article",
		"abstract": "While Transformers have enabled tremendous progress in various application settings, such architectures still trail behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than the $A^*$ implementation that was used for training initially. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics.",
		"DOI": "10.48550/arXiv.2402.14083",
		"note": "arXiv:2402.14083 [cs]",
		"number": "arXiv:2402.14083",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
		"title-short": "Beyond A*",
		"URL": "http://arxiv.org/abs/2402.14083",
		"author": [
			{
				"family": "Lehnert",
				"given": "Lucas"
			},
			{
				"family": "Sukhbaatar",
				"given": "Sainbayar"
			},
			{
				"family": "Su",
				"given": "DiJia"
			},
			{
				"family": "Zheng",
				"given": "Qinqing"
			},
			{
				"family": "Mcvay",
				"given": "Paul"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "Tian",
				"given": "Yuandong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					26
				]
			]
		}
	},
	{
		"id": "hendersonRethinkingMachineLearning2024",
		"type": "paper-conference",
		"abstract": "Benchmarking efforts for machine learning have often mimicked (or even explicitly used) professional licensing exams to assess capabilities in a given area, focusing primarily on accuracy as the metric of choice. However, this approach neglects a variety of essential skills required in professional settings. We propose that professional codes of conduct and rules can guide machine learning researchers to address potential gaps in benchmark construction. These guidelines frequently account for situations professionals may encounter and must handle with care. A model may excel on an exam but still fall short in critical scenarios, deemed unacceptable under professional codes or rules. To motivate this idea, we conduct a case study and comparative examination of machine translation in legal settings. We point out several areas where standard deployments and benchmarks do not assess key requirements under professional rules. We suggest further refinements that would bring the two closer together, including requiring a measurement of uncertainty so that models opt out of uncertain translations. We then share broader insights on constructing and deploying foundation models, particularly in critical domains like law and legal translation.",
		"collection-title": "CSLAW '24",
		"container-title": "Proceedings of the Symposium on Computer Science and Law",
		"DOI": "10.1145/3614407.3643708",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0333-1",
		"page": "109–120",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Rethinking Machine Learning Benchmarks in the Context of Professional Codes of Conduct",
		"URL": "https://doi.org/10.1145/3614407.3643708",
		"author": [
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Hu",
				"given": "Jieru"
			},
			{
				"family": "Diab",
				"given": "Mona"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					12
				]
			]
		}
	},
	{
		"id": "dufort-labbeMaxwellDemonWork2024",
		"type": "article",
		"abstract": "When training deep neural networks, the phenomenon of $\\textit{dying neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero during training$\\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization.",
		"DOI": "10.48550/arXiv.2403.07688",
		"note": "arXiv:2403.07688 [cs]",
		"number": "arXiv:2403.07688",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
		"title-short": "Maxwell's Demon at Work",
		"URL": "http://arxiv.org/abs/2403.07688",
		"author": [
			{
				"family": "Dufort-Labbé",
				"given": "Simon"
			},
			{
				"family": "D'Oro",
				"given": "Pierluca"
			},
			{
				"family": "Nikishin",
				"given": "Evgenii"
			},
			{
				"family": "Pascanu",
				"given": "Razvan"
			},
			{
				"family": "Bacon",
				"given": "Pierre-Luc"
			},
			{
				"family": "Baratin",
				"given": "Aristide"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					12
				]
			]
		}
	},
	{
		"id": "tambonBugsLargeLanguage2024",
		"type": "article",
		"abstract": "Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",
		"DOI": "10.48550/arXiv.2403.08937",
		"note": "arXiv:2403.08937 [cs]",
		"number": "arXiv:2403.08937",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bugs in Large Language Models Generated Code: An Empirical Study",
		"title-short": "Bugs in Large Language Models Generated Code",
		"URL": "http://arxiv.org/abs/2403.08937",
		"author": [
			{
				"family": "Tambon",
				"given": "Florian"
			},
			{
				"family": "Dakhel",
				"given": "Arghavan Moradi"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Desmarais",
				"given": "Michel C."
			},
			{
				"family": "Antoniol",
				"given": "Giuliano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					18
				]
			]
		}
	},
	{
		"id": "barin-pacelaIdentifiabilityQuantizedFactors2024a",
		"type": "paper-conference",
		"abstract": "Disentanglement  aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors is proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations. In this work, however, we demonstrate that it is possible to recover quantized latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have independent discontinuities in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed quantized factor identifiability, and provide a comprehensive proof of the recovery of the quantized factors.",
		"container-title": "Proceedings of the Third Conference on Causal Learning and Reasoning",
		"event-title": "Causal Learning and Reasoning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "384-422",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "On the Identifiability of Quantized Factors",
		"URL": "https://proceedings.mlr.press/v236/barin-pacela24a.html",
		"author": [
			{
				"family": "Barin-Pacela",
				"given": "Vitória"
			},
			{
				"family": "Ahuja",
				"given": "Kartik"
			},
			{
				"family": "Lacoste-Julien",
				"given": "Simon"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					15
				]
			]
		}
	},
	{
		"id": "clymerSafetyCasesHow2024",
		"type": "article",
		"abstract": "As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.",
		"DOI": "10.48550/arXiv.2403.10462",
		"note": "arXiv:2403.10462 [cs]",
		"number": "arXiv:2403.10462",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safety Cases: How to Justify the Safety of Advanced AI Systems",
		"title-short": "Safety Cases",
		"URL": "http://arxiv.org/abs/2403.10462",
		"author": [
			{
				"family": "Clymer",
				"given": "Joshua"
			},
			{
				"family": "Gabrieli",
				"given": "Nick"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Larsen",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					18
				]
			]
		}
	},
	{
		"id": "chehbouniRepresentationalHarmsQualityofService2024",
		"type": "article",
		"abstract": "Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.",
		"DOI": "10.48550/arXiv.2403.13213",
		"note": "arXiv:2403.13213 [cs]",
		"number": "arXiv:2403.13213",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
		"title-short": "From Representational Harms to Quality-of-Service Harms",
		"URL": "http://arxiv.org/abs/2403.13213",
		"author": [
			{
				"family": "Chehbouni",
				"given": "Khaoula"
			},
			{
				"family": "Roshan",
				"given": "Megha"
			},
			{
				"family": "Ma",
				"given": "Emmanuel"
			},
			{
				"family": "Wei",
				"given": "Futian Andrew"
			},
			{
				"family": "Taik",
				"given": "Afaf"
			},
			{
				"family": "Cheung",
				"given": "Jackie CK"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					5
				]
			]
		}
	},
	{
		"id": "nanfackAdversarialAttacksInterpretation2024",
		"type": "article-journal",
		"abstract": "Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v38i5.28228",
		"ISSN": "2374-3468",
		"issue": "5",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"note": "number: 5",
		"page": "4315-4324",
		"source": "ojs.aaai.org",
		"title": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/28228",
		"volume": "38",
		"author": [
			{
				"family": "Nanfack",
				"given": "Geraldin"
			},
			{
				"family": "Fulleringer",
				"given": "Alexander"
			},
			{
				"family": "Marty",
				"given": "Jonathan"
			},
			{
				"family": "Eickenberg",
				"given": "Michael"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					24
				]
			]
		}
	},
	{
		"id": "zengGeneralizingTemporalDomains2024",
		"type": "article-journal",
		"abstract": "In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Neural Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v38i15.29604",
		"ISSN": "2374-3468",
		"issue": "15",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"note": "number: 15",
		"page": "16651-16659",
		"source": "ojs.aaai.org",
		"title": "Generalizing across Temporal Domains with Koopman Operators",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/29604",
		"volume": "38",
		"author": [
			{
				"family": "Zeng",
				"given": "Qiuhao"
			},
			{
				"family": "Wang",
				"given": "Wei"
			},
			{
				"family": "Zhou",
				"given": "Fan"
			},
			{
				"family": "Xu",
				"given": "Gezheng"
			},
			{
				"family": "Pu",
				"given": "Ruizhi"
			},
			{
				"family": "Shui",
				"given": "Changjian"
			},
			{
				"family": "Gagné",
				"given": "Christian"
			},
			{
				"family": "Yang",
				"given": "Shichun"
			},
			{
				"family": "Ling",
				"given": "Charles X."
			},
			{
				"family": "Wang",
				"given": "Boyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					24
				]
			]
		}
	},
	{
		"id": "kartikSyntheticDataGeneration2024",
		"type": "article",
		"abstract": "The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.",
		"DOI": "10.48550/arXiv.2403.16771",
		"note": "arXiv:2403.16771 [cs]",
		"number": "arXiv:2403.16771",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation",
		"URL": "http://arxiv.org/abs/2403.16771",
		"author": [
			{
				"family": "Kartik",
				"given": "Kartik"
			},
			{
				"family": "Soni",
				"given": "Sanjana"
			},
			{
				"family": "Kunchukuttan",
				"given": "Anoop"
			},
			{
				"family": "Chakraborty",
				"given": "Tanmoy"
			},
			{
				"family": "Akhtar",
				"given": "Md Shad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					29
				]
			]
		}
	},
	{
		"id": "rolnickApplicationDrivenInnovationMachine2024",
		"type": "article",
		"abstract": "As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.",
		"DOI": "10.48550/arXiv.2403.17381",
		"note": "arXiv:2403.17381 [cs]",
		"number": "arXiv:2403.17381",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Application-Driven Innovation in Machine Learning",
		"URL": "http://arxiv.org/abs/2403.17381",
		"author": [
			{
				"family": "Rolnick",
				"given": "David"
			},
			{
				"family": "Aspuru-Guzik",
				"given": "Alan"
			},
			{
				"family": "Beery",
				"given": "Sara"
			},
			{
				"family": "Dilkina",
				"given": "Bistra"
			},
			{
				"family": "Donti",
				"given": "Priya L."
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			},
			{
				"family": "Kerner",
				"given": "Hannah"
			},
			{
				"family": "Monteleoni",
				"given": "Claire"
			},
			{
				"family": "Rolf",
				"given": "Esther"
			},
			{
				"family": "Tambe",
				"given": "Milind"
			},
			{
				"family": "White",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					26
				]
			]
		}
	},
	{
		"id": "manasImprovingTexttoImageConsistency2024",
		"type": "article",
		"abstract": "Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.",
		"DOI": "10.48550/arXiv.2403.17804",
		"note": "arXiv:2403.17804 [cs]",
		"number": "arXiv:2403.17804",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization",
		"URL": "http://arxiv.org/abs/2403.17804",
		"author": [
			{
				"family": "Mañas",
				"given": "Oscar"
			},
			{
				"family": "Astolfi",
				"given": "Pietro"
			},
			{
				"family": "Hall",
				"given": "Melissa"
			},
			{
				"family": "Ross",
				"given": "Candace"
			},
			{
				"family": "Urbanek",
				"given": "Jack"
			},
			{
				"family": "Williams",
				"given": "Adina"
			},
			{
				"family": "Agrawal",
				"given": "Aishwarya"
			},
			{
				"family": "Romero-Soriano",
				"given": "Adriana"
			},
			{
				"family": "Drozdzal",
				"given": "Michal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					26
				]
			]
		}
	},
	{
		"id": "braiekMachineLearningRobustness2024",
		"type": "article",
		"abstract": "This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms. It covers non-adversarial data shifts and nuances of Deep Learning (DL) software testing methodologies. The discussion progresses to explore amelioration strategies for bolstering robustness, starting with data-centric approaches like debiasing and augmentation. Further examination includes a variety of model-centric methods such as transfer learning, adversarial training, and randomized smoothing. Lastly, post-training methods are discussed, including ensemble techniques, pruning, and model repairs, emerging as cost-effective strategies to make models more resilient against the unpredictable. This chapter underscores the ongoing challenges and limitations in estimating and achieving ML robustness by existing approaches. It offers insights and directions for future research on this crucial concept, as a prerequisite for trustworthy AI systems.",
		"DOI": "10.48550/arXiv.2404.00897",
		"note": "arXiv:2404.00897 [cs]",
		"number": "arXiv:2404.00897",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Machine Learning Robustness: A Primer",
		"title-short": "Machine Learning Robustness",
		"URL": "http://arxiv.org/abs/2404.00897",
		"author": [
			{
				"family": "Braiek",
				"given": "Houssem Ben"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					3
				]
			]
		}
	},
	{
		"id": "behnamghaderLLM2VecLargeLanguage2024",
		"type": "article",
		"abstract": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.",
		"DOI": "10.48550/arXiv.2404.05961",
		"note": "arXiv:2404.05961 [cs]",
		"number": "arXiv:2404.05961",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
		"title-short": "LLM2Vec",
		"URL": "http://arxiv.org/abs/2404.05961",
		"author": [
			{
				"family": "BehnamGhader",
				"given": "Parishad"
			},
			{
				"family": "Adlakha",
				"given": "Vaibhav"
			},
			{
				"family": "Mosbach",
				"given": "Marius"
			},
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Chapados",
				"given": "Nicolas"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					8
				]
			]
		}
	},
	{
		"id": "rahmanCausalDeepLearning2024",
		"type": "paper-conference",
		"abstract": "Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.",
		"collection-title": "ICSE '24",
		"container-title": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
		"DOI": "10.1145/3597503.3639170",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0217-4",
		"page": "1–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Towards Causal Deep Learning for Vulnerability Detection",
		"URL": "https://doi.org/10.1145/3597503.3639170",
		"author": [
			{
				"family": "Rahman",
				"given": "Md Mahbubur"
			},
			{
				"family": "Ceka",
				"given": "Ira"
			},
			{
				"family": "Mao",
				"given": "Chengzhi"
			},
			{
				"family": "Chakraborty",
				"given": "Saikat"
			},
			{
				"family": "Ray",
				"given": "Baishakhi"
			},
			{
				"family": "Le",
				"given": "Wei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					12
				]
			]
		}
	},
	{
		"id": "kamathScopeAmbiguitiesLarge2024",
		"type": "article-journal",
		"abstract": "Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models -- GPT-2, GPT-3/3.5, Llama 2 and GPT-4 -- treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).",
		"container-title": "Transactions of the Association for Computational Linguistics",
		"DOI": "10.1162/tacl_a_00670",
		"ISSN": "2307-387X",
		"note": "arXiv:2404.04332 [cs]",
		"page": "738-754",
		"source": "arXiv.org",
		"title": "Scope Ambiguities in Large Language Models",
		"URL": "http://arxiv.org/abs/2404.04332",
		"volume": "12",
		"author": [
			{
				"family": "Kamath",
				"given": "Gaurav"
			},
			{
				"family": "Schuster",
				"given": "Sebastian"
			},
			{
				"family": "Vajjala",
				"given": "Sowmya"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					4
				]
			]
		}
	},
	{
		"id": "cohenRegulatingAdvancedArtificial2024",
		"type": "article-journal",
		"abstract": "Technical experts and policy-makers have increasingly emphasized the need to address extinction risk from artificial intelligence (AI) systems that might circumvent safeguards and thwart attempts to control them (1). Reinforcement learning (RL) agents that plan over a long time horizon far more effectively than humans present particular risks. Giving an advanced AI system the objective to maximize its reward and, at some point, withholding reward from it, strongly incentivizes the AI system to take humans out of the loop, if it has the opportunity. The incentive to deceive humans and thwart human control arises not only for RL agents but for long-term planning agents (LTPAs) more generally. Because empirical testing of sufficiently capable LTPAs is unlikely to uncover these dangerous tendencies, our core regulatory proposal is simple: Developers should not be permitted to build sufficiently capable LTPAs, and the resources required to build them should be subject to stringent controls. Governments are turning their attention to these risks, alongside current and anticipated risks arising from algorithmic bias, privacy concerns, and misuse. At a 2023 global summit on AI safety, the attending countries, including the United States, United Kingdom, Canada, China, India, and members of the European Union (EU), issued a joint statement warning that, as AI continues to advance, “Substantial risks may arise from…unintended issues of control relating to alignment with human intent” (2). This broad consensus concerning the potential inability to keep advanced AI under control is also reflected in President Biden’s 2023 executive order that introduces reporting requirements for AI that could “eva[de] human control or oversight through means of deception or obfuscation” (3). Building on these efforts, now is the time for governments to develop regulatory institutions and frameworks that specifically target the existential risks from advanced artificial agents.",
		"container-title": "Science",
		"DOI": "10.1126/science.adl0625",
		"issue": "6691",
		"note": "publisher: American Association for the Advancement of Science",
		"page": "36-38",
		"source": "science.org (Atypon)",
		"title": "Regulating advanced artificial agents",
		"URL": "https://www.science.org/doi/10.1126/science.adl0625",
		"volume": "384",
		"author": [
			{
				"family": "Cohen",
				"given": "Michael K."
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			},
			{
				"family": "Russell",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					5
				]
			]
		}
	},
	{
		"id": "kasettyEvaluatingInterventionalReasoning2024",
		"type": "article",
		"abstract": "Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts.",
		"DOI": "10.48550/arXiv.2404.05545",
		"note": "arXiv:2404.05545 [cs, stat]",
		"number": "arXiv:2404.05545",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Evaluating Interventional Reasoning Capabilities of Large Language Models",
		"URL": "http://arxiv.org/abs/2404.05545",
		"author": [
			{
				"family": "Kasetty",
				"given": "Tejas"
			},
			{
				"family": "Mahajan",
				"given": "Divyat"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			},
			{
				"family": "Drouin",
				"given": "Alexandre"
			},
			{
				"family": "Sridhar",
				"given": "Dhanya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					8
				]
			]
		}
	},
	{
		"id": "yuWhatYourFavorite2024",
		"type": "article",
		"abstract": "Bias is a disproportionate prejudice in favor of one side against another. Due to the success of transformer-based Masked Language Models (MLMs) and their impact on many NLP tasks, a systematic evaluation of bias in these models is needed more than ever. While many studies have evaluated gender bias in English MLMs, only a few works have been conducted for the task in other languages. This paper proposes a multilingual approach to estimate gender bias in MLMs from 5 languages: Chinese, English, German, Portuguese, and Spanish. Unlike previous work, our approach does not depend on parallel corpora coupled with English to detect gender bias in other languages using multilingual lexicons. Moreover, a novel model-based method is presented to generate sentence pairs for a more robust analysis of gender bias, compared to the traditional lexicon-based method. For each language, both the lexicon-based and model-based methods are applied to create two datasets respectively, which are used to evaluate gender bias in an MLM specifically trained for that language using one existing and 3 new scoring metrics. Our results show that the previous approach is data-sensitive and not stable as it does not remove contextual dependencies irrelevant to gender. In fact, the results often flip when different scoring metrics are used on the same dataset, suggesting that gender bias should be studied on a large dataset using multiple evaluation metrics for best practice.",
		"DOI": "10.48550/arXiv.2404.06621",
		"note": "arXiv:2404.06621 [cs]",
		"number": "arXiv:2404.06621",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models",
		"title-short": "What is Your Favorite Gender, MLM?",
		"URL": "http://arxiv.org/abs/2404.06621",
		"author": [
			{
				"family": "Yu",
				"given": "Jeongrok"
			},
			{
				"family": "Kim",
				"given": "Seong Ug"
			},
			{
				"family": "Choi",
				"given": "Jacob"
			},
			{
				"family": "Choi",
				"given": "Jinho D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					9
				]
			]
		}
	},
	{
		"id": "bengioGovernmentInterventionsAvert2024",
		"type": "article-journal",
		"abstract": "This essay is a revised transcription of Yoshua Bengio’s July 2023 testimony in front of the U.S. Senate Subcommittee on Privacy, Technology, and the Law meeting on the topic of oversight of AI. It argues for caution and government interventions in regulation and research investments to mitigate the potentially catastrophic outcomes from future advances in AI as the technology approaches human-level cognitive abilities. It summarizes the trends in advancing capabilities and the uncertain timeline to these future advances, as well as the different types of catastrophic scenarios that could follow, including both intentional and unintentional cases, misuse by bad actors, and intentional as well as unintentional loss of control of powerful AIs. It makes public policy recommendations that include national regulation, international agreements, and public research investments in AI safety as well as classified research investments to design aligned AI systems that can safely protect us from bad actors and uncontrolled dangerous AI systems. It highlights the need for strong democratic governance processes to control the safety and ethical use of future powerful AI systems, whether they are in private hands or under government authority.",
		"container-title": "Harvard Data Science Review",
		"DOI": "10.1162/99608f92.d949f941",
		"ISSN": "2644-2353, 2688-8513",
		"issue": "Special Issue 5",
		"language": "en",
		"note": "publisher: The MIT Press",
		"source": "hdsr.mitpress.mit.edu",
		"title": "Government Interventions to Avert Future Catastrophic AI Risks",
		"URL": "https://hdsr.mitpress.mit.edu/pub/w974bwb0/release/2",
		"author": [
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					4
				]
			]
		}
	},
	{
		"id": "shankarWhoValidatesValidators2024",
		"type": "article",
		"abstract": "Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.",
		"DOI": "10.48550/arXiv.2404.12272",
		"note": "arXiv:2404.12272 [cs]",
		"number": "arXiv:2404.12272",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
		"title-short": "Who Validates the Validators?",
		"URL": "http://arxiv.org/abs/2404.12272",
		"author": [
			{
				"family": "Shankar",
				"given": "Shreya"
			},
			{
				"family": "Zamfirescu-Pereira",
				"given": "J. D."
			},
			{
				"family": "Hartmann",
				"given": "Björn"
			},
			{
				"family": "Parameswaran",
				"given": "Aditya G."
			},
			{
				"family": "Arawjo",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					18
				]
			]
		}
	},
	{
		"id": "vidgenIntroducingV0AI2024",
		"type": "article",
		"abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.",
		"DOI": "10.48550/arXiv.2404.12241",
		"note": "arXiv:2404.12241 [cs]",
		"number": "arXiv:2404.12241",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons",
		"URL": "http://arxiv.org/abs/2404.12241",
		"author": [
			{
				"family": "Vidgen",
				"given": "Bertie"
			},
			{
				"family": "Agrawal",
				"given": "Adarsh"
			},
			{
				"family": "Ahmed",
				"given": "Ahmed M."
			},
			{
				"family": "Akinwande",
				"given": "Victor"
			},
			{
				"family": "Al-Nuaimi",
				"given": "Namir"
			},
			{
				"family": "Alfaraj",
				"given": "Najla"
			},
			{
				"family": "Alhajjar",
				"given": "Elie"
			},
			{
				"family": "Aroyo",
				"given": "Lora"
			},
			{
				"family": "Bavalatti",
				"given": "Trupti"
			},
			{
				"family": "Bartolo",
				"given": "Max"
			},
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Bollacker",
				"given": "Kurt"
			},
			{
				"family": "Bomassani",
				"given": "Rishi"
			},
			{
				"family": "Boston",
				"given": "Marisa Ferrara"
			},
			{
				"family": "Campos",
				"given": "Siméon"
			},
			{
				"family": "Chakra",
				"given": "Kal"
			},
			{
				"family": "Chen",
				"given": "Canyu"
			},
			{
				"family": "Coleman",
				"given": "Cody"
			},
			{
				"family": "Coudert",
				"given": "Zacharie Delpierre"
			},
			{
				"family": "Derczynski",
				"given": "Leon"
			},
			{
				"family": "Dutta",
				"given": "Debojyoti"
			},
			{
				"family": "Eisenberg",
				"given": "Ian"
			},
			{
				"family": "Ezick",
				"given": "James"
			},
			{
				"family": "Frase",
				"given": "Heather"
			},
			{
				"family": "Fuller",
				"given": "Brian"
			},
			{
				"family": "Gandikota",
				"given": "Ram"
			},
			{
				"family": "Gangavarapu",
				"given": "Agasthya"
			},
			{
				"family": "Gangavarapu",
				"given": "Ananya"
			},
			{
				"family": "Gealy",
				"given": "James"
			},
			{
				"family": "Ghosh",
				"given": "Rajat"
			},
			{
				"family": "Goel",
				"given": "James"
			},
			{
				"family": "Gohar",
				"given": "Usman"
			},
			{
				"family": "Goswami",
				"given": "Sujata"
			},
			{
				"family": "Hale",
				"given": "Scott A."
			},
			{
				"family": "Hutiri",
				"given": "Wiebke"
			},
			{
				"family": "Imperial",
				"given": "Joseph Marvin"
			},
			{
				"family": "Jandial",
				"given": "Surgan"
			},
			{
				"family": "Judd",
				"given": "Nick"
			},
			{
				"family": "Juefei-Xu",
				"given": "Felix"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Kailkhura",
				"given": "Bhavya"
			},
			{
				"family": "Kirk",
				"given": "Hannah Rose"
			},
			{
				"family": "Klyman",
				"given": "Kevin"
			},
			{
				"family": "Knotz",
				"given": "Chris"
			},
			{
				"family": "Kuchnik",
				"given": "Michael"
			},
			{
				"family": "Kumar",
				"given": "Shachi H."
			},
			{
				"family": "Kumar",
				"given": "Srijan"
			},
			{
				"family": "Lengerich",
				"given": "Chris"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Liao",
				"given": "Zeyi"
			},
			{
				"family": "Long",
				"given": "Eileen Peters"
			},
			{
				"family": "Lu",
				"given": "Victor"
			},
			{
				"family": "Luger",
				"given": "Sarah"
			},
			{
				"family": "Mai",
				"given": "Yifan"
			},
			{
				"family": "Mammen",
				"given": "Priyanka Mary"
			},
			{
				"family": "Manyeki",
				"given": "Kelvin"
			},
			{
				"family": "McGregor",
				"given": "Sean"
			},
			{
				"family": "Mehta",
				"given": "Virendra"
			},
			{
				"family": "Mohammed",
				"given": "Shafee"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Nachman",
				"given": "Lama"
			},
			{
				"family": "Naganna",
				"given": "Dinesh Jinenhally"
			},
			{
				"family": "Nikanjam",
				"given": "Amin"
			},
			{
				"family": "Nushi",
				"given": "Besmira"
			},
			{
				"family": "Oala",
				"given": "Luis"
			},
			{
				"family": "Orr",
				"given": "Iftach"
			},
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Patlak",
				"given": "Cigdem"
			},
			{
				"family": "Pietri",
				"given": "William"
			},
			{
				"family": "Poursabzi-Sangdeh",
				"given": "Forough"
			},
			{
				"family": "Presani",
				"given": "Eleonora"
			},
			{
				"family": "Puletti",
				"given": "Fabrizio"
			},
			{
				"family": "Röttger",
				"given": "Paul"
			},
			{
				"family": "Sahay",
				"given": "Saurav"
			},
			{
				"family": "Santos",
				"given": "Tim"
			},
			{
				"family": "Scherrer",
				"given": "Nino"
			},
			{
				"family": "Sebag",
				"given": "Alice Schoenauer"
			},
			{
				"family": "Schramowski",
				"given": "Patrick"
			},
			{
				"family": "Shahbazi",
				"given": "Abolfazl"
			},
			{
				"family": "Sharma",
				"given": "Vin"
			},
			{
				"family": "Shen",
				"given": "Xudong"
			},
			{
				"family": "Sistla",
				"given": "Vamsi"
			},
			{
				"family": "Tang",
				"given": "Leonard"
			},
			{
				"family": "Testuggine",
				"given": "Davide"
			},
			{
				"family": "Thangarasa",
				"given": "Vithursan"
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Weiss",
				"given": "Rebecca"
			},
			{
				"family": "Welty",
				"given": "Chris"
			},
			{
				"family": "Wilbers",
				"given": "Tyler"
			},
			{
				"family": "Williams",
				"given": "Adina"
			},
			{
				"family": "Wu",
				"given": "Carole-Jean"
			},
			{
				"family": "Yadav",
				"given": "Poonam"
			},
			{
				"family": "Yang",
				"given": "Xianjun"
			},
			{
				"family": "Zeng",
				"given": "Yi"
			},
			{
				"family": "Zhang",
				"given": "Wenhui"
			},
			{
				"family": "Zhdanov",
				"given": "Fedor"
			},
			{
				"family": "Zhu",
				"given": "Jiacheng"
			},
			{
				"family": "Liang",
				"given": "Percy"
			},
			{
				"family": "Mattson",
				"given": "Peter"
			},
			{
				"family": "Vanschoren",
				"given": "Joaquin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					13
				]
			]
		}
	},
	{
		"id": "vaniSPAROSelectiveAttention2024",
		"type": "article",
		"abstract": "Selective attention helps us focus on task-relevant aspects in the constant flood of our sensory input. This constraint in our perception allows us to robustly generalize under distractions and to new compositions of perceivable concepts. Transformers employ a similar notion of attention in their architecture, but representation learning models with transformer backbones like CLIP and DINO often fail to demonstrate robustness and compositionality. We highlight a missing architectural prior: unlike human perception, transformer encodings do not separately attend over individual concepts. In response, we propose SPARO, a read-out mechanism that partitions encodings into separately-attended slots, each produced by a single attention head. Using SPARO with CLIP imparts an inductive bias that the vision and text modalities are different views of a shared compositional world with the same corresponding concepts. Using SPARO, we demonstrate improvements on downstream recognition, robustness, retrieval, and compositionality benchmarks with CLIP (up to +14% for ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe for ImageNet with DINO (+3% each). We also showcase a powerful ability to intervene and select individual SPARO concepts to further improve downstream task performance (up from +4% to +9% for SugarCrepe) and use this ability to study the robustness of SPARO's representation structure. Finally, we provide insights through ablation experiments and visualization of learned concepts.",
		"DOI": "10.48550/arXiv.2404.15721",
		"note": "arXiv:2404.15721 [cs]",
		"number": "arXiv:2404.15721",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision",
		"title-short": "SPARO",
		"URL": "http://arxiv.org/abs/2404.15721",
		"author": [
			{
				"family": "Vani",
				"given": "Ankit"
			},
			{
				"family": "Nguyen",
				"given": "Bac"
			},
			{
				"family": "Lavoie",
				"given": "Samuel"
			},
			{
				"family": "Krishna",
				"given": "Ranjay"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					24
				]
			]
		}
	},
	{
		"id": "meadeUniversalAdversarialTriggers2024",
		"type": "article",
		"abstract": "Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models. These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not universal. We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models.",
		"DOI": "10.48550/arXiv.2404.16020",
		"note": "arXiv:2404.16020 [cs]",
		"number": "arXiv:2404.16020",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Universal Adversarial Triggers Are Not Universal",
		"URL": "http://arxiv.org/abs/2404.16020",
		"author": [
			{
				"family": "Meade",
				"given": "Nicholas"
			},
			{
				"family": "Patel",
				"given": "Arkil"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					24
				]
			]
		}
	},
	{
		"id": "arefinUnsupervisedConceptDiscovery2024",
		"type": "article",
		"abstract": "Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT.",
		"DOI": "10.48550/arXiv.2402.13368",
		"note": "arXiv:2402.13368 [cs]",
		"number": "arXiv:2402.13368",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unsupervised Concept Discovery Mitigates Spurious Correlations",
		"URL": "http://arxiv.org/abs/2402.13368",
		"author": [
			{
				"family": "Arefin",
				"given": "Md Rifat"
			},
			{
				"family": "Zhang",
				"given": "Yan"
			},
			{
				"family": "Baratin",
				"given": "Aristide"
			},
			{
				"family": "Locatello",
				"given": "Francesco"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Liu",
				"given": "Dianbo"
			},
			{
				"family": "Kawaguchi",
				"given": "Kenji"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					16
				]
			]
		}
	},
	{
		"id": "farebrotherStopRegressingTraining2024",
		"type": "article",
		"abstract": "Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.",
		"DOI": "10.48550/arXiv.2403.03950",
		"note": "arXiv:2403.03950 [cs, stat]",
		"number": "arXiv:2403.03950",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
		"title-short": "Stop Regressing",
		"URL": "http://arxiv.org/abs/2403.03950",
		"author": [
			{
				"family": "Farebrother",
				"given": "Jesse"
			},
			{
				"family": "Orbay",
				"given": "Jordi"
			},
			{
				"family": "Vuong",
				"given": "Quan"
			},
			{
				"family": "Taïga",
				"given": "Adrien Ali"
			},
			{
				"family": "Chebotar",
				"given": "Yevgen"
			},
			{
				"family": "Xiao",
				"given": "Ted"
			},
			{
				"family": "Irpan",
				"given": "Alex"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			},
			{
				"family": "Castro",
				"given": "Pablo Samuel"
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			},
			{
				"family": "Kumar",
				"given": "Aviral"
			},
			{
				"family": "Agarwal",
				"given": "Rishabh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					6
				]
			]
		}
	},
	{
		"id": "barStochasticPositionalEmbeddings2024",
		"type": "paper-conference",
		"abstract": "Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty to MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a gaussian distribution. We show that using StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, using StoP improves downstream MIM performance on a variety of downstream tasks. For example, linear probing on ImageNet using ViT-B is improved by $+1.7\\%$, and by $2.5\\%$ for ViT-H using 1% of the data.",
		"event-title": "Forty-first International Conference on Machine Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "Stochastic positional embeddings improve masked image modeling",
		"URL": "https://openreview.net/forum?id=hr8OXXMb7a",
		"author": [
			{
				"family": "Bar",
				"given": "Amir"
			},
			{
				"family": "Bordes",
				"given": "Florian"
			},
			{
				"family": "Shocher",
				"given": "Assaf"
			},
			{
				"family": "Assran",
				"given": "Mido"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			},
			{
				"family": "Ballas",
				"given": "Nicolas"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			},
			{
				"family": "Globerson",
				"given": "Amir"
			},
			{
				"family": "LeCun",
				"given": "Yann"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					6
				]
			]
		}
	},
	{
		"id": "carliniStealingPartProduction2024",
		"type": "article",
		"abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
		"DOI": "10.48550/arXiv.2403.06634",
		"note": "arXiv:2403.06634 [cs]",
		"number": "arXiv:2403.06634",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Stealing Part of a Production Language Model",
		"URL": "http://arxiv.org/abs/2403.06634",
		"author": [
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Paleka",
				"given": "Daniel"
			},
			{
				"family": "Dvijotham",
				"given": "Krishnamurthy Dj"
			},
			{
				"family": "Steinke",
				"given": "Thomas"
			},
			{
				"family": "Hayase",
				"given": "Jonathan"
			},
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Lee",
				"given": "Katherine"
			},
			{
				"family": "Jagielski",
				"given": "Matthew"
			},
			{
				"family": "Nasr",
				"given": "Milad"
			},
			{
				"family": "Conmy",
				"given": "Arthur"
			},
			{
				"family": "Yona",
				"given": "Itay"
			},
			{
				"family": "Wallace",
				"given": "Eric"
			},
			{
				"family": "Rolnick",
				"given": "David"
			},
			{
				"family": "Tramèr",
				"given": "Florian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					9
				]
			]
		}
	},
	{
		"id": "chenSelfIESelfInterpretationLarge2024",
		"type": "article",
		"abstract": "How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",
		"DOI": "10.48550/arXiv.2403.10949",
		"note": "arXiv:2403.10949 [cs]",
		"number": "arXiv:2403.10949",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
		"title-short": "SelfIE",
		"URL": "http://arxiv.org/abs/2403.10949",
		"author": [
			{
				"family": "Chen",
				"given": "Haozhe"
			},
			{
				"family": "Vondrick",
				"given": "Carl"
			},
			{
				"family": "Mao",
				"given": "Chengzhi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					25
				]
			]
		}
	},
	{
		"id": "madsenFaithfulnessMeasurableMasked2024",
		"type": "article",
		"abstract": "A common approach to explaining NLP models is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues, and existing solutions that address this are computationally expensive and employ proxy models. Furthermore, other metrics are very limited in scope. This work proposes an inherently faithfulness measurable model that addresses these challenges. This is achieved using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to 16 different datasets and validate it using statistical in-distribution tests. The faithfulness is then measured with 9 different importance measures. Because masking is in-distribution, importance measures that themselves use masking become consistently more faithful. Additionally, because the model makes faithfulness cheap to measure, we can optimize explanations towards maximal faithfulness; thus, our model becomes indirectly inherently explainable.",
		"DOI": "10.48550/arXiv.2310.07819",
		"note": "arXiv:2310.07819 [cs]",
		"number": "arXiv:2310.07819",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Faithfulness Measurable Masked Language Models",
		"URL": "http://arxiv.org/abs/2310.07819",
		"author": [
			{
				"family": "Madsen",
				"given": "Andreas"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					9
				]
			]
		}
	},
	{
		"id": "johnsonExpertsDonCheat2024a",
		"type": "article",
		"abstract": "Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.",
		"DOI": "10.48550/arXiv.2402.08733",
		"note": "arXiv:2402.08733 [cs]",
		"number": "arXiv:2402.08733",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
		"title-short": "Experts Don't Cheat",
		"URL": "http://arxiv.org/abs/2402.08733",
		"author": [
			{
				"family": "Johnson",
				"given": "Daniel D."
			},
			{
				"family": "Tarlow",
				"given": "Daniel"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			},
			{
				"family": "Maddison",
				"given": "Chris J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					27
				]
			]
		}
	},
	{
		"id": "wiltzerDistributionalAnalogueSuccessor2024",
		"type": "article",
		"abstract": "This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.",
		"DOI": "10.48550/arXiv.2402.08530",
		"note": "arXiv:2402.08530 [cs, stat]",
		"number": "arXiv:2402.08530",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Distributional Analogue to the Successor Representation",
		"URL": "http://arxiv.org/abs/2402.08530",
		"author": [
			{
				"family": "Wiltzer",
				"given": "Harley"
			},
			{
				"family": "Farebrother",
				"given": "Jesse"
			},
			{
				"family": "Gretton",
				"given": "Arthur"
			},
			{
				"family": "Tang",
				"given": "Yunhao"
			},
			{
				"family": "Barreto",
				"given": "André"
			},
			{
				"family": "Dabney",
				"given": "Will"
			},
			{
				"family": "Bellemare",
				"given": "Marc G."
			},
			{
				"family": "Rowland",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					24
				]
			]
		}
	},
	{
		"id": "venutoCodeRewardEmpowering2024",
		"type": "article",
		"abstract": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.",
		"DOI": "10.48550/arXiv.2402.04764",
		"note": "arXiv:2402.04764 [cs]",
		"number": "arXiv:2402.04764",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
		"title-short": "Code as Reward",
		"URL": "http://arxiv.org/abs/2402.04764",
		"author": [
			{
				"family": "Venuto",
				"given": "David"
			},
			{
				"family": "Islam",
				"given": "Sami Nur"
			},
			{
				"family": "Klissarov",
				"given": "Martin"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Yang",
				"given": "Sherry"
			},
			{
				"family": "Anand",
				"given": "Ankit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					7
				]
			]
		}
	},
	{
		"id": "poursoltaniRobustDatadrivenPrescriptiveness2024",
		"type": "article",
		"abstract": "The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual shortest path problem, we evaluate the robustness of the resulting policies against alternative methods when the out-of-sample dataset is subject to varying amounts of distribution shift.",
		"DOI": "10.48550/arXiv.2306.05937",
		"note": "arXiv:2306.05937 [cs, math, stat]",
		"number": "arXiv:2306.05937",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Robust Data-driven Prescriptiveness Optimization",
		"URL": "http://arxiv.org/abs/2306.05937",
		"author": [
			{
				"family": "Poursoltani",
				"given": "Mehran"
			},
			{
				"family": "Delage",
				"given": "Erick"
			},
			{
				"family": "Georghiou",
				"given": "Angelos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					3
				]
			]
		}
	},
	{
		"id": "hossainPersuasiveApproachCombating2024",
		"type": "article",
		"abstract": "Bayesian Persuasion is proposed as a tool for social media platforms to combat the spread of misinformation. Since platforms can use machine learning to predict the popularity and misinformation features of to-be-shared posts, and users are largely motivated to share popular content, platforms can strategically signal this informational advantage to change user beliefs and persuade them not to share misinformation. We characterize the optimal signaling scheme with imperfect predictions as a linear program and give sufficient and necessary conditions on the classifier to ensure optimal platform utility is non-decreasing and continuous. Next, this interaction is considered under a performative model, wherein platform intervention affects the user's future behaviour. The convergence and stability of optimal signaling under this performative process are fully characterized. Lastly, we experimentally validate that our approach significantly reduces misinformation in both the single round and performative setting and discuss the broader scope of using information design to combat misinformation.",
		"DOI": "10.48550/arXiv.2310.12065",
		"note": "arXiv:2310.12065 [cs]",
		"number": "arXiv:2310.12065",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Persuasive Approach to Combating Misinformation",
		"URL": "http://arxiv.org/abs/2310.12065",
		"author": [
			{
				"family": "Hossain",
				"given": "Safwan"
			},
			{
				"family": "Mladenovic",
				"given": "Andjela"
			},
			{
				"family": "Chen",
				"given": "Yiling"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					13
				]
			]
		}
	},
	{
		"id": "gloecklerAllinoneSimulationbasedInference2024",
		"type": "article",
		"abstract": "Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.",
		"DOI": "10.48550/arXiv.2404.09636",
		"note": "arXiv:2404.09636 [cs, stat]",
		"number": "arXiv:2404.09636",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "All-in-one simulation-based inference",
		"URL": "http://arxiv.org/abs/2404.09636",
		"author": [
			{
				"family": "Gloeckler",
				"given": "Manuel"
			},
			{
				"family": "Deistler",
				"given": "Michael"
			},
			{
				"family": "Weilbach",
				"given": "Christian"
			},
			{
				"family": "Wood",
				"given": "Frank"
			},
			{
				"family": "Macke",
				"given": "Jakob H."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					15
				]
			]
		}
	},
	{
		"id": "arawjoChainForgeVisualToolkit2024",
		"type": "paper-conference",
		"abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",
		"collection-title": "CHI '24",
		"container-title": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
		"DOI": "10.1145/3613904.3642016",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0330-0",
		"page": "1–18",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
		"title-short": "ChainForge",
		"URL": "https://doi.org/10.1145/3613904.3642016",
		"author": [
			{
				"family": "Arawjo",
				"given": "Ian"
			},
			{
				"family": "Swoopes",
				"given": "Chelse"
			},
			{
				"family": "Vaithilingam",
				"given": "Priyan"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Glassman",
				"given": "Elena L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					11
				]
			]
		}
	},
	{
		"id": "ostapenkoModularLLMsBuilding2024",
		"type": "article",
		"abstract": "The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
		"DOI": "10.48550/arXiv.2405.11157",
		"note": "arXiv:2405.11157 [cs]",
		"number": "arXiv:2405.11157",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
		"URL": "http://arxiv.org/abs/2405.11157",
		"author": [
			{
				"family": "Ostapenko",
				"given": "Oleksiy"
			},
			{
				"family": "Su",
				"given": "Zhan"
			},
			{
				"family": "Ponti",
				"given": "Edoardo Maria"
			},
			{
				"family": "Charlin",
				"given": "Laurent"
			},
			{
				"family": "Roux",
				"given": "Nicolas Le"
			},
			{
				"family": "Pereira",
				"given": "Matheus"
			},
			{
				"family": "Caccia",
				"given": "Lucas"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					17
				]
			]
		}
	},
	{
		"id": "wuWhatMechanismsDoes2024",
		"type": "paper-conference",
		"abstract": "Knowledge distillation is a commonly-used compression method in ML due to the popularity of increasingly large-scale models, but it is unclear if all the information a teacher model contains is distilled into the smaller student model. We aim to formalize the concept of ‘knowledge’ to investigate how knowledge is transferred during distillation, focusing on shared invariant outputs to counterfactual changes of dataset latent variables (we call these latents mechanisms). We define a student model to be a good stand-in model for a teacher if it shares the teacher’s learned mechanisms, and find that Jacobian matching and contrastive representation learning are viable methods by which to train such models. While these methods do not result in perfect transfer of mechanisms, we show they often improve student fidelity or mitigate simplicity bias (as measured by the teacher-to-student KL divergence and accuracy on various out-of-distribution test datasets), especially on datasets with spurious statistical correlations.",
		"container-title": "Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models",
		"event-title": "Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "60-75",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "What Mechanisms Does Knowledge Distillation Distill?",
		"URL": "https://proceedings.mlr.press/v243/wu24a.html",
		"author": [
			{
				"family": "Wu",
				"given": "Cindy"
			},
			{
				"family": "Lubana",
				"given": "Ekdeep Singh"
			},
			{
				"family": "Mlodozeniec",
				"given": "Bruno Kacper"
			},
			{
				"family": "Kirk",
				"given": "Robert"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					14
				]
			]
		}
	},
	{
		"id": "fathiDeCoDExConfounderDetector2024",
		"type": "article",
		"abstract": "Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability. Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.",
		"DOI": "10.48550/arXiv.2405.09288",
		"note": "arXiv:2405.09288 [cs]",
		"number": "arXiv:2405.09288",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations",
		"title-short": "DeCoDEx",
		"URL": "http://arxiv.org/abs/2405.09288",
		"author": [
			{
				"family": "Fathi",
				"given": "Nima"
			},
			{
				"family": "Kumar",
				"given": "Amar"
			},
			{
				"family": "Nichyporuk",
				"given": "Brennan"
			},
			{
				"family": "Havaei",
				"given": "Mohammad"
			},
			{
				"family": "Arbel",
				"given": "Tal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					15
				]
			]
		}
	},
	{
		"id": "hoppeGlobalRewardsMultiAgent2024",
		"type": "article",
		"abstract": "We study vehicle dispatching in autonomous mobility on demand (AMoD) systems, where a central operator assigns vehicles to customer requests or rejects these with the aim of maximizing its total profit. Recent approaches use multi-agent deep reinforcement learning (MADRL) to realize scalable yet performant algorithms, but train agents based on local rewards, which distorts the reward signal with respect to the system-wide profit, leading to lower performance. We therefore propose a novel global-rewards-based MADRL algorithm for vehicle dispatching in AMoD systems, which resolves so far existing goal conflicts between the trained agents and the operator by assigning rewards to agents leveraging a counterfactual baseline. Our algorithm shows statistically significant improvements across various settings on real-world data compared to state-of-the-art MADRL algorithms with local rewards. We further provide a structural analysis which shows that the utilization of global rewards can improve implicit vehicle balancing and demand forecasting abilities. Our code is available at https://github.com/tumBAIS/GR-MADRL-AMoD.",
		"DOI": "10.48550/arXiv.2312.08884",
		"note": "arXiv:2312.08884 [cs, eess]",
		"number": "arXiv:2312.08884",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Global Rewards in Multi-Agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems",
		"URL": "http://arxiv.org/abs/2312.08884",
		"author": [
			{
				"family": "Hoppe",
				"given": "Heiko"
			},
			{
				"family": "Enders",
				"given": "Tobias"
			},
			{
				"family": "Cappart",
				"given": "Quentin"
			},
			{
				"family": "Schiffer",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					19
				]
			]
		}
	},
	{
		"id": "vaithilingamImaginingFutureDesigning2024",
		"type": "paper-conference",
		"abstract": "We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.",
		"collection-title": "DIS '24",
		"container-title": "Proceedings of the 2024 ACM Designing Interactive Systems Conference",
		"DOI": "10.1145/3643834.3661525",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0583-0",
		"page": "289–300",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation",
		"title-short": "Imagining a Future of Designing with AI",
		"URL": "https://doi.org/10.1145/3643834.3661525",
		"author": [
			{
				"family": "Vaithilingam",
				"given": "Priyan"
			},
			{
				"family": "Arawjo",
				"given": "Ian"
			},
			{
				"family": "Glassman",
				"given": "Elena L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		}
	},
	{
		"id": "kaushalLORDLowRank2023",
		"type": "article",
		"abstract": "Low Rank Decomposition of matrix - splitting a large matrix into a product of two smaller matrix offers a means for compression that reduces the parameters of a model without sparsification, and hence delivering more speedup on modern hardware. Moreover, unlike quantization, the compressed linear layers remain fully differentiable and all the parameters trainable, while being able to leverage the existing highly efficient kernels over floating point matrices. We study the potential to compress Large Language Models (LLMs) for monolingual Code generation via Low Rank Decomposition (LoRD) and observe that ranks for the linear layers in these models can be reduced by upto 39.58% with less than 1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single A100. The compressed models speeds up inference by up to 22.35% with just a single line of change in code over huggingface's implementation with pytorch backend. Low Rank Decomposition (LoRD) models remain compatible with state of the art near-lossless quantization method such as SpQR, which allows leveraging further compression gains of quantization. Lastly, QLoRA over Low Rank Decomposition (LoRD) model further reduces memory requirements by as much as 21.2% over vanilla QLoRA while offering similar gains from parameter efficient fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new paradigm for LLM compression.",
		"DOI": "10.48550/arXiv.2309.14021",
		"note": "arXiv:2309.14021 [cs]",
		"number": "arXiv:2309.14021",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression",
		"title-short": "LORD",
		"URL": "http://arxiv.org/abs/2309.14021",
		"author": [
			{
				"family": "Kaushal",
				"given": "Ayush"
			},
			{
				"family": "Vaidhya",
				"given": "Tejas"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					25
				]
			]
		}
	},
	{
		"id": "ibrahimSimpleScalableStrategies2024",
		"type": "article",
		"abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.",
		"DOI": "10.48550/arXiv.2403.08763",
		"note": "arXiv:2403.08763 [cs]",
		"number": "arXiv:2403.08763",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
		"URL": "http://arxiv.org/abs/2403.08763",
		"author": [
			{
				"family": "Ibrahim",
				"given": "Adam"
			},
			{
				"family": "Thérien",
				"given": "Benjamin"
			},
			{
				"family": "Gupta",
				"given": "Kshitij"
			},
			{
				"family": "Richter",
				"given": "Mats L."
			},
			{
				"family": "Anthony",
				"given": "Quentin"
			},
			{
				"family": "Lesort",
				"given": "Timothée"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					26
				]
			]
		}
	},
	{
		"id": "muRuleBasedRewards2024",
		"type": "article-journal",
		"abstract": "Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior. However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental. Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader. In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating. We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.",
		"language": "en",
		"source": "Zotero",
		"title": "Rule Based Rewards for Language Model Safety",
		"author": [
			{
				"family": "Mu",
				"given": "Tong"
			},
			{
				"family": "Helyar",
				"given": "Alec"
			},
			{
				"family": "Heidecke",
				"given": "Johannes"
			},
			{
				"family": "Achiam",
				"given": "Joshua"
			},
			{
				"family": "Vallone",
				"given": "Andrea"
			},
			{
				"family": "Kivlichan",
				"given": "Ian"
			},
			{
				"family": "Lin",
				"given": "Molly"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chanMLEbenchEvaluatingMachine2024",
		"type": "article",
		"abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.",
		"DOI": "10.48550/arXiv.2410.07095",
		"note": "arXiv:2410.07095",
		"number": "arXiv:2410.07095",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
		"title-short": "MLE-bench",
		"URL": "http://arxiv.org/abs/2410.07095",
		"author": [
			{
				"family": "Chan",
				"given": "Jun Shern"
			},
			{
				"family": "Chowdhury",
				"given": "Neil"
			},
			{
				"family": "Jaffe",
				"given": "Oliver"
			},
			{
				"family": "Aung",
				"given": "James"
			},
			{
				"family": "Sherburn",
				"given": "Dane"
			},
			{
				"family": "Mays",
				"given": "Evan"
			},
			{
				"family": "Starace",
				"given": "Giulio"
			},
			{
				"family": "Liu",
				"given": "Kevin"
			},
			{
				"family": "Maksin",
				"given": "Leon"
			},
			{
				"family": "Patwardhan",
				"given": "Tejal"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Mądry",
				"given": "Aleksander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					24
				]
			]
		}
	},
	{
		"id": "eloundouFirstPersonFairnessChatbots2024",
		"type": "article-journal",
		"abstract": "Chatbots like ChatGPT are used by hundreds of millions of people for diverse purposes, ranging from re´sume´ writing to entertainment. These real-world applications are different from the institutional uses, such as re´sume´ screening or credit scoring, which have been the focus of much of AI research on bias and fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. In this work, we study “first-person fairness,” which means fairness toward the user who is interacting with a chatbot. This includes providing high-quality responses to all users regardless of their identity or background, and avoiding harmful stereotypes. We propose a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, we assess potential bias linked to users’ names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. Our method leverages a second language model to privately analyze name-sensitivity in the chatbot’s responses. We verify the validity of these annotations through independent human evaluation. Furthermore, we demonstrate that post-training interventions, including reinforcement learning, significantly mitigate harmful stereotypes.",
		"language": "en",
		"source": "Zotero",
		"title": "First-Person Fairness in Chatbots",
		"author": [
			{
				"family": "Eloundou",
				"given": "Tyna"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Robinson",
				"given": "David G"
			},
			{
				"family": "Gu-Lemberg",
				"given": "Keren"
			},
			{
				"family": "Brakman",
				"given": "Anna-Luisa"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Shah",
				"given": "Meghan"
			},
			{
				"family": "Heidecke",
				"given": "Johannes"
			},
			{
				"family": "Weng",
				"given": "Lilian"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024",
					10
				]
			]
		}
	},
	{
		"id": "luSimplifyingStabilizingScaling2024",
		"type": "article",
		"abstract": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.",
		"DOI": "10.48550/arXiv.2410.11081",
		"note": "arXiv:2410.11081",
		"number": "arXiv:2410.11081",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
		"URL": "http://arxiv.org/abs/2410.11081",
		"author": [
			{
				"family": "Lu",
				"given": "Cheng"
			},
			{
				"family": "Song",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					14
				]
			]
		}
	},
	{
		"id": "humeniukReinforcementLearningInformed2024",
		"type": "article-journal",
		"abstract": "Evolutionary search-based techniques are commonly used for testing autonomous robotic systems. However, these approaches often rely on computationally expensive simulator-based models for test scenario evaluation. To improve the computational efficiency of the search-based testing, we propose augmenting the evolutionary search (ES) with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge. In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm. By incorporating an RL agent into the search process, we aim to guide the algorithm towards promising regions of the search space from the start, enabling more efficient exploration of the solution space. We evaluate RIGAA on two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system. In both case studies, RIGAA reveals more failures of a high level of diversity, than the compared baselines. RIGAA also outperforms the state-of-the-art tools for vehicle lane keeping assist system testing, such as AmbieGen, CRAG, WOGAN and Frenetic in terms of number of revealed failures in a two-hour budget.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3680468",
		"ISSN": "1049-331X",
		"note": "Just Accepted",
		"source": "ACM Digital Library",
		"title": "Reinforcement Learning Informed Evolutionary Search for Autonomous Systems Testing",
		"URL": "https://dl.acm.org/doi/10.1145/3680468",
		"author": [
			{
				"family": "Humeniuk",
				"given": "Dmytro"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Antoniol",
				"given": "Giuliano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					27
				]
			]
		}
	},
	{
		"id": "openjaEmpiricalStudyTesting2024",
		"type": "article-journal",
		"abstract": "Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively). Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies. Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow. Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems. Results: Our findings reveal several key insights: 1.) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. 2.) A wide range of  \\(17\\)  ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency, Correctness, and Efficiency. 3.) Bias and Fairness is more tested in Recommendation (6%) and CV (3.9%) systems, while Security &amp; Privacy is tested in CV (2%), Application Platforms (0.9%), and NLP (0.5%). 4.) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing. Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.",
		"container-title": "ACM Trans. Softw. Eng. Methodol.",
		"DOI": "10.1145/3680463",
		"ISSN": "1049-331X",
		"note": "Just Accepted",
		"source": "ACM Digital Library",
		"title": "An empirical study of testing machine learning in the wild",
		"URL": "https://dl.acm.org/doi/10.1145/3680463",
		"author": [
			{
				"family": "Openja",
				"given": "Moses"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Foundjem",
				"given": "Armstrong"
			},
			{
				"family": "Jiang",
				"given": "Zhen Ming (Jack)"
			},
			{
				"family": "Abidi",
				"given": "Mouna"
			},
			{
				"family": "Hassan",
				"given": "Ahmed E."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					24
				]
			]
		}
	},
	{
		"id": "barcoTacklingProblemDistributional2024",
		"type": "article",
		"abstract": "Bayesian inference for inverse problems hinges critically on the choice of priors. In the absence of specific prior information, population-level distributions can serve as effective priors for parameters of interest. With the advent of machine learning, the use of data-driven population-level distributions (encoded, e.g., in a trained deep neural network) as priors is emerging as an appealing alternative to simple parametric priors in a variety of inverse problems. However, in many astrophysical applications, it is often difficult or even impossible to acquire independent and identically distributed samples from the underlying data-generating process of interest to train these models. In these cases, corrupted data or a surrogate, e.g. a simulator, is often used to produce training samples, meaning that there is a risk of obtaining misspecified priors. This, in turn, can bias the inferred posteriors in ways that are difficult to quantify, which limits the potential applicability of these models in real-world scenarios. In this work, we propose addressing this issue by iteratively updating the population-level distributions by retraining the model with posterior samples from different sets of observations and showcase the potential of this method on the problem of background image reconstruction in strong gravitational lensing when score-based models are used as data-driven priors. We show that starting from a misspecified prior distribution, the updated distribution becomes progressively closer to the underlying population-level distribution, and the resulting posterior samples exhibit reduced bias after several updates.",
		"DOI": "10.48550/arXiv.2407.17667",
		"note": "arXiv:2407.17667",
		"number": "arXiv:2407.17667",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tackling the Problem of Distributional Shifts: Correcting Misspecified, High-Dimensional Data-Driven Priors for Inverse Problems",
		"title-short": "Tackling the Problem of Distributional Shifts",
		"URL": "http://arxiv.org/abs/2407.17667",
		"author": [
			{
				"family": "Barco",
				"given": "Gabriel Missael"
			},
			{
				"family": "Adam",
				"given": "Alexandre"
			},
			{
				"family": "Stone",
				"given": "Connor"
			},
			{
				"family": "Hezaveh",
				"given": "Yashar"
			},
			{
				"family": "Perreault-Levasseur",
				"given": "Laurence"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					24
				]
			]
		}
	},
	{
		"id": "benkiraneMachineTranslationHallucination2024",
		"type": "article",
		"abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
		"DOI": "10.48550/arXiv.2407.16470",
		"note": "arXiv:2407.16470",
		"number": "arXiv:2407.16470",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
		"URL": "http://arxiv.org/abs/2407.16470",
		"author": [
			{
				"family": "Benkirane",
				"given": "Kenza"
			},
			{
				"family": "Gongas",
				"given": "Laura"
			},
			{
				"family": "Pelles",
				"given": "Shahar"
			},
			{
				"family": "Fuchs",
				"given": "Naomi"
			},
			{
				"family": "Darmon",
				"given": "Joshua"
			},
			{
				"family": "Stenetorp",
				"given": "Pontus"
			},
			{
				"family": "Adelani",
				"given": "David Ifeoluwa"
			},
			{
				"family": "Sánchez",
				"given": "Eduardo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					20
				]
			]
		}
	},
	{
		"id": "madsenAreSelfexplanationsLarge2024",
		"type": "paper-conference",
		"abstract": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",
		"container-title": "Findings of the Association for Computational Linguistics: ACL 2024",
		"DOI": "10.18653/v1/2024.findings-acl.19",
		"event-place": "Bangkok, Thailand",
		"event-title": "Findings 2024",
		"page": "295–337",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "Are self-explanations from Large Language Models faithful?",
		"URL": "https://aclanthology.org/2024.findings-acl.19",
		"author": [
			{
				"family": "Madsen",
				"given": "Andreas"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			},
			{
				"family": "Reddy",
				"given": "Siva"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "haoLearningRewriteGeneralized2024",
		"type": "article",
		"abstract": "Large language models (LLMs) can be abused at scale to create non-factual content and spread disinformation. Detecting LLM-generated content is essential to mitigate these risks, but current classifiers often fail to generalize in open-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated content less frequently, which can be used for detection and naturally generalizes to unforeseen data. However, we find that the rewriting edit distance between human and LLM content can be indistinguishable across domains, leading to detection failures. We propose training an LLM to rewrite input text, producing minimal edits for LLM-generated content and more edits for human-written text, deriving a distinguishable and generalizable edit distance difference across different domains. Experiments on text from 21 independent domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that our classifier outperforms the state-of-the-art zero-shot classifier by up to 20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work suggests that LLM can effectively detect machine-generated text if they are trained properly.",
		"DOI": "10.48550/arXiv.2408.04237",
		"note": "arXiv:2408.04237",
		"number": "arXiv:2408.04237",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Rewrite: Generalized LLM-Generated Text Detection",
		"title-short": "Learning to Rewrite",
		"URL": "http://arxiv.org/abs/2408.04237",
		"author": [
			{
				"family": "Hao",
				"given": "Wei"
			},
			{
				"family": "Li",
				"given": "Ran"
			},
			{
				"family": "Zhao",
				"given": "Weiliang"
			},
			{
				"family": "Yang",
				"given": "Junfeng"
			},
			{
				"family": "Mao",
				"given": "Chengzhi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					8
				]
			]
		}
	},
	{
		"id": "bengioCanBayesianOracle2024",
		"type": "article",
		"abstract": "Is there a way to design powerful AI systems based on machine learning methods that would satisfy probabilistic safety guarantees? With the long-term goal of obtaining a probabilistic guarantee that would apply in every context, we consider estimating a context-dependent bound on the probability of violating a given safety specification. Such a risk evaluation would need to be performed at run-time to provide a guardrail against dangerous actions of an AI. Noting that different plausible hypotheses about the world could produce very different outcomes, and because we do not know which one is right, we derive bounds on the safety violation probability predicted under the true but unknown hypothesis. Such bounds could be used to reject potentially dangerous actions. Our main results involve searching for cautious but plausible hypotheses, obtained by a maximization that involves Bayesian posteriors over hypotheses. We consider two forms of this result, in the iid case and in the non-iid case, and conclude with open problems towards turning such theoretical results into practical AI guardrails.",
		"DOI": "10.48550/arXiv.2408.05284",
		"note": "arXiv:2408.05284",
		"number": "arXiv:2408.05284",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Can a Bayesian Oracle Prevent Harm from an Agent?",
		"URL": "http://arxiv.org/abs/2408.05284",
		"author": [
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Cohen",
				"given": "Michael K."
			},
			{
				"family": "Malkin",
				"given": "Nikolay"
			},
			{
				"family": "MacDermott",
				"given": "Matt"
			},
			{
				"family": "Fornasiere",
				"given": "Damiano"
			},
			{
				"family": "Greiner",
				"given": "Pietro"
			},
			{
				"family": "Kaddar",
				"given": "Younesse"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					22
				]
			]
		}
	},
	{
		"id": "bardesRevisitingFeaturePrediction2024",
		"type": "article",
		"abstract": "This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",
		"DOI": "10.48550/arXiv.2404.08471",
		"note": "arXiv:2404.08471",
		"number": "arXiv:2404.08471",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
		"URL": "http://arxiv.org/abs/2404.08471",
		"author": [
			{
				"family": "Bardes",
				"given": "Adrien"
			},
			{
				"family": "Garrido",
				"given": "Quentin"
			},
			{
				"family": "Ponce",
				"given": "Jean"
			},
			{
				"family": "Chen",
				"given": "Xinlei"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "LeCun",
				"given": "Yann"
			},
			{
				"family": "Assran",
				"given": "Mahmoud"
			},
			{
				"family": "Ballas",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					15
				]
			]
		}
	},
	{
		"id": "humayunUnderstandingLocalGeometry2024",
		"type": "article",
		"abstract": "Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training. For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fr\\'echet Inception Distance using a large number of generated and real samples. However, generative model performance is not uniform across the learned manifold, e.g., for \\textit{foundation models} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised. In this paper we study the relationship between the \\textit{local geometry of the learned manifold} and downstream generation. Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to characterize a pre-trained generative model manifold locally. We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization. Finally we demonstrate that training a \\textit{reward model} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution.",
		"DOI": "10.48550/arXiv.2408.08307",
		"note": "arXiv:2408.08307",
		"number": "arXiv:2408.08307",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding the Local Geometry of Generative Model Manifolds",
		"URL": "http://arxiv.org/abs/2408.08307",
		"author": [
			{
				"family": "Humayun",
				"given": "Ahmed Imtiaz"
			},
			{
				"family": "Amara",
				"given": "Ibtihel"
			},
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Havaei",
				"given": "Mohammad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					15
				]
			]
		}
	},
	{
		"id": "didolkarZeroShotObjectCentricRepresentation2024",
		"type": "article",
		"abstract": "The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing pre-trained self-supervised features. However, so far, object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the wider trend in machine learning towards general-purpose models directly applicable to unseen data and tasks. Thus, in this work, we study current object-centric methods through the lens of zero-shot generalization by introducing a benchmark comprising eight different synthetic and real-world datasets. We analyze the factors influencing zero-shot performance and find that training on diverse real-world images improves transferability to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.",
		"DOI": "10.48550/arXiv.2408.09162",
		"note": "arXiv:2408.09162",
		"number": "arXiv:2408.09162",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Zero-Shot Object-Centric Representation Learning",
		"URL": "http://arxiv.org/abs/2408.09162",
		"author": [
			{
				"family": "Didolkar",
				"given": "Aniket"
			},
			{
				"family": "Zadaianchuk",
				"given": "Andrii"
			},
			{
				"family": "Goyal",
				"given": "Anirudh"
			},
			{
				"family": "Mozer",
				"given": "Mike"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Martius",
				"given": "Georg"
			},
			{
				"family": "Seitzer",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					17
				]
			]
		}
	},
	{
		"id": "alverAttentiveApproachBuilding2024",
		"type": "article-journal",
		"abstract": "We study the problem of building reasoning agents that are able to generalize in an effective manner. Towards this goal, we propose an end-to-end approach for building model-based reinforcement learning agents that dynamically focus their reasoning to the relevant aspects of the environment: after automatically identifying the distinct aspects of the environment, these agents dynamically filter out the relevant ones and then pass them to their simulator to perform partial reasoning. Unlike existing approaches, our approach works with pixel-based inputs and it allows for interpreting the focal points of the agent. Our quantitative analyses show that the proposed approach allows for effective generalization in high-dimensional domains with raw observational inputs. We also perform ablation analyses to validate of design choices. Finally, we demonstrate through qualitative analyses that our approach actually allows for building agents that focus their reasoning on the relevant aspects of the environment.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "An Attentive Approach for Building Partial Reasoning Agents from Pixels",
		"URL": "https://openreview.net/forum?id=S3FUKFMRw8",
		"author": [
			{
				"family": "Alver",
				"given": "Safa"
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					18
				]
			]
		}
	},
	{
		"id": "kumarTrainingLanguageModels2024",
		"type": "article",
		"abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.",
		"DOI": "10.48550/arXiv.2409.12917",
		"note": "arXiv:2409.12917",
		"number": "arXiv:2409.12917",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training Language Models to Self-Correct via Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2409.12917",
		"author": [
			{
				"family": "Kumar",
				"given": "Aviral"
			},
			{
				"family": "Zhuang",
				"given": "Vincent"
			},
			{
				"family": "Agarwal",
				"given": "Rishabh"
			},
			{
				"family": "Su",
				"given": "Yi"
			},
			{
				"family": "Co-Reyes",
				"given": "John D."
			},
			{
				"family": "Singh",
				"given": "Avi"
			},
			{
				"family": "Baumli",
				"given": "Kate"
			},
			{
				"family": "Iqbal",
				"given": "Shariq"
			},
			{
				"family": "Bishop",
				"given": "Colton"
			},
			{
				"family": "Roelofs",
				"given": "Rebecca"
			},
			{
				"family": "Zhang",
				"given": "Lei M."
			},
			{
				"family": "McKinney",
				"given": "Kay"
			},
			{
				"family": "Shrivastava",
				"given": "Disha"
			},
			{
				"family": "Paduraru",
				"given": "Cosmin"
			},
			{
				"family": "Tucker",
				"given": "George"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Behbahani",
				"given": "Feryal"
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					4
				]
			]
		}
	},
	{
		"id": "abdalwhabLearningMultiagentMultimachine2024",
		"type": "article",
		"abstract": "Robotics can help address the growing worker shortage challenge of the manufacturing industry. As such, machine tending is a task collaborative robots can tackle that can also highly boost productivity. Nevertheless, existing robotics systems deployed in that sector rely on a fixed single-arm setup, whereas mobile robots can provide more flexibility and scalability. In this work, we introduce a multi-agent multi-machine tending learning framework by mobile robots based on Multi-agent Reinforcement Learning (MARL) techniques with the design of a suitable observation and reward. Moreover, an attention-based encoding mechanism is developed and integrated into Multi-agent Proximal Policy Optimization (MAPPO) algorithm to boost its performance for machine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new challenging scenario in terms of task success, safety, and resources utilization. Furthermore, we provided an extensive ablation study to support our various design decisions.",
		"DOI": "10.48550/arXiv.2408.16875",
		"note": "arXiv:2408.16875",
		"number": "arXiv:2408.16875",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Multi-agent Multi-machine Tending by Mobile Robots",
		"URL": "http://arxiv.org/abs/2408.16875",
		"author": [
			{
				"family": "Abdalwhab",
				"given": "Abdalwhab"
			},
			{
				"family": "Beltrame",
				"given": "Giovanni"
			},
			{
				"family": "Kahou",
				"given": "Samira Ebrahimi"
			},
			{
				"family": "St-Onge",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					29
				]
			]
		}
	},
	{
		"id": "anwarFoundationalChallengesAssuring2024b",
		"type": "article",
		"abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.",
		"DOI": "10.48550/arXiv.2404.09932",
		"note": "arXiv:2404.09932",
		"number": "arXiv:2404.09932",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models",
		"URL": "http://arxiv.org/abs/2404.09932",
		"author": [
			{
				"family": "Anwar",
				"given": "Usman"
			},
			{
				"family": "Saparov",
				"given": "Abulhair"
			},
			{
				"family": "Rando",
				"given": "Javier"
			},
			{
				"family": "Paleka",
				"given": "Daniel"
			},
			{
				"family": "Turpin",
				"given": "Miles"
			},
			{
				"family": "Hase",
				"given": "Peter"
			},
			{
				"family": "Lubana",
				"given": "Ekdeep Singh"
			},
			{
				"family": "Jenner",
				"given": "Erik"
			},
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Sourbut",
				"given": "Oliver"
			},
			{
				"family": "Edelman",
				"given": "Benjamin L."
			},
			{
				"family": "Zhang",
				"given": "Zhaowei"
			},
			{
				"family": "Günther",
				"given": "Mario"
			},
			{
				"family": "Korinek",
				"given": "Anton"
			},
			{
				"family": "Hernandez-Orallo",
				"given": "Jose"
			},
			{
				"family": "Hammond",
				"given": "Lewis"
			},
			{
				"family": "Bigelow",
				"given": "Eric"
			},
			{
				"family": "Pan",
				"given": "Alexander"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Zhang",
				"given": "Heidi"
			},
			{
				"family": "Zhong",
				"given": "Ruiqi"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			},
			{
				"family": "Recchia",
				"given": "Gabriel"
			},
			{
				"family": "Corsi",
				"given": "Giulio"
			},
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Edwards",
				"given": "Lilian"
			},
			{
				"family": "Petrov",
				"given": "Aleksandar"
			},
			{
				"family": "Witt",
				"given": "Christian Schroeder",
				"dropping-particle": "de"
			},
			{
				"family": "Motwan",
				"given": "Sumeet Ramesh"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Chen",
				"given": "Danqi"
			},
			{
				"family": "Torr",
				"given": "Philip H. S."
			},
			{
				"family": "Albanie",
				"given": "Samuel"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Foerster",
				"given": "Jakob"
			},
			{
				"family": "Tramer",
				"given": "Florian"
			},
			{
				"family": "He",
				"given": "He"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					6
				]
			]
		}
	},
	{
		"id": "leRobustSaliencyMaps2024",
		"type": "paper-conference",
		"abstract": "Saliency maps are one of the most popular tools to interpret the operation of a neural network: they compute input features deemed relevant to the final prediction, which are often subsets of pixels that are easily understandable by a human being. However, it is known that relying solely on human assessment to judge a saliency map method can be misleading. In this work, we propose a new neural network verification specification called saliency-robustness, which aims to use formal methods to prove a relationship between Vanilla Gradient (VG) -- a simple yet surprisingly effective saliency map method -- and the network's prediction: given a network, if an input $x$ emits a certain VG saliency map, it is mathematically proven (or disproven) that the network must classify $x$ in a certain way. We then introduce a novel method that combines both Marabou and Crown -- two state-of-the-art neural network verifiers, to solve the proposed specification. Experiments on our synthetic dataset and MNIST show that Vanilla Gradient is surprisingly effective as a certification for the predicted output.",
		"event-title": "The 16th Asian Conference on Machine Learning (Conference Track)",
		"language": "en",
		"source": "openreview.net",
		"title": "Towards Robust Saliency Maps",
		"URL": "https://openreview.net/forum?id=2tv0Ubg3o7",
		"author": [
			{
				"family": "Le",
				"given": "Nham"
			},
			{
				"family": "Gurfinkel",
				"given": "Arie"
			},
			{
				"family": "Si",
				"given": "Xujie"
			},
			{
				"family": "Geng",
				"given": "Chuqin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "berrySheddingLightLarge2024",
		"type": "article",
		"abstract": "Generative diffusion models, notable for their large parameter count (exceeding 100 million) and operation within high-dimensional image spaces, pose significant challenges for traditional uncertainty estimation methods due to computational demands. In this work, we introduce an innovative framework, Diffusion Ensembles for Capturing Uncertainty (DECU), designed for estimating epistemic uncertainty for diffusion models. The DECU framework introduces a novel method that efficiently trains ensembles of conditional diffusion models by incorporating a static set of pre-trained parameters, drastically reducing the computational burden and the number of parameters that require training. Additionally, DECU employs Pairwise-Distance Estimators (PaiDEs) to accurately measure epistemic uncertainty by evaluating the mutual information between model outputs and weights in high-dimensional spaces. The effectiveness of this framework is demonstrated through experiments on the ImageNet dataset, highlighting its capability to capture epistemic uncertainty, specifically in under-sampled image classes.",
		"DOI": "10.48550/arXiv.2406.18580",
		"note": "arXiv:2406.18580",
		"number": "arXiv:2406.18580",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Shedding Light on Large Generative Networks: Estimating Epistemic Uncertainty in Diffusion Models",
		"title-short": "Shedding Light on Large Generative Networks",
		"URL": "http://arxiv.org/abs/2406.18580",
		"author": [
			{
				"family": "Berry",
				"given": "Lucas"
			},
			{
				"family": "Brando",
				"given": "Axel"
			},
			{
				"family": "Meger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					5
				]
			]
		}
	},
	{
		"id": "abbassiTrimmingRiskReliable2024",
		"type": "article",
		"abstract": "The industry increasingly relies on deep learning (DL) technology for manufacturing inspections, which are challenging to automate with rule-based machine vision algorithms. DL-powered inspection systems derive defect patterns from labeled images, combining human-like agility with the consistency of a computerized system. However, finite labeled datasets often fail to encompass all natural variations necessitating Continuous Training (CT) to regularly adjust their models with recent data. Effective CT requires fresh labeled samples from the original distribution; otherwise, selfgenerated labels can lead to silent performance degradation. To mitigate this risk, we develop a robust CT-based maintenance approach that updates DL models using reliable data selections through a two-stage filtering process. The initial stage filters out low-confidence predictions, as the model inherently discredits them. The second stage uses variational auto-encoders and histograms to generate image embeddings that capture latent and pixel characteristics, then rejects the inputs of substantially shifted embeddings as drifted data with erroneous overconfidence. Then, a fine-tuning of the original DL model is executed on the filtered inputs while validating on a mixture of recent production and original datasets. This strategy mitigates catastrophic forgetting and ensures the model adapts effectively to new operational conditions. Evaluations on industrial inspection systems for popsicle stick prints and glass bottles using critical real-world datasets showed less than 9% of erroneous self-labeled data are retained after filtering and used for fine-tuning, improving model performance on production data by up to 14% without compromising its results on original validation data.",
		"DOI": "10.48550/arXiv.2409.09108",
		"note": "arXiv:2409.09108",
		"number": "arXiv:2409.09108",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Trimming the Risk: Towards Reliable Continuous Training for Deep Learning Inspection Systems",
		"title-short": "Trimming the Risk",
		"URL": "http://arxiv.org/abs/2409.09108",
		"author": [
			{
				"family": "Abbassi",
				"given": "Altaf Allah"
			},
			{
				"family": "Braiek",
				"given": "Houssem Ben"
			},
			{
				"family": "Khomh",
				"given": "Foutse"
			},
			{
				"family": "Reid",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					13
				]
			]
		}
	},
	{
		"id": "darvishibayaziWhenMachinesOutshine2024",
		"type": "article-journal",
		"abstract": "In the field of vision science, recent endeavours have aimed to assess the comparative performance of artificial neural network models against human vision. Methodologies often involve the utilization of benchmarks that intentionally perturb or disturb images, thereby measuring noise sensitivity to gain insights into important features for object recognition. Recent studies employing critical frequency band masking have unveiled a perspective, positing that neural networks strategically exploit a wider band and less stable frequency channel compared to the one-octave band of human vision. In this work, we extend the inquiry to encompass diverse modern computer vision models, it becomes apparent that a considerable number of recently developed models outperform human capabilities in the presence of frequency noise. This ascendancy is not merely attributable to conventional techniques such as input image data augmentation but also crucially stems from the proficient exploitation of semantic information within expansive datasets, coupled with rigorous model scaling. Conceiving semantic information from multimodal training as a variant of output augmentation, we posit that augmenting input images and labels holds the potential to improve artificial neural networks to go beyond human performance in the current benchmarks. These advantages establish the idea that these models can be complementary agents for humans, particularly in challenging conditions. Despite acknowledging this progress, we must recognize a limitation in computer vision benchmarks, as they do not comprehensively quantify human vision. Consequently, we emphasize the imperative for vision science-inspired datasets to measure the alignment between models and human vision.",
		"container-title": "Journal of Vision",
		"DOI": "10.1167/jov.24.10.1523",
		"ISSN": "1534-7362",
		"issue": "10",
		"journalAbbreviation": "Journal of Vision",
		"page": "1523",
		"source": "Silverchair",
		"title": "When Machines Outshine Humans in Object Recognition, Benchmarking Dilemma",
		"URL": "https://doi.org/10.1167/jov.24.10.1523",
		"volume": "24",
		"author": [
			{
				"family": "Darvishi Bayazi",
				"given": "Mohammad Javad"
			},
			{
				"family": "Arefin",
				"given": "Md Rifat"
			},
			{
				"family": "Faubert",
				"given": "Jocelyn"
			},
			{
				"family": "rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					15
				]
			]
		}
	},
	{
		"id": "diazRethinkingTeacherStudentCurriculum2024",
		"type": "article",
		"abstract": "Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning. It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences. Despite its success, understanding the conditions under which TSCL is effective remains challenging. In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL. We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that is found by TSCL approaches. To do so, we demonstrate that for every TSCL problem, an equivalent cooperative game exists, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles. The framework and experimental setup we present in this work represents a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning.",
		"DOI": "10.48550/arXiv.2404.03084",
		"note": "arXiv:2404.03084",
		"number": "arXiv:2404.03084",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience",
		"URL": "http://arxiv.org/abs/2404.03084",
		"author": [
			{
				"family": "Diaz",
				"given": "Manfred"
			},
			{
				"family": "Paull",
				"given": "Liam"
			},
			{
				"family": "Tacchetti",
				"given": "Andrea"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					12
				]
			]
		}
	},
	{
		"id": "zhangChainBuddyAIAgent2024",
		"type": "article",
		"abstract": "As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-specific tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the \"blank page\" problem. ChainBuddy, an AI assistant for generating evaluative LLM pipelines built into the ChainForge platform, aims to tackle this issue. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior, making the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants reported a less demanding workload and felt more confident setting up evaluation pipelines of LLM behavior. We derive insights for the future of interfaces that assist users in the open-ended evaluation of AI.",
		"DOI": "10.48550/arXiv.2409.13588",
		"note": "arXiv:2409.13588",
		"number": "arXiv:2409.13588",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ChainBuddy: An AI Agent System for Generating LLM Pipelines",
		"title-short": "ChainBuddy",
		"URL": "http://arxiv.org/abs/2409.13588",
		"author": [
			{
				"family": "Zhang",
				"given": "Jingyue"
			},
			{
				"family": "Arawjo",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					20
				]
			]
		}
	},
	{
		"id": "knottAIContentDetection2024",
		"type": "article-journal",
		"abstract": "The world is about to be swamped by an unprecedented wave of AI-generated content. We need reliable ways of identifying such content, to supplement the many existing social institutions that enable trust between people and organisations and ensure social resilience. In this paper, we begin by highlighting an important new development: providers of AI content generators have new obligations to support the creation of reliable detectors for the content they generate. These new obligations arise mainly from the EU’s newly finalised AI Act, but they are enhanced by the US President’s recent Executive Order on AI, and by several considerations of self-interest. These new steps towards reliable detection mechanisms are by no means a panacea—but we argue they will usher in a new adversarial landscape, in which reliable methods for identifying AI-generated content are commonly available. In this landscape, many new questions arise for policymakers. Firstly, if reliable AI-content detection mechanisms are available, who should be required to use them? And how should they be used? We argue that new duties arise for media and Web search companies arise for media companies, and for Web search companies, in the deployment of AI-content detectors. Secondly, what broader regulation of the tech ecosystem will maximise the likelihood of reliable AI-content detectors? We argue for a range of new duties, relating to provenance-authentication protocols, open-source AI generators, and support for research and enforcement. Along the way, we consider how the production of AI-generated content relates to ‘free expression’, and discuss the important case of content that is generated jointly by humans and AIs.",
		"container-title": "Ethics and Information Technology",
		"DOI": "10.1007/s10676-024-09795-1",
		"ISSN": "1572-8439",
		"issue": "4",
		"journalAbbreviation": "Ethics Inf Technol",
		"language": "en",
		"page": "63",
		"source": "Springer Link",
		"title": "AI content detection in the emerging information ecosystem: new obligations for media and tech companies",
		"title-short": "AI content detection in the emerging information ecosystem",
		"URL": "https://doi.org/10.1007/s10676-024-09795-1",
		"volume": "26",
		"author": [
			{
				"family": "Knott",
				"given": "Alistair"
			},
			{
				"family": "Pedreschi",
				"given": "Dino"
			},
			{
				"family": "Jitsuzumi",
				"given": "Toshiya"
			},
			{
				"family": "Leavy",
				"given": "Susan"
			},
			{
				"family": "Eyers",
				"given": "David"
			},
			{
				"family": "Chakraborti",
				"given": "Tapabrata"
			},
			{
				"family": "Trotman",
				"given": "Andrew"
			},
			{
				"family": "Sundareswaran",
				"given": "Sundar"
			},
			{
				"family": "Baeza-Yates",
				"given": "Ricardo"
			},
			{
				"family": "Biecek",
				"given": "Przemyslaw"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Teal",
				"given": "Paul D."
			},
			{
				"family": "Basu",
				"given": "Subhadip"
			},
			{
				"family": "Haklidir",
				"given": "Mehmet"
			},
			{
				"family": "Morini",
				"given": "Virginia"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					21
				]
			]
		}
	},
	{
		"id": "yangToxiSightInsightsDetected2024",
		"type": "paper-conference",
		"abstract": "We present a comprehensive explainability dashboard designed for in-game chat toxicity. This dashboard integrates various existing explainable AI (XAI) techniques, including token importance analysis, model output visualization, and attribution to the training dataset. It also provides insights through the closest positive and negative examples, facilitating a deeper understanding and potential correction of the training data. Additionally, the dashboard includes word sense analysis—particularly useful for new moderators—and offers free-text explanations for both positive and negative predictions. This multi-faceted approach enhances the interpretability and transparency of toxicity detection models.",
		"event-title": "The 7th BlackboxNLP Workshop",
		"language": "en",
		"source": "openreview.net",
		"title": "ToxiSight: Insights Towards Detected Chat Toxicity",
		"title-short": "ToxiSight",
		"URL": "https://openreview.net/forum?id=iL6zxTh2HW",
		"author": [
			{
				"family": "Yang",
				"given": "Zachary"
			},
			{
				"family": "Tullo",
				"given": "Domenico"
			},
			{
				"family": "Rabbany",
				"given": "Reihaneh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					21
				]
			]
		}
	},
	{
		"id": "hameedNotOnlyLastLayer2024",
		"type": "article",
		"abstract": "Spurious correlations are a major source of errors for machine learning models, in particular when aiming for group-level fairness. It has been recently shown that a powerful approach to combat spurious correlations is to re-train the last layer on a balanced validation dataset, isolating robust features for the predictor. However, key attributes can sometimes be discarded by neural networks towards the last layer. In this work, we thus consider retraining a classifier on a set of features derived from all layers. We utilize a recently proposed feature selection strategy to select unbiased features from all the layers. We observe this approach gives significant improvements in worst-group accuracy on several standard benchmarks.",
		"DOI": "10.48550/arXiv.2409.14637",
		"note": "arXiv:2409.14637",
		"number": "arXiv:2409.14637",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Not Only the Last-Layer Features for Spurious Correlations: All Layer Deep Feature Reweighting",
		"title-short": "Not Only the Last-Layer Features for Spurious Correlations",
		"URL": "http://arxiv.org/abs/2409.14637",
		"author": [
			{
				"family": "Hameed",
				"given": "Humza Wajid"
			},
			{
				"family": "Nanfack",
				"given": "Geraldin"
			},
			{
				"family": "Belilovsky",
				"given": "Eugene"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					23
				]
			]
		}
	},
	{
		"id": "darrinWhenEmbeddingModel2024",
		"type": "article",
		"abstract": "Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks. The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison. However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming. In this paper, we present a unified approach to evaluate embedders. First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness. We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure. We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology. This effectively offers practitioners a valuable tool for prioritizing model trials.",
		"DOI": "10.48550/arXiv.2406.07640",
		"note": "arXiv:2406.07640",
		"number": "arXiv:2406.07640",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "When is an Embedding Model More Promising than Another?",
		"URL": "http://arxiv.org/abs/2406.07640",
		"author": [
			{
				"family": "Darrin",
				"given": "Maxime"
			},
			{
				"family": "Formont",
				"given": "Philippe"
			},
			{
				"family": "Ayed",
				"given": "Ismail Ben"
			},
			{
				"family": "Cheung",
				"given": "Jackie CK"
			},
			{
				"family": "Piantanida",
				"given": "Pablo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					11
				]
			]
		}
	},
	{
		"id": "altstidlScalabilityCertifiedAdversarial2024",
		"type": "paper-conference",
		"abstract": "Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\\ell_2$ ($\\epsilon = 36/255$) and $\\ell_{\\infty}$ ($\\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "On the Scalability of Certified Adversarial Robustness with Generated Data",
		"URL": "https://openreview.net/forum?id=TFAG9UznPv",
		"author": [
			{
				"family": "Altstidl",
				"given": "Thomas"
			},
			{
				"family": "Dobre",
				"given": "David"
			},
			{
				"family": "Kosmala",
				"given": "Arthur"
			},
			{
				"family": "Eskofier",
				"given": "Bjoern"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			},
			{
				"family": "Schwinn",
				"given": "Leo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "collins-woodfinHighLineExact2024",
		"type": "paper-conference",
		"abstract": "We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provide our code for evaluation at https://github.com/amackenzie1/highline2024.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms",
		"title-short": "The High Line",
		"URL": "https://openreview.net/forum?id=4VWnC5unAV",
		"author": [
			{
				"family": "Collins-Woodfin",
				"given": "Elizabeth"
			},
			{
				"family": "Seroussi",
				"given": "Inbar"
			},
			{
				"family": "Malaxechebarría",
				"given": "Begoña García"
			},
			{
				"family": "Mackenzie",
				"given": "Andrew"
			},
			{
				"family": "Paquette",
				"given": "Elliot"
			},
			{
				"family": "Paquette",
				"given": "Courtney"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "kitouniFactorizationCurseWhich2024",
		"type": "paper-conference",
		"abstract": "Today's best language models still struggle with \"hallucinations\", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The *reversal curse*, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval. To better understand these limitations, we reframe the reversal curse as a *factorization curse* --- a failure of models to learn the same joint distribution under different factorizations. We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing *WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
		"title-short": "The Factorization Curse",
		"URL": "https://openreview.net/forum?id=f70e6YYFHF",
		"author": [
			{
				"family": "Kitouni",
				"given": "Ouail"
			},
			{
				"family": "Nolte",
				"given": "Niklas"
			},
			{
				"family": "Williams",
				"given": "Adina"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			},
			{
				"family": "Ibrahim",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "schwinnSoftPromptThreats2024",
		"type": "article",
		"abstract": "Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. Trigger Warning: the appendix contains LLM-generated text with violence and harassment.",
		"DOI": "10.48550/arXiv.2402.09063",
		"note": "arXiv:2402.09063",
		"number": "arXiv:2402.09063",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
		"title-short": "Soft Prompt Threats",
		"URL": "http://arxiv.org/abs/2402.09063",
		"author": [
			{
				"family": "Schwinn",
				"given": "Leo"
			},
			{
				"family": "Dobre",
				"given": "David"
			},
			{
				"family": "Xhonneux",
				"given": "Sophie"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			},
			{
				"family": "Gunnemann",
				"given": "Stephan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					14
				]
			]
		}
	},
	{
		"id": "ferbachSelfConsumingGenerativeModels2024",
		"type": "article",
		"abstract": "The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models. Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step. However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users. In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an \\emph{implicit preference optimization mechanism}. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.",
		"DOI": "10.48550/arXiv.2407.09499",
		"note": "arXiv:2407.09499",
		"number": "arXiv:2407.09499",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences",
		"URL": "http://arxiv.org/abs/2407.09499",
		"author": [
			{
				"family": "Ferbach",
				"given": "Damien"
			},
			{
				"family": "Bertrand",
				"given": "Quentin"
			},
			{
				"family": "Bose",
				"given": "Avishek Joey"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					12
				]
			]
		}
	},
	{
		"id": "lauQGFNControllableGreediness2024",
		"type": "article",
		"abstract": "Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.",
		"DOI": "10.48550/arXiv.2402.05234",
		"note": "arXiv:2402.05234",
		"number": "arXiv:2402.05234",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "QGFN: Controllable Greediness with Action Values",
		"title-short": "QGFN",
		"URL": "http://arxiv.org/abs/2402.05234",
		"author": [
			{
				"family": "Lau",
				"given": "Elaine"
			},
			{
				"family": "Lu",
				"given": "Stephen Zhewen"
			},
			{
				"family": "Pan",
				"given": "Ling"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Bengio",
				"given": "Emmanuel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					1
				]
			]
		}
	},
	{
		"id": "chungPredictingFutureActions2024",
		"type": "paper-conference",
		"abstract": "As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Predicting Future Actions of Reinforcement Learning Agents",
		"URL": "https://openreview.net/forum?id=QgaGs7peYe",
		"author": [
			{
				"family": "Chung",
				"given": "Stephen"
			},
			{
				"family": "Niekum",
				"given": "Scott"
			},
			{
				"family": "Krueger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "chungParsevalRegularizationContinual2024",
		"type": "paper-conference",
		"abstract": "Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Parseval Regularization for Continual Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=RB1F2h5YEx",
		"author": [
			{
				"family": "Chung",
				"given": "Wesley"
			},
			{
				"family": "Cherif",
				"given": "Lynn"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Meger",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "ishfaqOfflineMultitaskRepresentation2024",
		"type": "article",
		"abstract": "We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.",
		"DOI": "10.48550/arXiv.2403.11574",
		"note": "arXiv:2403.11574",
		"number": "arXiv:2403.11574",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Offline Multitask Representation Learning for Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2403.11574",
		"author": [
			{
				"family": "Ishfaq",
				"given": "Haque"
			},
			{
				"family": "Nguyen-Tang",
				"given": "Thanh"
			},
			{
				"family": "Feng",
				"given": "Songtao"
			},
			{
				"family": "Arora",
				"given": "Raman"
			},
			{
				"family": "Wang",
				"given": "Mengdi"
			},
			{
				"family": "Yin",
				"given": "Ming"
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					31
				]
			]
		}
	},
	{
		"id": "lyleNormalizationEffectiveLearning2024",
		"type": "article",
		"abstract": "Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting effective learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.",
		"DOI": "10.48550/arXiv.2407.01800",
		"note": "arXiv:2407.01800",
		"number": "arXiv:2407.01800",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Normalization and effective learning rates in reinforcement learning",
		"URL": "http://arxiv.org/abs/2407.01800",
		"author": [
			{
				"family": "Lyle",
				"given": "Clare"
			},
			{
				"family": "Zheng",
				"given": "Zeyu"
			},
			{
				"family": "Khetarpal",
				"given": "Khimya"
			},
			{
				"family": "Martens",
				"given": "James"
			},
			{
				"family": "Hasselt",
				"given": "Hado",
				"dropping-particle": "van"
			},
			{
				"family": "Pascanu",
				"given": "Razvan"
			},
			{
				"family": "Dabney",
				"given": "Will"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					1
				]
			]
		}
	},
	{
		"id": "didolkarMetacognitiveCapabilitiesLLMs2024",
		"type": "article",
		"abstract": "Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",
		"DOI": "10.48550/arXiv.2405.12205",
		"note": "arXiv:2405.12205",
		"number": "arXiv:2405.12205",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
		"title-short": "Metacognitive Capabilities of LLMs",
		"URL": "http://arxiv.org/abs/2405.12205",
		"author": [
			{
				"family": "Didolkar",
				"given": "Aniket"
			},
			{
				"family": "Goyal",
				"given": "Anirudh"
			},
			{
				"family": "Ke",
				"given": "Nan Rosemary"
			},
			{
				"family": "Guo",
				"given": "Siyuan"
			},
			{
				"family": "Valko",
				"given": "Michal"
			},
			{
				"family": "Lillicrap",
				"given": "Timothy"
			},
			{
				"family": "Rezende",
				"given": "Danilo"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Mozer",
				"given": "Michael"
			},
			{
				"family": "Arora",
				"given": "Sanjeev"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					20
				]
			]
		}
	},
	{
		"id": "agarwalManyShotContextLearning2024",
		"type": "article",
		"abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.",
		"DOI": "10.48550/arXiv.2404.11018",
		"note": "arXiv:2404.11018",
		"number": "arXiv:2404.11018",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Many-Shot In-Context Learning",
		"URL": "http://arxiv.org/abs/2404.11018",
		"author": [
			{
				"family": "Agarwal",
				"given": "Rishabh"
			},
			{
				"family": "Singh",
				"given": "Avi"
			},
			{
				"family": "Zhang",
				"given": "Lei M."
			},
			{
				"family": "Bohnet",
				"given": "Bernd"
			},
			{
				"family": "Rosias",
				"given": "Luis"
			},
			{
				"family": "Chan",
				"given": "Stephanie"
			},
			{
				"family": "Zhang",
				"given": "Biao"
			},
			{
				"family": "Anand",
				"given": "Ankesh"
			},
			{
				"family": "Abbas",
				"given": "Zaheer"
			},
			{
				"family": "Nova",
				"given": "Azade"
			},
			{
				"family": "Co-Reyes",
				"given": "John D."
			},
			{
				"family": "Chu",
				"given": "Eric"
			},
			{
				"family": "Behbahani",
				"given": "Feryal"
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			},
			{
				"family": "Larochelle",
				"given": "Hugo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					17
				]
			]
		}
	},
	{
		"id": "liLLMsBuildWorld2024",
		"type": "paper-conference",
		"abstract": "How do large language models (LLMs) encode the state of the world, including the status of entities and their relations, as described by a text? While existing work directly probes for a complete state of the world, our research explores whether and how LLMs abstract this world state in their internal representations. We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks. To instantiate this framework, we design a text-based planning task, where an LLM acts as an agent in an environment and interacts with objects in containers to achieve a specified goal state. Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Do LLMs Build World Representations? Probing Through the Lens of State Abstraction",
		"title-short": "Do LLMs Build World Representations?",
		"URL": "https://openreview.net/forum?id=lzfzjYuWgY",
		"author": [
			{
				"family": "Li",
				"given": "Zichao"
			},
			{
				"family": "Cao",
				"given": "Yanshuai"
			},
			{
				"family": "Cheung",
				"given": "Jackie CK"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "paissanListenableMapsZeroShot2024",
		"type": "article",
		"abstract": "Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio Classifiers in the Zero-Shot context), which, to the best of our knowledge, is the first decoder-based post-hoc interpretation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that maximizes the faithfulness to the original similarity between a given text-and-audio pair. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.",
		"DOI": "10.48550/arXiv.2405.17615",
		"note": "arXiv:2405.17615",
		"number": "arXiv:2405.17615",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Listenable Maps for Zero-Shot Audio Classifiers",
		"URL": "http://arxiv.org/abs/2405.17615",
		"author": [
			{
				"family": "Paissan",
				"given": "Francesco"
			},
			{
				"family": "Libera",
				"given": "Luca Della"
			},
			{
				"family": "Ravanelli",
				"given": "Mirco"
			},
			{
				"family": "Subakan",
				"given": "Cem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					27
				]
			]
		}
	},
	{
		"id": "chuaLearningSuccessorFeatures2024",
		"type": "paper-conference",
		"abstract": "In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid) and 3D (Miniworld) mazes, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Learning Successor Features the Simple Way",
		"URL": "https://openreview.net/forum?id=rI7oZj1WMc",
		"author": [
			{
				"family": "Chua",
				"given": "Raymond"
			},
			{
				"family": "Ghosh",
				"given": "Arna"
			},
			{
				"family": "Kaplanis",
				"given": "Christos"
			},
			{
				"family": "Richards",
				"given": "Blake Aaron"
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "marksInterpretingLearnedFeedback2024",
		"type": "paper-conference",
		"abstract": "Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the **safety** and **alignment** of LLMs.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Interpreting Learned Feedback Patterns in Large Language Models",
		"URL": "https://openreview.net/forum?id=xUoNgR1Byy",
		"author": [
			{
				"family": "Marks",
				"given": "Luke"
			},
			{
				"family": "Abdullah",
				"given": "Amir"
			},
			{
				"family": "Neo",
				"given": "Clement"
			},
			{
				"family": "Arike",
				"given": "Rauno"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Torr",
				"given": "Philip"
			},
			{
				"family": "Barez",
				"given": "Fazl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "tangImprovingDeepReinforcement2024",
		"type": "article",
		"abstract": "Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings, as well as a scaling setting.",
		"DOI": "10.48550/arXiv.2409.04792",
		"note": "arXiv:2409.04792",
		"number": "arXiv:2409.04792",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn",
		"URL": "http://arxiv.org/abs/2409.04792",
		"author": [
			{
				"family": "Tang",
				"given": "Hongyao"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					7
				]
			]
		}
	},
	{
		"id": "senderaImprovedPolicyTraining2024",
		"type": "paper-conference",
		"abstract": "We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for future work on diffusion models for amortized inference.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Improved off-policy training of diffusion samplers",
		"URL": "https://openreview.net/forum?id=vieIamY2Gi",
		"author": [
			{
				"family": "Sendera",
				"given": "Marcin"
			},
			{
				"family": "Kim",
				"given": "Minsu"
			},
			{
				"family": "Mittal",
				"given": "Sarthak"
			},
			{
				"family": "Lemos",
				"given": "Pablo"
			},
			{
				"family": "Scimeca",
				"given": "Luca"
			},
			{
				"family": "Rector-Brooks",
				"given": "Jarrid"
			},
			{
				"family": "Adam",
				"given": "Alexandre"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Malkin",
				"given": "Nikolay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "pitisImprovingContextAwarePreference2024",
		"type": "article",
		"abstract": "While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.",
		"DOI": "10.48550/arXiv.2407.14916",
		"note": "arXiv:2407.14916",
		"number": "arXiv:2407.14916",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Context-Aware Preference Modeling for Language Models",
		"URL": "http://arxiv.org/abs/2407.14916",
		"author": [
			{
				"family": "Pitis",
				"given": "Silviu"
			},
			{
				"family": "Xiao",
				"given": "Ziang"
			},
			{
				"family": "Roux",
				"given": "Nicolas Le"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "ghoshHarnessingSmallProjectors2024",
		"type": "paper-conference",
		"abstract": "Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions. However, there are few theoretically grounded principles to guide practice, so practical implementation of each SSL framework requires several heuristics to achieve competitive performance. In this work, we build on recent analytical results to design practical recommendations for competitive and efficient SSL that are grounded in theory. Specifically, recent theory tells us that existing SSL frameworks are actually minimizing the same idealized loss, which is to learn features that best match the data similarity kernel defined by the augmentations used. We show how this idealized loss can be reformulated to a functionally equivalent loss that is more efficient to compute. We study the implicit bias of using gradient descent to minimize our reformulated loss function, and find that using a stronger orthogonalization constraint with a reduced projector dimensionality should yield good representations. Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence. We empirically verify our findings on CIFAR, STL and Imagenet datasets, wherein we demonstrate an improved linear readout performance when training a ResNet-backbone using our theoretically grounded recommendations. Remarkably, we also demonstrate that by leveraging these insights, we can reduce the pretraining dataset size by up to 2$\\times$ while maintaining downstream accuracy simply by using more data augmentations. Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Harnessing small projectors and multiple views for efficient vision pretraining",
		"URL": "https://openreview.net/forum?id=Y5DPSJzpra",
		"author": [
			{
				"family": "Ghosh",
				"given": "Arna"
			},
			{
				"family": "Agrawal",
				"given": "Kumar Krishna"
			},
			{
				"family": "Sodhani",
				"given": "Shagun"
			},
			{
				"family": "Oberman",
				"given": "Adam"
			},
			{
				"family": "Richards",
				"given": "Blake Aaron"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "szotGroundingMultimodalLarge2024",
		"type": "article",
		"abstract": "Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.",
		"DOI": "10.48550/arXiv.2406.07904",
		"note": "arXiv:2406.07904",
		"number": "arXiv:2406.07904",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Grounding Multimodal Large Language Models in Actions",
		"URL": "http://arxiv.org/abs/2406.07904",
		"author": [
			{
				"family": "Szot",
				"given": "Andrew"
			},
			{
				"family": "Mazoure",
				"given": "Bogdan"
			},
			{
				"family": "Agrawal",
				"given": "Harsh"
			},
			{
				"family": "Hjelm",
				"given": "Devon"
			},
			{
				"family": "Kira",
				"given": "Zsolt"
			},
			{
				"family": "Toshev",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					12
				]
			]
		}
	},
	{
		"id": "mazzagliaGenRLMultimodalfoundationWorld2024",
		"type": "paper-conference",
		"abstract": "Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "GenRL: Multimodal-foundation world models for generalization in embodied agents",
		"title-short": "GenRL",
		"URL": "https://openreview.net/forum?id=za9Jx8yqUA",
		"author": [
			{
				"family": "Mazzaglia",
				"given": "Pietro"
			},
			{
				"family": "Verbelen",
				"given": "Tim"
			},
			{
				"family": "Dhoedt",
				"given": "Bart"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Rajeswar",
				"given": "Sai"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "arnobEfficientReinforcementLearning2024",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5% of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Efficient Reinforcement Learning by Discovering Neural Pathways",
		"URL": "https://openreview.net/forum?id=WEoOreP0n5",
		"author": [
			{
				"family": "Arnob",
				"given": "Samin Yeasar"
			},
			{
				"family": "Ohib",
				"given": "Riyasat"
			},
			{
				"family": "Plis",
				"given": "Sergey M."
			},
			{
				"family": "Zhang",
				"given": "Amy"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "ngnaweDetectingBrittleDecisions2024",
		"type": "article",
		"abstract": "Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively and confidently use the logit margin to detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to assess adversarial vulnerability in deployment scenarios efficiently.",
		"DOI": "10.48550/arXiv.2406.18451",
		"note": "arXiv:2406.18451",
		"number": "arXiv:2406.18451",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers",
		"title-short": "Detecting Brittle Decisions for Free",
		"URL": "http://arxiv.org/abs/2406.18451",
		"author": [
			{
				"family": "Ngnawé",
				"given": "Jonas"
			},
			{
				"family": "Sahoo",
				"given": "Sabyasachi"
			},
			{
				"family": "Pequignot",
				"given": "Yann"
			},
			{
				"family": "Precioso",
				"given": "Frédéric"
			},
			{
				"family": "Gagné",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					1
				]
			]
		}
	},
	{
		"id": "xhonneuxEfficientAdversarialTraining2024",
		"type": "article",
		"abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.",
		"DOI": "10.48550/arXiv.2405.15589",
		"note": "arXiv:2405.15589",
		"number": "arXiv:2405.15589",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
		"URL": "http://arxiv.org/abs/2405.15589",
		"author": [
			{
				"family": "Xhonneux",
				"given": "Sophie"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			},
			{
				"family": "Günnemann",
				"given": "Stephan"
			},
			{
				"family": "Gidel",
				"given": "Gauthier"
			},
			{
				"family": "Schwinn",
				"given": "Leo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					1
				]
			]
		}
	},
	{
		"id": "linConformalInverseOptimization2024",
		"type": "paper-conference",
		"abstract": "Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data. We show that such a point estimation is insufficient in a prescriptive setting where the estimated parameters are used to prescribe new decisions. The prescribed decisions may be low-quality and misaligned with human intuition and thus are unlikely to be adopted. To tackle this challenge, we propose conformal inverse optimization, which seeks to learn an uncertainty set for the unknown parameters and then solve a robust optimization model to prescribe new decisions. Under mild assumptions, we show that our method enjoys provable guarantees on solution quality, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters. Our method demonstrates strong empirical performance compared to classic inverse optimization.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Conformal Inverse Optimization",
		"URL": "https://openreview.net/forum?id=Y2NWKlrDrX",
		"author": [
			{
				"family": "Lin",
				"given": "Bo"
			},
			{
				"family": "Delage",
				"given": "Erick"
			},
			{
				"family": "Chan",
				"given": "Timothy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "tangCodeRepairLLMs2024",
		"type": "paper-conference",
		"abstract": "Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.",
		"event-title": "The Thirty-eighth Annual Conference on Neural Information Processing Systems",
		"language": "en",
		"source": "openreview.net",
		"title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
		"URL": "https://openreview.net/forum?id=o863gX6DxA",
		"author": [
			{
				"family": "Tang",
				"given": "Hao"
			},
			{
				"family": "Hu",
				"given": "Keya"
			},
			{
				"family": "Zhou",
				"given": "Jin Peng"
			},
			{
				"family": "Zhong",
				"given": "Si Cheng"
			},
			{
				"family": "Zheng",
				"given": "Wei-Long"
			},
			{
				"family": "Si",
				"given": "Xujie"
			},
			{
				"family": "Ellis",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "venkatramanAmortizingIntractableInference2024",
		"type": "article",
		"abstract": "Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm post}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\\mathbf{x})$ and a black-box constraint or likelihood function $r(\\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.",
		"DOI": "10.48550/arXiv.2405.20971",
		"note": "arXiv:2405.20971",
		"number": "arXiv:2405.20971",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Amortizing intractable inference in diffusion models for vision, language, and control",
		"URL": "http://arxiv.org/abs/2405.20971",
		"author": [
			{
				"family": "Venkatraman",
				"given": "Siddarth"
			},
			{
				"family": "Jain",
				"given": "Moksh"
			},
			{
				"family": "Scimeca",
				"given": "Luca"
			},
			{
				"family": "Kim",
				"given": "Minsu"
			},
			{
				"family": "Sendera",
				"given": "Marcin"
			},
			{
				"family": "Hasan",
				"given": "Mohsin"
			},
			{
				"family": "Rowe",
				"given": "Luke"
			},
			{
				"family": "Mittal",
				"given": "Sarthak"
			},
			{
				"family": "Lemos",
				"given": "Pablo"
			},
			{
				"family": "Bengio",
				"given": "Emmanuel"
			},
			{
				"family": "Adam",
				"given": "Alexandre"
			},
			{
				"family": "Rector-Brooks",
				"given": "Jarrid"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			},
			{
				"family": "Malkin",
				"given": "Nikolay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					31
				]
			]
		}
	},
	{
		"id": "jainAdaptiveExplorationDataEfficient2024",
		"type": "article",
		"abstract": "General Value Functions (GVFs) (Sutton et al., 2011) represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique reward. Existing methods relying on fixed behavior policies or pre-collected data often face data efficiency issues when learning multiple GVFs in parallel using off-policy methods. To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel. Our method optimizes the behavior policy by minimizing the total variance in return across GVFs, thereby reducing the required environmental interactions. We use an existing temporal-difference-style variance estimator to approximate the return variance. We prove that each behavior policy update decreases the overall mean squared error in GVF predictions. We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.",
		"DOI": "10.48550/arXiv.2405.07838",
		"note": "arXiv:2405.07838",
		"number": "arXiv:2405.07838",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adaptive Exploration for Data-Efficient General Value Function Evaluations",
		"URL": "http://arxiv.org/abs/2405.07838",
		"author": [
			{
				"family": "Jain",
				"given": "Arushi"
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			},
			{
				"family": "Precup",
				"given": "Doina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					13
				]
			]
		}
	},
	{
		"id": "fengWereRNNsAll2024",
		"type": "article",
		"abstract": "The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.",
		"DOI": "10.48550/arXiv.2410.01201",
		"note": "arXiv:2410.01201",
		"number": "arXiv:2410.01201",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Were RNNs All We Needed?",
		"URL": "http://arxiv.org/abs/2410.01201",
		"author": [
			{
				"family": "Feng",
				"given": "Leo"
			},
			{
				"family": "Tung",
				"given": "Frederick"
			},
			{
				"family": "Ahmed",
				"given": "Mohamed Osama"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Hajimirsadegh",
				"given": "Hossein"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					4
				]
			]
		}
	},
	{
		"id": "kimAdaptiveTeachersAmortized2024",
		"type": "article",
		"abstract": "Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is implemented as a sequential decision-making process, reinforcement learning (RL) methods, such as generative flow networks, can be used to train the sampling policy. Off-policy RL training facilitates the discovery of diverse, high-reward candidates, but existing methods still face challenges in efficient exploration. We propose to use an adaptive training distribution (the Teacher) to guide the training of the primary amortized sampler (the Student) by prioritizing high-loss regions. The Teacher, an auxiliary behavior model, is trained to sample high-error regions of the Student and can generalize across unexplored modes, thereby enhancing mode coverage by providing an efficient training curriculum. We validate the effectiveness of this approach in a synthetic environment designed to present an exploration challenge, two diffusion-based sampling tasks, and four biochemical discovery tasks demonstrating its ability to improve sample efficiency and mode coverage.",
		"DOI": "10.48550/arXiv.2410.01432",
		"note": "arXiv:2410.01432",
		"number": "arXiv:2410.01432",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adaptive teachers for amortized samplers",
		"URL": "http://arxiv.org/abs/2410.01432",
		"author": [
			{
				"family": "Kim",
				"given": "Minsu"
			},
			{
				"family": "Choi",
				"given": "Sanghyeok"
			},
			{
				"family": "Yun",
				"given": "Taeyoung"
			},
			{
				"family": "Bengio",
				"given": "Emmanuel"
			},
			{
				"family": "Feng",
				"given": "Leo"
			},
			{
				"family": "Rector-Brooks",
				"given": "Jarrid"
			},
			{
				"family": "Ahn",
				"given": "Sungsoo"
			},
			{
				"family": "Park",
				"given": "Jinkyoo"
			},
			{
				"family": "Malkin",
				"given": "Nikolay"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					2
				]
			]
		}
	},
	{
		"id": "fanEfficientDesignControlAutomation2024",
		"type": "paper-conference",
		"abstract": "Seeking good designs is a central goal of many important domains, such as robotics, integrated circuits (IC), medicine, and materials science. These design problems are expensive, time-consuming, and traditionally performed by human experts. Moreover, the barriers to domain knowledge make it challenging to propose a universal solution that generalizes to different design problems. In this paper, we propose a new method called Efficient Design and Stable Control (EDiSon) for automatic design and control in different design problems. The key ideas of our method are (1) interactive sequential modeling of the design and control process and (2) adaptive exploration and design replay. To decompose the difficulty of learning design and control as a whole, we leverage sequential modeling for both the design process and control process, with a design policy to generate step-by-step design proposals and a control policy to optimize the objective by operating the design. With deep reinforcement learning (RL), the policies learn to find good designs by maximizing a reward signal that evaluates the quality of designs. Furthermore, we propose an adaptive exploration and replay mechanism based on a design memory that maintains high-quality designs generated so far. By regulating between constructing a design from scratch or replaying a design from memory to refine it, EDiSon balances the trade-off between exploration and exploitation in the design space and stabilizes the learning of the control policy. In the experiments, we evaluate our method in robotic morphology design and Tetris-based design tasks. Our framework has the potential to significantly accelerate the discovery of optimized designs across diverse domains, including automated materials discovery, by improving the exploration in design space while ensuring efficiency.",
		"event-title": "AI for Accelerated Materials Design - NeurIPS 2024",
		"language": "en",
		"source": "openreview.net",
		"title": "Efficient Design-and-Control Automation with Reinforcement Learning and Adaptive Exploration",
		"URL": "https://openreview.net/forum?id=stiehhc5y6",
		"author": [
			{
				"family": "Fan",
				"given": "Jiajun"
			},
			{
				"family": "Tang",
				"given": "Hongyao"
			},
			{
				"family": "Przystupa",
				"given": "Michael"
			},
			{
				"family": "Phielipp",
				"given": "Mariano"
			},
			{
				"family": "Miret",
				"given": "Santiago"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					3
				]
			]
		}
	},
	{
		"id": "klissarovModelingCapabilitiesLarge2024",
		"type": "article",
		"abstract": "Large pretrained models are showing increasingly better performance in reasoning and planning tasks across different modalities, opening the possibility to leverage them for complex sequential decision making problems. In this paper, we investigate the capabilities of Large Language Models (LLMs) for reinforcement learning (RL) across a diversity of interactive domains. We evaluate their ability to produce decision-making policies, either directly, by generating actions, or indirectly, by first generating reward models to train an agent with RL. Our results show that, even without task-specific fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards through artificial intelligence (AI) feedback yields the most generally applicable approach and can enhance performance by improving credit assignment and exploration. Finally, in environments with unfamiliar dynamics, we explore how fine-tuning LLMs with synthetic data can significantly improve their reward modeling capabilities while mitigating catastrophic forgetting, further broadening their utility in sequential decision-making tasks.",
		"DOI": "10.48550/arXiv.2410.05656",
		"note": "arXiv:2410.05656",
		"number": "arXiv:2410.05656",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making",
		"URL": "http://arxiv.org/abs/2410.05656",
		"author": [
			{
				"family": "Klissarov",
				"given": "Martin"
			},
			{
				"family": "Hjelm",
				"given": "Devon"
			},
			{
				"family": "Toshev",
				"given": "Alexander"
			},
			{
				"family": "Mazoure",
				"given": "Bogdan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					8
				]
			]
		}
	},
	{
		"id": "oncelAdaptationOdysseyLLMs2024",
		"type": "article",
		"abstract": "In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs. To this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity observations reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.",
		"DOI": "10.48550/arXiv.2410.05581",
		"note": "arXiv:2410.05581",
		"number": "arXiv:2410.05581",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?",
		"title-short": "Adaptation Odyssey in LLMs",
		"URL": "http://arxiv.org/abs/2410.05581",
		"author": [
			{
				"family": "Öncel",
				"given": "Fırat"
			},
			{
				"family": "Bethge",
				"given": "Matthias"
			},
			{
				"family": "Ermis",
				"given": "Beyza"
			},
			{
				"family": "Ravanelli",
				"given": "Mirco"
			},
			{
				"family": "Subakan",
				"given": "Cem"
			},
			{
				"family": "Yıldız",
				"given": "Çağatay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "mahajanCompositionalRiskMinimization2024",
		"type": "article",
		"abstract": "In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test distribution. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.",
		"DOI": "10.48550/arXiv.2410.06303",
		"note": "arXiv:2410.06303",
		"number": "arXiv:2410.06303",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Compositional Risk Minimization",
		"URL": "http://arxiv.org/abs/2410.06303",
		"author": [
			{
				"family": "Mahajan",
				"given": "Divyat"
			},
			{
				"family": "Pezeshki",
				"given": "Mohammad"
			},
			{
				"family": "Mitliagkas",
				"given": "Ioannis"
			},
			{
				"family": "Ahuja",
				"given": "Kartik"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					8
				]
			]
		}
	},
	{
		"id": "lanSparseAutoencodersReveal2024",
		"type": "article",
		"abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.",
		"DOI": "10.48550/arXiv.2410.06981",
		"note": "arXiv:2410.06981",
		"number": "arXiv:2410.06981",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models",
		"URL": "http://arxiv.org/abs/2410.06981",
		"author": [
			{
				"family": "Lan",
				"given": "Michael"
			},
			{
				"family": "Torr",
				"given": "Philip"
			},
			{
				"family": "Meek",
				"given": "Austin"
			},
			{
				"family": "Khakzar",
				"given": "Ashkan"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Barez",
				"given": "Fazl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					9
				]
			]
		}
	},
	{
		"id": "hosseiniNotAllLLM2024",
		"type": "article",
		"abstract": "We study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, we evaluate their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. Our findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. Our analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates.",
		"DOI": "10.48550/arXiv.2410.01748",
		"note": "arXiv:2410.01748",
		"number": "arXiv:2410.01748",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Not All LLM Reasoners Are Created Equal",
		"URL": "http://arxiv.org/abs/2410.01748",
		"author": [
			{
				"family": "Hosseini",
				"given": "Arian"
			},
			{
				"family": "Sordoni",
				"given": "Alessandro"
			},
			{
				"family": "Toyama",
				"given": "Daniel"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Agarwal",
				"given": "Rishabh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					2
				]
			]
		}
	},
	{
		"id": "tommasoLLMsPersonalitiesInconsistencies2024",
		"type": "paper-conference",
		"abstract": "This study investigates the application of human psychometric assessments to large language models (LLMs) to examine their consistency and malleability in exhibiting personality traits. We administered the Big Five Inventory (BFI) and the Eysenck Personality Questionnaire-Revised (EPQ-R) to various LLMs across different model sizes and persona prompts. Our results reveal substantial variability in responses due to question order shuffling, challenging the notion of a stable LLM \"personality.\" We find that larger models demonstrate more consistent responses across most personas, though this scaling behavior varies significantly by trait and persona type. The assistant persona showed the most predictable scaling patterns, while clinical personas exhibited more variable and sometimes extreme trait expressions. Including conversation history unexpectedly increased response variability. These findings have important implications for understanding LLM behavior under different conditions and reflect on the consequences of scaling.",
		"event-title": "NeurIPS 2024 Workshop on Behavioral Machine Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "LLMs and Personalities: Inconsistencies Across Scales",
		"title-short": "LLMs and Personalities",
		"URL": "https://openreview.net/forum?id=vBg3OvsHwv",
		"author": [
			{
				"family": "Tommaso",
				"given": "Tosato"
			},
			{
				"family": "Hegazy",
				"given": "Mahmood"
			},
			{
				"family": "Lemay",
				"given": "David"
			},
			{
				"family": "Abukalam",
				"given": "Mohammed"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Dumas",
				"given": "Guillaume"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "berlot-attwellLibraryLearningDoesnt2024",
		"type": "paper-conference",
		"abstract": "Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of *tools*, such as formal Isabelle lemmas or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts, but do current methods actually learn reusable libraries of tools? We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover and TroVE. We find that function reuse is extremely infrequent on miniF2F and MATH. Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case.",
		"event-title": "The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24",
		"language": "en",
		"source": "openreview.net",
		"title": "Library Learning Doesn’t: The Curious Case of the Single-Use “Library”",
		"title-short": "Library Learning Doesn’t",
		"URL": "https://openreview.net/forum?id=et2T8SKF1O",
		"author": [
			{
				"family": "Berlot-Attwell",
				"given": "Ian"
			},
			{
				"family": "Rudzicz",
				"given": "Frank"
			},
			{
				"family": "Si",
				"given": "Xujie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "bayatPitfallsMemorizationWhen2024",
		"type": "paper-conference",
		"abstract": "Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This leads to poor generalization when the learned explanations are spurious. In this work, we formalize $\\textit{the interplay between memorization and generalization}$, showing that spurious correlations, when combined with memorization, can reduce the training loss to zero, leaving no incentive to learn robust, generalizable patterns. To address this issue, we introduce $\\textit{memorization-aware training}$ (MAT). MAT leverages the flip side of memorization by using held-out predictions to adjust a model's logits, guiding it towards learning robust patterns that remain invariant from training to test, thereby enhancing generalization under distribution shifts.",
		"event-title": "NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "The Pitfalls of Memorization: When Memorization Hinders Generalization",
		"title-short": "The Pitfalls of Memorization",
		"URL": "https://openreview.net/forum?id=m24s1yUDFX",
		"author": [
			{
				"family": "Bayat",
				"given": "Reza"
			},
			{
				"family": "Pezeshki",
				"given": "Mohammad"
			},
			{
				"family": "Dohmatob",
				"given": "Elvis"
			},
			{
				"family": "Lopez-Paz",
				"given": "David"
			},
			{
				"family": "Vincent",
				"given": "Pascal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					9
				]
			]
		}
	},
	{
		"id": "liuNeuroplasticExpansionDeep2024",
		"type": "article",
		"abstract": "The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, Neuroplastic Expansion (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (1) elastic neuron generation based on potential gradients, (2) dormant neuron pruning to optimize network expressivity, and (3) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.",
		"DOI": "10.48550/arXiv.2410.07994",
		"note": "arXiv:2410.07994",
		"number": "arXiv:2410.07994",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Neuroplastic Expansion in Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2410.07994",
		"author": [
			{
				"family": "Liu",
				"given": "Jiashun"
			},
			{
				"family": "Obando-Ceron",
				"given": "Johan"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Pan",
				"given": "Ling"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "whiteLearningStochasticRainbow2024",
		"type": "paper-conference",
		"abstract": "Random feature models are a popular approach for studying network learning that can capture important behaviors while remaining simpler than traditional training. Guth et al. [2024] introduced “rainbow” networks which model the distribution of trained weights as correlated random features conditioned on previous layer activity. Sampling new weights from distributions fit to learned networks led to similar performance in entirely untrained networks, and the observed weight covariance were found to be low rank. This provided evidence that random feature models could be extended to some networks away from initialization, but White et al. [2024] failed to replicate their results in the deeper ResNet18 architecture. Here we ask whether the rainbow formulation can succeed in deeper networks by directly training a stochastic ensemble of random features, which we call stochastic rainbow networks. At every gradient descent iteration, new weights are sampled for all intermediate layers and features aligned layer-wise. We find: (1) this approach scales to deeper models, which outperform shallow networks at large widths; (2) ensembling multiple samples from the stochastic model is better than retraining the classifier head; and (3) low-rank parameterization of the learnable weight covariances can approach the accuracy of full-rank networks. This offers more evidence for rainbow and other structured random feature networks as reduced models of deep learning.",
		"event-title": "NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "Learning Stochastic Rainbow Networks",
		"URL": "https://openreview.net/forum?id=bvJ4UGTRtk",
		"author": [
			{
				"family": "White",
				"given": "Vivian"
			},
			{
				"family": "Chaudhary",
				"given": "Muawiz Sajjad"
			},
			{
				"family": "Wolf",
				"given": "Guy"
			},
			{
				"family": "Lajoie",
				"given": "Guillaume"
			},
			{
				"family": "Harris",
				"given": "Kameron Decker"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					9
				]
			]
		}
	},
	{
		"id": "mohamedLearningRobustRepresentations2024",
		"type": "paper-conference",
		"abstract": "Learning transferable representations for deep reinforcement learning (RL) is a challenging problem due to the inherent non-stationarity, distribution shift, and unstable training dynamics. To be useful, a transferable representation needs to be robust to such factors. In this work, we introduce a new architecture and training strategy for learning robust representations for transfer learning in RL. We propose leveraging multiple CNN encoders and training them not to specialize in areas of the state space but instead to match each other's representation. We find that learned representations transfer well across many Atari tasks, resulting in better transfer learning performance and data efficiency than training from scratch.",
		"event-title": "NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability",
		"language": "en",
		"source": "openreview.net",
		"title": "Learning Robust Representations for Transfer in Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=tHa7MhPCdg",
		"author": [
			{
				"family": "Mohamed",
				"given": "Faisal"
			},
			{
				"family": "Castanyer",
				"given": "Roger Creus"
			},
			{
				"family": "Tang",
				"given": "Hongyao"
			},
			{
				"family": "Sheikhbahaee",
				"given": "Zahra"
			},
			{
				"family": "Berseth",
				"given": "Glen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "sahooLayerSelectionApproach2024",
		"type": "paper-conference",
		"abstract": "Test Time Adaptation (TTA) addresses the problem of distribution shift by adapting a pretrained model to a new domain during inference. When faced with challenging shifts, most methods collapse and perform worse than the original pretrained model. In this paper, we find that not all layers are equally receptive to the adaptation, and the layers with the most misaligned gradients often cause performance degradation. To address this, we propose GALA, a novel layer selection criterion to identify the most beneficial updates to perform during test time adaptation. This criterion can also filter out unreliable samples with noisy gradients. Its simplicity allows seamless integration with existing TTA loss functions, thereby preventing degradation and focusing adaptation on the most trainable layers. This approach also helps to regularize adaptation to preserve the pretrained features, which are crucial for handling unseen domains. Through extensive experiments, we demonstrate that the proposed layer selection framework improves the performance of existing TTA approaches across multiple datasets, domain shifts, model architectures, and TTA losses.",
		"event-title": "NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability",
		"language": "en",
		"source": "openreview.net",
		"title": "A Layer Selection Approach to Test Time Adaptation",
		"URL": "https://openreview.net/forum?id=WhYuW9n1At",
		"author": [
			{
				"family": "Sahoo",
				"given": "Sabyasachi"
			},
			{
				"family": "ElAraby",
				"given": "Mostafa"
			},
			{
				"family": "Ngnawe",
				"given": "Jonas"
			},
			{
				"family": "Pequignot",
				"given": "Yann Batiste"
			},
			{
				"family": "Precioso",
				"given": "Frederic"
			},
			{
				"family": "Gagné",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "mirceaLanguageModelScaling2024",
		"type": "paper-conference",
		"abstract": "This work aims to understand how, in terms of training dynamics, scaling up language model size yields predictable loss improvements. We find that these improvements can be tied back to loss deceleration, an abrupt transition in the rate of loss improvement, characterized by piece-wise linear behavior in log-log space. Notably, improvements from increased model size appear to be a result of (1) improving the loss at which this transition occurs; and (2) improving the rate of loss improvement after this transition. As an explanation for the mechanism underlying this transition (and the effect of model size on loss it mediates), we propose the zero-sum learning (ZSL) hypothesis. In ZSL, per-token gradients become systematically opposed, leading to degenerate training dynamics where the model can't improve loss on one token without harming it on another; bottlenecking the overall rate at which loss can improve. We find compelling evidence of ZSL, as well as unexpected results which shed light on other factors contributing to ZSL.",
		"event-title": "NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "Language model scaling laws and zero-sum learning",
		"URL": "https://openreview.net/forum?id=yBq2g832Go",
		"author": [
			{
				"family": "Mircea",
				"given": "Andrei"
			},
			{
				"family": "Lobacheva",
				"given": "Ekaterina"
			},
			{
				"family": "Chakraborty",
				"given": "Supriyo"
			},
			{
				"family": "Chitsazan",
				"given": "Nima"
			},
			{
				"family": "Rish",
				"given": "Irina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					9
				]
			]
		}
	},
	{
		"id": "lobachevaHowLearningRates2024",
		"type": "paper-conference",
		"abstract": "The learning rate is a key hyperparameter that affects both the speed of training and the generalization performance of neural networks. Through a new {\\it loss-based example ranking} analysis, we show that networks trained with different learning rates focus their capacity on different parts of the data distribution, leading to solutions with different generalization properties. These findings, which hold across architectures and datasets, provide new insights into how learning rates affect model performance and example-level dynamics in neural networks.",
		"event-title": "NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning",
		"language": "en",
		"source": "openreview.net",
		"title": "How Learning Rates Shape Neural Network Focus: Insights from Example Ranking",
		"title-short": "How Learning Rates Shape Neural Network Focus",
		"URL": "https://openreview.net/forum?id=NeetGrZ22b",
		"author": [
			{
				"family": "Lobacheva",
				"given": "Ekaterina"
			},
			{
				"family": "Jordan",
				"given": "Keller"
			},
			{
				"family": "Baratin",
				"given": "Aristide"
			},
			{
				"family": "Roux",
				"given": "Nicolas Le"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					9
				]
			]
		}
	},
	{
		"id": "noukhovitchFasterMoreEfficient2024",
		"type": "paper-conference",
		"abstract": "To achieve state-of-the-art chatbots, large language models are finetuned with reinforcement learning (RL), frequently to optimize human feedback (RLHF). This process is computationally expensive and can take weeks. Offline approaches, like DPO, learn on a static dataset and are efficient but not performant. The dominant paradigm, online and on-policy---synchronously generating from the model, labelling with a reward model, and learning on feedback from the model's own outputs---is performant but not efficient. Following prior work in the generall deep RL setting, we propose separating the actor and learner in RLHF. This enables the asynchronously generation of new samples while learning on prior samples, thus leading to overall faster training and better scaling. But this requires a novel regime for RLHF, online but off-policy: learning on samples from a previous version of our model. We ask a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? We find that a contrastive loss, Online DPO, is most robust to off-policy data and that robustness increases with the scale of the policy model. We show even further compute optimizations but demonstrate that they come at a performance cost, giving rise to a trade-off. Finally, we verify our design choices by training LLaMA 3.1 8B with RLHF as a helpful chatbot in half the time of a synchronous run while matching final performance.",
		"event-title": "NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability",
		"language": "en",
		"source": "openreview.net",
		"title": "Faster, More Efficient RLHF through Off-Policy Asynchronous Learning",
		"URL": "https://openreview.net/forum?id=ND3io3eses",
		"author": [
			{
				"family": "Noukhovitch",
				"given": "Michael"
			},
			{
				"family": "Huang",
				"given": "Shengyi"
			},
			{
				"family": "Xhonneux",
				"given": "Sophie"
			},
			{
				"family": "Hosseini",
				"given": "Arian"
			},
			{
				"family": "Agarwal",
				"given": "Rishabh"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "ashokContextKeyBenchmark2024",
		"type": "paper-conference",
		"abstract": "Forecasting is a task of pinnacle importance in decision making across various fields. Numerical data alone often lacks crucial information for accurate forecasting, and in many cases, humans possess additional contextual information that is essential for forecasting, such as background knowledge or constraints on the quantity to predict. One convenient way to provide such essential information to models is through natural language. Yet, the extent to which existing forecasting approaches can effectively utilize contextual information in text is still an open question. To address this, we propose Context is Key (CiK), a time series forecasting benchmark consisting of tasks that combine numerical data with diverse kinds of textual context, requiring models to leverage a variety of skills to succeed. We evaluate a range of approaches and introduce a simple LLM prompting method that serves as a strong baseline. By presenting this challenging benchmark, we aim to foster progress in multimodal forecasting, paving the way for advancements that will lead to forecasting methods accessible to decision-makers irrespective of their technical expertise. The benchmark can be visualized at https://anon-forecast.github.io/benchmark_report/.",
		"event-title": "NeurIPS Workshop on Time Series in the Age of Large Models",
		"language": "en",
		"source": "openreview.net",
		"title": "Context is Key: A Benchmark for Forecasting with Essential Textual Information",
		"title-short": "Context is Key",
		"URL": "https://openreview.net/forum?id=ReSNVjuPpw",
		"author": [
			{
				"family": "Ashok",
				"given": "Arjun"
			},
			{
				"family": "Williams",
				"given": "Andrew Robert"
			},
			{
				"family": "Marcotte",
				"given": "Étienne"
			},
			{
				"family": "Zantedeschi",
				"given": "Valentina"
			},
			{
				"family": "Subramanian",
				"given": "Jithendaraa"
			},
			{
				"family": "Riachi",
				"given": "Roland"
			},
			{
				"family": "Requeima",
				"given": "James"
			},
			{
				"family": "Lacoste",
				"given": "Alexandre"
			},
			{
				"family": "Rish",
				"given": "Irina"
			},
			{
				"family": "Chapados",
				"given": "Nicolas"
			},
			{
				"family": "Drouin",
				"given": "Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					10
				]
			]
		}
	},
	{
		"id": "chengAmOneOnly2024",
		"type": "article",
		"abstract": "Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. In this perspective, we argue that we cannot thoroughly map the social impacts of generative AI without mapping the social impacts of anthropomorphic AI, and outline a call to action.",
		"DOI": "10.48550/arXiv.2410.08526",
		"note": "arXiv:2410.08526",
		"number": "arXiv:2410.08526",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "\"I Am the One and Only, Your Cyber BFF\": Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI",
		"title-short": "I Am the One and Only, Your Cyber BFF",
		"URL": "http://arxiv.org/abs/2410.08526",
		"author": [
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "DeVrio",
				"given": "Alicia"
			},
			{
				"family": "Egede",
				"given": "Lisa"
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					11
				]
			]
		}
	},
	{
		"id": "fuPoisonBenchAssessingLarge2024",
		"type": "article",
		"abstract": "Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.",
		"DOI": "10.48550/arXiv.2410.08811",
		"note": "arXiv:2410.08811",
		"number": "arXiv:2410.08811",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning",
		"title-short": "PoisonBench",
		"URL": "http://arxiv.org/abs/2410.08811",
		"author": [
			{
				"family": "Fu",
				"given": "Tingchen"
			},
			{
				"family": "Sharma",
				"given": "Mrinank"
			},
			{
				"family": "Torr",
				"given": "Philip"
			},
			{
				"family": "Cohen",
				"given": "Shay B."
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Barez",
				"given": "Fazl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					11
				]
			]
		}
	},
	{
		"id": "touzelSimulationSystemSolving2024",
		"type": "article",
		"abstract": "The rise of AI-driven manipulation poses significant risks to societal trust and democratic processes. Yet, studying these effects in real-world settings at scale is ethically and logistically impractical, highlighting a need for simulation tools that can model these dynamics in controlled settings to enable experimentation with possible defenses. We present a simulation environment designed to address this. We elaborate upon the Concordia framework that simulates offline, `real life' activity by adding online interactions to the simulation through social media with the integration of a Mastodon server. We improve simulation efficiency and information flow, and add a set of measurement tools, particularly longitudinal surveys. We demonstrate the simulator with a tailored example in which we track agents' political positions and show how partisan manipulation of agents can affect election results.",
		"DOI": "10.48550/arXiv.2410.13915",
		"note": "arXiv:2410.13915",
		"number": "arXiv:2410.13915",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Simulation System Towards Solving Societal-Scale Manipulation",
		"URL": "http://arxiv.org/abs/2410.13915",
		"author": [
			{
				"family": "Touzel",
				"given": "Maximilian Puelma"
			},
			{
				"family": "Sarangi",
				"given": "Sneheel"
			},
			{
				"family": "Welch",
				"given": "Austin"
			},
			{
				"family": "Krishnakumar",
				"given": "Gayatri"
			},
			{
				"family": "Zhao",
				"given": "Dan"
			},
			{
				"family": "Yang",
				"given": "Zachary"
			},
			{
				"family": "Yu",
				"given": "Hao"
			},
			{
				"family": "Kosak-Hine",
				"given": "Ethan"
			},
			{
				"family": "Gibbs",
				"given": "Tom"
			},
			{
				"family": "Musulan",
				"given": "Andreea"
			},
			{
				"family": "Thibault",
				"given": "Camille"
			},
			{
				"family": "Gurbuz",
				"given": "Busra Tugce"
			},
			{
				"family": "Rabbany",
				"given": "Reihaneh"
			},
			{
				"family": "Godbout",
				"given": "Jean-François"
			},
			{
				"family": "Pelrine",
				"given": "Kellin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					17
				]
			]
		}
	},
	{
		"id": "ghafouriEpistemicIntegrityLarge2024",
		"type": "article",
		"abstract": "Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.",
		"DOI": "10.48550/arXiv.2411.06528",
		"note": "arXiv:2411.06528",
		"number": "arXiv:2411.06528",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Epistemic Integrity in Large Language Models",
		"URL": "http://arxiv.org/abs/2411.06528",
		"author": [
			{
				"family": "Ghafouri",
				"given": "Bijean"
			},
			{
				"family": "Mohammadzadeh",
				"given": "Shahrad"
			},
			{
				"family": "Zhou",
				"given": "James"
			},
			{
				"family": "Nair",
				"given": "Pratheeksha"
			},
			{
				"family": "Tian",
				"given": "Jacob-Junqi"
			},
			{
				"family": "Goel",
				"given": "Mayank"
			},
			{
				"family": "Rabbany",
				"given": "Reihaneh"
			},
			{
				"family": "Godbout",
				"given": "Jean-François"
			},
			{
				"family": "Pelrine",
				"given": "Kellin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					10
				]
			]
		}
	},
	{
		"id": "mohammadzadehHallucinationDetoxSensitive2024",
		"type": "article",
		"abstract": "As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.",
		"DOI": "10.48550/arXiv.2410.15460",
		"note": "arXiv:2410.15460",
		"number": "arXiv:2410.15460",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training",
		"title-short": "Hallucination Detox",
		"URL": "http://arxiv.org/abs/2410.15460",
		"author": [
			{
				"family": "Mohammadzadeh",
				"given": "Shahrad"
			},
			{
				"family": "Guerra",
				"given": "Juan David"
			},
			{
				"family": "Bonizzato",
				"given": "Marco"
			},
			{
				"family": "Rabbany",
				"given": "Reihaneh"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					20
				]
			]
		}
	},
	{
		"id": "zhaoIdentifyingAddressingDelusions2024",
		"type": "article",
		"abstract": "Target-directed agents utilize self-generated targets, to guide their behaviors for better generalization. These agents are prone to blindly chasing problematic targets, resulting in worse generalization and safety catastrophes. We show that these behaviors can be results of delusions, stemming from improper designs around training: the agent may naturally come to hold false beliefs about certain targets. We identify different types of delusions via intuitive examples in controlled environments, and investigate their causes and mitigations. With the insights, we demonstrate how we can make agents address delusions preemptively and autonomously. We validate empirically the effectiveness of the proposed strategies in correcting delusional behaviors and improving out-of-distribution generalization.",
		"DOI": "10.48550/arXiv.2410.07096",
		"note": "arXiv:2410.07096",
		"number": "arXiv:2410.07096",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Identifying and Addressing Delusions for Target-Directed Decision-Making",
		"URL": "http://arxiv.org/abs/2410.07096",
		"author": [
			{
				"family": "Zhao",
				"given": "Mingde"
			},
			{
				"family": "Sylvain",
				"given": "Tristan"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "suhDynamicAbstractionsBuilding2024a",
		"type": "paper-conference",
		"abstract": "This workshop provides a forum to discuss, brainstorm, and prototype the next generation of interfaces that leverage the dynamic experiences enabled by recent advances in AI and the generative capabilities of foundation models. These models simplify complex tasks by generating outputs in various representations (e.g., text, images, videos) through diverse input modalities like natural language, voice, and sketch. They interpret user intent to generate and transform representations, potentially changing how we interact with information and express ideas. Inspired by this potential, technologists, theorists, and researchers are exploring new forms of interaction by building demos and communities dedicated to concretizing and advancing the vision of working with dynamic abstractions. This UIST workshop provides a timely space to discuss AI’s impact on how we might design and use cognitive tools (e.g., languages, notations, diagrams). We will explore the challenges, critiques, and opportunities of this space by thinking through and prototyping use cases across various domains.",
		"container-title": "The 37th Annual ACM Symposium on User Interface Software and Technology",
		"DOI": "10.1145/3672539.3686706",
		"event-place": "Pittsburgh PA USA",
		"event-title": "UIST '24: The 37th Annual ACM Symposium on User Interface Software and Technology",
		"ISBN": "979-8-4007-0718-6",
		"language": "en",
		"page": "1-3",
		"publisher": "ACM",
		"publisher-place": "Pittsburgh PA USA",
		"source": "DOI.org (Crossref)",
		"title": "Dynamic Abstractions: Building the Next Generation of Cognitive Tools and Interfaces",
		"title-short": "Dynamic Abstractions",
		"URL": "https://dl.acm.org/doi/10.1145/3672539.3686706",
		"author": [
			{
				"family": "Suh",
				"given": "Sangho"
			},
			{
				"family": "Dang",
				"given": "Hai"
			},
			{
				"family": "Yen",
				"given": "Ryan"
			},
			{
				"family": "Pollock",
				"given": "Josh M."
			},
			{
				"family": "Arawjo",
				"given": "Ian"
			},
			{
				"family": "Kazi",
				"given": "Rubaiat Habib"
			},
			{
				"family": "Subramonyam",
				"given": "Hariharan"
			},
			{
				"family": "Li",
				"given": "Jingyi"
			},
			{
				"family": "Saquib",
				"given": "Nazmus"
			},
			{
				"family": "Satyanarayan",
				"given": "Arvind"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					13
				]
			]
		}
	},
	{
		"id": "suDualformerControllableFast2024",
		"type": "article",
		"abstract": "In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.",
		"DOI": "10.48550/arXiv.2410.09918",
		"note": "arXiv:2410.09918",
		"number": "arXiv:2410.09918",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces",
		"title-short": "Dualformer",
		"URL": "http://arxiv.org/abs/2410.09918",
		"author": [
			{
				"family": "Su",
				"given": "DiJia"
			},
			{
				"family": "Sukhbaatar",
				"given": "Sainbayar"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "Tian",
				"given": "Yuandong"
			},
			{
				"family": "Zheng",
				"given": "Qinqing"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					13
				]
			]
		}
	},
	{
		"id": "guoMechanisticUnlearningRobust2024",
		"type": "article",
		"abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
		"DOI": "10.48550/arXiv.2410.12949",
		"note": "arXiv:2410.12949",
		"number": "arXiv:2410.12949",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
		"title-short": "Mechanistic Unlearning",
		"URL": "http://arxiv.org/abs/2410.12949",
		"author": [
			{
				"family": "Guo",
				"given": "Phillip"
			},
			{
				"family": "Syed",
				"given": "Aaquib"
			},
			{
				"family": "Sheshadri",
				"given": "Abhay"
			},
			{
				"family": "Ewart",
				"given": "Aidan"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "huangRobotSnakesDream2024",
		"type": "article",
		"abstract": "The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to \\textit{hallucinate} false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.",
		"DOI": "10.48550/arXiv.2410.17477",
		"note": "arXiv:2410.17477",
		"number": "arXiv:2410.17477",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination",
		"title-short": "Do Robot Snakes Dream like Electric Sheep?",
		"URL": "http://arxiv.org/abs/2410.17477",
		"author": [
			{
				"family": "Huang",
				"given": "Jerry"
			},
			{
				"family": "Parthasarathi",
				"given": "Prasanna"
			},
			{
				"family": "Rezagholizadeh",
				"given": "Mehdi"
			},
			{
				"family": "Chen",
				"given": "Boxing"
			},
			{
				"family": "Chandar",
				"given": "Sarath"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					29
				]
			]
		}
	},
	{
		"id": "tanStickbreakingAttention2024",
		"type": "article",
		"abstract": "The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We propose an alternative attention mechanism based on the stick-breaking process: For each token before the current, we determine a break point $\\beta_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token. We repeat the process until the stick is fully allocated, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et. al., 2017). We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.",
		"DOI": "10.48550/arXiv.2410.17980",
		"note": "arXiv:2410.17980",
		"number": "arXiv:2410.17980",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Stick-breaking Attention",
		"URL": "http://arxiv.org/abs/2410.17980",
		"author": [
			{
				"family": "Tan",
				"given": "Shawn"
			},
			{
				"family": "Shen",
				"given": "Yikang"
			},
			{
				"family": "Yang",
				"given": "Songlin"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Panda",
				"given": "Rameswar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					23
				]
			]
		}
	},
	{
		"id": "mcleanOvercomingStateAction2024",
		"type": "paper-conference",
		"abstract": "Current multi-task reinforcement learning (MTRL) methods have the ability to perform a large number of tasks with a single policy. However when attempting to interact with a new domain, the MTRL agent would need to be re-trained due to differences in domain dynamics and structure. Because of these limitations, we are forced to train multiple policies even though tasks may have shared dynamics, leading to needing more samples and is thus sample inefficient. In this work, we explore the ability of MTRL agents to learn in various domains with various dynamics by simultaneously learning in multiple domains, without the need to fine-tune extra policies. In doing so we find that a MTRL agent trained in multiple domains induces an increase in sample efficiency of up to 70\\% while maintaining the overall success rate of the MTRL agent.",
		"event-title": "[CoRL 2024] Morphology-Aware Policy and Design Learning Workshop (MAPoDeL)",
		"language": "en",
		"source": "openreview.net",
		"title": "Overcoming State and Action Space Disparities in Multi-Domain, Multi-Task Reinforcement Learning",
		"URL": "https://openreview.net/forum?id=T7bA2zjobB",
		"author": [
			{
				"family": "McLean",
				"given": "Reginald"
			},
			{
				"family": "Yuan",
				"given": "Kai"
			},
			{
				"family": "Woungang",
				"given": "Isaac"
			},
			{
				"family": "Farsad",
				"given": "Nariman"
			},
			{
				"family": "Castro",
				"given": "Pablo Samuel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					4
				]
			]
		}
	},
	{
		"id": "cacciolaStructuredPruningNeural2024",
		"type": "article-journal",
		"abstract": "In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity in applications such as cancer treatment, algorithmic configuration, and chemical process optimization. This integration often uses Mixed Integer Programming (MIP) formulations to represent the chosen ML model, that is often an Artificial Neural Networks (ANNs) due to their widespread use. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations impractical to solve. In this paper we showcase the effectiveness of a ANN pruning, when applied to models prior to their integration into MIPs. We discuss why pruning is more suitable in this context than other ML compression techniques, and we highlight the potential of appropriate pruning strategies via experiments on MIPs used to construct adversarial examples to ANNs. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.",
		"container-title": "Operations Research Letters",
		"DOI": "10.1016/j.orl.2024.107194",
		"ISSN": "0167-6377",
		"journalAbbreviation": "Operations Research Letters",
		"page": "107194",
		"source": "ScienceDirect",
		"title": "Structured pruning of neural networks for constraints learning",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0167637724001305",
		"volume": "57",
		"author": [
			{
				"family": "Cacciola",
				"given": "Matteo"
			},
			{
				"family": "Frangioni",
				"given": "Antonio"
			},
			{
				"family": "Lodi",
				"given": "Andrea"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					1
				]
			]
		}
	},
	{
		"id": "balutaUnlearningVsOutofdistribution2024",
		"type": "article",
		"abstract": "Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.",
		"DOI": "10.48550/arXiv.2411.04388",
		"note": "arXiv:2411.04388",
		"number": "arXiv:2411.04388",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
		"URL": "http://arxiv.org/abs/2411.04388",
		"author": [
			{
				"family": "Baluta",
				"given": "Teodora"
			},
			{
				"family": "Lamblin",
				"given": "Pascal"
			},
			{
				"family": "Tarlow",
				"given": "Daniel"
			},
			{
				"family": "Pedregosa",
				"given": "Fabian"
			},
			{
				"family": "Dziugaite",
				"given": "Gintare Karolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					7
				]
			]
		}
	},
	{
		"id": "variciGeneralIdentifiabilityAchievability2024",
		"type": "article-journal",
		"abstract": "This paper focuses on causal representation learning (CRL) under a general nonparametric latent causal model and a general transformation model that maps the latent data to the observational data. It establishes identifiability and achievability results using two hard uncoupled interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, additionally, recovers the identifiability result for two hard coupled interventions, that is when metadata about the pair of environments that have the same node intervened is known. This paper also shows that when observational data is available, additional faithfulness assumptions that are adopted by the existing literature are unnecessary.",
		"language": "en",
		"source": "Zotero",
		"title": "General Identifiability and Achievability for Causal Representation Learning",
		"author": [
			{
				"family": "Varıcı",
				"given": "Burak"
			},
			{
				"family": "Acartürk",
				"given": "Emre"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Tajer",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "tennenholtzDemystifyingEmbeddingSpaces2023",
		"type": "paper-conference",
		"abstract": "Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Demystifying Embedding Spaces using Large Language Models",
		"URL": "https://openreview.net/forum?id=qoYogklIPz",
		"author": [
			{
				"family": "Tennenholtz",
				"given": "Guy"
			},
			{
				"family": "Chow",
				"given": "Yinlam"
			},
			{
				"family": "Hsu",
				"given": "ChihWei"
			},
			{
				"family": "Jeong",
				"given": "Jihwan"
			},
			{
				"family": "Shani",
				"given": "Lior"
			},
			{
				"family": "Tulepbergenov",
				"given": "Azamat"
			},
			{
				"family": "Ramachandran",
				"given": "Deepak"
			},
			{
				"family": "Mladenov",
				"given": "Martin"
			},
			{
				"family": "Boutilier",
				"given": "Craig"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "paceDelphicOfflineReinforcement2023",
		"type": "paper-conference",
		"abstract": "A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding",
		"URL": "https://openreview.net/forum?id=lUYY2qsRTI&noteId=NBlfr4LHx0",
		"author": [
			{
				"family": "Pace",
				"given": "Alizée"
			},
			{
				"family": "Yèche",
				"given": "Hugo"
			},
			{
				"family": "Schölkopf",
				"given": "Bernhard"
			},
			{
				"family": "Ratsch",
				"given": "Gunnar"
			},
			{
				"family": "Tennenholtz",
				"given": "Guy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "wengIntentionalApproachManaging2024",
		"type": "article-journal",
		"abstract": "Advances in machine learning for health care have brought concerns about bias from the research community; specifically, the introduction, perpetuation, or exacerbation of care disparities. Reinforcing these concerns is the finding that medical images often reveal signals about sensitive attributes in ways that are hard to pinpoint by both algorithms and people. This finding raises a question about how to best design general purpose pretrained embeddings (GPPEs, defined as embeddings meant to support a broad array of use cases) for building downstream models that are free from particular types of bias. The downstream model should be carefully evaluated for bias, and audited and improved as appropriate. However, in our view, well intentioned attempts to prevent the upstream components—GPPEs—from learning sensitive attributes can have unintended consequences on the downstream models. Despite producing a veneer of technical neutrality, the resultant end-to-end system might still be biased or poorly performing. We present reasons, by building on previously published data, to support the reasoning that GPPEs should ideally contain as much information as the original data contain, and highlight the perils of trying to remove sensitive attributes from a GPPE. We also emphasise that downstream prediction models trained for specific tasks and settings, whether developed using GPPEs or not, should be carefully designed and evaluated to avoid bias that makes models vulnerable to issues such as distributional shift. These evaluations should be done by a diverse team, including social scientists, on a diverse cohort representing the full breadth of the patient population for which the final model is intended.",
		"container-title": "The Lancet Digital Health",
		"DOI": "10.1016/S2589-7500(23)00227-3",
		"ISSN": "2589-7500",
		"issue": "2",
		"journalAbbreviation": "The Lancet Digital Health",
		"language": "English",
		"note": "publisher: Elsevier\nPMID: 38278614",
		"page": "e126-e130",
		"source": "www.thelancet.com",
		"title": "An intentional approach to managing bias in general purpose embedding models",
		"URL": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00227-3/fulltext",
		"volume": "6",
		"author": [
			{
				"family": "Weng",
				"given": "Wei-Hung"
			},
			{
				"family": "Sellergen",
				"given": "Andrew"
			},
			{
				"family": "Kiraly",
				"given": "Atilla P."
			},
			{
				"family": "D’Amour",
				"given": "Alexander"
			},
			{
				"family": "Park",
				"given": "Jungyeon"
			},
			{
				"family": "Pilgrim",
				"given": "Rory"
			},
			{
				"family": "Pfohl",
				"given": "Stephen"
			},
			{
				"family": "Lau",
				"given": "Charles"
			},
			{
				"family": "Natarajan",
				"given": "Vivek"
			},
			{
				"family": "Azizi",
				"given": "Shekoofeh"
			},
			{
				"family": "Karthikesalingam",
				"given": "Alan"
			},
			{
				"family": "Cole-Lewis",
				"given": "Heather"
			},
			{
				"family": "Matias",
				"given": "Yossi"
			},
			{
				"family": "Corrado",
				"given": "Greg S."
			},
			{
				"family": "Webster",
				"given": "Dale R."
			},
			{
				"family": "Shetty",
				"given": "Shravya"
			},
			{
				"family": "Prabhakara",
				"given": "Shruthi"
			},
			{
				"family": "Eswaran",
				"given": "Krish"
			},
			{
				"family": "Celi",
				"given": "Leo A. G."
			},
			{
				"family": "Liu",
				"given": "Yun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					1
				]
			]
		}
	},
	{
		"id": "havaldarLearningLabelProportions2024",
		"type": "article",
		"abstract": "Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (up to 15%) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples.",
		"DOI": "10.48550/arXiv.2310.08056",
		"note": "arXiv:2310.08056",
		"number": "arXiv:2310.08056",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation",
		"title-short": "Learning from Label Proportions",
		"URL": "http://arxiv.org/abs/2310.08056",
		"author": [
			{
				"family": "Havaldar",
				"given": "Shreyas"
			},
			{
				"family": "Sharma",
				"given": "Navodita"
			},
			{
				"family": "Sareen",
				"given": "Shubhi"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Raghuveer",
				"given": "Aravindan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		}
	},
	{
		"id": "jainSelectiveClassificationUsing2024",
		"type": "article",
		"abstract": "Predictive uncertainty-a model's self awareness regarding its accuracy on an input-is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditioned reweighting approach that captures predictive uncertainty using an auxiliary network and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing the dropout variance, an approximation of Bayesian Predictive uncertainty. We show in controlled experiments that we effectively capture the diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings-selective classification, label noise, domain adaptation, calibration-and across datasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto 3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX.",
		"DOI": "10.48550/arXiv.2212.05987",
		"note": "arXiv:2212.05987",
		"number": "arXiv:2212.05987",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Selective classification using a robust meta-learning approach",
		"URL": "http://arxiv.org/abs/2212.05987",
		"author": [
			{
				"family": "Jain",
				"given": "Nishant"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Shenoy",
				"given": "Pradeep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					2
				]
			]
		}
	},
	{
		"id": "quachConformalLanguageModeling2024",
		"type": "article",
		"abstract": "We propose a novel approach to conformal prediction for generative language models (LMs). Standard conformal prediction produces prediction sets -- in place of single predictions -- that have rigorous, statistical performance guarantees. LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components -- such as phrases or sentences -- that are each independently correct (e.g., that are not \"hallucinations\"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.",
		"DOI": "10.48550/arXiv.2306.10193",
		"note": "arXiv:2306.10193",
		"number": "arXiv:2306.10193",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Conformal Language Modeling",
		"URL": "http://arxiv.org/abs/2306.10193",
		"author": [
			{
				"family": "Quach",
				"given": "Victor"
			},
			{
				"family": "Fisch",
				"given": "Adam"
			},
			{
				"family": "Schuster",
				"given": "Tal"
			},
			{
				"family": "Yala",
				"given": "Adam"
			},
			{
				"family": "Sohn",
				"given": "Jae Ho"
			},
			{
				"family": "Jaakkola",
				"given": "Tommi S."
			},
			{
				"family": "Barzilay",
				"given": "Regina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					1
				]
			]
		}
	},
	{
		"id": "havaldarFairnessCovariateShift2024",
		"type": "article",
		"abstract": "Covariate shift in the test data is a common practical phenomena that can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups under covariate shift is of paramount importance due to societal implications like criminal justice. We operate in the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards improving fairness under this highly challenging yet realistic scenario, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines.",
		"DOI": "10.48550/arXiv.2310.07535",
		"note": "arXiv:2310.07535",
		"number": "arXiv:2310.07535",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fairness under Covariate Shift: Improving Fairness-Accuracy tradeoff with few Unlabeled Test Samples",
		"title-short": "Fairness under Covariate Shift",
		"URL": "http://arxiv.org/abs/2310.07535",
		"author": [
			{
				"family": "Havaldar",
				"given": "Shreyas"
			},
			{
				"family": "Chauhan",
				"given": "Jatin"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Nandy",
				"given": "Jay"
			},
			{
				"family": "Raghuveer",
				"given": "Aravindan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					8
				]
			]
		}
	},
	{
		"id": "donahueWhenAreTwo2024",
		"type": "article",
		"abstract": "Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \\in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.",
		"DOI": "10.48550/arXiv.2308.11721",
		"note": "arXiv:2308.11721",
		"number": "arXiv:2308.11721",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making",
		"title-short": "When Are Two Lists Better than One?",
		"URL": "http://arxiv.org/abs/2308.11721",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Gollapudi",
				"given": "Sreenivas"
			},
			{
				"family": "Kollias",
				"given": "Kostas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					26
				]
			]
		}
	},
	{
		"id": "bharadwajMultimodalModelingSpoken2023",
		"type": "article",
		"abstract": "Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.",
		"DOI": "10.48550/arXiv.2309.10567",
		"note": "arXiv:2309.10567",
		"number": "arXiv:2309.10567",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multimodal Modeling For Spoken Language Identification",
		"URL": "http://arxiv.org/abs/2309.10567",
		"author": [
			{
				"family": "Bharadwaj",
				"given": "Shikhar"
			},
			{
				"family": "Ma",
				"given": "Min"
			},
			{
				"family": "Vashishth",
				"given": "Shikhar"
			},
			{
				"family": "Bapna",
				"given": "Ankur"
			},
			{
				"family": "Ganapathy",
				"given": "Sriram"
			},
			{
				"family": "Axelrod",
				"given": "Vera"
			},
			{
				"family": "Dalmia",
				"given": "Siddharth"
			},
			{
				"family": "Han",
				"given": "Wei"
			},
			{
				"family": "Zhang",
				"given": "Yu"
			},
			{
				"family": "Esch",
				"given": "Daan",
				"dropping-particle": "van"
			},
			{
				"family": "Ritchie",
				"given": "Sandy"
			},
			{
				"family": "Talukdar",
				"given": "Partha"
			},
			{
				"family": "Riesa",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					19
				]
			]
		}
	},
	{
		"id": "jainInstanceconditionalTimescalesDecay2024",
		"type": "paper-conference",
		"abstract": "Slow concept drift is a ubiquitous, yet under-studied problem in practical machine learning systems. In such settings, although recent data is more indicative of future data, naively prioritizing recent instances runs the risk of losing valuable information from the past. We propose an optimization-driven approach towards balancing instance importance over large training windows. First, we model instance relevance using a mixture of multiple timescales of decay, allowing us to capture rich temporal trends. Second, we learn an auxiliary scorer model that recovers the appropriate mixture of timescales as a function of the instance itself. Finally, we propose a nested optimization objective for learning the scorer, by which it maximizes forward transfer for the learned model. Experiments on a large real-world dataset of 39M photos over a 9 year period show upto 15% relative gains in accuracy compared to other robust learning baselines. We replicate our gains on two collections of real-world datasets for non-stationary learning, and extend our work to continual learning settings where, too, we beat SOTA methods by large margins.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 11",
		"page": "12773–12781",
		"source": "Google Scholar",
		"title": "Instance-conditional timescales of decay for non-stationary learning",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/29173",
		"volume": "38",
		"author": [
			{
				"family": "Jain",
				"given": "Nishant"
			},
			{
				"family": "Shenoy",
				"given": "Pradeep"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "zhuangSetwiseApproachEffective2024",
		"type": "paper-conference",
		"abstract": "We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at https://github.com/ielab/llm-rankers.",
		"container-title": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval",
		"DOI": "10.1145/3626772.3657813",
		"event-place": "Washington DC USA",
		"event-title": "SIGIR 2024: The 47th International ACM SIGIR Conference on Research and Development in Information Retrieval",
		"ISBN": "979-8-4007-0431-4",
		"language": "en",
		"page": "38-47",
		"publisher": "ACM",
		"publisher-place": "Washington DC USA",
		"source": "DOI.org (Crossref)",
		"title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
		"URL": "https://dl.acm.org/doi/10.1145/3626772.3657813",
		"author": [
			{
				"family": "Zhuang",
				"given": "Shengyao"
			},
			{
				"family": "Zhuang",
				"given": "Honglei"
			},
			{
				"family": "Koopman",
				"given": "Bevan"
			},
			{
				"family": "Zuccon",
				"given": "Guido"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					10
				]
			]
		}
	},
	{
		"id": "keBridgingPreferenceGap2024",
		"type": "article",
		"abstract": "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-“friendly” information and assembling a LLM-“friendly” context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.",
		"language": "en",
		"note": "arXiv:2401.06954 [cs]",
		"number": "arXiv:2401.06954",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Bridging the Preference Gap between Retrievers and LLMs",
		"URL": "http://arxiv.org/abs/2401.06954",
		"author": [
			{
				"family": "Ke",
				"given": "Zixuan"
			},
			{
				"family": "Kong",
				"given": "Weize"
			},
			{
				"family": "Li",
				"given": "Cheng"
			},
			{
				"family": "Zhang",
				"given": "Mingyang"
			},
			{
				"family": "Mei",
				"given": "Qiaozhu"
			},
			{
				"family": "Bendersky",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					20
				]
			]
		}
	},
	{
		"id": "liuWeNeedStructured2024",
		"type": "paper-conference",
		"abstract": "Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.",
		"collection-title": "CHI EA '24",
		"container-title": "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems",
		"DOI": "10.1145/3613905.3650756",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0331-7",
		"page": "1–9",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "\"We Need Structured Output\": Towards User-centered Constraints on Large Language Model Output",
		"title-short": "We Need Structured Output",
		"URL": "https://dl.acm.org/doi/10.1145/3613905.3650756",
		"author": [
			{
				"family": "Liu",
				"given": "Michael Xieyang"
			},
			{
				"family": "Liu",
				"given": "Frederick"
			},
			{
				"family": "Fiannaca",
				"given": "Alexander J."
			},
			{
				"family": "Koo",
				"given": "Terry"
			},
			{
				"family": "Dixon",
				"given": "Lucas"
			},
			{
				"family": "Terry",
				"given": "Michael"
			},
			{
				"family": "Cai",
				"given": "Carrie J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					11
				]
			]
		}
	},
	{
		"id": "bowenDataPoisoningLLMs2024",
		"type": "article",
		"abstract": "LLMs produce harmful and undesirable behavior when trained on poisoned datasets that contain a small fraction of corrupted or harmful data. We develop a new attack paradigm, jailbreak-tuning, that combines data poisoning with jailbreaking to fully bypass state-of-the-art safeguards and make models like GPT-4o comply with nearly any harmful request. Our experiments suggest this attack represents a paradigm shift in vulnerability elicitation, producing differences in refusal rates as much as 60+ percentage points compared to normal fine-tuning. Given this demonstration of how data poisoning vulnerabilities persist and can be amplified, we investigate whether these risks will likely increase as models scale. We evaluate three threat models—malicious fine-tuning, imperfect data curation, and intentional data contamination—across 23 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.",
		"language": "en",
		"note": "arXiv:2408.02946 [cs]",
		"number": "arXiv:2408.02946",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws",
		"title-short": "Data Poisoning in LLMs",
		"URL": "http://arxiv.org/abs/2408.02946",
		"author": [
			{
				"family": "Bowen",
				"given": "Dillon"
			},
			{
				"family": "Murphy",
				"given": "Brendan"
			},
			{
				"family": "Cai",
				"given": "Will"
			},
			{
				"family": "Khachaturov",
				"given": "David"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Pelrine",
				"given": "Kellin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					29
				]
			]
		}
	},
	{
		"id": "howeEffectsScaleLanguage2024",
		"type": "article",
		"abstract": "Language models exhibit scaling laws, whereby increasing model and dataset size yields predictable decreases in negative log likelihood, unlocking a dazzling array of capabilities. This phenomenon spurs many companies to train ever larger models in pursuit of ever improved performance. Yet, these models are vulnerable to adversarial inputs such as ``jailbreaks'' and prompt injections that induce models to perform undesired behaviors, posing a growing risk as models become more capable. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale? We study this question empirically in the classification setting, finding that without explicit defense training, larger models tend to be modestly more robust on most tasks, though the effect is not reliable. Even with the advantage conferred by scale, undefended models remain easy to attack in absolute terms, and we thus turn our attention to explicitly training models for adversarial robustness, which we show to be a much more compute-efficient defense than scaling model size alone. In this setting, we also observe that adversarially trained larger models generalize faster and better to modified attacks not seen during training when compared with smaller models. Finally, we analyze the offense/defense balance of increasing compute, finding parity in some settings and an advantage for offense in others, suggesting that adversarial training alone is not sufficient to solve robustness, even at greater model scales.",
		"language": "en",
		"note": "arXiv:2407.18213 [cs]",
		"number": "arXiv:2407.18213",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Effects of Scale on Language Model Robustness",
		"URL": "http://arxiv.org/abs/2407.18213",
		"author": [
			{
				"family": "Howe",
				"given": "Nikolaus"
			},
			{
				"family": "McKenzie",
				"given": "Ian"
			},
			{
				"family": "Hollinsworth",
				"given": "Oskar"
			},
			{
				"family": "Zajac",
				"given": "Michał"
			},
			{
				"family": "Tseng",
				"given": "Tom"
			},
			{
				"family": "Tucker",
				"given": "Aaron"
			},
			{
				"family": "Bacon",
				"given": "Pierre-Luc"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					24
				]
			]
		}
	},
	{
		"id": "taufeequePlanningRecurrentNeural2024",
		"type": "article",
		"abstract": "How a neural network (NN) generalizes to novel situations depends on whether it has learned to select actions heuristically or via a planning process. \"An investigation of model-free planning\" (Guez et al. 2019) found that a recurrent NN (RNN) trained to play Sokoban appears to plan, with extra computation steps improving the RNN's success rate. We replicate and expand on their behavioral analysis, finding the RNN learns to give itself extra computation steps in complex situations by \"pacing\" in cycles. Moreover, we train linear probes that predict the future actions taken by the network and find that intervening on the hidden state using these probes controls the agent's subsequent actions. Leveraging these insights, we perform model surgery, enabling the convolutional NN to generalize beyond its 10x10 architectural limit to arbitrarily sized inputs. The resulting model solves challenging, highly off-distribution levels. We open-source our model and code, and believe the neural network's small size (1.29M parameters) makes it an excellent model organism to deepen our understanding of learned planning.",
		"language": "en",
		"note": "arXiv:2407.15421 [cs]",
		"number": "arXiv:2407.15421",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Planning in a recurrent neural network that plays Sokoban",
		"URL": "http://arxiv.org/abs/2407.15421",
		"author": [
			{
				"family": "Taufeeque",
				"given": "Mohammad"
			},
			{
				"family": "Quirke",
				"given": "Philip"
			},
			{
				"family": "Li",
				"given": "Maximilian"
			},
			{
				"family": "Cundy",
				"given": "Chris"
			},
			{
				"family": "Tucker",
				"given": "Aaron David"
			},
			{
				"family": "Gleave",
				"given": "Adam"
			},
			{
				"family": "Garriga-Alonso",
				"given": "Adrià"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					24
				]
			]
		}
	},
	{
		"id": "guptaInterpBenchSemiSyntheticTransformers2024",
		"type": "article",
		"abstract": "Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents INTERPBENCH, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model’s output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr’s original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",
		"language": "en",
		"note": "arXiv:2407.14494 [cs]",
		"number": "arXiv:2407.14494",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
		"title-short": "InterpBench",
		"URL": "http://arxiv.org/abs/2407.14494",
		"author": [
			{
				"family": "Gupta",
				"given": "Rohan"
			},
			{
				"family": "Arcuschin",
				"given": "Iván"
			},
			{
				"family": "Kwa",
				"given": "Thomas"
			},
			{
				"family": "Garriga-Alonso",
				"given": "Adrià"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		}
	},
	{
		"id": "kwaCatastrophicGoodhartRegularizing2024",
		"type": "article",
		"abstract": "When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model–a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization.",
		"language": "en",
		"note": "arXiv:2407.14503 [cs]",
		"number": "arXiv:2407.14503",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification",
		"title-short": "Catastrophic Goodhart",
		"URL": "http://arxiv.org/abs/2407.14503",
		"author": [
			{
				"family": "Kwa",
				"given": "Thomas"
			},
			{
				"family": "Thomas",
				"given": "Drake"
			},
			{
				"family": "Garriga-Alonso",
				"given": "Adrià"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					8
				]
			]
		}
	},
	{
		"id": "ensignInvestigatingIndirectObject2024",
		"type": "article",
		"abstract": "How well will current interpretability techniques generalize to future models? A relevant case study is Mamba, a recent recurrent architecture with scaling comparable to Transformers. We adapt pre-Mamba techniques to Mamba and partially reverse-engineer the circuit responsible for the Indirect Object Identification (IOI) task. Our techniques provide evidence that 1) Layer 39 is a key bottleneck, 2) Convolutions in layer 39 shift names one position forward, and 3) The name entities are stored linearly in Layer 39’s SSM. Finally, we adapt an automatic circuit discovery tool, positional Edge Attribution Patching, to identify a Mamba IOI circuit. Our contributions provide initial evidence that circuit-based mechanistic interpretability tools work well for the Mamba architecture.",
		"language": "en",
		"note": "arXiv:2407.14008 [cs]",
		"number": "arXiv:2407.14008",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Investigating the Indirect Object Identification circuit in Mamba",
		"URL": "http://arxiv.org/abs/2407.14008",
		"author": [
			{
				"family": "Ensign",
				"given": "Danielle"
			},
			{
				"family": "Garriga-Alonso",
				"given": "Adrià"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					22
				]
			]
		}
	},
	{
		"id": "millerTransformerCircuitFaithfulness2024",
		"type": "article",
		"abstract": "Mechanistic interpretability work attempts to reverse engineer the learned algorithms present inside neural networks. One focus of this work has been to discover ‘circuits’ – subgraphs of the full model that explain behaviour on specific tasks. But how do we measure the performance of such circuits? Prior work has attempted to measure circuit ‘faithfulness’ – the degree to which the circuit replicates the performance of the full model. In this work, we survey many considerations for designing experiments that measure circuit faithfulness by ablating portions of the model’s computation. Concerningly, we find existing methods are highly sensitive to seemingly insignificant changes in the ablation methodology. We conclude that existing circuit faithfulness scores reflect both the methodological choices of researchers as well as the actual components of the circuit - the task a circuit is required to perform depends on the ablation used to test it. The ultimate goal of mechanistic interpretability work is to understand neural networks, so we emphasize the need for more clarity in the precise claims being made about circuits. We open source a library at this https URL that includes highly efficient implementations of a wide range of ablation methodologies and circuit discovery algorithms.",
		"language": "en",
		"note": "arXiv:2407.08734 [cs]",
		"number": "arXiv:2407.08734",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Transformer Circuit Faithfulness Metrics are not Robust",
		"URL": "http://arxiv.org/abs/2407.08734",
		"author": [
			{
				"family": "Miller",
				"given": "Joseph"
			},
			{
				"family": "Chughtai",
				"given": "Bilal"
			},
			{
				"family": "Saunders",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					11
				]
			]
		}
	},
	{
		"id": "hakimSwimTillYou2024",
		"type": "article",
		"abstract": "During 2023, two interesting results were proven about the limit behavior of game dynamics: First, it was shown that there is a game for which no dynamics converges to the Nash equilibria. Second, it was shown that the sink equilibria of a game adequately capture the limit behavior of natural game dynamics. These two results have created a need and opportunity to articulate a principled computational theory of the meaning of the game that is based on game dynamics. Given any game in normal form, and any prior distribution of play, we study the problem of computing the asymptotic behavior of a class of natural dynamics called the noisy replicator dynamics as a limit distribution over the sink equilibria of the game. When the prior distribution has pure strategy support, we prove this distribution can be computed efficiently, in near-linear time to the size of the best-response graph. When the distribution can be sampled -- for example, if it is the uniform distribution over all mixed strategy profiles -- we show through experiments that the limit distribution of reasonably large games can be estimated quite accurately through sampling and simulation.",
		"DOI": "10.48550/arXiv.2408.11146",
		"note": "arXiv:2408.11146",
		"number": "arXiv:2408.11146",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Swim till You Sink: Computing the Limit of a Game",
		"title-short": "Swim till You Sink",
		"URL": "http://arxiv.org/abs/2408.11146",
		"author": [
			{
				"family": "Hakim",
				"given": "Rashida"
			},
			{
				"family": "Milionis",
				"given": "Jason"
			},
			{
				"family": "Papadimitriou",
				"given": "Christos"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					20
				]
			]
		}
	},
	{
		"id": "vodrahalliMichelangeloLongContext2024",
		"type": "article",
		"abstract": "We introduce Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to ``chisel away'' the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, we query the model for details of the structure. Using LSQ, we produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. We perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information.",
		"DOI": "10.48550/arXiv.2409.12640",
		"note": "arXiv:2409.12640",
		"number": "arXiv:2409.12640",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries",
		"title-short": "Michelangelo",
		"URL": "http://arxiv.org/abs/2409.12640",
		"author": [
			{
				"family": "Vodrahalli",
				"given": "Kiran"
			},
			{
				"family": "Ontanon",
				"given": "Santiago"
			},
			{
				"family": "Tripuraneni",
				"given": "Nilesh"
			},
			{
				"family": "Xu",
				"given": "Kelvin"
			},
			{
				"family": "Jain",
				"given": "Sanil"
			},
			{
				"family": "Shivanna",
				"given": "Rakesh"
			},
			{
				"family": "Hui",
				"given": "Jeffrey"
			},
			{
				"family": "Dikkala",
				"given": "Nishanth"
			},
			{
				"family": "Kazemi",
				"given": "Mehran"
			},
			{
				"family": "Fatemi",
				"given": "Bahare"
			},
			{
				"family": "Anil",
				"given": "Rohan"
			},
			{
				"family": "Dyer",
				"given": "Ethan"
			},
			{
				"family": "Shakeri",
				"given": "Siamak"
			},
			{
				"family": "Vij",
				"given": "Roopali"
			},
			{
				"family": "Mehta",
				"given": "Harsh"
			},
			{
				"family": "Ramasesh",
				"given": "Vinay"
			},
			{
				"family": "Le",
				"given": "Quoc"
			},
			{
				"family": "Chi",
				"given": "Ed"
			},
			{
				"family": "Lu",
				"given": "Yifeng"
			},
			{
				"family": "Firat",
				"given": "Orhan"
			},
			{
				"family": "Lazaridou",
				"given": "Angeliki"
			},
			{
				"family": "Lespiau",
				"given": "Jean-Baptiste"
			},
			{
				"family": "Attaluri",
				"given": "Nithya"
			},
			{
				"family": "Olszewska",
				"given": "Kate"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					20
				]
			]
		}
	},
	{
		"id": "lampinenLearnedFeatureRepresentations2024",
		"type": "article-journal",
		"abstract": "Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. We then illustrate the downstream effects of these biases on various commonly-used methods for analyzing or intervening on representations. These results highlight a key challenge for interpretability---or for comparing the representations of models and brains---disentangling extraneous biases from the computationally important aspects of a system's internal representations.",
		"container-title": "Transactions on Machine Learning Research",
		"ISSN": "2835-8856",
		"language": "en",
		"source": "openreview.net",
		"title": "Learned feature representations are biased by complexity, learning order, position, and more",
		"URL": "https://openreview.net/forum?id=aY2nsgE97a",
		"author": [
			{
				"family": "Lampinen",
				"given": "Andrew Kyle"
			},
			{
				"family": "Chan",
				"given": "Stephanie C. Y."
			},
			{
				"family": "Hermann",
				"given": "Katherine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					13
				]
			]
		}
	},
	{
		"id": "morrisPromptingConsideredHarmful2024",
		"type": "article-journal",
		"abstract": "As a computer scientist with one foot in artificial intelligence (AI) research and the other in human-computer interaction (HCI) research, I have become increasingly concerned that promptinga has transitioned from what was essentially a test and debugging interface for machine-learning (ML) engineers into the de facto interaction paradigm for end users of large language models (LLMs) and their multimodal generative AI counterparts. It is my professional opinion that prompting is a poor user interface for generative AI systems, which should be phased out as quickly as possible. My concerns about prompting are twofold. First, prompt-based interfaces are confusing and non-optimal for end users (and ought not to be conflated with true natural-language interactions). Second, prompt-based interfaces are also risky for AI experts—we risk building a body of apps and research atop a shaky foundation of prompt engineering. I will discuss each of these issues in turn, below.",
		"container-title": "Communications of the ACM",
		"DOI": "10.1145/3673861",
		"ISSN": "0001-0782, 1557-7317",
		"journalAbbreviation": "Commun. ACM",
		"language": "en",
		"page": "3673861",
		"source": "DOI.org (Crossref)",
		"title": "Prompting Considered Harmful",
		"URL": "https://dl.acm.org/doi/10.1145/3673861",
		"author": [
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					17
				]
			]
		}
	},
	{
		"id": "tesslerAICanHelp2024",
		"type": "article-journal",
		"abstract": "Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human ...",
		"archive_location": "world",
		"container-title": "Science",
		"DOI": "10.1126/science.adq2852",
		"language": "EN",
		"license": "Copyright © 2024 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works",
		"note": "publisher: American Association for the Advancement of Science",
		"source": "www.science.org",
		"title": "AI can help humans find common ground in democratic deliberation",
		"URL": "https://www.science.org/doi/10.1126/science.adq2852",
		"author": [
			{
				"family": "Tessler",
				"given": "Michael Henry"
			},
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Jarrett",
				"given": "Daniel"
			},
			{
				"family": "Sheahan",
				"given": "Hannah"
			},
			{
				"family": "Chadwick",
				"given": "Martin J."
			},
			{
				"family": "Koster",
				"given": "Raphael"
			},
			{
				"family": "Evans",
				"given": "Georgina"
			},
			{
				"family": "Campbell-Gillingham",
				"given": "Lucy"
			},
			{
				"family": "Collins",
				"given": "Tantum"
			},
			{
				"family": "Parkes",
				"given": "David C."
			},
			{
				"family": "Botvinick",
				"given": "Matthew"
			},
			{
				"family": "Summerfield",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					18
				]
			]
		}
	},
	{
		"id": "barredoScientometricsAIBenchmarks2020",
		"type": "paper-conference",
		"abstract": "The widespread use of experimental benchmarks in AI research has created new competition and collaboration dynamics that are still poorly understood. In this paper we provide an innovative methodology to explore this dynamics and analyse the way different entrants in these competitions, from academia to tech giants, behave and react depending on their own or others’ achievements. We perform an analysis of over twenty popular benchmarks in AI, linking their underlying research papers. We identify links between researchers and institutions (i.e., communities) beyond the standard co-authorship relations, and we explore a series of hypotheses about their behaviour as well as some aggregated results in terms of activity, breakthroughs and efﬁciency. As a result, we detect and characterise the dynamics of research communities at different levels of abstraction, including organisation, afﬁliation, trajectories, results and activity.",
		"container-title": "1st International Workshop on Evaluating Progress in Artificial Intelligence",
		"event-title": "ECAI 2020",
		"language": "en",
		"source": "Zotero",
		"title": "The Scientometrics of AI Benchmarks: Unveiling the Underlying Mechanics of AI Research",
		"author": [
			{
				"family": "Barredo",
				"given": "Pablo"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "whittlestoneTensionOpennessPrudence2019",
		"type": "paper-conference",
		"abstract": "This paper explores the tension between openness and prudence in AI research, evident in two core principles of the Montréal Declaration for Responsible AI. While the AI community has strong norms around open sharing of research, concerns about the potential harms arising from misuse of research are growing, prompting some to consider whether the ﬁeld of AI needs to reconsider publication norms. We discuss how different beliefs and values can lead to differing perspectives on how the AI community should manage this tension, and explore implications for what responsible publication norms in AI research might look like in practice.",
		"container-title": "AI for Social Good workshop at NeurIPS",
		"language": "en",
		"source": "Zotero",
		"title": "The tension between openness and prudence in responsible AI research",
		"author": [
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Ovadya",
				"given": "Aviv"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "tragerInternationalGovernanceCivilian2023",
		"type": "article",
		"abstract": "This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified jurisdictions. Indeed, both the import and export standards could be required for certification. As international actors reach consensus on risks of and minimum standards for advanced AI, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.",
		"DOI": "10.48550/arXiv.2308.15514",
		"note": "arXiv:2308.15514",
		"number": "arXiv:2308.15514",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "International Governance of Civilian AI: A Jurisdictional Certification Approach",
		"title-short": "International Governance of Civilian AI",
		"URL": "http://arxiv.org/abs/2308.15514",
		"author": [
			{
				"family": "Trager",
				"given": "Robert"
			},
			{
				"family": "Harack",
				"given": "Ben"
			},
			{
				"family": "Reuel",
				"given": "Anka"
			},
			{
				"family": "Carnegie",
				"given": "Allison"
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Ho",
				"given": "Lewis"
			},
			{
				"family": "Kreps",
				"given": "Sarah"
			},
			{
				"family": "Lall",
				"given": "Ranjit"
			},
			{
				"family": "Larter",
				"given": "Owen"
			},
			{
				"family": "hÉigeartaigh",
				"given": "Seán Ó"
			},
			{
				"family": "Staffell",
				"given": "Simon"
			},
			{
				"family": "Villalobos",
				"given": "José Jaime"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					11
				]
			]
		}
	},
	{
		"id": "hernandez-oralloGeneralIntelligenceDisentangled2021",
		"type": "article-journal",
		"abstract": "Success in all sorts of situations is the most classical interpretation of general intelligence. Under limited resources, however, the capability of an agent must necessarily be limited too, and generality needs to be understood as comprehensive performance up to a level of difficulty. The degree of generality then refers to the way an agent’s capability is distributed as a function of task difficulty. This dissects the notion of general intelligence into two non-populational measures, generality and capability, which we apply to individuals and groups of humans, other animals and AI systems, on several cognitive and perceptual tests. Our results indicate that generality and capability can decouple at the individual level: very specialised agents can show high capability and vice versa. The metrics also decouple at the population level, and we rarely see diminishing returns in generality for those groups of high capability. We relate the individual measure of generality to traditional notions of general intelligence and cognitive efficiency in humans, collectives, non-human animals and machines. The choice of the difficulty function now plays a prominent role in this new conception of generality, which brings a quantitative tool for shedding light on long-standing questions about the evolution of general intelligence and the evaluation of progress in Artificial General Intelligence.",
		"container-title": "Scientific Reports",
		"DOI": "10.1038/s41598-021-01997-7",
		"ISSN": "2045-2322",
		"issue": "1",
		"journalAbbreviation": "Sci Rep",
		"language": "en",
		"license": "2021 The Author(s)",
		"note": "publisher: Nature Publishing Group",
		"page": "22822",
		"source": "www.nature.com",
		"title": "General intelligence disentangled via a generality metric for natural and artificial intelligence",
		"URL": "https://www.nature.com/articles/s41598-021-01997-7",
		"volume": "11",
		"author": [
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Loe",
				"given": "Bao Sheng"
			},
			{
				"family": "Cheke",
				"given": "Lucy"
			},
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Ó hÉigeartaigh",
				"given": "Seán"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					24
				]
			]
		}
	},
	{
		"id": "maasMilitaryArtificialIntelligence2022",
		"type": "article",
		"abstract": "Recent years have seen growing attention for the use of AI technologies in warfare, which has been rapidly advancing. This chapter explores in what ways such military AI technologies might contribute to Global Catastrophic Risks (GCR). After reviewing the GCR field’s limited previous engagement with military AI, and giving an overview of recent advances in military AI, this chapter focuses on two risk scenarios that have been proposed. First, we discuss arguments around the use of swarms of Lethal Autonomous Weapons Systems, and suggest that while these systems are concerning, they appear not yet likely to be a GCR in the near-term, on the basis of current and anticipated production limits and costs which make these systems still uncompetitive with extant systems for mass destruction. Second, we delve into the intersection of military AI and nuclear weapons, which we argue has a significantly higher GCR potential. We review historical debates over when, where, and why nuclear weapons could lead to GCR, along with recent geopolitical developments that could raise these risks further. We then outline six ways in which the use of AI systems in-, around-, or against- nuclear weapons and their command infrastructures could increase the likelihood of nuclear escalation and global catastrophe. The chapter concludes with suggestions for a research agenda that can gain a more comprehensive and multidisciplinary understanding of the potential risks from military AI, both today and in the future.",
		"DOI": "10.2139/ssrn.4115010",
		"event-place": "Rochester, NY",
		"genre": "SSRN Scholarly Paper",
		"language": "en",
		"number": "4115010",
		"publisher": "Social Science Research Network",
		"publisher-place": "Rochester, NY",
		"source": "papers.ssrn.com",
		"title": "Military Artificial Intelligence as Contributor to Global Catastrophic Risk",
		"URL": "https://papers.ssrn.com/abstract=4115010",
		"author": [
			{
				"family": "Maas",
				"given": "Matthijs M."
			},
			{
				"family": "Matteucci",
				"given": "Kayla"
			},
			{
				"family": "Cooke",
				"given": "Di"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					22
				]
			]
		}
	},
	{
		"id": "tzachorResponsibleArtificialIntelligence2022",
		"type": "article-journal",
		"abstract": "Global agriculture is poised to benefit from the rapid advance and diffusion of artificial intelligence (AI) technologies. AI in agriculture could improve crop management and agricultural productivity through plant phenotyping, rapid diagnosis of plant disease, efficient application of agrochemicals and assistance for growers with location-relevant agronomic advice. However, the ramifications of machine learning (ML) models, expert systems and autonomous machines for farms, farmers and food security are poorly understood and under-appreciated. Here, we consider systemic risk factors of AI in agriculture. Namely, we review risks relating to interoperability, reliability and relevance of agricultural data, unintended socio-ecological consequences resulting from ML models optimized for yields, and safety and security concerns associated with deployment of ML platforms at scale. As a response, we suggest risk-mitigation measures, including inviting rural anthropologists and applied ecologists into the technology design process, applying frameworks for responsible and human-centred innovation, setting data cooperatives for improved data transparency and ownership rights, and initial deployment of agricultural AI in digital sandboxes.",
		"container-title": "Nature Machine Intelligence",
		"DOI": "10.1038/s42256-022-00440-4",
		"ISSN": "2522-5839",
		"issue": "2",
		"journalAbbreviation": "Nat Mach Intell",
		"language": "en",
		"license": "2022 Springer Nature Limited",
		"note": "publisher: Nature Publishing Group",
		"page": "104-109",
		"source": "www.nature.com",
		"title": "Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities",
		"URL": "https://www.nature.com/articles/s42256-022-00440-4",
		"volume": "4",
		"author": [
			{
				"family": "Tzachor",
				"given": "Asaf"
			},
			{
				"family": "Devare",
				"given": "Medha"
			},
			{
				"family": "King",
				"given": "Brian"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Ó hÉigeartaigh",
				"given": "Seán"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2
				]
			]
		}
	},
	{
		"id": "zoecremerArtificialCanariesEarly2021",
		"type": "article-journal",
		"abstract": "We propose a method for identifying early warning signs of transformative progress in artificial intelligence (AI), and discuss how these can support the anticipatory and democratic governance of AI. We call these early warning signs ‘canaries’, based on the use of canaries to provide early warnings of unsafe air pollution in coal mines. Our method combines expert elicitation and collaborative causal graphs to identify key milestones and identify the relationships between them. We present two illustrations of how this method could be used: to identify early warnings of harmful impacts of language models; and of progress towards high-level machine intelligence. Identifying early warning signs of transformative applications can support more efficient monitoring and timely regulation of progress in AI: as AI advances, its impacts on society may be too great to be governed retrospectively. It is essential that those impacted by AI have a say in how it is governed. Early warnings can give the public time and focus to influence emerging technologies using democratic, participatory technology assessments. We discuss the challenges in identifying early warning signals and propose directions for future work.",
		"container-title": "International Journal of Interactive Multimedia and Artificial Intelligence",
		"DOI": "10.9781/ijimai.2021.02.011",
		"ISSN": "1989-1660",
		"issue": "5",
		"journalAbbreviation": "IJIMAI",
		"language": "en",
		"page": "100",
		"source": "DOI.org (Crossref)",
		"title": "Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI",
		"title-short": "Artificial Canaries",
		"URL": "https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf",
		"volume": "6",
		"author": [
			{
				"family": "Zoe Cremer",
				"given": "Carla"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "belfieldComputeAntitrust2022",
		"type": "article-journal",
		"abstract": "Compute or computing power refers to a software and hardware stack, such as in a data centre or computer, engineered for AI-specific applications. We argue that the antitrust and regulatory literature to date has failed to pay sufficient attention to compute, despite compute being a key input to AI progress and services, the potentially substantial market power of companies in the supply chain, and the advantages of compute as a ‘unit’ of regulation in terms of detection and remedies.",
		"container-title": "Verfassungsblog",
		"DOI": "10.17176/20220819-181907-0",
		"ISSN": "2366-7044",
		"language": "eng",
		"license": "CC BY-SA 4.0",
		"note": "publisher: Verfassungsblog",
		"source": "verfassungsblog.de",
		"title": "Compute and Antitrust",
		"URL": "https://verfassungsblog.de/compute-and-antitrust/",
		"author": [
			{
				"family": "Belfield",
				"given": "Haydn"
			},
			{
				"family": "Hua",
				"given": "Shin-Shin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					19
				]
			]
		}
	},
	{
		"id": "whittlestoneSocietalImplicationsDeep2021",
		"type": "article-journal",
		"abstract": "Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications.",
		"DOI": "10.1613/jair.1.12360",
		"ISSN": "1076-9757",
		"language": "eng",
		"note": "publisher: AI Access Foundation",
		"source": "www.repository.cam.ac.uk",
		"title": "The Societal Implications of Deep Reinforcement Learning",
		"URL": "https://www.repository.cam.ac.uk/handle/1810/318700",
		"author": [
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Arulkumaran",
				"given": "Kai"
			},
			{
				"family": "Crosby",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					8
				]
			]
		}
	},
	{
		"id": "gruetzemacherTransformativePotentialArtificial2022",
		"type": "article-journal",
		"abstract": "The terms ‘human-level artificial intelligence’ and ‘artificial general intelligence’ are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term ‘transformative AI’ is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be ‘transformative’ is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.",
		"container-title": "Futures",
		"DOI": "10.1016/j.futures.2021.102884",
		"ISSN": "0016-3287",
		"journalAbbreviation": "Futures",
		"page": "102884",
		"source": "ScienceDirect",
		"title": "The transformative potential of artificial intelligence",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0016328721001932",
		"volume": "135",
		"author": [
			{
				"family": "Gruetzemacher",
				"given": "Ross"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					1
				]
			]
		}
	},
	{
		"id": "huaAIAntitrustReconciling2021",
		"type": "article-journal",
		"abstract": "Cooperation between companies developing artificial intelligence (AI) can help them create AI systems that are safe, secure, and with broadly shared benefits. Researchers have proposed a range of cooperation strategies, ranging from redistributing “windfall” profits to assistance to address the harmful dynamics of a competitive race for technological superiority.\n\nA critical tension arises, however, between cooperation and the goal of competition law, which is to protect the very process of competition between rival companies. Whilst these potential conflicts are significant, they are currently underexplored in the literature. This paper examines the relationship between proposed forms of AI cooperation and competition law, focusing on the competition law of the European Union (EU).\n\nEU competition law governs the behavior of the world’s largest AI companies, though many are based abroad, especially in the US. Its jurisdiction can extend to any foreign company that is active in the EU. Scrutiny of US “Big Tech” is also an area of strategic focus for the European Commission (EC).\n\nThis paper seeks to reconcile the cooperative AI development and competition law. It examines fourteen forms of AI cooperation, both those that are applicable today and longer-term strategies that will apply when AI development is more advanced. Where we identify potential tensions with EU competition law, we suggest mitigation steps. Our aim is to ensure the long-term sustainability of these important safeguards to the responsible and beneficial development of AI.",
		"language": "en",
		"source": "Zotero",
		"title": "AI & Antitrust: Reconciling Tensions Between Competition Law and Cooperative AI Development",
		"author": [
			{
				"family": "Hua",
				"given": "Shin-Shin"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "jayanthiItTakesVillage2024",
		"type": "chapter",
		"abstract": "Expectations around future capabilities of lethal autonomous weapons systems (LAWS) have raised concerns for military risks, ethics, and accountability. The U.K.’s position, as presented among various international voices at the UN’s Convention on Certain Conventional Weapons (CCW) meetings, has attempted to address these concerns through a focused look at the weapons review process, humanmachine teaming or “meaningful human control” (see e.g. JCN1/18), and the ability of autonomous systems to adhere to the Rules of Engagement. Further, the U.K. has stated that the existing governance structures—both domestic and international—around weapons systems are sufficient in dealing with any concerns around the development, deployment, and accountability for emerging LAWS; there is no need for novel agreements on the control of these weapons systems. In an effort to better understand and test the U.K. position on LAWS, the Centre for the Study of Existential Risk has run a research project in which we interviewed experts in multiple relevant organisations, structured around a mock parliamentary inquiry of a hypothetical LAWS-related civilian death. The responses to this scenario have highlighted different, sometimes complementary and sometimes contradicting, conceptions of future systems, challenges, and accountability measures. They have provided rich \"on the ground” perspectives, while also highlighting key gaps that should be addressed by every military that is considering acquisition and deployment of autonomous and semi-autonomous weapon systems.",
		"container-title": "An Anthology of Global Risk",
		"edition": "1",
		"event-place": "Cambridge, UK",
		"ISBN": "978-1-80511-114-6",
		"language": "en",
		"license": "https://creativecommons.org/licenses/by-nc/4.0/",
		"note": "DOI: 10.11647/obp.0360.21",
		"page": "603-612",
		"publisher": "Open Book Publishers",
		"publisher-place": "Cambridge, UK",
		"source": "DOI.org (Crossref)",
		"title": "It Takes a Village: The Shared Responsibility of “Raising” an Autonomous Weapon",
		"title-short": "21. It Takes a Village",
		"URL": "https://www.openbookpublishers.com/books/10.11647/obp.0360/chapters/10.11647/obp.0360.21",
		"editor": [
			{
				"family": "Beard",
				"given": "Sj"
			},
			{
				"family": "Hobson",
				"given": "Tom"
			}
		],
		"author": [
			{
				"family": "Jayanthi",
				"given": "Amritha"
			},
			{
				"family": "Avin",
				"given": "Shahar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					3
				]
			]
		}
	},
	{
		"id": "cihonFragmentationFutureInvestigating2020",
		"type": "article-journal",
		"abstract": "The international governance of artificial intelligence (AI) is at a crossroads: should it remain fragmented or be centralised? We draw on the history of environment, trade, and security regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak for centralisation. The risk of creating a slow and brittle institution, and the difficulty of pairing deep rules with adequate participation, speak against it. Other considerations depend on the specific design. A centralised body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial, and fragmented institutions could self-organise. In sum, these trade-offs should inform development of the AI governance architecture, which is only now emerging. We apply the trade-offs to the case of the potential development of high-level machine intelligence. We conclude with two recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, fragmentation will likely persist for now. The developing landscape should be monitored to see if it is self-organising or simply inadequate.",
		"container-title": "Global Policy",
		"DOI": "10.1111/1758-5899.12890",
		"ISSN": "1758-5899",
		"issue": "5",
		"language": "en",
		"license": "© 2020 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd",
		"note": "_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12890",
		"page": "545-556",
		"source": "Wiley Online Library",
		"title": "Fragmentation and the Future: Investigating Architectures for International AI Governance",
		"title-short": "Fragmentation and the Future",
		"URL": "https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12890",
		"volume": "11",
		"author": [
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			},
			{
				"family": "Kemp",
				"given": "Luke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "stixBridgingGapCase2021",
		"type": "article-journal",
		"abstract": "Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.",
		"container-title": "AI and Ethics",
		"DOI": "10.1007/s43681-020-00037-w",
		"ISSN": "2730-5961",
		"issue": "3",
		"journalAbbreviation": "AI Ethics",
		"language": "en",
		"page": "261-271",
		"source": "Springer Link",
		"title": "Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy",
		"title-short": "Bridging the gap",
		"URL": "https://doi.org/10.1007/s43681-020-00037-w",
		"volume": "1",
		"author": [
			{
				"family": "Stix",
				"given": "Charlotte"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8,
					1
				]
			]
		}
	},
	{
		"id": "whittlestoneWhyHowGovernments2021",
		"type": "article",
		"abstract": "In this paper we outline a proposal for improving the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this would give governments greater information about the AI ecosystem, equipping them to more effectively direct AI development and deployment in the most societally and economically beneficial directions. It would also create infrastructure that could rapidly identify potential threats or harms that could occur as a consequence of changes in the AI ecosystem, such as the emergence of strategically transformative capabilities, or the deployment of harmful systems. We begin by outlining the problem which motivates this proposal: in brief, traditional governance approaches struggle to keep pace with the speed of progress in AI. We then present our proposal for addressing this problem: governments must invest in measurement and monitoring infrastructure. We discuss this proposal in detail, outlining what specific things governments could focus on measuring and monitoring, and the kinds of benefits this would generate for policymaking. Finally, we outline some potential pilot projects and some considerations for implementing this in practice.",
		"language": "en",
		"note": "arXiv:2108.12427 [cs]",
		"number": "arXiv:2108.12427",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Why and How Governments Should Monitor AI Development",
		"URL": "http://arxiv.org/abs/2108.12427",
		"author": [
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Clark",
				"given": "Jack"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8,
					31
				]
			]
		}
	},
	{
		"id": "maasAligningAIRegulation2021",
		"type": "article",
		"abstract": "How do we regulate a changing technology, with changing uses, in a changing world? This chapter argues that while existing (inter)national AI governance approaches are important, they are often siloed. Technology-centric approaches focus on individual AI applications; law-centric approaches emphasize AI’s effects on pre-existing legal fields or doctrines. This chapter argues that to foster a more systematic, functional and effective AI regulatory ecosystem, policy actors should instead complement these approaches with a regulatory perspective that emphasizes how, when, and why AI applications enable patterns of ‘sociotechnical change’. Drawing on theories from the emerging field of ‘TechLaw’, it explores how this perspective can provide informed, more nuanced, and actionable perspectives on AI regulation. A focus on sociotechnical change can help analyze when and why AI applications actually do create a meaningful rationale for new regulation — and how they are consequently best approached as targets for regulatory intervention, considering not just the technology, but also six distinct ‘problem logics’ that appear around AI issues across domains. The chapter concludes by briefly reviewing concrete institutional and regulatory actions that can draw on this approach in order to improve the regulatory triage, tailoring, timing & responsiveness, and design of AI policy.",
		"DOI": "10.2139/ssrn.3871635",
		"event-place": "Rochester, NY",
		"genre": "SSRN Scholarly Paper",
		"language": "en",
		"number": "3871635",
		"publisher": "Social Science Research Network",
		"publisher-place": "Rochester, NY",
		"source": "papers.ssrn.com",
		"title": "Aligning AI Regulation to Sociotechnical Change",
		"URL": "https://papers.ssrn.com/abstract=3871635",
		"author": [
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					16
				]
			]
		}
	},
	{
		"id": "maasAIGovernanceDisplacement2021",
		"type": "article-journal",
		"abstract": "The emergence, proliferation, and use of new general-purpose technologies can often produce significant political, redistributive, normative and legal effects on the world. Artificial intelligence (AI) has been identified as one such transformative technology. Many of its impacts may require global governance responses. However, what are the direct and indirect effects of AI technologies on the viability, form, or functioning of the international legal order itself? What, if any, are the prospects, peril or promise of AI-driven legal automation at the international level? This paper draws on an ‘AI Governance Disruption’ framework to understanding AI’s impacts on the global governance architecture. Focusing particularly on the potential for legal automation at the international law level, it explores three potential pathways of such ‘legal displacement’: (1) the automation of rule creation and arbitration; (2) the automation of monitoring & enforcement; or (3) the ‘replacement’ of international law with new architectural modes of (international) behaviour control. It then focuses on the effects of these trends on the architecture of international law. It distinguishes 10 different roles that AI applications could play, with distinct effects on the international legal order. That is, AI systems can serve as (1) legal ‘canary in the coal mine’, highlighting the need for greater cross-regime harmonization. However, it can also serve as (2) tough knot or (3) generator of regime fault lines. Under even modest scenarios of legal automation, AI systems may serve variably as a (4) shield, (5) patch, (6) cure, or (7) accelerator of international legal fragmentation. Finally, AI tools may serve as (8) differential enabler; (9) driver of value shifts, or (10) asymmetric weapon, potentially contributing to trends of contestation or erosion in the international legal order. The paper concludes with a brief review of the ways in which international lawyers or regime scholars might approach the risks and opportunities of increasing automation in international law, in order to leverage these trends and tools towards improved efficacy, resilience, and legitimacy of global governance.",
		"container-title": "SSRN Electronic Journal",
		"DOI": "10.2139/ssrn.3806624",
		"ISSN": "1556-5068",
		"journalAbbreviation": "SSRN Journal",
		"language": "en",
		"source": "DOI.org (Crossref)",
		"title": "AI, Governance Displacement, and the (De)Fragmentation of International Law",
		"URL": "https://www.ssrn.com/abstract=3806624",
		"author": [
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "Cook2022",
		"type": "webpage",
		"abstract": "When should funders wanting to increase the probability of AGI going well spend their money? We have created a tool to calculate the optimum spending schedule and tentatively conclude funders collectively should be spending at least 5% of their capital each year on AI risk interventions and in some cases up to 35%.\n\nThis is likely higher than the current AI risk community spending rate which is at most 3%48. In most cases, we find that the optimal spending schedule is between 5% and 15% better than the ‘default’ strategy of just spending the interest one accrues and from 15% to 50% better than a naive projection of the community’s spending rate49.\n\nWe strongly encourage users to put their own inputs into the tool to draw their own conclusions.\n\nThe key finding of a higher spending rate is supported by two distinct models we have created, one that splits spending of capital into research and influence, and a second model  (the ‘alternate model’) that supposes we can spend our stock of things that grow on direct work. We focus on the former with the latter described in the appendix since its output is more obviously action-guiding50.\n\nThe table below shows our best guess for the optimal spending schedule using the former model when varying the difficulty of achieving a good AGI outcome and AGI timelines. We keep other inputs, such as diminishing returns to spending and interest rate constant51.\n\nSome of the critical limitations of our model include: poorly modelling exogenous research, which is particularly important for those with longer timelines, and many parts of the model - such as diminishing returns - remaining constant over time.\n\nFurther, we find that robust spending strategies - those that work in a wide variety of worlds - also support a higher spending rate. We show the results of a Monte Carlo simulation in the appendix60.",
		"note": "tex.howpublished: Effective Altruism Forum\ntex.pubstate: published\ntex.tppubtype: online",
		"title": "The optimal timing of spending on AGI safety work; why we should probably be spending more now",
		"URL": "https://longtermrisk.org/the-optimal-timing-of-spending-on-agi-safety-work-why-we-should-probably-be-spending-more-now/, HTML https://forum.effectivealtruism.org/posts/Ne8ZS6iJJp7EpzztP/the-optimal-timing-of-spending-on-agi-safety-work-why-we, EA Forum",
		"author": [
			{
				"family": "Cook",
				"given": "Tristan"
			},
			{
				"family": "Corlouer",
				"given": "Guillaume"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					29
				]
			]
		}
	},
	{
		"id": "clifton-collaborative-game-2021",
		"type": "webpage",
		"abstract": "Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient. \n\n In another post, I described the \"prior selection problem\", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on solution concepts or surrogate goals / safe Pareto improvements seem to require agents to have a common, explicit game-theoretic model.\n\nIn this post, I introduce collaborative game specification (CGS), a family of techniques designed to address the problem of agents lacking a shared model. In CGS, agents agree on a common model of their bargaining situation and use this to come to an agreement. Here is the basic idea:\n\n    Two agents are playing an unknown game. They each have private models of this game. (These may be explicit models, as in model-based reinforcement learning, or models implicit in a black-box policy which can be extracted.) By default, they will use these models to make a decision. The problem is that their models may differ, possibly leading to bad outcomes and precluding the use of bargaining protocols which require a shared, explicit model.\n     Rather than using these default strategies, agents agree on a common model, talk, and use this model to reach an agreement.\n\nOf course, when agreeing on a common model, agents must handle incentives for their counterparts to deceive each other. In the toy illustration below, we’ll see how handling incentives to misrepresent one’s model can be handled in a pure cheap-talk setting. \n\nHow might we use CGS to reduce the risks of conflict involving powerful AI systems? One use is to provide demonstrations of good bargaining behavior. Some approaches to AI development may involve training AI systems to imitate the behavior of some demonstrator (e.g., imitative amplification), and so we may need to be able to provide many demonstrations of good bargaining behavior to ensure that the resulting system is robustly able and motivated to bargain successfully. Another is to facilitate bargaining between humans with powerful AI tools, e.g. in a comprehensive AI services scenario. \n\nAside from actually implementing CGS in AI systems, studying protocols of this kind can give us a better understanding of the limits on agents’ ability to overcome differences in their private models. Under the simple version of CGS discussed here, because agents have to incentivize truth-telling by refusing to engage in CGS sometimes, agents will fail to agree on a common model with positive probability in equilibrium.  \n\nI will first give a toy example of CGS (Section 1), and then discuss how it might be implemented in practice (Section 2). I close by discussing some potential problems and open questions for CGS (Section 3). In the Appendix, I discuss a game-theoretic formalism in which CGS can be given a more rigorous basis.",
		"note": "tex.howpublished: Working paper\ntex.pubstate: published\ntex.tppubtype: online",
		"title": "Collaborative game specification: arriving at common models in bargaining",
		"URL": "https://longtermrisk.org/collaborative-game-specification/, HTML",
		"author": [
			{
				"family": "Clifton",
				"given": "Jesse"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					6
				]
			]
		}
	},
	{
		"id": "hsuSafetyLivenessGuarantees2021",
		"type": "paper-conference",
		"abstract": "Reach-avoid optimal control problems; in which the system must reach certain goal conditions while staying clear of unacceptable failure modes; are central to safety and liveness assurance for autonomous robotic systems; but their exact solutions are intractable for complex dynamics and environments. Recent successes in the use of reinforcement learning methods to approximately solve optimal control problems with performance objectives make their application to certification problems attractive; however; the Lagrange-type objective (cumulative costs or rewards over time) used in reinforcement learning is not suitable to encode temporal logic requirements. Recent work has shown promise in extending the reinforcement learning machinery to safety-type problems; whose objective is not a sum but a minimum over time. In this work; we generalize the reinforcement learning formulation to handle all optimal control problems in the reach-avoid category. We derive a time-discounted reach-avoid Bellman backup with contraction mapping properties and prove that the resulting reach-avoid Q-learning algorithm converges under analogous conditions to the traditional Lagrange-type problem; yielding an arbitrarily tight conservative approximation to the reach-avoid set. We further demonstrate the use of this formulation with deep reinforcement learning methods; by treating their approximate solutions as untrusted oracles in a supervisory control framework. We evaluate our proposed framework on a range of nonlinear systems; validating the results against analytic and numerical solutions; and through Monte Carlo simulation in previously intractable problems. Our results open the door to a range of learning-based methods for safe-and-live autonomous behavior; with applications across robotics and automation.",
		"event-title": "Robotics: Science and Systems XVII",
		"ISBN": "978-0-9923747-7-8",
		"source": "www.roboticsproceedings.org",
		"title": "Safety and Liveness Guarantees through Reach-Avoid Reinforcement Learning",
		"URL": "https://www.roboticsproceedings.org/rss17/p077.html",
		"volume": "17",
		"author": [
			{
				"family": "Hsu",
				"given": "Kai-Chieh"
			},
			{
				"family": "Rubies-Royo",
				"given": "Vicenç"
			},
			{
				"family": "Tomlin",
				"given": "Claire"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					12
				]
			]
		}
	},
	{
		"id": "huSHARPShieldingAwareRobust2022",
		"type": "article-journal",
		"abstract": "Jointly achieving safety and efficiency in human-robot interaction settings is a challenging problem, as the robot’s planning objectives may be at odds with the human’s own intent and expectations. Recent approaches ensure safe robot operation in uncertain environments through a supervisory control scheme, sometimes called “shielding,” which overrides the robot’s nominal plan with a safety fallback strategy when a safety-critical event is imminent. These reactive “last-resort” strategies (typically in the form of aggressive emergency maneuvers) focus on preserving safety without efficiency considerations; when the nominal planner is unaware of possible safety overrides, shielding can be activated more frequently than necessary, leading to degraded performance. In this letter, we propose a new shielding-based planning approach that allows the robot to plan efficiently by explicitly accounting for possible future shielding events. Leveraging recent work on Bayesian human motion prediction, the resulting robot policy proactively balances nominal performance with the risk of high-cost emergency maneuvers triggered by low-probability human behaviors. We formalize Shielding-Aware Robust Planning (SHARP) as a stochastic optimal control problem and propose a computationally efficient framework for finding tractable approximate solutions at runtime. Our method outperforms the shielding-agnostic motion planning baseline (equipped with the same human intent inference scheme) on simulated driving examples with human trajectories taken from the recently released Waymo Open Motion Dataset.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2022.3155229",
		"ISSN": "2377-3766",
		"issue": "2",
		"note": "event-title: IEEE Robotics and Automation Letters",
		"page": "5591-5598",
		"source": "IEEE Xplore",
		"title": "SHARP: Shielding-Aware Robust Planning for Safe and Efficient Human-Robot Interaction",
		"title-short": "SHARP",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9723544",
		"volume": "7",
		"author": [
			{
				"family": "Hu",
				"given": "Haimin"
			},
			{
				"family": "Nakamura",
				"given": "Kensuke"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4
				]
			]
		}
	},
	{
		"id": "huActiveUncertaintyReduction2023",
		"type": "chapter",
		"abstract": "The ability to accurately predict human behavior is central to the safety and efficiency of robot autonomy in interactive settings. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as people’s goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning, mainly due to the fundamental coupling between robot trajectory optimization and human intent inference. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for a broad class of predictive human models with both continuous and categorical uncertainty. The efficacy of our approach is demonstrated with simulated driving examples.",
		"container-title": "Algorithmic Foundations of Robotics XV",
		"event-place": "Cham",
		"ISBN": "978-3-031-21089-1",
		"language": "en",
		"note": "collection-title: Springer Proceedings in Advanced Robotics\nDOI: 10.1007/978-3-031-21090-7_23",
		"page": "385-401",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Active Uncertainty Reduction for Human-Robot Interaction: An Implicit Dual Control Approach",
		"title-short": "Active Uncertainty Reduction for Human-Robot Interaction",
		"URL": "https://link.springer.com/10.1007/978-3-031-21090-7_23",
		"volume": "25",
		"editor": [
			{
				"family": "LaValle",
				"given": "Steven M."
			},
			{
				"family": "O’Kane",
				"given": "Jason M."
			},
			{
				"family": "Otte",
				"given": "Michael"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Tokekar",
				"given": "Pratap"
			}
		],
		"author": [
			{
				"family": "Hu",
				"given": "Haimin"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hsuSimLabRealSafeReinforcement2023",
		"type": "article-journal",
		"abstract": "Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.",
		"container-title": "Artificial Intelligence",
		"DOI": "10.1016/j.artint.2022.103811",
		"ISSN": "0004-3702",
		"journalAbbreviation": "Artificial Intelligence",
		"page": "103811",
		"source": "ScienceDirect",
		"title": "Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees",
		"title-short": "Sim-to-Lab-to-Real",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0004370222001515",
		"volume": "314",
		"author": [
			{
				"family": "Hsu",
				"given": "Kai-Chieh"
			},
			{
				"family": "Ren",
				"given": "Allen Z."
			},
			{
				"family": "Nguyen",
				"given": "Duy P."
			},
			{
				"family": "Majumdar",
				"given": "Anirudha"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					1
				]
			]
		}
	},
	{
		"id": "hsuISAACSIterativeSoft2023a",
		"type": "paper-conference",
		"abstract": "The deployment of robots in uncontrolled environments requires them to operate \nrobustly under previously unseen scenarios, like irregular terrain and wind conditions. \nUnfortunately, while rigorous safety frameworks from robust optimal control theory \nscale poorly to high-dimensional nonlinear dynamics, control policies computed by \nmore tractable “deep” methods lack guarantees and tend to exhibit little robustness \nto uncertain operating conditions. This work introduces a novel approach enabling scalable \nsynthesis of robust safety-preserving controllers for robotic systems with general \nnonlinear dynamics subject to bounded modeling error, by combining game-theoretic safety \nanalysis with adversarial reinforcement learning in simulation. Following a soft actor-\ncritic scheme, a safety-seeking fallback policy is co-trained with an adversarial \n“disturbance” agent that aims to invoke the worst-case realization of model error and \ntraining-to-deployment discrepancy allowed by the designer’s uncertainty. While the \nlearned control policy does not intrinsically guarantee safety, it is used to construct a\nreal-time safety filter with robust safety guarantees based on forward reachability \nrollouts. This safety filter can be used in conjunction with a safety-agnostic control\npolicy, precluding any task-driven actions that could result in loss of safety. We \nevaluate our learning-based safety approach in a 5D race car simulator, compare the \nlearned safety policy to the numerically obtained optimal solution, and empiricall \nvalidate the robust safety guarantee of our proposed safety filter against worst-case \nmodel discrepancy.",
		"container-title": "Proceedings of The 5th Annual Learning for Dynamics and Control Conference",
		"event-title": "Learning for Dynamics and Control Conference",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "90-103",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "ISAACS: Iterative Soft Adversarial Actor-Critic for Safety",
		"title-short": "ISAACS",
		"URL": "https://proceedings.mlr.press/v211/hsu23a.html",
		"author": [
			{
				"family": "Hsu",
				"given": "Kai-Chieh"
			},
			{
				"family": "Nguyen",
				"given": "Duy Phuong"
			},
			{
				"family": "Fisac",
				"given": "Jaime Fernàndez"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					6
				]
			]
		}
	},
	{
		"id": "hsuInterpretableTrajectoryPrediction2023",
		"type": "paper-conference",
		"abstract": "The ability to anticipate surrounding agents' behaviors is critical to enable safe and seamless autonomous vehicles (AVs). While phenomenological methods have successfully predicted future trajectories from scene context, these predictions lack interpretability. On the other hand, ontological approaches assume an underlying structure able to describe the interaction dynamics or agents' internal decision processes. Still, they often suffer from poor scalability or cannot reflect diverse human behaviors. This work proposes an interpretability framework for a phenomenological method through responsibility evaluations. We formulate responsibility as a measure of how much an agent takes into account the welfare of other agents through counterfactual reasoning. Additionally, this framework abstracts the computed responsibility sequences into different responsibility levels and grounds these latent levels into reward functions. The proposed responsibility-based interpretability framework is modular and easily integrated into a wide range of prediction models. To demonstrate the utility of the proposed framework in providing added interpretability, we adapt an existing AV prediction model and perform a simulation study on a real-world nuScenes traffic dataset. Experimental results show that we can perform offline ex-post traffic analysis by incorporating the responsibility signal and rendering interpretable but accurate online trajectory predictions.",
		"container-title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"DOI": "10.1109/IROS55552.2023.10341712",
		"event-title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"note": "ISSN: 2153-0866",
		"page": "5918-5925",
		"source": "IEEE Xplore",
		"title": "Interpretable Trajectory Prediction for Autonomous Vehicles via Counterfactual Responsibility",
		"URL": "https://ieeexplore.ieee.org/document/10341712",
		"author": [
			{
				"family": "Hsu",
				"given": "Kai-Chieh"
			},
			{
				"family": "Leung",
				"given": "Karen"
			},
			{
				"family": "Chen",
				"given": "Yuxiao"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			},
			{
				"family": "Pavone",
				"given": "Marco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10
				]
			]
		}
	},
	{
		"id": "huDeceptionGameClosing2023",
		"type": "article",
		"abstract": "An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot's evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot's learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework's ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.",
		"DOI": "10.48550/arXiv.2309.01267",
		"note": "arXiv:2309.01267",
		"number": "arXiv:2309.01267",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy",
		"title-short": "Deception Game",
		"URL": "http://arxiv.org/abs/2309.01267",
		"author": [
			{
				"family": "Hu",
				"given": "Haimin"
			},
			{
				"family": "Zhang",
				"given": "Zixu"
			},
			{
				"family": "Nakamura",
				"given": "Kensuke"
			},
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "Fisac",
				"given": "Jaime F."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					1
				]
			]
		}
	},
	{
		"id": "liangIntrospectivePlanningAligning2024",
		"type": "article",
		"abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty–aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries. Code is available at https://github.com/kevinliang888/IntroPlan.",
		"language": "en",
		"note": "arXiv:2402.06529 [cs]",
		"number": "arXiv:2402.06529",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity",
		"title-short": "Introspective Planning",
		"URL": "http://arxiv.org/abs/2402.06529",
		"author": [
			{
				"family": "Liang",
				"given": "Kaiqu"
			},
			{
				"family": "Zhang",
				"given": "Zixu"
			},
			{
				"family": "Fisac",
				"given": "Jaime Fernández"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					4
				]
			]
		}
	},
	{
		"id": "mandiEffectivenessFinetuningMetareinforcement2023",
		"type": "article",
		"abstract": "Intelligent agents should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly and efficiently. Meta-learning approaches have emerged as a popular solution to achieve this. However, meta-reinforcement learning (meta-RL) algorithms have thus far been restricted to simple environments with narrow task distributions. Moreover, the paradigm of pretraining followed by fine-tuning to adapt to new tasks has emerged as a simple yet effective solution in supervised and self-supervised learning. This calls into question the benefits of meta-learning approaches also in reinforcement learning, which typically come at the cost of high complexity. We hence investigate meta-RL approaches in a variety of vision-based benchmarks, including Procgen, RLBench, and Atari, where evaluations are made on completely novel tasks. Our findings show that when meta-learning approaches are evaluated on different tasks (rather than different variations of the same task), multi-task pretraining with fine-tuning on new tasks performs equally as well, or better, than meta-pretraining with meta test-time adaptation. This is encouraging for future research, as multi-task pretraining tends to be simpler and computationally cheaper than meta-RL. From these findings, we advocate for evaluating future meta-RL methods on more challenging tasks and including multi-task pretraining with fine-tuning as a simple, yet strong baseline.",
		"DOI": "10.48550/arXiv.2206.03271",
		"note": "arXiv:2206.03271",
		"number": "arXiv:2206.03271",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning",
		"URL": "http://arxiv.org/abs/2206.03271",
		"author": [
			{
				"family": "Mandi",
				"given": "Zhao"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "James",
				"given": "Stephen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					16
				]
			]
		}
	},
	{
		"id": "laskinCICContrastiveIntrinsic2022",
		"type": "article",
		"abstract": "We introduce Contrastive Intrinsic Control (CIC), an algorithm for unsupervised skill discovery that maximizes the mutual information between state-transitions and latent skill vectors. CIC utilizes contrastive learning between state-transitions and skills to learn behavior embeddings and maximizes the entropy of these embeddings as an intrinsic reward to encourage behavioral diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. CIC substantially improves over prior methods in terms of adaptation efficiency, outperforming prior unsupervised skill discovery methods by 1.79x and the next leading overall exploration algorithm by 1.18x.",
		"DOI": "10.48550/arXiv.2202.00161",
		"note": "arXiv:2202.00161",
		"number": "arXiv:2202.00161",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery",
		"title-short": "CIC",
		"URL": "http://arxiv.org/abs/2202.00161",
		"author": [
			{
				"family": "Laskin",
				"given": "Michael"
			},
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Peng",
				"given": "Xue Bin"
			},
			{
				"family": "Yarats",
				"given": "Denis"
			},
			{
				"family": "Rajeswaran",
				"given": "Aravind"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					30
				]
			]
		}
	},
	{
		"id": "yangChainThoughtImitation2022",
		"type": "article",
		"abstract": "Imitation learning aims to extract high-performance policies from logged demonstrations of expert behavior. It is common to frame imitation learning as a supervised learning problem in which one fits a function approximator to the input-output mapping exhibited by the logged demonstrations (input observations to output actions). While the framing of imitation learning as a supervised input-output learning problem allows for applicability in a wide variety of settings, it is also an overly simplistic view of the problem in situations where the expert demonstrations provide much richer insight into expert behavior. For example, applications such as path navigation, robot manipulation, and strategy games acquire expert demonstrations via planning, search, or some other multi-step algorithm, revealing not just the output action to be imitated but also the procedure for how to determine this action. While these intermediate computations may use tools not available to the agent during inference (e.g., environment simulators), they are nevertheless informative as a way to explain an expert's mapping of state to actions. To properly leverage expert procedure information without relying on the privileged tools the expert may have used to perform the procedure, we propose procedure cloning, which applies supervised sequence prediction to imitate the series of expert computations. This way, procedure cloning learns not only what to do (i.e., the output action), but how and why to do it (i.e., the procedure). Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, we show that imitating the intermediate computations of an expert's behavior enables procedure cloning to learn policies exhibiting significant generalization to unseen environment configurations, including those configurations for which running the expert's procedure directly is infeasible.",
		"DOI": "10.48550/arXiv.2205.10816",
		"note": "arXiv:2205.10816",
		"number": "arXiv:2205.10816",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Chain of Thought Imitation with Procedure Cloning",
		"URL": "http://arxiv.org/abs/2205.10816",
		"author": [
			{
				"family": "Yang",
				"given": "Mengjiao"
			},
			{
				"family": "Schuurmans",
				"given": "Dale"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Nachum",
				"given": "Ofir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					22
				]
			]
		}
	},
	{
		"id": "tamirisaTamperResistantSafeguardsOpenWeight2024",
		"type": "article",
		"abstract": "Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after thousands of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that tamper-resistance is a tractable problem, opening up a promising new avenue to improve the safety and security of open-weight LLMs.",
		"language": "en",
		"note": "arXiv:2408.00761 [cs]",
		"number": "arXiv:2408.00761",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Tamper-Resistant Safeguards for Open-Weight LLMs",
		"URL": "http://arxiv.org/abs/2408.00761",
		"author": [
			{
				"family": "Tamirisa",
				"given": "Rishub"
			},
			{
				"family": "Bharathi",
				"given": "Bhrugu"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Zhou",
				"given": "Andy"
			},
			{
				"family": "Gatti",
				"given": "Alice"
			},
			{
				"family": "Suresh",
				"given": "Tarun"
			},
			{
				"family": "Lin",
				"given": "Maxwell"
			},
			{
				"family": "Wang",
				"given": "Justin"
			},
			{
				"family": "Wang",
				"given": "Rowan"
			},
			{
				"family": "Arel",
				"given": "Ron"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					9,
					14
				]
			]
		}
	},
	{
		"id": "renSafetywashingAISafety2024",
		"type": "article",
		"abstract": "As artificial intelligence systems grow more powerful, there has been increasing interest in “AI safety” research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling “safetywashing”—where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.",
		"language": "en",
		"note": "arXiv:2407.21792 [cs]",
		"number": "arXiv:2407.21792",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
		"title-short": "Safetywashing",
		"URL": "http://arxiv.org/abs/2407.21792",
		"author": [
			{
				"family": "Ren",
				"given": "Richard"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Khoja",
				"given": "Adam"
			},
			{
				"family": "Gatti",
				"given": "Alice"
			},
			{
				"family": "Phan",
				"given": "Long"
			},
			{
				"family": "Yin",
				"given": "Xuwang"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Pan",
				"given": "Alexander"
			},
			{
				"family": "Mukobi",
				"given": "Gabriel"
			},
			{
				"family": "Kim",
				"given": "Ryan H."
			},
			{
				"family": "Fitz",
				"given": "Stephen"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					7,
					31
				]
			]
		}
	},
	{
		"id": "bentonSabotageEvaluationsFrontier2024",
		"type": "article-journal",
		"abstract": "Sufficiently capable models could subvert human oversight and decisionmaking in important contexts. For example, in the context of AI development, models could covertly sabotage efforts to evaluate their own dangerous capabilities, to monitor their behavior, or to make decisions about their deployment. We refer to this family of abilities as sabotage capabilities. We develop a set of related threat models and evaluations. These evaluations are designed to provide evidence that a given model, operating under a given set of mitigations, could not successfully sabotage a frontier model developer or other large organization’s activities in any of these ways. We demonstrate these evaluations on Anthropic’s Claude 3 Opus and Claude 3.5 Sonnet models. Our results suggest that for these models, minimal mitigations are currently sufficient to address sabotage risks, but that more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve. We also survey related evaluations we tried and abandoned. Finally, we discuss the advantages of mitigation-aware capability evaluations, and of simulating large-scale deployments using smallscale statistics.",
		"language": "en",
		"source": "Zotero",
		"title": "Sabotage Evaluations for Frontier Models",
		"author": [
			{
				"family": "Benton",
				"given": "Joe"
			},
			{
				"family": "Wagner",
				"given": "Misha"
			},
			{
				"family": "Christiansen",
				"given": "Eric"
			},
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Srivastav",
				"given": "Jai"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Karnofsky",
				"given": "Holden"
			},
			{
				"family": "Hubinger",
				"given": "Evan"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "Bowman",
				"given": "Samuel R"
			},
			{
				"family": "Duvenaud",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chenreddyEndendConditionalRobust2024",
		"type": "article-journal",
		"abstract": "The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality in our training loss. We show that the proposed training algorithms produce decisions that outperform the traditional “estimate then optimize” approaches.",
		"language": "en",
		"source": "Zotero",
		"title": "End-to-end Conditional Robust Optimization",
		"author": [
			{
				"family": "Chenreddy",
				"given": "Abhilash Reddy"
			},
			{
				"family": "Delage",
				"given": "Erick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ahmedQuantumSecureAuthenticationKey2021",
		"type": "article",
		"abstract": "Current methods for authentication and key agreement based on public-key cryptography are vulnerable to quantum computing. We propose a novel approach based on artificial intelligence research in which communicating parties are viewed as autonomous agents which interact repeatedly using their private decision models. Authentication and key agreement are decided based on the agents' observed behaviors during the interaction. The security of this approach rests upon the difficulty of modeling the decisions of interacting agents from limited observations, a problem which we conjecture is also hard for quantum computing. We release PyAMI, a prototype authentication and key agreement system based on the proposed method. We empirically validate our method for authenticating legitimate users while detecting different types of adversarial attacks. Finally, we show how reinforcement learning techniques can be used to train server models which effectively probe a client's decisions to achieve more sample-efficient authentication.",
		"DOI": "10.48550/arXiv.2007.09327",
		"note": "arXiv:2007.09327",
		"number": "arXiv:2007.09327",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction",
		"URL": "http://arxiv.org/abs/2007.09327",
		"author": [
			{
				"family": "Ahmed",
				"given": "Ibrahim H."
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			},
			{
				"family": "Fosong",
				"given": "Elliot"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					9
				]
			]
		}
	},
	{
		"id": "fosongFewShotTeamwork2022",
		"type": "article",
		"abstract": "We propose the novel few-shot teamwork (FST) problem, where skilled agents trained in a team to complete one task are combined with skilled agents from different tasks, and together must learn to adapt to an unseen but related task. We discuss how the FST problem can be seen as addressing two separate problems: one of reducing the experience required to train a team of agents to complete a complex task; and one of collaborating with unfamiliar teammates to complete a new task. Progress towards solving FST could lead to progress in both multi-agent reinforcement learning and ad hoc teamwork.",
		"DOI": "10.48550/arXiv.2207.09300",
		"note": "arXiv:2207.09300",
		"number": "arXiv:2207.09300",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Few-Shot Teamwork",
		"URL": "http://arxiv.org/abs/2207.09300",
		"author": [
			{
				"family": "Fosong",
				"given": "Elliot"
			},
			{
				"family": "Rahman",
				"given": "Arrasy"
			},
			{
				"family": "Carlucho",
				"given": "Ignacio"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					19
				]
			]
		}
	},
	{
		"id": "zhongRobustPolicySampling2022",
		"type": "article",
		"abstract": "Reinforcement learning (RL) algorithms are often categorized as either on-policy or off-policy depending on whether they use data from a target policy of interest or from a different behavior policy. In this paper, we study a subtle distinction between on-policy data and on-policy sampling in the context of the RL sub-problem of policy evaluation. We observe that on-policy sampling may fail to match the expected distribution of on-policy data after observing only a finite number of trajectories and this failure hinders data-efficient policy evaluation. Towards improved data-efficiency, we show how non-i.i.d., off-policy sampling can produce data that more closely matches the expected on-policy data distribution and consequently increases the accuracy of the Monte Carlo estimator for policy evaluation. We introduce a method called Robust On-Policy Sampling and demonstrate theoretically and empirically that it produces data that converges faster to the expected on-policy distribution compared to on-policy sampling. Empirically, we show that this faster convergence leads to lower mean squared error policy value estimates.",
		"DOI": "10.48550/arXiv.2111.14552",
		"note": "arXiv:2111.14552",
		"number": "arXiv:2111.14552",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2111.14552",
		"author": [
			{
				"family": "Zhong",
				"given": "Rujie"
			},
			{
				"family": "Zhang",
				"given": "Duohan"
			},
			{
				"family": "Schäfer",
				"given": "Lukas"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					10
				]
			]
		}
	},
	{
		"id": "andresUsingOfflineData2023",
		"type": "article",
		"abstract": "One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently during online RL training both consistently improve the sample-efficiency while converging to optimal policies. Furthermore, we show that pre-training a policy from as few as two trajectories can make the difference between learning an optimal policy at the end of online training and not learning at all. Our findings motivate the widespread adoption of IL for pre-training and concurrent IL in procedurally generated environments whenever offline trajectories are available or can be generated.",
		"DOI": "10.48550/arXiv.2304.09825",
		"note": "arXiv:2304.09825",
		"number": "arXiv:2304.09825",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments",
		"URL": "http://arxiv.org/abs/2304.09825",
		"author": [
			{
				"family": "Andres",
				"given": "Alain"
			},
			{
				"family": "Schäfer",
				"given": "Lukas"
			},
			{
				"family": "Villar-Rodriguez",
				"given": "Esther"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Ser",
				"given": "Javier Del"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					18
				]
			]
		}
	},
	{
		"id": "dunionTemporalDisentanglementRepresentations2023a",
		"type": "article",
		"abstract": "Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).",
		"DOI": "10.48550/arXiv.2207.05480",
		"note": "arXiv:2207.05480",
		"number": "arXiv:2207.05480",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2207.05480",
		"author": [
			{
				"family": "Dunion",
				"given": "Mhairi"
			},
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Luck",
				"given": "Kevin Sebastian"
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					27
				]
			]
		}
	},
	{
		"id": "mccallumFeedbackAllYou2023",
		"type": "article",
		"abstract": "Despite numerous successes, the field of reinforcement learning (RL) remains far from matching the impressive generalisation power of human behaviour learning. One possible way to help bridge this gap be to provide RL agents with richer, more human-like feedback expressed in natural language. To investigate this idea, we first extend BabyAI to automatically generate language feedback from the environment dynamics and goal condition success. Then, we modify the Decision Transformer architecture to take advantage of this additional signal. We find that training with language feedback either in place of or in addition to the return-to-go or goal descriptions improves agents' generalisation performance, and that agents can benefit from feedback even when this is only available during training, but not at inference.",
		"DOI": "10.48550/arXiv.2312.04736",
		"note": "arXiv:2312.04736",
		"number": "arXiv:2312.04736",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning",
		"title-short": "Is Feedback All You Need?",
		"URL": "http://arxiv.org/abs/2312.04736",
		"author": [
			{
				"family": "McCallum",
				"given": "Sabrina"
			},
			{
				"family": "Taylor-Davies",
				"given": "Max"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Suglia",
				"given": "Alessandro"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					7
				]
			]
		}
	},
	{
		"id": "garcinHowLevelSampling2023",
		"type": "article",
		"abstract": "A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (UED) methods, which adaptively generate new training levels and minimise MI more effectively than methods sampling from a fixed set. However, we find UED methods significantly shift the training distribution, resulting in over-generalisation and worse ZSG performance over the distribution of interest. To prevent both instance overfitting and over-generalisation, we introduce self-supervised environment design (SSED). SSED generates levels using a variational autoencoder, effectively reducing MI while minimising the shift with the distribution of interest, and leads to statistically significant improvements in ZSG over fixed-set level sampling strategies and UED methods.",
		"DOI": "10.48550/arXiv.2310.03494",
		"note": "arXiv:2310.03494",
		"number": "arXiv:2310.03494",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "How the level sampling process impacts zero-shot generalisation in deep reinforcement learning",
		"URL": "http://arxiv.org/abs/2310.03494",
		"author": [
			{
				"family": "Garcin",
				"given": "Samuel"
			},
			{
				"family": "Doran",
				"given": "James"
			},
			{
				"family": "Guo",
				"given": "Shangmin"
			},
			{
				"family": "Lucas",
				"given": "Christopher G."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					11
				]
			]
		}
	},
	{
		"id": "schaferLearningTaskEmbeddings2023",
		"type": "article",
		"abstract": "Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks.",
		"DOI": "10.48550/arXiv.2207.02249",
		"note": "arXiv:2207.02249",
		"number": "arXiv:2207.02249",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2207.02249",
		"author": [
			{
				"family": "Schäfer",
				"given": "Lukas"
			},
			{
				"family": "Christianos",
				"given": "Filippos"
			},
			{
				"family": "Storkey",
				"given": "Amos"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "dunionConditionalMutualInformation2023",
		"type": "article",
		"abstract": "Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the conditional mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well as improving the training performance of RL algorithms in the presence of correlated features.",
		"DOI": "10.48550/arXiv.2305.14133",
		"note": "arXiv:2305.14133",
		"number": "arXiv:2305.14133",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Conditional Mutual Information for Disentangled Representations in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2305.14133",
		"author": [
			{
				"family": "Dunion",
				"given": "Mhairi"
			},
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Luck",
				"given": "Kevin Sebastian"
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					12
				]
			]
		}
	},
	{
		"id": "rahmanGeneratingTeammatesTraining2023",
		"type": "article",
		"abstract": "Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.",
		"DOI": "10.48550/arXiv.2207.14138",
		"note": "arXiv:2207.14138",
		"number": "arXiv:2207.14138",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
		"URL": "http://arxiv.org/abs/2207.14138",
		"author": [
			{
				"family": "Rahman",
				"given": "Arrasy"
			},
			{
				"family": "Fosong",
				"given": "Elliot"
			},
			{
				"family": "Carlucho",
				"given": "Ignacio"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					24
				]
			]
		}
	},
	{
		"id": "rahmanGeneralLearningFramework2023",
		"type": "article",
		"abstract": "Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully observable case to compute an agent's optimal policy under partial observability in open ad hoc teamwork. Empirical results demonstrate that our solution can learn efficient policies in open ad hoc teamwork in fully and partially observable cases. Further analysis demonstrates that our methods' success is a result of effectively learning the effects of teammates' actions while also inferring the inherent state of the environment under partial observability.",
		"DOI": "10.48550/arXiv.2210.05448",
		"note": "arXiv:2210.05448",
		"number": "arXiv:2210.05448",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning",
		"URL": "http://arxiv.org/abs/2210.05448",
		"author": [
			{
				"family": "Rahman",
				"given": "Arrasy"
			},
			{
				"family": "Carlucho",
				"given": "Ignacio"
			},
			{
				"family": "Höpner",
				"given": "Niklas"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					28
				]
			]
		}
	},
	{
		"id": "yuSkillawareMutualInformation2024",
		"type": "article",
		"abstract": "Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviour). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\\log$-$K$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the $\\log$-$K$ curse.",
		"DOI": "10.48550/arXiv.2406.04815",
		"note": "arXiv:2406.04815",
		"number": "arXiv:2406.04815",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2406.04815",
		"author": [
			{
				"family": "Yu",
				"given": "Xuehui"
			},
			{
				"family": "Dunion",
				"given": "Mhairi"
			},
			{
				"family": "Li",
				"given": "Xin"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					11,
					6
				]
			]
		}
	},
	{
		"id": "hanLLMPersonalizeAligningLLM2024",
		"type": "article",
		"abstract": "Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.",
		"DOI": "10.48550/arXiv.2404.14285",
		"note": "arXiv:2404.14285",
		"number": "arXiv:2404.14285",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots",
		"title-short": "LLM-Personalize",
		"URL": "http://arxiv.org/abs/2404.14285",
		"author": [
			{
				"family": "Han",
				"given": "Dongge"
			},
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Jelley",
				"given": "Adam"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Bell",
				"given": "Peter"
			},
			{
				"family": "Storkey",
				"given": "Amos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					22
				]
			]
		}
	},
	{
		"id": "guoLpNTKBetterGeneralisation2024",
		"type": "article",
		"abstract": "Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of samples and forgetting events during learning. Moreover, we also show that using lpNTK to identify and remove poisoning training samples does not hurt the generalisation performance of ANNs.",
		"DOI": "10.48550/arXiv.2401.08808",
		"note": "arXiv:2401.08808",
		"number": "arXiv:2401.08808",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "lpNTK: Better Generalisation with Less Data via Sample Interaction During Learning",
		"title-short": "lpNTK",
		"URL": "http://arxiv.org/abs/2401.08808",
		"author": [
			{
				"family": "Guo",
				"given": "Shangmin"
			},
			{
				"family": "Ren",
				"given": "Yi"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Smith",
				"given": "Kenny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					14
				]
			]
		}
	},
	{
		"id": "azranContextualPreplanningReward2024",
		"type": "article",
		"abstract": "Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RMs), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Empirical results show that our representations improve sample efficiency and few-shot transfer in a variety of domains.",
		"DOI": "10.48550/arXiv.2307.05209",
		"note": "arXiv:2307.05209",
		"number": "arXiv:2307.05209",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2307.05209",
		"author": [
			{
				"family": "Azran",
				"given": "Guy"
			},
			{
				"family": "Danesh",
				"given": "Mohamad H."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Keren",
				"given": "Sarah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					21
				]
			]
		}
	},
	{
		"id": "fosongLearningComplexTeamwork2024",
		"type": "article",
		"abstract": "Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, and propose a simple and scalable method to address these problems which augments existing actor-critic algorithms. We demonstrate the empirical benefits of our proposed method, enabling sub-task decomposition approaches to be deployed in diverse multi-agent tasks.",
		"DOI": "10.48550/arXiv.2302.04944",
		"note": "arXiv:2302.04944",
		"number": "arXiv:2302.04944",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition",
		"URL": "http://arxiv.org/abs/2302.04944",
		"author": [
			{
				"family": "Fosong",
				"given": "Elliot"
			},
			{
				"family": "Rahman",
				"given": "Arrasy"
			},
			{
				"family": "Carlucho",
				"given": "Ignacio"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					15
				]
			]
		}
	},
	{
		"id": "garcinDREDZeroShotTransfer2024",
		"type": "article",
		"abstract": "Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce data-regularised environment design (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods. Our code and experimental data are available at https://github.com/uoe-agents/dred.",
		"DOI": "10.48550/arXiv.2402.03479",
		"note": "arXiv:2402.03479",
		"number": "arXiv:2402.03479",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design",
		"title-short": "DRED",
		"URL": "http://arxiv.org/abs/2402.03479",
		"author": [
			{
				"family": "Garcin",
				"given": "Samuel"
			},
			{
				"family": "Doran",
				"given": "James"
			},
			{
				"family": "Guo",
				"given": "Shangmin"
			},
			{
				"family": "Lucas",
				"given": "Christopher G."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					11
				]
			]
		}
	},
	{
		"id": "mcinroePlanningGoOutDistribution2024",
		"type": "article",
		"abstract": "Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modification, and UCB methods are myopic and it is unclear which learned-component's ensemble to use for action selection. We then introduce an algorithm for planning to go out-of-distribution (PTGOOD) that avoids these issues. PTGOOD uses a non-myopic planning procedure that targets exploration in relatively high-reward regions of the state-action space unlikely to be visited by the behavior policy. By leveraging concepts from the Conditional Entropy Bottleneck, PTGOOD encourages data collected online to provide new information relevant to improving the final deployment policy without altering rewards. We show empirically in several continuous control tasks that PTGOOD significantly improves agent returns during online fine-tuning and avoids the suboptimal policy convergence that many of our baselines exhibit in several environments.",
		"DOI": "10.48550/arXiv.2310.05723",
		"note": "arXiv:2310.05723",
		"number": "arXiv:2310.05723",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2310.05723",
		"author": [
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Jelley",
				"given": "Adam"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Storkey",
				"given": "Amos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					21
				]
			]
		}
	},
	{
		"id": "dunionMultiviewDisentanglementReinforcement2024",
		"type": "article",
		"abstract": "The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images. Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL. However, hardware constraints may limit the availability of multiple cameras in real-world deployment. Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training. To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that is robust to a reduction in the number of cameras to generalise to any single camera from the training set. Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific. We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera.",
		"DOI": "10.48550/arXiv.2404.14064",
		"note": "arXiv:2404.14064",
		"number": "arXiv:2404.14064",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras",
		"URL": "http://arxiv.org/abs/2404.14064",
		"author": [
			{
				"family": "Dunion",
				"given": "Mhairi"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					21
				]
			]
		}
	},
	{
		"id": "mcinroe2023hksl",
		"type": "article-journal",
		"abstract": "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions. Instead, we propose Hierarchical k-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks and find that HKSL either reaches higher episodic returns or converges to maximum performance more quickly than several current baselines. Also, we find that levels in HKSL's hierarchy can learn to specialize in long- or short-term consequences of agent actions, thereby providing the downstream control policy with more informative representations. Finally, we determine that communication channels between hierarchy levels organize information based on both sides of the communication process, which improves sample efficiency.",
		"container-title": "Transactions on Machine Learning Research (TMLR)",
		"title": "Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning",
		"author": [
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Schäfer",
				"given": "Lukas"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "liuAlignBenchBenchmarkingChinese2024",
		"type": "article",
		"abstract": "Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still largely unexplored. To fill in this gap, we introduce ALIGNBENCH, a comprehensive multi-dimensional benchmark for evaluating LLMs’ alignment in Chinese. We design a human-in-the-loop data curation pipeline, containing eight main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledgeintensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLMas-Judge (Zheng et al., 2023) approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. All evaluation code, data, and LLM generations are available at https:// github.com/THUDM/AlignBench. Since its release, AlignBench has been adopted by top (Chinese) LLMs for evaluating their alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi, Baichuan, and Abab.",
		"language": "en",
		"note": "arXiv:2311.18743 [cs]",
		"number": "arXiv:2311.18743",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
		"title-short": "AlignBench",
		"URL": "http://arxiv.org/abs/2311.18743",
		"author": [
			{
				"family": "Liu",
				"given": "Xiao"
			},
			{
				"family": "Lei",
				"given": "Xuanyu"
			},
			{
				"family": "Wang",
				"given": "Shengyuan"
			},
			{
				"family": "Huang",
				"given": "Yue"
			},
			{
				"family": "Feng",
				"given": "Zhuoer"
			},
			{
				"family": "Wen",
				"given": "Bosi"
			},
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Xu",
				"given": "Yifan"
			},
			{
				"family": "Tam",
				"given": "Weng Lam"
			},
			{
				"family": "Zhang",
				"given": "Xiaohan"
			},
			{
				"family": "Sun",
				"given": "Lichao"
			},
			{
				"family": "Gu",
				"given": "Xiaotao"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Zhang",
				"given": "Jing"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Dong",
				"given": "Yuxiao"
			},
			{
				"family": "Tang",
				"given": "Jie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					25
				]
			]
		}
	},
	{
		"id": "zhangSafetyBenchEvaluatingSafety2024",
		"type": "article",
		"abstract": "With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.",
		"language": "en",
		"note": "arXiv:2309.07045 [cs]",
		"number": "arXiv:2309.07045",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SafetyBench: Evaluating the Safety of Large Language Models",
		"title-short": "SafetyBench",
		"URL": "http://arxiv.org/abs/2309.07045",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Lei",
				"given": "Leqi"
			},
			{
				"family": "Wu",
				"given": "Lindong"
			},
			{
				"family": "Sun",
				"given": "Rui"
			},
			{
				"family": "Huang",
				"given": "Yongkang"
			},
			{
				"family": "Long",
				"given": "Chong"
			},
			{
				"family": "Liu",
				"given": "Xiao"
			},
			{
				"family": "Lei",
				"given": "Xuanyu"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					24
				]
			]
		}
	},
	{
		"id": "zhangConstructingHighlyInductive2022",
		"type": "paper-conference",
		"abstract": "Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at https://github.com/thu-coai/Reverse_Generation.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2022",
		"DOI": "10.18653/v1/2022.findings-emnlp.270",
		"event-place": "Abu Dhabi, United Arab Emirates",
		"event-title": "Findings 2022",
		"page": "3684–3697",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates",
		"source": "ACLWeb",
		"title": "Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation",
		"URL": "https://aclanthology.org/2022.findings-emnlp.270",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Wang",
				"given": "Yasheng"
			},
			{
				"family": "Shang",
				"given": "Lifeng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Zhang",
				"given": "Yue"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "wenAutoCADAutomaticallyGenerate2022",
		"type": "paper-conference",
		"abstract": "Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models' reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals, thereby impeding CAD's applicability to a broad range of NLU tasks. In this paper, we present AutoCAD, a fully automatic and task-agnostic CAD generation framework. AutoCAD first leverages a classifier to unsupervisedly identify rationales as spans to be intervened, which disentangles spurious and causal features. Then, AutoCAD performs controllable generation enhanced by unlikelihood training to produce diverse counterfactuals. Extensive evaluations on multiple out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently and significantly boosts the out-of-distribution performance of powerful pre-trained models across different NLU tasks, which is comparable or even better than previous state-of-the-art human-in-the-loop or task-specific CAD methods.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2022",
		"DOI": "10.18653/v1/2022.findings-emnlp.170",
		"event-place": "Abu Dhabi, United Arab Emirates",
		"event-title": "Findings 2022",
		"page": "2302–2317",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates",
		"source": "ACLWeb",
		"title": "AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning",
		"title-short": "AutoCAD",
		"URL": "https://aclanthology.org/2022.findings-emnlp.170",
		"author": [
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Zhu",
				"given": "Yeshuang"
			},
			{
				"family": "Zhang",
				"given": "Jinchao"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Zhang",
				"given": "Yue"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "liuEmotionalSupportDialog2021",
		"type": "paper-conference",
		"abstract": "Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.",
		"container-title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2021.acl-long.269",
		"event-place": "Online",
		"event-title": "ACL-IJCNLP 2021",
		"page": "3469–3483",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "ACLWeb",
		"title": "Towards Emotional Support Dialog Systems",
		"URL": "https://aclanthology.org/2021.acl-long.269",
		"author": [
			{
				"family": "Liu",
				"given": "Siyang"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Demasi",
				"given": "Orianna"
			},
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Li",
				"given": "Yu"
			},
			{
				"family": "Yu",
				"given": "Zhou"
			},
			{
				"family": "Jiang",
				"given": "Yong"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Zong",
				"given": "Chengqing"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Li",
				"given": "Wenjie"
			},
			{
				"family": "Navigli",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "wangDiversifyingDialogGeneration2021",
		"type": "paper-conference",
		"abstract": "Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.",
		"container-title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2021.acl-long.272",
		"event-place": "Online",
		"event-title": "ACL-IJCNLP 2021",
		"page": "3507–3520",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "ACLWeb",
		"title": "Diversifying Dialog Generation via Adaptive Label Smoothing",
		"URL": "https://aclanthology.org/2021.acl-long.272",
		"author": [
			{
				"family": "Wang",
				"given": "Yida"
			},
			{
				"family": "Zheng",
				"given": "Yinhe"
			},
			{
				"family": "Jiang",
				"given": "Yong"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Zong",
				"given": "Chengqing"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Li",
				"given": "Wenjie"
			},
			{
				"family": "Navigli",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "shaoMutualInformationMaximization2021",
		"type": "paper-conference",
		"abstract": "Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.",
		"container-title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2021.acl-long.318",
		"event-place": "Online",
		"event-title": "ACL-IJCNLP 2021",
		"page": "4111–4124",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "ACLWeb",
		"title": "A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering",
		"URL": "https://aclanthology.org/2021.acl-long.318",
		"author": [
			{
				"family": "Shao",
				"given": "Zhihong"
			},
			{
				"family": "Shang",
				"given": "Lifeng"
			},
			{
				"family": "Liu",
				"given": "Qun"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Zong",
				"given": "Chengqing"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Li",
				"given": "Wenjie"
			},
			{
				"family": "Navigli",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "shiROBUSTNESSVERIFICATIONTRANSFORMERS2020",
		"type": "article-journal",
		"abstract": "Robustness veriﬁcation that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness veriﬁcation problem for Transformers. Transformers have complex self-attention layers that pose many challenges for veriﬁcation, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the ﬁrst robustness veriﬁcation algorithm for Transformers. The certiﬁed robustness bounds computed by our method are signiﬁcantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reﬂect the importance of different words in sentiment analysis.",
		"language": "en",
		"source": "Zotero",
		"title": "ROBUSTNESS VERIFICATION FOR TRANSFORMERS",
		"author": [
			{
				"family": "Shi",
				"given": "Zhouxing"
			},
			{
				"family": "Zhang",
				"given": "Huan"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Hsieh",
				"given": "Cho-Jui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liuRobustnessTestingLanguage2021",
		"type": "paper-conference",
		"abstract": "Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.",
		"container-title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2021.acl-long.192",
		"event-place": "Online",
		"event-title": "ACL-IJCNLP 2021",
		"page": "2467–2480",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "ACLWeb",
		"title": "Robustness Testing of Language Understanding in Task-Oriented Dialog",
		"URL": "https://aclanthology.org/2021.acl-long.192",
		"author": [
			{
				"family": "Liu",
				"given": "Jiexi"
			},
			{
				"family": "Takanobu",
				"given": "Ryuichi"
			},
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Wan",
				"given": "Dazhen"
			},
			{
				"family": "Li",
				"given": "Hongguang"
			},
			{
				"family": "Nie",
				"given": "Weiran"
			},
			{
				"family": "Li",
				"given": "Cheng"
			},
			{
				"family": "Peng",
				"given": "Wei"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Zong",
				"given": "Chengqing"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Li",
				"given": "Wenjie"
			},
			{
				"family": "Navigli",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "zhangLearningGoalorientedDialogue2020",
		"type": "paper-conference",
		"abstract": "Most existing approaches for goal-oriented dialogue policy learning used reinforcement learning, which focuses on the target agent policy and simply treats the opposite agent policy as part of the environment. While in real-world scenarios, the behavior of an opposite agent often exhibits certain patterns or underlies hidden policies, which can be inferred and utilized by the target agent to facilitate its own decision making. This strategy is common in human mental simulation by first imaging a specific action and the probable results before really acting it. We therefore propose an opposite behavior aware framework for policy learning in goal-oriented dialogues. We estimate the opposite agent's policy from its behavior and use this estimation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines.",
		"container-title": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
		"event-place": "Suzhou, China",
		"event-title": "AACL 2020",
		"page": "122–132",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Suzhou, China",
		"source": "ACLWeb",
		"title": "Learning Goal-oriented Dialogue Policy with opposite Agent Awareness",
		"URL": "https://aclanthology.org/2020.aacl-main.16",
		"author": [
			{
				"family": "Zhang",
				"given": "Zheng"
			},
			{
				"family": "Liao",
				"given": "Lizi"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Chua",
				"given": "Tat-Seng"
			},
			{
				"family": "Liu",
				"given": "Zitao"
			},
			{
				"family": "Huang",
				"given": "Yan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Wong",
				"given": "Kam-Fai"
			},
			{
				"family": "Knight",
				"given": "Kevin"
			},
			{
				"family": "Wu",
				"given": "Hua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "zhuContinualPromptTuning2022",
		"type": "paper-conference",
		"abstract": "A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens' embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines.",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2022.acl-long.80",
		"event-place": "Dublin, Ireland",
		"event-title": "ACL 2022",
		"page": "1124–1137",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Dublin, Ireland",
		"source": "ACLWeb",
		"title": "Continual Prompt Tuning for Dialog State Tracking",
		"URL": "https://aclanthology.org/2022.acl-long.80",
		"author": [
			{
				"family": "Zhu",
				"given": "Qi"
			},
			{
				"family": "Li",
				"given": "Bing"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Muresan",
				"given": "Smaranda"
			},
			{
				"family": "Nakov",
				"given": "Preslav"
			},
			{
				"family": "Villavicencio",
				"given": "Aline"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "keCTRLEvalUnsupervisedReferenceFree2022",
		"type": "paper-conference",
		"abstract": "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overﬁt task-speciﬁc data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text inﬁlling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities1.",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2022.acl-long.164",
		"event-place": "Dublin, Ireland",
		"event-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"language": "en",
		"page": "2306-2319",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Dublin, Ireland",
		"source": "DOI.org (Crossref)",
		"title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
		"title-short": "CTRLEval",
		"URL": "https://aclanthology.org/2022.acl-long.164",
		"author": [
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Lin",
				"given": "Yankai"
			},
			{
				"family": "Li",
				"given": "Peng"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuRethinkingRefiningDistinct2022",
		"type": "paper-conference",
		"abstract": "Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct.",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
		"DOI": "10.18653/v1/2022.acl-short.86",
		"event-place": "Dublin, Ireland",
		"event-title": "ACL 2022",
		"page": "762–770",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Dublin, Ireland",
		"source": "ACLWeb",
		"title": "Rethinking and Refining the Distinct Metric",
		"URL": "https://aclanthology.org/2022.acl-short.86",
		"author": [
			{
				"family": "Liu",
				"given": "Siyang"
			},
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Zheng",
				"given": "Yinhe"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Muresan",
				"given": "Smaranda"
			},
			{
				"family": "Nakov",
				"given": "Preslav"
			},
			{
				"family": "Villavicencio",
				"given": "Aline"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "sunSafetyConversationalModels2022",
		"type": "paper-conference",
		"abstract": "Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems.",
		"container-title": "Findings of the Association for Computational Linguistics: ACL 2022",
		"DOI": "10.18653/v1/2022.findings-acl.308",
		"event-place": "Dublin, Ireland",
		"event-title": "Findings 2022",
		"page": "3906–3923",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Dublin, Ireland",
		"source": "ACLWeb",
		"title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark",
		"title-short": "On the Safety of Conversational Models",
		"URL": "https://aclanthology.org/2022.findings-acl.308",
		"author": [
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Xu",
				"given": "Guangxuan"
			},
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Peng",
				"given": "Nanyun"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Muresan",
				"given": "Smaranda"
			},
			{
				"family": "Nakov",
				"given": "Preslav"
			},
			{
				"family": "Villavicencio",
				"given": "Aline"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "guanLOTStoryCentricBenchmark2022",
		"type": "article-journal",
		"abstract": "Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT.",
		"container-title": "Transactions of the Association for Computational Linguistics",
		"DOI": "10.1162/tacl_a_00469",
		"ISSN": "2307-387X",
		"language": "en",
		"page": "434-451",
		"source": "DOI.org (Crossref)",
		"title": "LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation",
		"title-short": "LOT",
		"URL": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00469/110537/LOT-A-Story-Centric-Benchmark-for-Evaluating",
		"volume": "10",
		"author": [
			{
				"family": "Guan",
				"given": "Jian"
			},
			{
				"family": "Feng",
				"given": "Zhuoer"
			},
			{
				"family": "Chen",
				"given": "Yamei"
			},
			{
				"family": "He",
				"given": "Ruilin"
			},
			{
				"family": "Mao",
				"given": "Xiaoxi"
			},
			{
				"family": "Fan",
				"given": "Changjie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					11
				]
			]
		}
	},
	{
		"id": "keCurriculumBasedSelfTrainingMakes2022",
		"type": "article",
		"abstract": "Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation.",
		"DOI": "10.48550/arXiv.2206.02712",
		"note": "arXiv:2206.02712",
		"number": "arXiv:2206.02712",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation",
		"URL": "http://arxiv.org/abs/2206.02712",
		"author": [
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Ji",
				"given": "Haozhe"
			},
			{
				"family": "Yang",
				"given": "Zhenyu"
			},
			{
				"family": "Huang",
				"given": "Yi"
			},
			{
				"family": "Feng",
				"given": "Junlan"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					6
				]
			]
		}
	},
	{
		"id": "huangDirectedAcyclicTransformer2022",
		"type": "article-journal",
		"abstract": "Non-autoregressive Transformers (NATs) significantly reduce the decoding latency by generating all tokens in parallel. However, such independent predictions prevent NATs from capturing the dependencies between the tokens for generating multiple possible translations. In this paper, we propose Directed Acyclic Transfomer (DATransformer), which represents the hidden states in a Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a specific translation. The whole DAG simultaneously captures multiple translations and facilitates fast predictions in a non-autoregressive fashion. Experiments on the raw training data of WMT benchmark show that DA-Transformer substantially outperforms previous NATs by about 3 BLEU on average, which is the first NAT model that achieves competitive results with autoregressive Transformers without relying on knowledge distillation.",
		"language": "en",
		"source": "Zotero",
		"title": "Directed Acyclic Transformer for Non-Autoregressive Machine Translation",
		"author": [
			{
				"family": "Huang",
				"given": "Fei"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Li",
				"given": "Hang"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yaoHumanVsGenerative2024",
		"type": "article-journal",
		"abstract": "The advent of generative AI (GenAI) technology produces a transformative impact on the content creation landscape, offering alternative approaches to produce diverse, good-quality content across media, thereby reshaping online ecosystems but also raising concerns about market oversaturation and the potential marginalization of human creativity. Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and GenAI. Our theory and simulations suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in GenAI.",
		"language": "en",
		"source": "Zotero",
		"title": "Human vs. Generative AI in Content Creation Competition:  Symbiosis or Conflict?",
		"author": [
			{
				"family": "Yao",
				"given": "Fan"
			},
			{
				"family": "Li",
				"given": "Chuanhao"
			},
			{
				"family": "Nekipelov",
				"given": "Denis"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Xu",
				"given": "Haifeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "shaoSyntheticPromptingGenerating2023",
		"type": "article-journal",
		"abstract": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through stepby-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce SYNTHETIC PROMPTING, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.",
		"language": "en",
		"source": "Zotero",
		"title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
		"author": [
			{
				"family": "Shao",
				"given": "Zhihong"
			},
			{
				"family": "Gong",
				"given": "Yeyun"
			},
			{
				"family": "Shen",
				"given": "Yelong"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Duan",
				"given": "Nan"
			},
			{
				"family": "Chen",
				"given": "Weizhu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "sunMoralDialFrameworkTrain2023",
		"type": "article-journal",
		"abstract": "Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users' values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems.",
		"language": "en",
		"source": "Zotero",
		"title": "MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions",
		"author": [
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Wang",
				"given": "Yasheng"
			},
			{
				"family": "Liu",
				"given": "Wei"
			},
			{
				"family": "Cui",
				"given": "Jianwei"
			},
			{
				"family": "Wang",
				"given": "Bin"
			},
			{
				"family": "Liu",
				"given": "Qun"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shaoAdvExpanderGeneratingNatural2022",
		"type": "article-journal",
		"abstract": "Adversarial examples are vital to expose vulnerability of machine learning models. Despite the success of the most popular word-level substitution-based attacks which substitute some words in the original examples, only substitution is insufficient to uncover all robustness issues of models. In this paper, we focus on perturbations beyond word-level substitution, and present AdvExpander, a method that crafts new adversarial examples by expanding text. We first utilize linguistic rules to determine which constituents to expand and what types of modifiers to expand with. We then expand each constituent by inserting an adversarial modifier searched from a pre-trained CVAE-based generative model. To ensure that our adversarial examples are label-preserving for text matching, we also constrain the modifications with a heuristic rule. Experiments on three classification tasks verify the effectiveness of AdvExpander and the validity of our adversarial examples. AdvExpander is significantly more effective than sentence-level attack baselines and is complementary to previous word substitution-based attacks, thus promising to reveal new robustness issues.",
		"container-title": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
		"DOI": "10.1109/TASLP.2021.3129339",
		"ISSN": "2329-9304",
		"note": "event-title: IEEE/ACM Transactions on Audio, Speech, and Language Processing",
		"page": "1184-1196",
		"source": "IEEE Xplore",
		"title": "AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text",
		"title-short": "AdvExpander",
		"URL": "https://ieeexplore.ieee.org/document/9622188",
		"volume": "30",
		"author": [
			{
				"family": "Shao",
				"given": "Zhihong"
			},
			{
				"family": "Wu",
				"given": "Zhongqin"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dengCOLDBenchmarkChinese2022",
		"type": "paper-conference",
		"abstract": "Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset –COLDATASET and a baseline detector –COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.",
		"container-title": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2022.emnlp-main.796",
		"event-place": "Abu Dhabi, United Arab Emirates",
		"event-title": "EMNLP 2022",
		"page": "11580–11599",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates",
		"source": "ACLWeb",
		"title": "COLD: A Benchmark for Chinese Offensive Language Detection",
		"title-short": "COLD",
		"URL": "https://aclanthology.org/2022.emnlp-main.796",
		"author": [
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Zhou",
				"given": "Jingyan"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Meng",
				"given": "Helen"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Zhang",
				"given": "Yue"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "guLearningInstructionsUnlabeled2022",
		"type": "paper-conference",
		"abstract": "Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of humanannotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT’s effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT.",
		"container-title": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2022.emnlp-main.105",
		"event-place": "Abu Dhabi, United Arab Emirates",
		"event-title": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
		"language": "en",
		"page": "1617-1634",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates",
		"source": "DOI.org (Crossref)",
		"title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization",
		"URL": "https://aclanthology.org/2022.emnlp-main.105",
		"author": [
			{
				"family": "Gu",
				"given": "Yuxian"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhengCDConvBenchmarkContradiction2022",
		"type": "paper-conference",
		"abstract": "Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains 12K multi-turn conversations annotated with three typical contradiction categories: Intra-sentence Contradiction, Role Confusion, and History Contradiction. To efficiently construct the CDConv conversations, we devise a series of methods for automatic conversation generation, which simulate common user behaviors that trigger chatbots to make contradictions. We conduct careful manual quality screening of the constructed conversations and show that state-of-the-art Chinese chatbots can be easily goaded into making contradictions. Experiments on CDConv show that properly modeling contextual information is critical for dialogue contradiction detection, but there are still unresolved challenges that require future research.",
		"container-title": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2022.emnlp-main.2",
		"event-place": "Abu Dhabi, United Arab Emirates",
		"event-title": "EMNLP 2022",
		"page": "18–29",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Abu Dhabi, United Arab Emirates",
		"source": "ACLWeb",
		"title": "CDConv: A Benchmark for Contradiction Detection in Chinese Conversations",
		"title-short": "CDConv",
		"URL": "https://aclanthology.org/2022.emnlp-main.2",
		"author": [
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Zhou",
				"given": "Jinfeng"
			},
			{
				"family": "Zheng",
				"given": "Yinhe"
			},
			{
				"family": "Peng",
				"given": "Libiao"
			},
			{
				"family": "Guo",
				"given": "Zhen"
			},
			{
				"family": "Wu",
				"given": "Wenquan"
			},
			{
				"family": "Niu",
				"given": "Zheng-Yu"
			},
			{
				"family": "Wu",
				"given": "Hua"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Zhang",
				"given": "Yue"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "zhengAugESCDialogueAugmentation2023",
		"type": "paper-conference",
		"abstract": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models’ generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks.",
		"container-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"DOI": "10.18653/v1/2023.findings-acl.99",
		"event-place": "Toronto, Canada",
		"event-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"language": "en",
		"page": "1552-1568",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "DOI.org (Crossref)",
		"title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
		"title-short": "AugESC",
		"URL": "https://aclanthology.org/2023.findings-acl.99",
		"author": [
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Zhang",
				"given": "Zheng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhengClickControllableText2023",
		"type": "paper-conference",
		"abstract": "It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Leo for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Leo outperforms strong baselines of controllable text generation and demonstrate the superiority of Leo’s sample construction strategy.",
		"container-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"DOI": "10.18653/v1/2023.findings-acl.65",
		"event-place": "Toronto, Canada",
		"event-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"language": "en",
		"page": "1022-1040",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "DOI.org (Crossref)",
		"title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning",
		"title-short": "Click",
		"URL": "https://aclanthology.org/2023.findings-acl.65",
		"author": [
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Zhang",
				"given": "Zheng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guanMitigatingLearningBias2023",
		"type": "paper-conference",
		"abstract": "Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.",
		"container-title": "Findings of the Association for Computational Linguistics: ACL 2023",
		"DOI": "10.18653/v1/2023.findings-acl.431",
		"event-place": "Toronto, Canada",
		"event-title": "Findings 2023",
		"page": "6897–6909",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation",
		"URL": "https://aclanthology.org/2023.findings-acl.431",
		"author": [
			{
				"family": "Guan",
				"given": "Jian"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "dengEnhancingOffensiveLanguage2023",
		"type": "article-journal",
		"abstract": "Offensive language detection has received important attention and plays a crucial role in promoting healthy communication on social platforms, as well as promoting the safe deployment of large language models. Training data is the basis for developing detectors; however, the available offense-related dataset in Chinese is severely limited in terms of data scale and coverage when compared to English resources. This significantly affects the accuracy of Chinese offensive language detectors in practical applications, especially when dealing with hard cases or out-of-domain samples. To alleviate the limitations posed by available datasets, we introduce AugCOLD (Augmented Chinese Offensive Language Dataset), a large-scale unsupervised dataset containing 1 million samples gathered by data crawling and model generation. Furthermore, we employ a multiteacher distillation framework to enhance detection performance with unsupervised data. That is, we build multiple teachers with publicly accessible datasets and use them to assign soft labels to AugCOLD. The soft labels serve as a bridge for knowledge to be distilled from both AugCOLD and multiteacher to the student network, i.e., the final offensive detector. We conduct experiments on multiple public test sets and our well-designed hard tests, demonstrating that our proposal can effectively improve the generalization and robustness of the offensive language detector.",
		"container-title": "Research",
		"DOI": "10.34133/research.0189",
		"note": "publisher: American Association for the Advancement of Science",
		"page": "0189",
		"source": "spj.science.org (Atypon)",
		"title": "Enhancing Offensive Language Detection with Data Augmentation and Knowledge Distillation",
		"URL": "https://spj.science.org/doi/10.34133/research.0189",
		"volume": "6",
		"author": [
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Chen",
				"given": "Zhuang"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Wu",
				"given": "Jincenzi"
			},
			{
				"family": "Nakagawa",
				"given": "Satoshi"
			},
			{
				"family": "Ren",
				"given": "Fuji"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					18
				]
			]
		}
	},
	{
		"id": "wenUnveilingImplicitToxicity2023",
		"type": "paper-conference",
		"abstract": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.",
		"container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2023.emnlp-main.84",
		"event-place": "Singapore",
		"event-title": "EMNLP 2023",
		"page": "1322–1338",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "Unveiling the Implicit Toxicity in Large Language Models",
		"URL": "https://aclanthology.org/2023.emnlp-main.84",
		"author": [
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Li",
				"given": "Chengfei"
			},
			{
				"family": "Bai",
				"given": "Jinfeng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "liMultiSourceProbingOpenDomain2023",
		"type": "paper-conference",
		"abstract": "Dialogue comprehension and generation are vital to the success of open-domain dialogue systems. Although pre-trained generative conversation models have made significant progress in generating fluent responses, people have difficulty judging whether they understand and efficiently model the contextual information of the conversation. In this study, we propose a Multi-Source Probing (MSP) method to probe the dialogue comprehension abilities of open-domain dialogue models. MSP aggregates features from multiple sources to accomplish diverse task goals and conducts downstream tasks in a generative manner that is consistent with dialogue model pre-training to leverage model capabilities. We conduct probing experiments on seven tasks that require various dialogue comprehension skills, based on the internal representations encoded by dialogue models. Experimental results show that open-domain dialogue models can encode semantic information in the intermediate hidden states, which facilitates dialogue comprehension tasks. Models of different scales and structures possess different conversational understanding capabilities. Our findings encourage a comprehensive evaluation and design of open-domain dialogue models.",
		"container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2023.emnlp-main.769",
		"event-place": "Singapore",
		"event-title": "EMNLP 2023",
		"page": "12491–12505",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "Multi-Source Probing for Open-Domain Conversational Understanding",
		"URL": "https://aclanthology.org/2023.emnlp-main.769",
		"author": [
			{
				"family": "Li",
				"given": "Yuanxi"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "liuTaskAdaptiveTokenizationEnhancing2023",
		"type": "paper-conference",
		"abstract": "We propose task-adaptive tokenization1 as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on taskspecific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model’s tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.",
		"container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2023.emnlp-main.944",
		"event-place": "Singapore",
		"event-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
		"language": "en",
		"page": "15264-15281",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "DOI.org (Crossref)",
		"title": "Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond",
		"title-short": "Task-Adaptive Tokenization",
		"URL": "https://aclanthology.org/2023.emnlp-main.944",
		"author": [
			{
				"family": "Liu",
				"given": "Siyang"
			},
			{
				"family": "Deng",
				"given": "Naihao"
			},
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Jia",
				"given": "Yilin"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Mihalcea",
				"given": "Rada"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangInstructSafetyUnifiedFramework2023",
		"type": "paper-conference",
		"abstract": "Safety detection has been an increasingly important topic in recent years and it has become even more necessary to develop reliable safety detection systems with the rapid development of large language models. However, currently available safety detection systems have limitations in terms of their versatility and interpretability. In this paper, we first introduce InstructSafety, a safety detection framework that unifies 7 common sub-tasks for safety detection. These tasks are unified into a similar form through different instructions. We then conduct a comprehensive survey of existing safety detection datasets and process 39 human-annotated datasets for instruction tuning. We also construct adversarial samples to enhance the model's robustness. After fine-tuning Flan-T5 on the collected data, we have developed Safety-Flan-T5, a multidimensional and explainable safety detector. We conduct comprehensive experiments on a variety of datasets and tasks, and demonstrate the strong performance of Safety-Flan-T5 in comparison to supervised baselines and served APIs (Perspective API, ChatGPT and InstructGPT). We will release the processed data, fine-tuned Safety-Flan-T5 and related code for public use.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2023",
		"DOI": "10.18653/v1/2023.findings-emnlp.700",
		"event-place": "Singapore",
		"event-title": "Findings 2023",
		"page": "10421–10436",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning",
		"title-short": "InstructSafety",
		"URL": "https://aclanthology.org/2023.findings-emnlp.700",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "shaoEnhancingRetrievalAugmentedLarge2023",
		"type": "paper-conference",
		"abstract": "Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2023",
		"DOI": "10.18653/v1/2023.findings-emnlp.620",
		"event-place": "Singapore",
		"event-title": "Findings 2023",
		"page": "9248–9274",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Singapore",
		"source": "ACLWeb",
		"title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
		"URL": "https://aclanthology.org/2023.findings-emnlp.620",
		"author": [
			{
				"family": "Shao",
				"given": "Zhihong"
			},
			{
				"family": "Gong",
				"given": "Yeyun"
			},
			{
				"family": "Shen",
				"given": "Yelong"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Duan",
				"given": "Nan"
			},
			{
				"family": "Chen",
				"given": "Weizhu"
			}
		],
		"editor": [
			{
				"family": "Bouamor",
				"given": "Houda"
			},
			{
				"family": "Pino",
				"given": "Juan"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "sabourCEMCommonsenseawareEmpathetic2021",
		"type": "article",
		"abstract": "A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user's emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user's emotion, cognitive understanding of the user's situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user's situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EmpatheticDialogues, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses.",
		"DOI": "10.48550/arXiv.2109.05739",
		"note": "arXiv:2109.05739",
		"number": "arXiv:2109.05739",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CEM: Commonsense-aware Empathetic Response Generation",
		"title-short": "CEM",
		"URL": "http://arxiv.org/abs/2109.05739",
		"author": [
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					6
				]
			]
		}
	},
	{
		"id": "guPreTrainingLearnContext2023",
		"type": "paper-conference",
		"abstract": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pretraining for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a large collection of “intrinsic tasks” in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widelyused text classification datasets and the SUPERNATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github. com/thu-coai/PICL.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.267",
		"event-place": "Toronto, Canada",
		"event-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"language": "en",
		"page": "4849-4870",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "DOI.org (Crossref)",
		"title": "Pre-Training to Learn in Context",
		"URL": "https://aclanthology.org/2023.acl-long.267",
		"author": [
			{
				"family": "Gu",
				"given": "Yuxian"
			},
			{
				"family": "Dong",
				"given": "Li"
			},
			{
				"family": "Wei",
				"given": "Furu"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouFacilitatingMultiturnEmotional2023",
		"type": "paper-conference",
		"abstract": "Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.96",
		"event-place": "Toronto, Canada",
		"event-title": "ACL 2023",
		"page": "1714–1729",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach",
		"title-short": "Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation",
		"URL": "https://aclanthology.org/2023.acl-long.96",
		"author": [
			{
				"family": "Zhou",
				"given": "Jinfeng"
			},
			{
				"family": "Chen",
				"given": "Zhuang"
			},
			{
				"family": "Wang",
				"given": "Bo"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "zhouCASEAligningCoarseFine2023",
		"type": "paper-conference",
		"abstract": "Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.457",
		"event-place": "Toronto, Canada",
		"event-title": "ACL 2023",
		"page": "8223–8237",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation",
		"title-short": "CASE",
		"URL": "https://aclanthology.org/2023.acl-long.457",
		"author": [
			{
				"family": "Zhou",
				"given": "Jinfeng"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Wang",
				"given": "Bo"
			},
			{
				"family": "Zhang",
				"given": "Zheng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "zhangETHICISTTargetedTraining2023",
		"type": "paper-conference",
		"abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named ETHICIST for targeted training data Extraction THrough loss smoothed soft prompting and calIbrated ConfIdence eSTimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that ETHICIST significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https://github.com/ thu-coai/Targeted-Data-Extraction.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.709",
		"event-place": "Toronto, Canada",
		"event-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"language": "en",
		"page": "12674-12687",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "DOI.org (Crossref)",
		"title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
		"title-short": "ETHICIST",
		"URL": "https://aclanthology.org/2023.acl-long.709",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "keDecompEvalEvaluatingGenerated2023",
		"type": "paper-conference",
		"abstract": "Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.",
		"container-title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2023.acl-long.539",
		"event-place": "Toronto, Canada",
		"event-title": "ACL 2023",
		"page": "9676–9691",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Toronto, Canada",
		"source": "ACLWeb",
		"title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering",
		"title-short": "DecompEval",
		"URL": "https://aclanthology.org/2023.acl-long.539",
		"author": [
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Huang",
				"given": "Fei"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Wang",
				"given": "Yasheng"
			},
			{
				"family": "Liu",
				"given": "Qun"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Boyd-Graber",
				"given": "Jordan"
			},
			{
				"family": "Okazaki",
				"given": "Naoaki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7
				]
			]
		}
	},
	{
		"id": "wenLearningTaskDecomposition2024",
		"type": "paper-conference",
		"abstract": "When using language models (LMs) to solve complex problems, humans might struggle to understand the LM-generated solutions and repair the flawed ones. To assist humans in repairing them, we propose to automatically decompose complex solutions into multiple simpler pieces that correspond to specific subtasks. We introduce a novel objective for learning task decomposition, termed assistive value (AssistV), which measures the feasibility and speed for humans to repair the decomposed solution. We collect a dataset of human repair experiences on different decomposed solutions. Utilizing the collected data as in-context examples, we then learn to critique, refine, and rank decomposed solutions to improve AssistV. We validate our method under competitive programming problems: under 177 hours of human study, our method enables non-experts to solve 33.3% more problems, speeds them up by 3.3x, and empowers them to match unassisted experts.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.629",
		"event-place": "Bangkok, Thailand",
		"event-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"language": "en",
		"page": "11700-11723",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "DOI.org (Crossref)",
		"title": "Learning Task Decomposition to Assist Humans in Competitive Programming",
		"URL": "https://aclanthology.org/2024.acl-long.629",
		"author": [
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Zhong",
				"given": "Ruiqi"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Shao",
				"given": "Zhihong"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "keCritiqueLLMInformativeCritique2024",
		"type": "paper-conference",
		"abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.704",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "13034–13054",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
		"title-short": "CritiqueLLM",
		"URL": "https://aclanthology.org/2024.acl-long.704",
		"author": [
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Wen",
				"given": "Bosi"
			},
			{
				"family": "Feng",
				"given": "Andrew"
			},
			{
				"family": "Liu",
				"given": "Xiao"
			},
			{
				"family": "Lei",
				"given": "Xuanyu"
			},
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Wang",
				"given": "Shengyuan"
			},
			{
				"family": "Zeng",
				"given": "Aohan"
			},
			{
				"family": "Dong",
				"given": "Yuxiao"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "chengBlackBoxPromptOptimization2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective—Black-Box Prompt Optimization (BPO)—to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.176",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "3201–3219",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training",
		"title-short": "Black-Box Prompt Optimization",
		"URL": "https://aclanthology.org/2024.acl-long.176",
		"author": [
			{
				"family": "Cheng",
				"given": "Jiale"
			},
			{
				"family": "Liu",
				"given": "Xiao"
			},
			{
				"family": "Zheng",
				"given": "Kehan"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Dong",
				"given": "Yuxiao"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "jiTailoringLanguageGeneration2023",
		"type": "article-journal",
		"abstract": "The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks.",
		"language": "en",
		"source": "Zotero",
		"title": "Tailoring Language Generation Models under Total Variation Distance",
		"author": [
			{
				"family": "Ji",
				"given": "Haozhe"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Hu",
				"given": "Zhipeng"
			},
			{
				"family": "Zhang",
				"given": "Rongsheng"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuCOKECognitiveKnowledge2024",
		"type": "paper-conference",
		"abstract": "Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.848",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "15984–16007",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind",
		"title-short": "COKE",
		"URL": "https://aclanthology.org/2024.acl-long.848",
		"author": [
			{
				"family": "Wu",
				"given": "Jincenzi"
			},
			{
				"family": "Chen",
				"given": "Zhuang"
			},
			{
				"family": "Deng",
				"given": "Jiawen"
			},
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Meng",
				"given": "Helen"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "guEVA20InvestigatingOpendomain2023",
		"type": "article-journal",
		"abstract": "Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems.",
		"container-title": "Machine Intelligence Research",
		"DOI": "10.1007/s11633-022-1387-3",
		"ISSN": "2731-5398",
		"issue": "2",
		"journalAbbreviation": "Mach. Intell. Res.",
		"language": "en",
		"page": "207-219",
		"source": "Springer Link",
		"title": "EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training",
		"title-short": "EVA2.0",
		"URL": "https://doi.org/10.1007/s11633-022-1387-3",
		"volume": "20",
		"author": [
			{
				"family": "Gu",
				"given": "Yuxian"
			},
			{
				"family": "Wen",
				"given": "Jiaxin"
			},
			{
				"family": "Sun",
				"given": "Hao"
			},
			{
				"family": "Song",
				"given": "Yi"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Zhang",
				"given": "Zheng"
			},
			{
				"family": "Yao",
				"given": "Jianzhu"
			},
			{
				"family": "Liu",
				"given": "Lei"
			},
			{
				"family": "Zhu",
				"given": "Xiaoyan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					1
				]
			]
		}
	},
	{
		"id": "niuBridgingGapSynthetic2023",
		"type": "article-journal",
		"abstract": "Semantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9% on KQA and +8.9% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1% for Hit@1).",
		"container-title": "Transactions of the Association for Computational Linguistics",
		"DOI": "10.1162/tacl_a_00552",
		"note": "publisher-place: Cambridge, MA\npublisher: MIT Press",
		"page": "367–383",
		"source": "ACLWeb",
		"title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing",
		"URL": "https://aclanthology.org/2023.tacl-1.22",
		"volume": "11",
		"author": [
			{
				"family": "Niu",
				"given": "Yilin"
			},
			{
				"family": "Huang",
				"given": "Fei"
			},
			{
				"family": "Liu",
				"given": "Wei"
			},
			{
				"family": "Cui",
				"given": "Jianwei"
			},
			{
				"family": "Wang",
				"given": "Bin"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jiEfficientExactOptimization2024",
		"type": "article",
		"abstract": "The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. However, we show that DPO derived based on the optimal solution of the problem leads to a compromised mean-seeking approximation of the optimal solution in practice. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. EXO is guaranteed to optimize in the same direction as RL algorithms asymptotically for arbitrary policy parametrization. This leads to the same mode-seeking solution, while enables efficient optimization by circumventing the complexities of RL. We also compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data. Code is available at https://github.com/haozheji/exact-optimization.",
		"DOI": "10.48550/arXiv.2402.00856",
		"note": "arXiv:2402.00856",
		"number": "arXiv:2402.00856",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Efficient Exact Optimization of Language Model Alignment",
		"URL": "http://arxiv.org/abs/2402.00856",
		"author": [
			{
				"family": "Ji",
				"given": "Haozhe"
			},
			{
				"family": "Lu",
				"given": "Cheng"
			},
			{
				"family": "Niu",
				"given": "Yilin"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Zhu",
				"given": "Jun"
			},
			{
				"family": "Tang",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6,
					5
				]
			]
		}
	},
	{
		"id": "chenToMBenchBenchmarkingTheory2024",
		"type": "paper-conference",
		"abstract": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.847",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "15959–15983",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
		"title-short": "ToMBench",
		"URL": "https://aclanthology.org/2024.acl-long.847",
		"author": [
			{
				"family": "Chen",
				"given": "Zhuang"
			},
			{
				"family": "Wu",
				"given": "Jincenzi"
			},
			{
				"family": "Zhou",
				"given": "Jinfeng"
			},
			{
				"family": "Wen",
				"given": "Bosi"
			},
			{
				"family": "Bi",
				"given": "Guanqun"
			},
			{
				"family": "Jiang",
				"given": "Gongyao"
			},
			{
				"family": "Cao",
				"given": "Yaru"
			},
			{
				"family": "Hu",
				"given": "Mengting"
			},
			{
				"family": "Lai",
				"given": "Yunghwei"
			},
			{
				"family": "Xiong",
				"given": "Zexuan"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "zhangDefendingLargeLanguage2024",
		"type": "paper-conference",
		"abstract": "While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. Our code is available at https://github.com/thu-coai/JailbreakDefense_GoalPriority.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.481",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "8865–8887",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
		"URL": "https://aclanthology.org/2024.acl-long.481",
		"author": [
			{
				"family": "Zhang",
				"given": "Zhexin"
			},
			{
				"family": "Yang",
				"given": "Junxiao"
			},
			{
				"family": "Ke",
				"given": "Pei"
			},
			{
				"family": "Mi",
				"given": "Fei"
			},
			{
				"family": "Wang",
				"given": "Hongning"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "guMiniLLMKnowledgeDistillation2023",
		"type": "paper-conference",
		"abstract": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "MiniLLM: Knowledge Distillation of Large Language Models",
		"title-short": "MiniLLM",
		"URL": "https://openreview.net/forum?id=5h0qf7IBZZ",
		"author": [
			{
				"family": "Gu",
				"given": "Yuxian"
			},
			{
				"family": "Dong",
				"given": "Li"
			},
			{
				"family": "Wei",
				"given": "Furu"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "sabourEmoBenchEvaluatingEmotional2024",
		"type": "paper-conference",
		"abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.",
		"container-title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.acl-long.326",
		"event-place": "Bangkok, Thailand",
		"event-title": "ACL 2024",
		"page": "5986–6004",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Bangkok, Thailand",
		"source": "ACLWeb",
		"title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
		"title-short": "EmoBench",
		"URL": "https://aclanthology.org/2024.acl-long.326",
		"author": [
			{
				"family": "Sabour",
				"given": "Sahand"
			},
			{
				"family": "Liu",
				"given": "Siyang"
			},
			{
				"family": "Zhang",
				"given": "Zheyuan"
			},
			{
				"family": "Liu",
				"given": "June"
			},
			{
				"family": "Zhou",
				"given": "Jinfeng"
			},
			{
				"family": "Sunaryo",
				"given": "Alvionna"
			},
			{
				"family": "Lee",
				"given": "Tatia"
			},
			{
				"family": "Mihalcea",
				"given": "Rada"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"editor": [
			{
				"family": "Ku",
				"given": "Lun-Wei"
			},
			{
				"family": "Martins",
				"given": "Andre"
			},
			{
				"family": "Srikumar",
				"given": "Vivek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8
				]
			]
		}
	},
	{
		"id": "guanLanguageModelsHallucinate2024",
		"type": "paper-conference",
		"abstract": "Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently “hallucinate,” resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B , the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.",
		"container-title": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2024.naacl-long.62",
		"event-place": "Mexico City, Mexico",
		"event-title": "NAACL-HLT 2024",
		"page": "1090–1111",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Mexico City, Mexico",
		"source": "ACLWeb",
		"title": "Language Models Hallucinate, but May Excel at Fact Verification",
		"URL": "https://aclanthology.org/2024.naacl-long.62",
		"author": [
			{
				"family": "Guan",
				"given": "Jian"
			},
			{
				"family": "Dodge",
				"given": "Jesse"
			},
			{
				"family": "Wadden",
				"given": "David"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Peng",
				"given": "Hao"
			}
		],
		"editor": [
			{
				"family": "Duh",
				"given": "Kevin"
			},
			{
				"family": "Gomez",
				"given": "Helena"
			},
			{
				"family": "Bethard",
				"given": "Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					6
				]
			]
		}
	},
	{
		"id": "zhengPromptDrivenSafeguardingLarge2024",
		"type": "paper-conference",
		"abstract": "Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not been revealed yet. In this work, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we further present a safety prompt optimization method in the Appendix. We demonstrate that the proposed method remarkably improves the safeguarding performance of human-crafted safety prompts without compromising the general model capability.",
		"event-title": "ICLR 2024 Workshop on Secure and Trustworthy Large Language Models",
		"language": "en",
		"source": "openreview.net",
		"title": "On Prompt-Driven Safeguarding for Large Language Models",
		"URL": "https://openreview.net/forum?id=lFwf7bnpUs&referrer=%5Bthe%20profile%20of%20Jie%20Zhou%5D(%2Fprofile%3Fid%3D~Jie_Zhou8)",
		"author": [
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Yin",
				"given": "Fan"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Meng",
				"given": "Fandong"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			},
			{
				"family": "Peng",
				"given": "Nanyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					14
				]
			]
		}
	},
	{
		"id": "zhengLargeLanguageModels2023",
		"type": "paper-conference",
		"abstract": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model’s prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Large Language Models Are Not Robust Multiple Choice Selectors",
		"URL": "https://openreview.net/forum?id=shr9PXz7T0",
		"author": [
			{
				"family": "Zheng",
				"given": "Chujie"
			},
			{
				"family": "Zhou",
				"given": "Hao"
			},
			{
				"family": "Meng",
				"given": "Fandong"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Huang",
				"given": "Minlie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "yuMultimodalFederatedLearning2022",
		"type": "paper-conference",
		"abstract": "With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose \\textit{Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL)}, a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (\\textit{modality gap} and \\textit{task gap}), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value.",
		"event-title": "The Eleventh International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "Multimodal Federated Learning via Contrastive Representation Ensemble",
		"URL": "https://openreview.net/forum?id=Hnk1WRMAYqg",
		"author": [
			{
				"family": "Yu",
				"given": "Qiying"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Wang",
				"given": "Yimu"
			},
			{
				"family": "Xu",
				"given": "Ke"
			},
			{
				"family": "Liu",
				"given": "Jingjing"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					29
				]
			]
		}
	},
	{
		"id": "changAdversarialAttackFramework2022",
		"type": "article",
		"abstract": "With the success of the graph embedding model in both academic and industry areas, the robustness of graph embedding against adversarial attack inevitably becomes a crucial problem in graph learning. Existing works usually perform the attack in a white-box fashion: they need to access the predictions/labels to construct their adversarial loss. However, the inaccessibility of predictions/labels makes the white-box attack impractical to a real graph learning system. This paper promotes current frameworks in a more general and flexible sense -- we demand to attack various kinds of graph embedding models with black-box driven. We investigate the theoretical connections between graph signal processing and graph embedding models and formulate the graph embedding model as a general graph signal process with a corresponding graph filter. Therefore, we design a generalized adversarial attacker: GF-Attack. Without accessing any labels and model predictions, GF-Attack can perform the attack directly on the graph filter in a black-box fashion. We further prove that GF-Attack can perform an effective attack without knowing the number of layers of graph embedding models. To validate the generalization of GF-Attack, we construct the attacker on four popular graph embedding models. Extensive experiments validate the effectiveness of GF-Attack on several benchmark datasets.",
		"DOI": "10.48550/arXiv.2105.12419",
		"note": "arXiv:2105.12419",
		"number": "arXiv:2105.12419",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge",
		"URL": "http://arxiv.org/abs/2105.12419",
		"author": [
			{
				"family": "Chang",
				"given": "Heng"
			},
			{
				"family": "Rong",
				"given": "Yu"
			},
			{
				"family": "Xu",
				"given": "Tingyang"
			},
			{
				"family": "Huang",
				"given": "Wenbing"
			},
			{
				"family": "Zhang",
				"given": "Honglei"
			},
			{
				"family": "Cui",
				"given": "Peng"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Zhu",
				"given": "Wenwu"
			},
			{
				"family": "Huang",
				"given": "Junzhou"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					1
				]
			]
		}
	},
	{
		"id": "wangOpenChatAdvancingOpensource2023",
		"type": "paper-conference",
		"abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
		"title-short": "OpenChat",
		"URL": "https://openreview.net/forum?id=AOJyfhWYHf",
		"author": [
			{
				"family": "Wang",
				"given": "Guan"
			},
			{
				"family": "Cheng",
				"given": "Sijie"
			},
			{
				"family": "Zhan",
				"given": "Xianyuan"
			},
			{
				"family": "Li",
				"given": "Xiangang"
			},
			{
				"family": "Song",
				"given": "Sen"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "linSelectingLargeLanguage2024",
		"type": "article",
		"abstract": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known \"power phase\" but also the previously unobserved \"pre-power phase\". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of \"pre-learned data size\" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.",
		"DOI": "10.48550/arXiv.2402.02314",
		"note": "arXiv:2402.02314",
		"number": "arXiv:2402.02314",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
		"URL": "http://arxiv.org/abs/2402.02314",
		"author": [
			{
				"family": "Lin",
				"given": "Haowei"
			},
			{
				"family": "Huang",
				"given": "Baizhou"
			},
			{
				"family": "Ye",
				"given": "Haotian"
			},
			{
				"family": "Chen",
				"given": "Qinyu"
			},
			{
				"family": "Wang",
				"given": "Zihao"
			},
			{
				"family": "Li",
				"given": "Sujian"
			},
			{
				"family": "Ma",
				"given": "Jianzhu"
			},
			{
				"family": "Wan",
				"given": "Xiaojun"
			},
			{
				"family": "Zou",
				"given": "James"
			},
			{
				"family": "Liang",
				"given": "Yitao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					28
				]
			]
		}
	},
	{
		"id": "zhangAmFollowerAlso2023",
		"type": "article",
		"abstract": "Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.",
		"DOI": "10.48550/arXiv.2302.03481",
		"note": "arXiv:2302.03481",
		"number": "arXiv:2302.03481",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "\"I am the follower, also the boss\": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired",
		"title-short": "I am the follower, also the boss",
		"URL": "http://arxiv.org/abs/2302.03481",
		"author": [
			{
				"family": "Zhang",
				"given": "Yan"
			},
			{
				"family": "Li",
				"given": "Ziang"
			},
			{
				"family": "Guo",
				"given": "Haole"
			},
			{
				"family": "Wang",
				"given": "Luyao"
			},
			{
				"family": "Chen",
				"given": "Qihe"
			},
			{
				"family": "Jiang",
				"given": "Wenjie"
			},
			{
				"family": "Fan",
				"given": "Mingming"
			},
			{
				"family": "Zhou",
				"given": "Guyue"
			},
			{
				"family": "Gong",
				"given": "Jiangtao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					7
				]
			]
		}
	},
	{
		"id": "suTransferabilityPromptTuning2022",
		"type": "paper-conference",
		"abstract": "Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts' stimulation to PLMs. The source code can be obtained from https://github.com/thunlp/Prompt-Transferability.",
		"container-title": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
		"DOI": "10.18653/v1/2022.naacl-main.290",
		"event-place": "Seattle, United States",
		"event-title": "NAACL-HLT 2022",
		"page": "3949–3969",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Seattle, United States",
		"source": "ACLWeb",
		"title": "On Transferability of Prompt Tuning for Natural Language Processing",
		"URL": "https://aclanthology.org/2022.naacl-main.290",
		"author": [
			{
				"family": "Su",
				"given": "Yusheng"
			},
			{
				"family": "Wang",
				"given": "Xiaozhi"
			},
			{
				"family": "Qin",
				"given": "Yujia"
			},
			{
				"family": "Chan",
				"given": "Chi-Min"
			},
			{
				"family": "Lin",
				"given": "Yankai"
			},
			{
				"family": "Wang",
				"given": "Huadong"
			},
			{
				"family": "Wen",
				"given": "Kaiyue"
			},
			{
				"family": "Liu",
				"given": "Zhiyuan"
			},
			{
				"family": "Li",
				"given": "Peng"
			},
			{
				"family": "Li",
				"given": "Juanzi"
			},
			{
				"family": "Hou",
				"given": "Lei"
			},
			{
				"family": "Sun",
				"given": "Maosong"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			}
		],
		"editor": [
			{
				"family": "Carpuat",
				"given": "Marine"
			},
			{
				"family": "Marneffe",
				"given": "Marie-Catherine",
				"non-dropping-particle": "de"
			},
			{
				"family": "Meza Ruiz",
				"given": "Ivan Vladimir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "zouVFLAIRResearchLibrary2023",
		"type": "paper-conference",
		"abstract": "Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. To address this need, we present an extensible and lightweight VFL framework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which supports VFL training with a variety of models, datasets and protocols, along with standardized modules for comprehensive evaluations of attacks and defense strategies. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios.",
		"event-title": "The Twelfth International Conference on Learning Representations",
		"language": "en",
		"source": "openreview.net",
		"title": "VFLAIR: A Research Library and Benchmark for Vertical Federated Learning",
		"title-short": "VFLAIR",
		"URL": "https://openreview.net/forum?id=sqRgz88TM3",
		"author": [
			{
				"family": "Zou",
				"given": "Tianyuan"
			},
			{
				"family": "Gu",
				"given": "Zixuan"
			},
			{
				"family": "He",
				"given": "Yu"
			},
			{
				"family": "Takahashi",
				"given": "Hideaki"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Zhang",
				"given": "Ya-Qin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "caiPrivacyPreservingFederatedCrossDomain2023",
		"type": "chapter",
		"abstract": "By combining user feedback on items with social networks, cross-domain social recommendations provide users with more accurate recommendation results. However, traditional cross-domain social recommendations require holding both data of ratings and social networks, which is not easy to achieve for both information-oriented and socialoriented websites. To promote cross-domain social network collaboration among the institutions holding different data, we propose a federated crossdomain social recommendation (FCSR) algorithm. The main innovation is applying Random Response mechanism to achieve sparsely maintained differential privacy for user connections and proposing Matrix Confusion Method to achieve efﬁcient encrypted user feature vector updates. Our experiments on three datasets show the practicality of FCSR in social recommendation and signiﬁcantly outperforms baselines.",
		"container-title": "Trustworthy Federated Learning",
		"event-place": "Cham",
		"ISBN": "978-3-031-28995-8",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-28996-5_11",
		"page": "144-158",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Privacy-Preserving Federated Cross-Domain Social Recommendation",
		"URL": "https://link.springer.com/10.1007/978-3-031-28996-5_11",
		"volume": "13448",
		"editor": [
			{
				"family": "Goebel",
				"given": "Randy"
			},
			{
				"family": "Yu",
				"given": "Han"
			},
			{
				"family": "Faltings",
				"given": "Boi"
			},
			{
				"family": "Fan",
				"given": "Lixin"
			},
			{
				"family": "Xiong",
				"given": "Zehui"
			}
		],
		"author": [
			{
				"family": "Cai",
				"given": "Jianping"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Liu",
				"given": "Ximeng"
			},
			{
				"family": "Li",
				"given": "Jiayin"
			},
			{
				"family": "Zhuang",
				"given": "Hongbin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chengEgoThinkEvaluatingFirstPerson2024",
		"type": "article",
		"abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.",
		"DOI": "10.48550/arXiv.2311.15596",
		"note": "arXiv:2311.15596",
		"number": "arXiv:2311.15596",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
		"title-short": "EgoThink",
		"URL": "http://arxiv.org/abs/2311.15596",
		"author": [
			{
				"family": "Cheng",
				"given": "Sijie"
			},
			{
				"family": "Guo",
				"given": "Zhicheng"
			},
			{
				"family": "Wu",
				"given": "Jingwen"
			},
			{
				"family": "Fang",
				"given": "Kechen"
			},
			{
				"family": "Li",
				"given": "Peng"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					28
				]
			]
		}
	},
	{
		"id": "yangUnifiedAlignmentAgents2024",
		"type": "article",
		"abstract": "The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.",
		"DOI": "10.48550/arXiv.2402.07744",
		"note": "arXiv:2402.07744",
		"number": "arXiv:2402.07744",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Unified Alignment Between Agents, Humans, and Environment",
		"URL": "http://arxiv.org/abs/2402.07744",
		"author": [
			{
				"family": "Yang",
				"given": "Zonghan"
			},
			{
				"family": "Liu",
				"given": "An"
			},
			{
				"family": "Liu",
				"given": "Zijun"
			},
			{
				"family": "Liu",
				"given": "Kaiming"
			},
			{
				"family": "Xiong",
				"given": "Fangzhou"
			},
			{
				"family": "Wang",
				"given": "Yile"
			},
			{
				"family": "Yang",
				"given": "Zeyuan"
			},
			{
				"family": "Hu",
				"given": "Qingyuan"
			},
			{
				"family": "Chen",
				"given": "Xinrui"
			},
			{
				"family": "Zhang",
				"given": "Zhenhe"
			},
			{
				"family": "Luo",
				"given": "Fuwen"
			},
			{
				"family": "Guo",
				"given": "Zhicheng"
			},
			{
				"family": "Li",
				"given": "Peng"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					14
				]
			]
		}
	},
	{
		"id": "kirchnerProverVerifierGamesImprove2024",
		"type": "article",
		"abstract": "One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, \"helpful\" provers to produce correct solutions that the verifier accepts, and \"sneaky\" provers to produce incorrect solutions that fool the verifier. We find that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models.",
		"DOI": "10.48550/arXiv.2407.13692",
		"note": "arXiv:2407.13692",
		"number": "arXiv:2407.13692",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Prover-Verifier Games improve legibility of LLM outputs",
		"URL": "http://arxiv.org/abs/2407.13692",
		"author": [
			{
				"family": "Kirchner",
				"given": "Jan Hendrik"
			},
			{
				"family": "Chen",
				"given": "Yining"
			},
			{
				"family": "Edwards",
				"given": "Harri"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "McAleese",
				"given": "Nat"
			},
			{
				"family": "Burda",
				"given": "Yuri"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					11,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					8,
					1
				]
			]
		}
	},
	{
		"id": "albrechtInterpretableGoalbasedPrediction2021",
		"type": "paper-conference",
		"abstract": "We propose an integrated prediction and planning system for autonomous driving which uses rational inverse planning to recognise the goals of other vehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS utilise a shared set of defined maneuvers and macro actions to construct plans which are explainable by means of rationality principles. Evaluation in simulations of urban driving scenarios demonstrate the system's ability to robustly recognise the goals of other vehicles, enabling our vehicle to exploit non-trivial opportunities to significantly reduce driving times. In each scenario, we extract intuitive explanations for the predictions which justify the system's decisions.",
		"container-title": "IEEE International Conference on Robotics and Automation (ICRA)",
		"license": "All rights reserved",
		"title": "Interpretable Goal-based Prediction and Planning for Autonomous Driving",
		"URL": "https://ieeexplore.ieee.org/document/9560849",
		"author": [
			{
				"family": "Albrecht",
				"given": "Stefano V."
			},
			{
				"family": "Brewitt",
				"given": "Cillian"
			},
			{
				"family": "Wilhelm",
				"given": "John"
			},
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Eiras",
				"given": "Francisco"
			},
			{
				"family": "Dobre",
				"given": "Mihai"
			},
			{
				"family": "Ramamoorthy",
				"given": "Subramanian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2021",
					10,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					15
				]
			]
		}
	},
	{
		"id": "gyevnarHumanCentricMethodGenerating2022",
		"type": "paper-conference",
		"abstract": "Inscrutable AI systems are difficult to trust, especially if they operate in safety-critical settings like autonomous driving. Therefore, there is a need to build transparent and queryable systems to increase trust levels. We propose a transparent, human-centric explanation generation method for autonomous vehicle motion planning and prediction based on an existing white-box system called IGP2. Our method integrates Bayesian networks with context-free generative rules and can give causal natural language explanations for the high-level driving behaviour of autonomous vehicles. Preliminary testing on simulated scenarios shows that our method captures the causes behind the actions of autonomous vehicles and generates intelligible explanations with varying complexity.",
		"container-title": "Workshop on Artificial Intelligence for Autonomous Driving",
		"DOI": "10.48550/arXiv.2206.08783",
		"event-place": "London",
		"event-title": "International Joint Conference on Artificial Intelligence (IJCAI)",
		"note": "arXiv:2206.08783 [cs]",
		"publisher-place": "London",
		"source": "arXiv.org",
		"title": "A Human-Centric Method for Generating Causal Explanations in Natural Language for Autonomous Vehicle Motion Planning",
		"URL": "http://arxiv.org/abs/2206.08783",
		"author": [
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Tamborski",
				"given": "Massimiliano"
			},
			{
				"family": "Wang",
				"given": "Cheng"
			},
			{
				"family": "Lucas",
				"given": "Christopher G."
			},
			{
				"family": "Cohen",
				"given": "Shay B."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2022",
					12,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "brewittGRITFastInterpretable2021",
		"type": "paper-conference",
		"abstract": "It is important for autonomous vehicles to have the ability to infer the goals of other vehicles (goal recognition), in order to safely interact with other vehicles and predict their future trajectories. This is a difficult problem, especially in urban environments with interactions between many vehicles. Goal recognition methods must be fast to run in real time and make accurate inferences. As autonomous driving is safety- critical, it is important to have methods which are human interpretable and for which safety can be formally verified. Existing goal recognition methods for autonomous vehicles fail to satisfy all four objectives of being fast, accurate, interpretable and verifiable. We propose Goal Recognition with Interpre table Trees (GRIT), a goal recognition system which achieves these objectives. GRIT makes use of decision trees trained on vehicle trajectory data. We evaluate GRIT on two datasets, showing that GRIT achieved fast inference speed and comparable accuracy to two deep learning baselines, a planning-based goal recognition method, and an ablation of GRIT. We show that the learned trees are human interpretable and demonstrate how properties of GRIT can be formally verified using a satisfiability modulo theories (SMT) solver.",
		"container-title": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"DOI": "10.1109/IROS51168.2021.9636279",
		"event-title": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"note": "ISSN: 2153-0866",
		"page": "1023-1030",
		"source": "IEEE Xplore",
		"title": "GRIT: Fast, Interpretable, and Verifiable Goal Recognition with Learned Decision Trees for Autonomous Driving",
		"title-short": "GRIT",
		"author": [
			{
				"family": "Brewitt",
				"given": "Cillian"
			},
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Garcin",
				"given": "Samuel"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					9
				]
			]
		}
	},
	{
		"id": "gyevnarBridgingTransparencyGap2023",
		"type": "chapter",
		"container-title": "ECAI 2023",
		"note": "DOI: 10.3233/FAIA230367",
		"page": "964-971",
		"publisher": "IOS Press",
		"source": "ebooks.iospress.nl",
		"title": "Bridging the Transparency Gap: What Can Explainable AI Learn from the AI Act?",
		"title-short": "Bridging the Transparency Gap",
		"URL": "https://ebooks.iospress.nl/doi/10.3233/FAIA230367",
		"abstract": "The European Union has proposed the Artificial Intelligence Act which introduces detailed requirements of transparency for AI systems. Many of these requirements can be addressed by the field of explainable AI (XAI), however, there is a fundamental difference between XAI and the Act regarding what transparency is. The Act views transparency as a means that supports wider values, such as accountability, human rights, and sustainable innovation. In contrast, XAI views transparency narrowly as an end in itself, focusing on explaining complex algorithmic properties without considering the socio-technical context. We call this difference the “transparency gap”. Failing to address the transparency gap, XAI risks leaving a range of transparency issues unaddressed. To begin to bridge this gap, we overview and clarify the terminology of how XAI and European regulation – the Act and the related General Data Protection Regulation (GDPR) – view basic definitions of transparency. By comparing the disparate views of XAI and regulation, we arrive at four axes where practical work could bridge the transparency gap: defining the scope of transparency, clarifying the legal status of XAI, addressing issues with conformity assessment, and building explainability for datasets.",
		"author": [
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Ferguson",
				"given": "Nick"
			},
			{
				"family": "Schafer",
				"given": "Burkhard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					10,
					7
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kuznietsovExplainableAISafe2024",
		"type": "article",
		"abstract": "Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxiliary explanations, and interpretable validation. Finally, we propose a modular framework called SafeX to integrate these contributions, enabling explanation delivery to users while simultaneously ensuring the safety of AI models.",
		"note": "arXiv:2402.10086 [cs]",
		"number": "arXiv:2402.10086",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review",
		"title-short": "Explainable AI for Safe and Trustworthy Autonomous Driving",
		"URL": "http://arxiv.org/abs/2402.10086",
		"author": [
			{
				"family": "Kuznietsov",
				"given": "Anton"
			},
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Wang",
				"given": "Cheng"
			},
			{
				"family": "Peters",
				"given": "Steven"
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					2,
					8
				]
			]
		}
	},
	{
		"id": "gyevnarObjectiveMetricsHumanSubjects2025",
		"type": "article",
		"abstract": "Explanation is a fundamentally human process. Understanding the goal and audience of the explanation is vital, yet existing work on explainable reinforcement learning (XRL) routinely does not consult humans in their evaluations. Even when they do, they routinely resort to subjective metrics, such as confidence or understanding, that can only inform researchers of users' opinions, not their practical effectiveness for a given problem. This paper calls on researchers to use objective human metrics for explanation evaluations based on observable and actionable behaviour to build more reproducible, comparable, and epistemically grounded research. To this end, we curate, describe, and compare several objective evaluation methodologies for applying explanations to debugging agent behaviour and supporting human-agent teaming, illustrating our proposed methods using a novel grid-based environment. We discuss how subjective and objective metrics complement each other to provide holistic validation and how future work needs to utilise standardised benchmarks for testing to enable greater comparisons between research.",
		"DOI": "10.48550/arXiv.2501.19256",
		"note": "arXiv:2501.19256 [cs]",
		"number": "arXiv:2501.19256",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Objective Metrics for Human-Subjects Evaluation in Explainable Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2501.19256",
		"author": [
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Towers",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2025",
					2,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2025",
					1,
					31
				]
			]
		}
	},
	{
		"id": "gyevnarAISafetyEveryone2025",
		"type": "article",
		"abstract": "Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.",
		"DOI": "10.48550/arXiv.2502.09288",
		"note": "arXiv:2502.09288 [cs]",
		"number": "arXiv:2502.09288",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI Safety for Everyone",
		"URL": "http://arxiv.org/abs/2502.09288",
		"author": [
			{
				"family": "Gyevnar",
				"given": "Balint"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2025",
					2,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2025",
					2,
					14
				]
			]
		}
	}
]